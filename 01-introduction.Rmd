\mainmatter

# Introduction {#intro}


Start with motivational examples.
Things that went wrong.

## What is machine learning and why is it important?
Black box

## Why is explainability important?
Machine learning has come to a state where you have to make a trade-off: Do you simply want to know **what** will happen? For example if a client will churn or if medication will work well for a patient.  Or do you want to know **why** something will happen and paying for the explainability with accuracy? In some cases you will not care why a decision was made, only the assurance that the accuracy was good on some test set is enough. But in other cases knowing the 'why' can help you understand more about the problem, the data and also know why a model might fail.

## Who this book is for
This book is for everyone who wants to learn how to make machine learning models more explainable. It is a recommended reading for machine learning practitioners, statisticians, data scientists and everyone who has contact with machine learning applications.  It contains one or the other formula, but it's kept at a manageable level of math.
This book is not for people who are trying to learn machine learning from scratch. If you want to learn machine learning, there are loads of books and other resources for learning the basics.

## Outline of the book



Definition of interpretability / explainability


System is interpretable if it falls into certain class of models, where authors claim that this class is interpretable and the authors present algorithms to optimize within that class. [Towards A Rigorous Science of Interpretable Machine Learning]

There is no good definition of interpretability yet?



Classic statistics usually aims at having interpretable models. That's why doctors, sociologists and banks tend to go to statisticians with research questions instead of computer scientists.



Some systems need more interpretability. The more impactful the decision, the more explainable it should be. A possibly incorrect product recommendation might is not as bad as a wrong diagnose and recommendation of an inappropriate treatment.


Need for explainability arises when something goes wrong. Because having an explanation for a faulty classification helps to understand the cause of the fault. It delivers a direction for how to fix the system. Consider an example of a husky versus wolf classifier, that missclassifies some huskys as wolfs. If there is an explanation to the classification you can see, that the missclassification happend due to the snow on the image. The classifier learned to use snow as a feature for classifying images as wolfs, which might make sense in terms of separating features in the training data set, but not in the real world use.  [TODO: Add image from Ribeiro + ask for permission]




As a machine learning practitioner, your main goal is to drive down the loss function while also keeping the learned model generalizable to other data sets.




## Is good user experience enough?
Maybe it is enough to show that the algorithm works and does what it is supposed to. The recommended movies are all reasonable and my self-driving car never had an accident.


We have to distinguish between between low and high stakes scenario and individual risks and systematic biases.


For low-risk applications (e.g. product recommender systems) there is not so much damage if something goes wrong. No one will die because they got a product recommendation on Amazon for something they are not interested in. Biggest risk is for the company deploying those algorithms that they do not work and loose customers to competitors.
But there is the risk for a systematic bias and while for each individual the impact might be negligible on a group or society level it is quite problematic. Social bubbles through newsfeeds? An example is a restaurant recommender system that would never recommend restaurants to a certain minority, because they are from that minority.

High risk applications are self-driving cars, AI doctors etc. Here it is quite important to have explainability. Only with explainability can you 'debug' why a car accident happend by a self-driving car or decide if you want to trust the diagnose of a machine learning algorithm.

## Scope of explainability
There are basically

### Algorithm
On this level of explainability, you are only asking how the algorithm learns, what kind of relationships it is capable of picking up. This level of explainability is what each good machine learning practitioner today usually has. If you are using convolutional neural networks for classifying images, you can explain that the algorithm learns edge detectors and filters on the lowest layers. This is an understanding of how the algorithm works, but not of the specific model that is learned in the end and not about how single decisions are reached. For this level of explainability only knowledge about the algorithm and the data is required. This book will talk a little about algorithmic explainability, but will focuse more on global and local explainability.

### Global model explainability
To explain the global model output, you need the trained model, knowledge about the algorithm and the data. This level of explainability is about understanding how the model makes the decisions, based on the features. Which features are the important ones and what kind of interactions are happening? Global model explainability helps to understand the distribution of your target variable based on the features.


### Explain a single observation
You can go all the way down to a single observation and examine what kind of classification or decision the model gives for this input, and why it gives this input. When you zoom in into one example, the conditional distribution of the target variable might behave more nicely. Locally it might depend only linearly or monotonic on some variables rather than having a complex dependency. For example the rent of an appartment might not depend linearly on the size, but if you only look at a specific appartment of 100 square meter and check how the prize changes going up plus and minus 10 square meters there is a chance that this sub region in your data space is linear. Local explanations can be more accurate compared to global explanations because of this.

## Measuring explainability / comprehensibility / interpretability

Model size is an easy way to measure, but might be too simplistic.


## Definitions
- An **Algorithm** is a set of rules that a machine follows to achieve a particular goal [1]
- **Machine learning algorithm** is an set of rules that a machine follows to learn how to a achieve a particular goal. The output of a machine learning algorithm is a machine learning model.
- **(Machine learning) Model** is the outcome of a machine learning algorithm. This can be a set of weights for a linear model or neural network plus the architecture.
- **Features** are the variables/information used for prediction/classification/clustering.
- **(machine learning) Task** can be classification, regression, survival analysis, clustering, outlier detection
- **Instance** One row in the dataset. 

## Terminology
Y is the target variable in supervised settings.
X are the features or covariates.
w are the weights.
$\beta% are regression weights.



[1] https://www.merriam-webster.com/dictionary/algorithm, accessed on Feb. 12th
