\mainmatter

# Introduction {#intro}


## What is machine learning and why is it important?
Recommendation of products, identifying street signs, counting people on the street, assessing a person's credit worthiness, detecting fraud: All these examples have in common that they can, and are increasingly, realized with machine learning algorithms. The tasks are different, but the approach is the same: Step 1 is to collect data. This can be images with and without street signs plus the information which sign is visible or the personal data from loan applicants together with the information if they repaid their loan or not. Step 2: Feed this information into a machine learning algorithm, which produces a sign detector model or a credit worthiness model. This model can then be used in Step 3: Integrate the model into the product or process, like an self-driving car or a loan application process.

There are a lot of tasks in which machines exceed humans. Even if the machine is as good as a human at a task, or slightly worse, there remains big advantages, and that is speed, reproducibility and scale. A machine learning model that has been implemented once, can do a task much faster than humans, will reliably produce the same results from the same input and can be copied endlessly.

<!--  TODO: Add examples -->



## Why is explainability important?
AI explainability is the ability of a machine learning system to explain or to present a decision in an understandable way for humans. Machine learning has come to a state where you have to make a trade-off: Do you simply want to know **what** will happen? For example if a client will churn or if medication will work well for a patient.  Or do you want to know **why** something will happen and paying for the explainability with accuracy? In some cases you will not care why a decision was made, only the assurance that the accuracy was good on some test set is enough. But in other cases knowing the 'why' can help you understand more about the problem, the data and also know why a model might fail.
Two sorts of problems might not need explanations, because they either are low risk (e.g. movie recommender system) or the method is already extensively studied and evaluated (e.g. optical character recognition).
The necessity for explainability comes from an incompleteness in the problem formalization [2], meaning that for certain problems/tasks it is not enough to get the answer (the "what"), but the model also has to give an explanation (the *why**)

- There is a shift in many scientific disciplines <!--TODO> Add example disciplines--> from qualitative to quantitative methods (e.g. sociology, psychology), and also towards machine learning (biology, genomics). The **goal of science** is to gain knowledge, but many problems can only be solved with big datasets and black box machine learning models. Explainability allows to extract additional knowledge.
- Machine learning models are taking over real world tasks, that demand **safety measurements** and testing. A self-driving car automatically detects cyclists, which is as desired. You want to bet 100% sure that the abstraction the system learned will be fail-safe, because running over cyclists is quite bad. An explanation might reveal that the most important feature learned is to recognize the two wheels of a bike and this explanation helps to think about edge cases like bikes with side bags, that partially cover the wheels.
- By default most machine learning models pick up biases from the training data. This can turn your machine learning models into racists (see Microsoft failed experiment: Tay) or discriminate in against other demographic, protected groups. Explainability helps you to get the **ethics** of your machine learning model right. From the data it might not be possible to see the biases, so it might also not be enough to implement protections against known biases. An example: The goal is to train a machine learning model to pre-sort applications for jobs in a big company. The company had some troubles in the past because they preferred to hire men over women, having the same qualifications. So for the model training the data scientist decides to remove all features that indicate the gender. But it turns out that the model picks up another, a priori unknown,  bias favoring younger people over older people. This could only be uncovered because the model is explainable.
- Sometimes it is not possible to optimize a goal directly, but only a proxy. Consider a marketing where a e-commerce company sends some gift to some customers in order to bind them to the business. It has already done similar campaigns, so there is data to learn a model to select the best customers to send the gifts to (sending it to all would be far to expensive). Ideally the company wants to optimize the likelihood that the customers with the gift are still their customers in five years from now, but there is no data for analysing this, because the first of theses kinds of campaigns was two years ago, meaning there is no suitable training data for this objective. So the company opts for using a proxy: The amount of money the customers will be spending in the next 6 months at your company. An explainable model can help to find problems that might arise due to using a proxy objective (possibly **mismatched objectives**). In the e-commerce case it might turn out that the customers selected by the algorithm are only international students that might spent heavily in a short time, even more after receiving a gift, but usually leave the country and therefore the company as a customer within a few years.
- In volatile environments a machine learning model might be outdated at some point because of a **data shift**. A data shift is a change in the relationships in the data. Explanation make it easier to detect when that happens or if one model is more prone than another to face problems soon. Spam filter for mails use works in a mail to detect if they should reach your inbox or be moved into the spam folder. 'Viagra' is one of the words that increase the likelihood that a mail is spam. When 'viagra' is suddenly used in a different context, for some obscure reason, suddenly non-spam mails will land in the spam folder (like 'Random forests are basically a bunch of decision trees on viagra'). An investigation of this problem might reveal very quickly that the data shift ('viagra'-occurrence / spam relationship) is caused by the new use of the former spam related word. An explainable model could highlight in the text which sections caused the spam classification.
- Sometimes training of machine learning models happens with training data that comes from a different distribution as the live data with which the model will be confronted with in production. The machine learning team basically trains the model on the dedicated training dataset, crosses their fingers and put it into production. The hope is, that the learned associations between the features and the output has a good **generalization from training to production**. The training data for a fracture detection  machine learning model might come from labeled radiographs that were done with a handful of different x-ray devices. But the company hopes to use their fracture detection algorithm in their software that they sell to hundreds of hospitals that likely use different machines. Explainable machine learning models can give the engineers the confidence that the model only picked up relationships that will generalize well for new datasets.
- Even if the problem formalization is complete, there might be an unknown **trade-off between objectives** like privacy and predictive ability.



## Who this book is for
This book is for everyone who wants to learn how to make machine learning models more explainable. It is a recommended reading for machine learning practitioners, statisticians, data scientists and everyone who has contact with machine learning applications.  It contains one or the other formula, but it's kept at a manageable level of math.
This book is not for people who are trying to learn machine learning from scratch. If you want to learn machine learning, there are loads of books and other resources for learning the basics.

## Outline of the book
Definition of interpretability / explainability and other terms.

Classic statistics usually aims at having interpretable models. That's why doctors, sociologists and banks tend to go to statisticians with research questions instead of computer scientists.

Some systems need more interpretability. The more impactful the decision, the more explainable it should be. A possibly incorrect product recommendation might is not as bad as a wrong diagnose and recommendation of an inappropriate treatment.


Need for explainability arises when something goes wrong. Because having an explanation for a faulty classification helps to understand the cause of the fault. It delivers a direction for how to fix the system. Consider an example of a husky versus wolf classifier, that missclassifies some huskys as wolfs. If there is an explanation to the classification you can see, that the missclassification happend due to the snow on the image. The classifier learned to use snow as a feature for classifying images as wolfs, which might make sense in terms of separating features in the training data set, but not in the real world use.  [TODO: Add image from Ribeiro + ask for permission]

As a machine learning practitioner, your main goal is to drive down the loss function while also keeping the learned model generalizable to other data sets.



## Is a good user experience enough?
Maybe it is enough to show that the algorithm works and does what it is supposed to. The recommended movies are all reasonable and my self-driving car never had an accident.

We have to distinguish between between low and high stakes scenario and individual risks and systematic biases.

For low-risk applications (e.g. product recommender systems) there is not so much damage if something goes wrong. No one will die because they got a product recommendation on Amazon for something they are not interested in. Biggest risk is for the company deploying those algorithms that they do not work and loose customers to competitors.
But there is the risk for a systematic bias and while for each individual the impact might be negligible on a group or society level it is quite problematic. Social bubbles through newsfeeds? An example is a restaurant recommender system that would never recommend restaurants to a certain minority, because they are from that minority.

High risk applications are self-driving cars, AI doctors etc. Here it is quite important to have explainability. Only with explainability can you 'debug' why a car accident happend by a self-driving car or decide if you want to trust the diagnose of a machine learning algorithm.

## Scope of explainability

### Algorithm
On this level of explainability, you are only asking how the algorithm learns, what kind of relationships it is capable of picking up. This level of explainability is what each good machine learning practitioner today usually has. If you are using convolutional neural networks for classifying images, you can explain that the algorithm learns edge detectors and filters on the lowest layers. This is an understanding of how the algorithm works, but not of the specific model that is learned in the end and not about how single decisions are reached. For this level of explainability only knowledge about the algorithm and the data is required. This book will talk a little about algorithmic explainability, but will focuse more on global and local explainability.

### Global model explainability
To explain the global model output, you need the trained model, knowledge about the algorithm and the data. This level of explainability is about understanding how the model makes the decisions, based on the features. Which features are the important ones and what kind of interactions are happening? Global model explainability helps to understand the distribution of your target variable based on the features.


### Explain a single observation
You can go all the way down to a single observation and examine what kind of classification or decision the model gives for this input, and why it gives this input. When you zoom in into one example, the conditional distribution of the target variable might behave more nicely. Locally it might depend only linearly or monotonic on some variables rather than having a complex dependency. For example the rent of an appartment might not depend linearly on the size, but if you only look at a specific appartment of 100 square meter and check how the prize changes going up plus and minus 10 square meters there is a chance that this sub region in your data space is linear. Local explanations can be more accurate compared to global explanations because of this.


## Tool vs. product
Explainability can be either a tool to analyse the machine learning model or a part of the final product.

## Evaluating explainability

There is no real consensus what explainability in machine learning is. Also it is not clear how to measure it.
Model size is an easy way to measure, but might be too simplistic.

Dimensions of interpretability:
- Model sparsity
- Monotonicity: Is there a Monotonicity constraint?
- Uncertainty
- Interactions
- How long does it take to understand the explanation. 
- Feature mode: What features were used? Word occurence? PCA components?
- Description length

If you can ensure that the machine learning model can explain decisions and are interpretable, following traits can also be checked more easily [2]
- Fairness: Unbiased, not discrimating against protected groups (implicit or explicit). An interpretable model can tell you why it decided it decided a certain person is not worthy of a credit and for a human it becomes easy to decide if the decision was based on a learned demographic (e.g. racial) bias.
- Privacy: sensitive information in the data is protected.
- Reliability/Robustness: Small changes in the input don't lead to big changes in the ouput/decision.
- Causality: Only causal relationships are picked up. So a predicted change in a decision due to arbitrary changes in the input values, are also happening in reality.
- Usability:
- Trust: It is easier for humans to trust into a system that explains it's decisions compared to a black box


### Approaches for evaluation of the explanation quality

- Application level evaluation (real task): Put the explanation into the product and let the end user test it. On an application level the radiologists would test the fracture detection software in order to evaluate the model. This requires a good experimental setup and an idea of how to assess the quality. A good baseline for this is always how good a human would be at explaining the same decision.
- Human level evaluation (simple task) is a  simplified application level evaluation. The difference is that these experiments are not conducted with the domain experts, but with lay humans. This makes experiments less expensive (especially when the domain experts are radiologists) and it is easier to find more humans. An example would be to show a user different explanations and the human would choose the best  .
- Function level evaluation (proxy task) does not require any humans. This works best when the class of models used is already evaluated by someone else in a human level evaluation.


## Definitions
- An **Algorithm** is a set of rules that a machine follows to achieve a particular goal [1]
- **Machine learning algorithm** is an set of rules that a machine follows to learn how to a achieve a particular goal. The output of a machine learning algorithm is a machine learning model.
- **(Machine learning) Model** is the outcome of a machine learning algorithm. This can be a set of weights for a linear model or neural network plus the architecture.
- **Features** are the variables/information used for prediction/classification/clustering.
- **(machine learning) Task** can be classification, regression, survival analysis, clustering, outlier detection
- **Instance** One row in the dataset.

## Terminology
Y is the target variable in supervised settings.
X are the features or covariates.
w are the weights.
$\beta% are regression weights.



[1] https://www.merriam-webster.com/dictionary/algorithm, accessed on Feb. 12th
[2] Doshi-Velez, F., & Kim, B. (2017). Towards A Rigorous Science of Interpretable Machine Learning, (Ml), 1–13. Retrieved from http://arxiv.org/abs/1702.08608
