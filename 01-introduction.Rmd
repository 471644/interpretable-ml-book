\mainmatter


```{r setup, cache=FALSE, include=FALSE}
library('knitr')
# output <- opts_knit$get("rmarkdown.pandoc.to")
# if (output=="html") opts_chunk$set(fig.width=11, fig.height=11)
# if (output=="docx") opts_chunk$set(fig.width=6,  fig.height=6)

opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
source('./src/initialize.R')

```

# Introduction {#intro}

![Guy on a pile of data explaining how math and data have to be stirred in a machine learning system until the right answers show up. Credits: xkcd.com](images/machine-learning-xkcd.png)


## What to expect from this book
The book will teach you how to make (supervised) machine learning models interpretable.
It contains one or the other mathematical formula, but it's kept at a manageable level of math.
This book is not for people who are trying to learn machine learning from scratch. If you are new to machine learning, there are loads of books and other resources for learning the basics. I recommend the book [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/) from (@Hastie2009) and [Andrew Ng's "Machine Learning" online course on coursera](https://www.coursera.org/learn/machine-learning) to get started with machine learning. Both the book and the course are available for free!

This book starts out with exploring the concepts of machine learning interpretability in Chapter \@ref(interpretability): It talks about when interpretability is important and different types of explanations.
Definitions used throughout the book can be looked up in Chapter \@ref(definitions).
All models and methods are explained and demonstrated with real data examples from Chapter \@ref(data).
One way to make machine learning interpretable is by using interpretable models, like linear models or decision trees.
Interpretable models are introduced in Chapter \@ref(simple).
The other option is to use model-agnostic interpretability methods, which are the topic of Chapter \@ref(agnostic).
This chapter covers methods like partial dependence plots and permutation feature importance.
Model-agnostic methods work by changing the input of the machine learning model and measuring changes in the output.

You can either read the book from start to end or directly jump to the methods you are interested in.
I hope you will enjoy the read!



## What is machine learning and why is it so important?

Machine learning is a method for teaching computers to make and improve predictions or behaviours based on data.
Predicting the prize of a house by learning from historical house sales can be done with machine learning.
The book focuses on supervised machine learning, which includes all problems where we know the label or outcome of interest (e.g. the past sale prizes of houses) and want to learn to predict.
Excluded from supervised learning are, for example, clustering tasks (=unsupervised learning), where we have no label, but want to find clusters of data points.
Also excluded are things like reinforcement learning, where an agent learns to optimise some reward by acting in an environment (e.g. a computer playing Tetris).
The goal in supervised learning is to learn a predictive model that maps features (e.g. house size, location, type of floor, ...) to an output (e.g. sales prize).
If the output is categorical it is called classification and if it is numerical, then regression.
Machine learning is a set of algorithms that can learn these mappings from training data, which are pairs of input features and output variable.
The machine learning algorithm learns a model by changing parameters (like linear weights) or learning structures (like trees).
The algorithm is guided by a score or loss function that is minimised.
In the house prize example, the machine minimise some form of difference between the estimated house prize and the predicted house prize.
A fully trained machine learning model can then be used to make predictions for new instances and be integrated into a product or process.

Estimating house prizes, recommending products, identifying street signs, counting people on the street, assessing a person's credit worthiness and detecting fraud:
All these examples have in common that they can and increasingly are realised with machine learning. The tasks are different, but the approach is the same: Step 1 is to collect data.
The more, the better.
The data needs to have the information you want to predict and additional information from which the prediction should be made.
For a street sign detector (is there a street sign in the image?) you would collect street images and label them accordingly with street sign yes vs. no.
For a loan default predictor you need historical data from actual loans, the information if the customers defaulted on their loans and data that helps you predict, like the customers income, age and so on.
For a house prize estimator, you would want to collect data from historical house sales and information about the real estate like size, location and so on.
Step 2: Feed this information into a machine learning algorithm, which produces a sign detector model, a credit worthiness model or a house prize estimator.
This model can then be used in Step 3:
Integrate the model into the product or process, like a self-driving car, a loan application process or a real estate marketplace website.

There are a lot of tasks in which machines exceed humans. Even if the machine is as good as a human at a task, or slightly worse, there remains big advantages, and that is speed, reproducibility and scale. A machine learning model that has been implemented once, can do a task much faster than humans, will reliably produce the same results from the same input and can be copied endlessly.
