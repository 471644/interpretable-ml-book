# Explainability {#explainability}


## What is explainability

UNDER CONSTRUCTION

## When is explainability important?
AI explainability is the ability of a machine learning system to explain or to present a decision in an understandable way for humans. Machine learning has come to a state where you have to make a trade-off: Do you simply want to know **what** will happen? For example if a client will churn or if medication will work well for a patient.  Or do you want to know **why** something will happen and paying for the explainability with accuracy? In some cases you will not care why a decision was made, only the assurance that the accuracy was good on some test set is enough. But in other cases knowing the 'why' can help you understand more about the problem, the data and also know why a model might fail.
Two sorts of problems might not need explanations, because they either are low risk (e.g. movie recommender system) or the method is already extensively studied and evaluated (e.g. optical character recognition).
The necessity for explainability comes from an incompleteness in the problem formalization [@Doshi-Velez2017], meaning that for certain problems/tasks it is not enough to get the answer (the "what"), but the model also has to give an explanation (the *why**)

- There is a shift in many scientific disciplines <!--TODO> Add example disciplines--> from qualitative to quantitative methods (e.g. sociology, psychology), and also towards machine learning (biology, genomics). The **goal of science** is to gain knowledge, but many problems can only be solved with big datasets and black box machine learning models. Explainability allows to extract additional knowledge.
- It is **human nature** wanting to understand things, to have some form of control.
- Machine learning models are taking over real world tasks, that demand **safety measurements** and testing. A self-driving car automatically detects cyclists, which is as desired. You want to bet 100% sure that the abstraction the system learned will be fail-safe, because running over cyclists is quite bad. An explanation might reveal that the most important feature learned is to recognize the two wheels of a bike and this explanation helps to think about edge cases like bikes with side bags, that partially cover the wheels.
- By default most machine learning models pick up biases from the training data. This can turn your machine learning models into racists (see Microsoft failed experiment: Tay) or discriminate in against other demographic, protected groups. Explainability helps you to get the **ethics** of your machine learning model right. From the data it might not be possible to see the biases, so it might also not be enough to implement protections against known biases. An example: The goal is to train a machine learning model to pre-sort applications for jobs in a big company. The company had some troubles in the past because they preferred to hire men over women, having the same qualifications. So for the model training the data scientist decides to remove all features that indicate the gender. But it turns out that the model picks up another, a priori unknown,  bias favoring younger people over older people. This could only be uncovered because the model is explainable.
- Sometimes it is not possible to optimize a goal directly, but only a proxy. Consider a marketing where a e-commerce company sends some gift to some customers in order to bind them to the business. It has already done similar campaigns, so there is data to learn a model to select the best customers to send the gifts to (sending it to all would be far to expensive). Ideally the company wants to optimize the likelihood that the customers with the gift are still their customers in five years from now, but there is no data for optimizing this goal directly, because the first of theses kinds of campaigns was two years ago, meaning there is no suitable training data for this objective. So the company opts for using a proxy: The amount of money the customers will be spending in the next 6 months at your company. An explainable model can help to find problems that might arise due to using a proxy objective (possibly **mismatched objectives**). In the e-commerce example it might turn out that the customers selected by the algorithm are only international students that might spent heavily in a short time, even more after receiving a gift, but usually leave the country and therefore the company as a customer within a few years.In  many scientific questions, like in longitudinal observational studies, the researcher wants to uncover causal relationships, like the therapy effect on some disease outcome measurement, but all algorithms only pick up and optimize the correlation which the researcher is only interested in if it is causal.
- In volatile environments a machine learning model might be outdated at some point because of a **data shift**. A data shift is a change in the relationships in the data. Explanation make it easier to detect when that happens or if one model is more prone than another to face problems soon. Spam filter for mails use works in a mail to detect if they should reach your inbox or be moved into the spam folder. 'Viagra' is one of the words that increase the likelihood that a mail is spam. When 'viagra' is suddenly used in a different context, for some obscure reason, suddenly non-spam mails will land in the spam folder (like 'Random forests are basically a bunch of decision trees on viagra'). An investigation of this problem might reveal very quickly that the data shift ('viagra'-occurrence / spam reluationship) is caused by the new use of the former spam related word. An explainable model could highlight in the text which sections caused the spam classification. A special case of a data shift is when the outcome of the machine learning model influences its environment and therefore its future training data. An example is the optimization of marketing campaigns. The machinee learning model influences which people will be targeted by the campaign and therefore influence who will be part of the future training set and also possibly invalidate its own predictions.
- Another topic related to data shift is **extrapolation**. With simpler, interpretable models it is clearer how predictions will be handled for data points outside of the training data cluster.
- Sometimes training of machine learning models happens with training data that comes from a different distribution as the live data with which the model will be confronted with in production. The machine learning team basically trains the model on the dedicated training dataset, crosses their fingers and put it into production. The hope is, that the learned associations between the features and the output has a good **generalization to new data**. The training data for a fracture detection  machine learning model might come from labeled radiographs that were done with a handful of different x-ray devices. But the company hopes to use their fracture detection algorithm in their software that they sell to hundreds of hospitals that likely use different machines. Explainable machine learning models can give the engineers the confidence that the model only picked up relationships that will generalize well for new datasets.
- Even if the problem formalization is complete, there might be an unknown **trade-off between objectives** like predictive ability and not being racis. A machine learning model that decides for a bank whether customers should receive a loan or not has many objectives to take care of: For the bank it is important that the customers will pay their loan back and not default. This can be directly optimized by training on historical data with customer data and information if they defaulted on a loan or not. The customers with the best scores (highest probability of paying back the loan without problems)  are the ones the bank would want to give a loan. At the same time the bank has to ensure that the algorithm is not discriminating against a demographic minority like race or religion. Also there might be other regulatory or legal requirements the bank has to be aware of when giving out loans. These additional objectives cannot be directly optimized, but explanations can fill the gap.
- There might be **legal requirements** to have an explainable decision making process.

Besides these scenarios with an incompleteness to of the problem formalization, there is also the task of debugging a machine learning model, which is crucial and difficult. Explainability is a useful debugging tool for black box algorithm.
So even in low risk environments (e.g. movie recommenders) explainability in the research and development stage is valuable. Also later when some model is used in a product, things can go wrong. And needed for explainability arises when something goes wrong. Because having an explanation for a faulty classification helps to understand the cause of the fault. It delivers a direction for how to fix the system. Consider an example of a husky versus wolf classifier, that missclassifies some huskies as wolfs. If there is an explanation to the classification you can see, that the missclassification happened due to the snow on the image. The classifier learned to use snow as a feature for classifying images as wolfs, which might make sense in terms of separating features in the training data set, but not in the real world use.  <!--TODO: Add image from Ribeiro + ask for permission]-->



## The bigger picture

UNDER CONSTRUCTION



## Scope of explainability
An algorithm trains a model, which produces the predictions. Each step can be evaluated in terms of transparency or explainability.

###  Algorithm transparency
How does the algorithm create the model?

Algorithm transparency is about how the algorithm learns a model from data and what kind of relationships it is capable of picking up. If you are using convolutional neural networks for classifying images, you can explain that the algorithm learns edge detectors and filters on the lowest layers. This is an understanding of how the algorithm works, but not of the specific model that is learned in the end and not about how single predictions are made. For this level of transparency only knowledge about the algorithm and not about the data or concrete learned models are required. This book  focuses on model explainability.
Algorithms like the least squares method for linear models are well studied and understood. They score high in transparency. Deep learning approaches (pushing a gradient through a network with millions of weights) are less understood and the inner workings are in the focus on-going research. It is not clear how they exactly work, so they are less transparent.


### Global, holistic model explainability
How does the trained model make predictions?

You could call a model explainable if you can comprehend the whole model at once [@Lipton2016].
To explain the global model output, you need the trained model, knowledge about the algorithm and the data. This level of explainability is about understanding how the model makes the decisions, based on a holistic few on it's features and each learned components like weights, parameters and structures. Which features are the important ones and what kind of interactions are happening? Global model explainability helps to understand the distribution of your target variable based on the features.
Arguably, global model explainability is very hard to achieve in practice. Any model that exceeds a handful of parameters or weights, probably won't fit an average human's brain capacity. I'd argue that you cannot really imagine a linear model with 5 features and draw in your head the hyperplane that the was estimated in the 5-dimensional feature space. Each feature space with more than 3 dimensions is just not imaginable for humans.
Usually when people try to comprehend a model, they look at parts of it, like the weights in linear models.

### Global model explainability on a modular level
How do parts of the model influence predictions?


You might not be able to comprehend a naive bayes model with many hundred features, because there is no way you could
hold all the feature weights in your brain's working memory. But you can understand a single weight easily. Not many models are explainable on a strict parameter level.While global model explainability is usually out of reach, there is a better chance to understand at least some models on a modular level. In the case of linear models parts to understand are the weights and the distribution of the features, for trees it would be splits (used feature and cut-off point) and leaf node predictions.
Linear models for example look like they would be, but the interpretation of a single weight is interlocked with all of the other weights. As you will see in Chapter [#limo], the interpretation of a single weight always comes with the footnote that the other input features stay at the same way value, which is not the case in many real world applications. A linear model predicting the rent of a flat, which takes into account both the size of the flat and the number of rooms might have a negative weight for the rooms feature, which is counter intuitive. But it can happen, because there is already the highly correlated flat size feature and in a market where people prefer bigger rooms, a flat with more rooms might be worth less than a flat with more rooms and same square meters. The weights only make sense in the light of the other features used in the model. But arguably a linear models weights still have better explainability than the weights of a deep neural network.


### Explain the decision for a single instance
Why did the model make a specific decision for an instance?

You can go all the way down to a single observation and examine what kind of classification or decision the model gives for this input, and why it made this decision. When you look at one example, the local distribution of the target variable might behave more nicely. Locally it might depend only linearly or monotonic on some variables rather than having a complex dependency on the features. For example the rent of an apartment might not depend linearly on the size, but if you only look at a specific apartment of 100 square meter and check how the prize changes going up plus and minus 10 square meters there is a chance that this sub region in your data space is linear. Local explanations can be more accurate compared to global explanations because of this.

### Explain the decisions for a group of instances
Why did the model make specific decisions for a group of instances?

The model output for multiple instances can be explained by using methods for global model explainability and single instance explanations. The global methods can be applied by taking the group of observations pretending it's the complete dataset and using the global methods on this subset. The single explanation methods can be used on each instance and listed or aggregated afterwards for the whole group.

## Evaluating explainability
There is no real consensus what explainability in machine learning is. Also it is not clear how to measure it.


### Approaches for evaluation of the explanation quality
[@Doshi-Velez2017] proposes 3 major levels of evaluating explainability.
- Application level evaluation (real task): Put the explanation into the product and let the end user test it. On an application level the radiologists would test the fracture detection software in order to evaluate the model. This requires a good experimental setup and an idea of how to assess the quality. A good baseline for this is always how good a human would be at explaining the same decision.
- Human level evaluation (simple task) is a  simplified application level evaluation. The difference is that these experiments are not conducted with the domain experts, but with lay humans. This makes experiments less expensive (especially when the domain experts are radiologists) and it is easier to find more humans. An example would be to show a user different explanations and the human would choose the best  .
- Function level evaluation (proxy task) does not require any humans. This works best when the class of models used is already evaluated by someone else in a human level evaluation.


#### Function level evaluation

Model size is an easy way to measure, but might be too simplistic.

Dimensions of interpretability:

- Model sparsity: How many features are being used by the explanation?
- Monotonicity: Is there a Monotonicity constraint?
- Uncertainty: Is a measurement of uncertainty part of the explanation?
- Interactions: Is the explanation able to include interaction of features?
- Cognitive processing time: How long does it take to understand the explanation.
- Feature complexity: What features were used for the explanation? PCA components are harder to understand than word occurrences for example.
- Description length of explanation


If you can ensure that the machine learning model can explain decisions, following traits can also be checked more easily [@Doshi-Velez2017].

- Fairness: Unbiased, not discrimating against protected groups (implicit or explicit). An interpretable model can tell you why it decided it decided a certain person is not worthy of a credit and for a human it becomes easy to decide if the decision was based on a learned demographic (e.g. racial) bias.
- Privacy: sensitive information in the data is protected.
- Reliability/Robustness: Small changes in the input don't lead to big changes in the ouput/decision.
- Causality: Only causal relationships are picked up. So a predicted change in a decision due to arbitrary changes in the input values, are also happening in reality.
- Usability:
- Trust: It is easier for humans to trust into a system that explains it's decisions compared to a black box
