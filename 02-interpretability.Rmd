# Interpretability {#interpretability}

So far, I haven't found a good scientific definition of "Machine learning model interpretability" or how to measure the goodness of an explanation.
Throughout the book, I will use this rather simple, yet elegant definition from \citet{miller2017explanation}: \textbf{Interpretability is the degree to which a human can understand the cause of a decision.} The higher the interpretability of a model, the easier it is for someone to comprehend why certain decisions (read: predictions) were made. A model has better interpretability than another model, if it's decisions are easier to comprehend for a human than decisions from the second model. I will be using both the terms interpretable and explainable equally.

## When is interpretability important?
Let' dive deeper into reasons why interpretability is important.
Interpretability in machine learning is the ability of a machine learning system to explain or to present a decision in an understandable way for humans.
Machine learning has come to a state where you have to make a trade-off:
Do you simply want to know **what** will happen?
For example if a client will churn or if medication will work well for a patient.
Or do you want to know **why** something will happen and paying for the interpretability with accuracy?
In some cases you don't  care why a decision was made, only the assurance that the accuracy was good on a test dataset is enough.
But in other cases knowing the 'why' can help you understand more about the problem, the data and also know why a model might fail.
Two sorts of problems might not need explanations, because they either are low risk (e.g. movie recommender system) or the method is already extensively studied and evaluated (e.g. optical character recognition).
The necessity for interpretability comes from an incompleteness in the problem formalisation [@Doshi-Velez2017], meaning that for certain problems/tasks it is not enough to get the answer (the "what"), but the model also has to give an explanation (the **why**)

- There is a shift in many scientific disciplines <!--TODO> Add example disciplines--> from qualitative to quantitative methods (e.g. sociology, psychology), and also towards machine learning (biology, genomics). The **goal of science** is to gain knowledge, but many problems can only be solved with big datasets and black box machine learning models. Interpretability allows to extract additional knowledge.
- It is **human nature** wanting to understand things, to have some form of control.
- Machine learning models are taking over real world tasks, that demand **safety measurements** and testing. A self-driving car automatically detects cyclists, which is as desired. You want to bet 100% sure that the abstraction the system learned will be fail-safe, because running over cyclists is quite bad. An explanation might reveal that the most important feature learned is to recognize the two wheels of a bike and this explanation helps to think about edge cases like bikes with side bags, that partially cover the wheels.
- By default most machine learning models pick up biases from the training data. This can turn your machine learning models into racists (see Microsoft failed experiment: Tay) or discriminate in against other demographic, protected groups. Interpretability is a useful debugging tool for black box algorithm.
So even in low risk environments (e.g. movie recommenders) explainability in the research and development stage is valuable. Also later when some model is used in a product, things can go wrong. And needed for explainability arises when something goes wrong. Because having an explanation for a faulty classification helps to understand the cause of the fault. It delivers a direction for how to fix the system. Consider an example of a husky versus wolf classifier, that missclassifies some huskies as wolfs. If there is an explanation to the classification you can see, that the missclassification happened due to the snow on the image. The classifier learned to use snow as a feature for classifying images as wolfs, which might make sense in terms of separating features in the training data set, but not in the real world use.  <!--TODO: Add image from Ribeiro + ask for permission]-->



## The bigger picture

Let's take a look from further away. What do we want to explain, and what kind of 'layers' are inbetween? The infographic displays the concepts, see Figure \@ref(fig:bigpicture). The bottom layer is the 'World'. This could literally be nature itself, like the biology of the human body and how it reacts to medication, but also human behaviour like if people payed back their loans. The 'World'-layer contains everything that can be observed and is of interest. Ultimately we want to learn something about the 'World' and interact with it.

The second layer is the 'Data'-layer. We have to digitalise the 'World' in to make it processable for computers and also to store information. The 'Data'-layer contains anything from images, texts, tabular data and so on.

With machine learning on top of the 'Data'-layer we get to the 'Black Box Model'-layer. Machine learning algorithms learns with data from the real world to make predictions / classifications or finds structures.

Now with the 'Interpretable models'-layer we come the part that this book is concerned with. On top of the 'Black-Box-Layer' we want to have something that helps us deal with the opaqueness of machine learning models. What were the important attributes for a particular diagnosis? Why was a financial transaction classified as fraud?

On top of that, there is the 'Explanations'-layer. I put it as a layer separate from 'Interpretable models', since the simple models deal with capturing associations and it is useful to think of the explanation as independent. There are different ways to present the results of a linear regression model for example, it could be a coefficient table, a coefficient plot with confidence intervals, a coloured bar chart, a few sentences, ...
It depends on the target audience what representation which explanation to choose.

The last layer is 'Human'. Look this one is waiving at you because you are reading this book and you are helping to provide better explanations for black box models! Humans are the consumers of the explanations ultimately.

```{r bigpicture, fig.cap="The big picture of explainable machine learning. The real world goes through many layers before it reaches the human in forms of explanations.", out.width="80%"}
knitr::include_graphics("images/big-picture.png")
```

This layered abstraction also helps in understanding what the difference between statisticians and machine learning practitioners is. Statistician are concerned with the 'Data' layer, like planning clinical trials or designing surveys. The they skip the 'Black Box Model'-layer and go right to the 'Interpretable Models' and from there to the explanations for our human. Machine learning specialists are also concerned with the 'Data'-layer, like collecting labeled samples of skin cancer images or crawling Wikipedia. Then comes the machine learning model. 'Interpretable models' and 'Explanations' are skipped and the human deals directly with the 'Black Box Model'. It's a nice thing, that in explainable machine learning, the work of a statistician and a machine learner fuses and becomes something better.

Of course this graphic does not capture everything: Data could come from simulations. Black box models also output predictions that might not even reach humans, but only feed other machines and so on. But overall it is a useful abstraction for understanding what explainable machine learning is.


## Scope of interpretability
An algorithm trains a model, which produces the predictions. Each step can be evaluated in terms of transparency or explainability.

###  Algorithm transparency
How does the algorithm create the model?

Algorithm transparency is about how the algorithm learns a model from data and what kind of relationships it is capable of picking up. If you are using convolutional neural networks for classifying images, you can explain that the algorithm learns edge detectors and filters on the lowest layers. This is an understanding of how the algorithm works, but not of the specific model that is learned in the end and not about how single predictions are made. For this level of transparency only knowledge about the algorithm and not about the data or concrete learned models are required. This book  focuses on model explainability.
Algorithms like the least squares method for linear models are well studied and understood. They score high in transparency. Deep learning approaches (pushing a gradient through a network with millions of weights) are less understood and the inner workings are in the focus on-going research. It is not clear how they exactly work, so they are less transparent.


### Global, holistic model explainability
How does the trained model make predictions?

You could call a model explainable if you can comprehend the whole model at once [@Lipton2016].
To explain the global model output, you need the trained model, knowledge about the algorithm and the data. This level of explainability is about understanding how the model makes the decisions, based on a holistic few on it's features and each learned components like weights, parameters and structures. Which features are the important ones and what kind of interactions are happening? Global model explainability helps to understand the distribution of your target variable based on the features.
Arguably, global model explainability is very hard to achieve in practice. Any model that exceeds a handful of parameters or weights, probably won't fit an average human's brain capacity. I'd argue that you cannot really imagine a linear model with 5 features and draw in your head the hyperplane that the was estimated in the 5-dimensional feature space. Each feature space with more than 3 dimensions is just not imaginable for humans.
Usually when people try to comprehend a model, they look at parts of it, like the weights in linear models.

### Global model explainability on a modular level
How do parts of the model influence predictions?


You might not be able to comprehend a naive bayes model with many hundred features, because there is no way you could
hold all the feature weights in your brain's working memory. But you can understand a single weight easily. Not many models are explainable on a strict parameter level.While global model explainability is usually out of reach, there is a better chance to understand at least some models on a modular level. In the case of linear models parts to understand are the weights and the distribution of the features, for trees it would be splits (used feature and cut-off point) and leaf node predictions.
Linear models for example look like they would be, but the interpretation of a single weight is interlocked with all of the other weights. As you will see in Chapter [#limo], the interpretation of a single weight always comes with the footnote that the other input features stay at the same way value, which is not the case in many real world applications. A linear model predicting the rent of a flat, which takes into account both the size of the flat and the number of rooms might have a negative weight for the rooms feature, which is counter intuitive. But it can happen, because there is already the highly correlated flat size feature and in a market where people prefer bigger rooms, a flat with more rooms might be worth less than a flat with more rooms and same square meters. The weights only make sense in the light of the other features used in the model. But arguably a linear models weights still have better explainability than the weights of a deep neural network.


### Explain the decision for a single instance
Why did the model make a specific decision for an instance?

You can go all the way down to a single observation and examine what kind of classification or decision the model gives for this input, and why it made this decision. When you look at one example, the local distribution of the target variable might behave more nicely. Locally it might depend only linearly or monotonic on some variables rather than having a complex dependency on the features. For example the rent of an apartment might not depend linearly on the size, but if you only look at a specific apartment of 100 square meter and check how the prize changes going up plus and minus 10 square meters there is a chance that this sub region in your data space is linear. Local explanations can be more accurate compared to global explanations because of this.

### Explain the decisions for a group of instances
Why did the model make specific decisions for a group of instances?

The model output for multiple instances can be explained by using methods for global model explainability and single instance explanations. The global methods can be applied by taking the group of observations pretending it's the complete dataset and using the global methods on this subset. The single explanation methods can be used on each instance and listed or aggregated afterwards for the whole group.

## Evaluating interpretability
There is no real consensus what explainability in machine learning is. Also it is not clear how to measure it.


### Approaches for evaluation of the explanation quality
[@Doshi-Velez2017] proposes 3 major levels of evaluating explainability.
- Application level evaluation (real task): Put the explanation into the product and let the end user test it. On an application level the radiologists would test the fracture detection software in order to evaluate the model. This requires a good experimental setup and an idea of how to assess the quality. A good baseline for this is always how good a human would be at explaining the same decision.
- Human level evaluation (simple task) is a  simplified application level evaluation. The difference is that these experiments are not conducted with the domain experts, but with lay humans. This makes experiments less expensive (especially when the domain experts are radiologists) and it is easier to find more humans. An example would be to show a user different explanations and the human would choose the best  .
- Function level evaluation (proxy task) does not require any humans. This works best when the class of models used is already evaluated by someone else in a human level evaluation.


#### Function level evaluation

Model size is an easy way to measure, but might be too simplistic.

Dimensions of interpretability:

- Model sparsity: How many features are being used by the explanation?
- Monotonicity: Is there a Monotonicity constraint?
- Uncertainty: Is a measurement of uncertainty part of the explanation?
- Interactions: Is the explanation able to include interaction of features?
- Cognitive processing time: How long does it take to understand the explanation.
- Feature complexity: What features were used for the explanation? PCA components are harder to understand than word occurrences for example.
- Description length of explanation


If you can ensure that the machine learning model can explain decisions, following traits can also be checked more easily [@Doshi-Velez2017].

- Fairness: Unbiased, not discrimating against protected groups (implicit or explicit). An interpretable model can tell you why it decided it decided a certain person is not worthy of a credit and for a human it becomes easy to decide if the decision was based on a learned demographic (e.g. racial) bias.
- Privacy: sensitive information in the data is protected.
- Reliability/Robustness: Small changes in the input don't lead to big changes in the ouput/decision.
- Causality: Only causal relationships are picked up. So a predicted change in a decision due to arbitrary changes in the input values, are also happening in reality.
- Usability:
- Trust: It is easier for humans to trust into a system that explains it's decisions compared to a black box
