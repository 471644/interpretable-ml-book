# Interpretability {#interpretability}



So far, I haven't found a good scientific definition of "Machine learning model interpretability" or how to measure the goodness of an explanation.
Throughout the book, I will use this rather simple, yet elegant definition from @miller2017explanation: **Interpretability is the degree to which a human can understand the cause of a decision.**
The higher the interpretability of a model, the easier it is for someone to comprehend why certain decisions (read: predictions) were made.
A model has better interpretability than another model, if it's decisions are easier to comprehend for a human than decisions from the second model.
I will be using both the terms interpretable and explainable equally.
<!--TODO: Distinguish between explanation and interpretability. Cite miller. Refer to explanation chapter where appropriate-->


## The Importance of Interpretability {#interpretability.importance}
If a machine learning model performs well, **why not just trust** it and ignore why it made a certain decision?

Let's dive deeper into the reasons why interpretability is so important.
Machine learning has come to a state where you have to make a trade-off:
Do you simply want to know **what** is predicted happen?
For example if a client will churn or if a medication will work well for a patient.
Or do you want to know **why** something is predicted to happen and paying for the interpretability with accuracy?
In some cases you don't care why a decision was made, only the assurance that the predictive performance was good on a test dataset is enough.
But in other cases knowing the 'why' can help you understand more about the problem, the data and it can also tell you why a model might fail.
Some problems might not need explanations, because they either are low risk, meaning a mistake has no severe consequences, (e.g. a movie recommender system) or the method has already been extensively studied and evaluated (e.g. optical character recognition).
The necessity for interpretability comes from an incompleteness in the problem formalisation [@Doshi-Velez2017], meaning that for certain problems or tasks it is not enough to get the answer (the **what**), but the model also has to give an explanation how it came to the answer (the **why**), because correctly predicting is not enough to solve the problem.
The following reasons drive the demand for interpretability:

- There is a shift in many scientific disciplines from qualitative to quantitative methods (e.g. sociology, psychology), and also towards machine learning (biology, genomics). The **goal of science** is to gain knowledge, but many problems can only be solved with big datasets and black box machine learning models. Interpretability allows to extract additional knowledge.
- It is **human nature** wanting to understand things and to have some form of control.
- Machine learning models are taking over real world tasks, that demand **safety measurements** and testing. A self-driving car automatically detects cyclists, which is as desired. You want to beÂ´ 100% sure that the abstraction the system learned will be fail-safe, because running over cyclists is quite bad. An explanation might reveal that the most important feature learned is to recognise the two wheels of a bike and this explanation helps to think about edge cases like bikes with side bags, that partially cover the wheels.
- By default most machine learning models pick up biases from the training data. This can turn your machine learning models into racists which discriminate against protected groups. Interpretability is a useful debugging tool to **detect bias** in machine learning models.
<!--TODO: Add Social acceptance of algorithms-->

Even in low risk environments, like movie recommendation, interpretability in the research and development stage is valuable.
Also later, when some model is used in a product, things can go wrong.
And need for interpretability arises when your model fucks up.
Because having an explanation for a faulty classification helps to understand the cause of the fault.
It delivers a direction for how to fix the system.
Consider an example of a husky versus wolf classifier, that misclassifies some huskies as wolfs.
There can be an explanation to the classification you can see, for instance, that the misclassification happened due to the snow on the image.
The classifier learned to use snow as a feature for classifying images as wolfs, which might make sense in terms of separating features in the training data set, but not in the real world use.

If you can ensure that the machine learning model can explain decisions, the following traits can also be checked more easily [@Doshi-Velez2017]:

- Fairness: Unbiased, not discriminating against protected groups (implicit or explicit). An interpretable model can tell you why it decided a certain person is not worthy of a credit and for a human it becomes easy to judge if the decision was based on a learned demographic (e.g. racial) bias.
- Privacy: Sensitive information in the data is protected.
- Reliability or Robustness: Small changes in the input don't lead to big changes in the prediction.
- Causality: Only causal relationships are picked up. Meaning a predicted change in a decision due to arbitrary changes in the input values are also happening in reality.
- Trust: It is easier for humans to trust into a system that explains it's decisions compared to a black box.


## Scope of Interpretability
An algorithm trains a model, which produces the predictions. Each step can be evaluated in terms of transparency or interpretability.

###  Algorithm transparency
*How does the algorithm create the model?*

Algorithm transparency is about how the algorithm learns a model from the data and what kind of relationships it is capable of picking up.
If you are using convolutional neural networks for classifying images, you can explain that the algorithm learns edge detectors and filters on the lowest layers.
This is an understanding of how the algorithm works, but not of the specific model that is learned in the end and not about how single predictions are made.
For this level of transparency, only knowledge about the algorithm and not about the data or concrete learned models are required.
This book  focuses on model interpretability and not algorithm transparency.
Algorithms like the least squares method for linear models are well studied and understood.
They score high in transparency.
<!-- Deep learning approaches (pushing a gradient through a network with millions of weights) are less well understood and the inner workings are in the focus of on-going research. -->
It is not clear how they exactly work, so they are less transparent.


### Global, Holistic Model Interpretability
*How does the trained model make predictions?*

You could call a model interpretable if you can comprehend the whole model at once [@Lipton2016].
To explain the global model output, you need the trained model, knowledge about the algorithm and the data.
This level of interpretability is about understanding how the model makes the decisions, based on a holistic view of its features and each of the learned components like weights, parameters, and structures.
Which features are the important ones and what kind of interactions are happening?
Global model interpretability helps to understand the distribution of your target variable based on the features.
Arguably, global model interpretability is very hard to achieve in practice.
Any model that exceeds a handful of parameters or weights, probably won't fit an average human's brain short term memory.
I'd argue that you cannot really imagine a linear model with 5 features and draw in your head the hyperplane that was estimated in the 5-dimensional feature space.
Each feature space with more than 3 dimensions is just not imaginable for humans.
Usually when people try to comprehend a model, they look at parts of it, like the weights in linear models.

### Global Model Interpretability on a Modular Level
*How do parts of the model influence predictions?*


You might not be able to comprehend a Naive Bayes model with many hundred features, because there is no way you could
hold all the feature weights in your brain's working memory.
But you can understand a single weight easily.
Not many models are interpretable on a strict parameter level.
While global model interpretability is usually out of reach, there is a better chance to understand at least some models on a modular level.
In the case of linear models, the interpretable parts are the weights and the distribution of the features, for trees it would be splits (used feature plus the cut-off point) and leaf node predictions.
Linear models for example look like they would be perfectly interpretable on a modular level, but the interpretation of a single weight is interlocked with all of the other weights.
As you will see in Chapter \@ref(limo), the interpretation of a single weight always comes with the footnote that the other input features stay at the same value, which is not the case in many real world applications.
A linear model predicting the value of a house, which takes into account both the size of the house and the number of rooms might have a negative weight for the rooms feature, which is counter intuitive.
But it can happen, because there is already the highly correlated flat size feature and in a market where people prefer bigger rooms, a flat with less rooms might be worth more than a flat with more rooms when both have the same size. The weights only make sense in the context of the other features used in the model.
But arguably the weights in a linear model still have better interpretability than the weights of a deep neural network.


### Explain the Prediction for a Single Instance
*Why did the model make a specific decision for an instance?*

You can zoom in on a single instance and examine what kind of prediction the model makes for this input, and why it made this decision.
When you look at one example, the local distribution of the target variable might behave more nicely.
Locally it might depend only linearly or monotonic on some features rather than having a complex dependence on the features.
For example the value of an apartment might not depend linearly on the size.
But if you only look at a specific apartment of 100 square meters and check how the price changes by going up and down by 10 square meters, there is a chance that this subregion in your data space is linear. Local explanations can be more accurate compared to global explanations because of this.
This book presents methods that can make single predictions more interpretable in Chapter \@ref(agnostic).

### Explain the Predictions for a Group of Instances
*Why did the model make specific decisions for a group of instances?*

The model predictions for multiple instances can be explained by either using methods for global model interpretability (on a modular level) or single instance explanations.
The global methods can be applied by taking the group of instances, pretending it's the complete dataset, and using the global methods on this subset.
The single explanation methods can be used on each instance and listed or aggregated afterwards for the whole group.

## Evaluating interpretability
There is no real consensus on what interpretability in machine learning is.
Also it is not clear how to measure it.
But there is some first research on it and the attempt to formulate some approaches for the evaluation, as described in the following section.


### Approaches for Evaluating the Explanation Quality
@Doshi-Velez2017 propose three major levels when evaluating explainability:

- **Application level evaluation (real task)**: Put the explanation into the product and let the end user test it. For example, on an application level, radiologists would test fracture detection software (which includes a machine learning component to suggest where fractures might be in an x-ray image) directly in order to evaluate the model. This requires a good experimental setup and an idea of how to assess the quality. A good baseline for this is always how good a human would be at explaining the same decision.
- **Human level evaluation (simple task)** is a  simplified application level evaluation. The difference is that these experiments are not conducted with the domain experts, but with lay humans. This makes experiments less expensive (especially when the domain experts are radiologists) and it is easier to find more humans. An example would be to show a user different explanations and the human would choose the best.
- **Function level evaluation (proxy task)** does not require any humans. This works best when the class of models used was already evaluated by someone else in a human level evaluation. For example it might be known that the end users understand decision trees. In this case, a proxy for explanation quality might be the depth of the tree. Shorter trees would get a better explainability rating. It would make sense to add the constraint that the predictive performance of the tree remains good and does not drop too much compared to a larger tree.


#### More on Function Level Evaluation
Model size is an easy way to measure explanation quality, but it is too simplistic.
For example, a sparse model with features that are themselves not interpretable is still not a good explanation.

There are more dimensions to interpretability:

- Model sparsity: How many features are being used by the explanation?
- Monotonicity: Is there a monotonicity constraint? Monotonicity means that a feature has a monotonic relationship with the target. If the feature increases, the target either always increases or always decreases, but never switches between increasing and decreasing.
- Uncertainty: Is a measurement of uncertainty part of the explanation?
- Interactions: Is the explanation able to include interactions of features?
- Cognitive processing time: How long does it take to understand the explanation?
- Feature complexity: What features were used for the explanation? PCA components are harder to understand than word occurrences, for example.
- Description length of explanation.



## Explanations

Let's dig deeper and ponder what we humans see as good explanations and what the implications for interpretable machine learning.
The humanities can help us to figure that out.
@miller2017explanation did a huge survey of publications about explanations.


After reading this chapter you should understand the difference between an explanation and causal attribution.

### What is an explanation?

The most straightforward definition is:
An explanation is the answer to a why-question (@miller2017explanation).
- "Why did the treatment not work on the patient?"
- "Why was my loan rejected?"
- "Why haven't we been contacted by alien live yet?"

The first two kind of questions can be answered with "everyday"-explanation, while the third is from the category "More general scientific phenomena and philosophical reasoning".
For interpretable machine learning, the "everyday"-type explanation is what we need to focus on.

Also questions starting with "how" can usually be turned into "why" questions:
"How was my loan rejected?" can be answered with the same explanation as "Why was my loan rejected".

The term "explanation" applies most to the interpretability on an instance level, as presented in Chapter \@ref(interpretability).
While reading the next paragraphs it is useful to keep in the back of your head that for machine learning "explanations" are answers to the question "Why did the machine learning model make a certain prediction?" (which is equivalent to "How did the machine learning model make a certain prediction?").


The term "explanation" can mean both the process of explaining something and the explanation as a product that results from a cognitive process.
This cognitive process can be conducted by either a human or a machine.
In case of machines the explanation as a product would be text, a plot or some other aggregated output, explaining why it made a certain prediction.
Seen as a process, an explanation is a cognitive process that identified the causes for some prediction, by for example considering counterfactuals, and then reducing this explanation to a sparse set of causes that form the explanation.
Explanation is also a social process of transferring knowledge from the explainer (the one doing the explanation) to the explainee (the one receiving the explanation).
The difference in cognitive and social process is the recipient of the explanation.
For cognitive the recipient is the explainer (e.g. the machine) itself, for social it is another human.

Things that are called explanation in machine learning:
- The cognitive process of computing an explanations.
- The social process of transferring the knowledge from the machine to the human.
- The explanation as a product, which can be a sentence, a graphic, etc..

## What constitutes a "good" explanation?

Machine learning model interpretability on a per-instance level as described in Chapter \@ref(interpretability) generates explanations, but as we will learn, not necessarily "good" ones.
The great literature survey buy @miller2017explanation takes a deep dive into the question:
What constitutes a 'good' explanations for humans?
Many artificial intelligence and machine learning publications and methods claim to be about "interpretable" or "explainable" methods, yet often this claim is only based on the authors intuition instead of hard facts and research.

The major findings about what explanations are (@miller2017explanation):
**Explanations are contrastive** (@lipton1990contrastive):
Humans usually don't ask *why* a certain decision was made, but rather why this decision was made instead of another decision.
We tend to think in counterfactual cases.
For a house value prediction, a person might be interested in why the predicted prize was high compared to the lower prize she expected.
When my loan application is rejected, I am not interested what in general constitutes a rejection or an approval.
I am interested in the factors of my application that would need to change so that it got accepted.
I want to know the contrast between my application and the would-be-accepted version of my application.
The realisation that contrastive explanations matter, is the most important finding for explainable machine learning.
As we will see, most interpretable models allow to extract some form of explanation that implicitly contrast it to some 'fictive data instance' or an average.
Going back to the question "Why did the treatment not work on the patient?", the doctor might ask for an explanation contrastive to a patient, where the treatment worked and who was maybe similar to the non-responsive patient.
Contrastive explanations are easier to understand than complete explanations.
A complete explanation to the doctor's why question (why does the treatment not work) might include:
The patient has the disease already since 10 years, 11 genes are over-expressed making the disease more severe, the patients body is very fast in breaking down the treatment into ineffective compounds , etc.. The contrastive explanation, which answers the question compared to the other patient, for whom the drug worked, might be much simpler:
The non-responsive has a certain combination of genes, that make the medication much less effective, compared to the other patient.


**Explanations are selected**:
People don't expect expectations to cover the actual and complete cause of an event.
We are used to selecting one or two causes from a huge number of possible causes as the explanation.
For proof, switch on the television and consume some news:
"The drop in share prices is blamed on a growing backlash against problems consumers are reporting with the latest software update.",
"The Tsubasa and his team lost the match because of a weak defence: they left their opponents to much free space to play out their strategy.",
"The increased distrust in established institutions and our government are the main factors that drove voters "


**Explanations are social**:
They are part of a conversation or interaction between the explainer and the receiver of the explanation.
The social context determines how the content and type of explanations.
If I wanted to explain why digital cryptocurrencies are worth so much technical person I would say things like:
"The decentralised, distributed blockchain-based ledger that cannot be controlled by a central entity resonates with peoples desire to secure their wealth, which explains the high demand and high prize.".
But to my grandma I might say:
"Look Grandma: Cryptocurrencies are a bit like gold, but it is just in your computer. Old people like and pay a lot for physical gold, young people like and pay a lot for digital gold".


Difference between causal attribution and causal explanation

Causal attribution might be needed in legal background, when you have to explain each cause for the prediction.
Also if you were to debug a system, you want causal attribution, which gives out the full picture of causes for a prediction, instead of a contrastive explanation.

Causal explanation is more useful, when it has to be fast and easy to explain for a human, especially lay-humans.
Contrastive explanations implicitly require that the you can easily understand an instance in your dataset.
Makes sense for: patients, houses, ...
Does not make sense for: gene dataset, ...
Maybe okay also to compare it to meaningful groups in order to create a contrastive explanation. Compare gene set of human who got disease to  average genes of healthy people.
The difference between causal attribution and an explanation is that the explanation has a conversational aspect.



## Why do humans want explanations?

<!--TODO: Combine with paragraphs below -->
<!--TODO: Maybe move this up to why interpretability is needed -->

- Curiosity and learning: Update mental models, when something unexpected happens. For example: Human feels unexpectedly sick and asks himself: "Why do I feel so sick?". He learns that this happened every time he ate those red berries. He updates his mental model and decides that the berries caused the sickness and therefore should be avoided. Another example: A doctor notes that some patients recover faster from surgery than others. She is curious: "Why are those patients recovering faster?". She conducts a survey and finds out that those patients did have more body fat than the others before surgery and she updates her mental model about how patients handle the aftermath of a surgery.
- Finding meaning in our environment: Reconcile contradictions or inconsistencies between elements of our knowledge structures. "Why did my dog bite me, even though it has never done so before?" a human might ask himself. The contraction is now between knowledge about the dog's past behaviour and the newly made experience. The explanation of the vet reconciles the dog holders contradiction: The dog was under stress and did bite, dogs are animals and this can happen.
- Manage social interactions: Create some shared meaning of something. Change and influence other actions/emotions/beliefs. Examples: "Why did the fields not yield enough crop this year?" a farmer asks herself. "Because you didn't pray enough to the crop-God", the priest answers and with his answer, creating a shared meaning, influencing the behaviour of the farmer to be more involved in the shared belief system the priest represents.

These reasons are obvious in human to human social interactions, but how are they relevant for human-machine interactions?
I'd argue that all three major reasons: **Curiosity and learning**, **Finding meaning**, and **Managing social interactions** are relevant for machine learning model explanations that have a direct human interaction component.
Let's walk through them:

**Curiosity and learning** is important for any machine learning model used in the research context, where scientific findings stay completely hidden, when the machine learning model only give predictions without explanations.
Also in the context of building machine learning models, it is important to know for the developers to have a mental model of what the model does to make sure that they will generalise to new data, to be aware of possible problems and not to discriminate against certain groups.

**Finding meaning** is required, when the machine learning model affects a persons life.
When a machine learning model rejects a loan application, this could be quite unexpected to some applicant.
He can only reconcile this inconsistency in expectation and reality by having some form of explanation.
Another example are recommender systems.
I personally always reflect on why certain product or movie recommendations have been made for me.
Often it is quite clear: The advertising is following me on the internet because I have bought a washing machine recently, and I know that I will be followed by washing machine advertisement the next days.
Yes, it makes sense to suggest gloves, when I already have a winter hat in my shopping basket.
Ok, this movie has been recommended to me by the algorithm, because users that liked other movies that I liked, also liked the recommended movie.
Increasingly, Internet companies are adding these explanations to their recommendations.
A good example is the Amazon product recommendation based on frequently bought product combinations, see Figure \@ref(fig:amazon-recommendation).
```{r amazon-recommendation, fig.cap='Recommended products when buying some paint from [Amazon](https://www.amazon.com/Colore-Acrylic-Paint-Set-12/dp/B014UMGA5W/). Visited on December 5th 2012.',  fig.align='center', out.width='80%'}
knitr::include_graphics("images/amazon-freq-bought-together.png")
```
**Managing social interactions** becomes the more important the more machines become part of our daily lives.
Machines have to "persuade" us, so that we believe that they can achieve their intended goal.
In order to allow a machine to interact with us, it might need to shape our emotions and beliefs.
I would not accept my robot vacuum cleaner if it would not yield explanations of it's behaviour.
The vacuum cleaner creates a shared meaning of for example an "accident" (get stuck on the bathroom carpet ... again) by explaining that it got stuck, instead of simply stopping to work without comment.
Interestingly there can be a misalignment between the goal of the explaining machine, which is generating trust and the goal of the recipients, which is to understand the prediction.
Maybe the correct explanation why my vacuum cleaner got stuck (by the way, it's name is "Doge") could be that the battery was very low, additionally one of the wheels is not working properly and on top of that there is a bug that causes the robot to re-try to go to the same spot, even though there was some resistance for the robot to move.
```{r doge-stuck, fig.cap="My vacuum cleaner 'Doge' got stuck. As an explanation for the accident, Doge told me that it needs to be on a flat surface.", out.width="80%"}
knitr::include_graphics("images/doge-stuck.png")
```
These reasons (and some more) caused the robot to get stuck, but it only explained that there was something in the way, and this was enough for me to trust it's behaviour,  and to get a shared meaning of that accident, which I can share with my girlfriend ("By the way, Doge got stuck again in the bathroom, we have to remove the carpets before we let it clean.").



### Conceptual framework of explaining behaviour
<!--TODO: Move this part here to the importance of interpretability-->
People attribute things like belief, desires intentions and so on to objects.
In a famous experiment @heider1944experimental showed participants videos of shapes, where for example a circle opened a door to enter a room.
The participants described the actions of the shapes as they would a human agent, assuming intentions and even emotions and personality traits for the shapes.
Again, a good example are robots, like my vacuum cleaner robot.
When 'Doge' gets stuck, I think "Doge wants to continue cleaning, but asks me for help because it got stuck." and later, when it is done and searches the home base to recharge I think: "Doge has the desire to recharge and intents to find the home base".
Also I attribute personality traits:
"Doge is a bit dumb, but in a cute way".
That's what I can't help but think when I find out that it kicked of some plant while dutifully cleaning the house.




### How do People select and evaluate explanations?

There are as many explanations as causes for a prediction.
Why was the house so expensive?
Because it is big.
Because it is in a good neighbourhood.
Because it is in good shape.
Because of good infrastructure.


Rashomon effect.
Rashomon is a Japanese movie, which tells 3 different stories that all have the same outcome.
Each story offers a different explanation for the outcome.



There are three processes involved:
1. causal connection: people identify the causes of an event
2. explanation selection: select a small subset of the causes as the explanation
3. explanation evaluation: evaluate the quality of an explanation

For a machine learning model explanation system:
1. Select all features, maybe a subset
2. Make explanation sparse
3. Evaluate fidelity and explainability


People focus more on abnormal causes (@kahneman1981simulation).
These are causes, that had a small likelihood, but happened anyways.
And removing these unlikely, abnormal causes would have changed the outcome a lot.
These kinds of "abnormal" causes are considered good explanations by humans.
Example:
The teacher tests each student and the student can either pass or fail.
The teacher can either be nice or mean.
The student can be prepared or unprepared.
We want to predict if a student will fail the test and explain our prediction.
Scenario one:
Most of the time teacher is very nice to students, so students that are prepared all pass the test.
Even unprepared, there is a good likelihood to pass the test, when the teacher is nice.
In the few cases where the teacher is mean, the unprepared students fail with high probability and the prepared have like a  50/50 chance.
When a student does not prepare, the teacher has a bad day and is mean and the student fails the test: What is the explanation for the failure?
The abnormal cause is that the teacher was mean, because it is such a rare event.


Scenario 2:
The teacher is rather mean (like 90 percent of the time) and the students usually prepare for the test.
Now a student fails the test, on a day where the teacher was mean and the student not prepared for the test.
Why did the student fail the test?
Because she did not prepare.

<!-- TODO: Check paper from where this example was and cite and improve.-->


So a good idea is to check what the abnormal attribute was for the prediction to be explained.

Also important is the temporality:
earlier events are considered less mutable than later events and therefore later events are perceived as more causal for an outcome.


people weight intentional causes more than non-intentional ones.
In the context of supervised machine learning this is probably not that important?
Maybe in explanations for behaviour of agents trained through reinforcement learning it is more important.
Labeling features as 'intentional' or 'non-intentional' could be useful for generating explanations.
Intentional features could be selected as an explanation.
This does not apply to all predictive models.
But consider we want to predict the churn probability of customers from an internet company.
As features we know the region the customer lives in, the duration of the contract, the advertisements for other products he received from internet provider, the increase of speed he got for being a long-term customer and so on.
The last to features are intentional actions by the company.
These would make good explanations, if someone wants to assess how likely someone is to churn.
Also it is useful, because those are actionable.
If the company sees, that the advertisements increase the likelihood of this particular customer to leave (as predicted by the machine learning model), they could stop sending advertisements.
But they can't act directly on the region the customer lives in.

People look at the difference between similar cases of which one of them resulted in an event, in the other case not.
The best explanation is the one that highlights the greatest difference between the object of interest and reference objects.

To explain why class A was predicted rather than class B, a good explanation consists of the difference between class A and B and not what constitutes class A (or class B) in general.


Abnormality of conditions:
Abnormal conditions are seen as causes for events.

Necessary causes are preferred to sufficient causes.
Necessary means, that without that cause, the prediction could never happen.
Sufficient means, that this feature enables the prediction, but there can be other sufficient features.

Responsible causes/features:
A feature that is fully responsible for an outcome is a necessary feature.
Responsibility is a degree of necessity.



How are people evaluating explanations?
The most important criteria: probability, simplicity, generalisation and coherence with prior believes.

Explanations should be simple and general.
The explanations should be coherent with their prior beliefs, otherwise they might not accept it.

Truth or high probability is an attribute of a good explanation, but it's not enough.

For class prediction, the abnormality rule for good explanations would support finding representative data points for classes, against which the class prediction can be compared:
For loan default, a predicted default could be compared to typical examples, where the loan would not default.



The "rules" for a conversation are (Grice's maxims @grice1975logic)
- only say what you mean (quality). For machine learning interpretability means, that the explanation should describe the machine learning model prediction as closely as possible. This is also called fidelity.
- Only say as much as necessary (quantity). This means both saying enough to be informative, but also not too informative. For machine learning interpretability this means the explanations should be as sparse as possible, without loosing too much fidelity. Difficult tradeoff to make.
- Only say what is relevant (relation). Also a strategy for achieving the maxim of quantity. For example don't use the models hyperparameters in an explanation, unless it was relevant for explaining the prediction. Relevance must also consider the mental model of the receiver of the explanation.
- Say it in a nice way (manner). This is about how rather then what is provided.


### What does this all mean for building explanation systems?

sparsity
social factor
Abnormality
Contrastive
General
Fidelity
