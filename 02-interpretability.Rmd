# Interpretability {#interpretability}

Throughout the book, I will use this rather simple, yet elegant definition of interpretability from @miller2017explanation: **Interpretability is the degree to which a human can understand the cause of a decision.**
The higher the interpretability of a model, the easier it is for someone to comprehend why certain decisions (read: predictions) were made.
A model has better interpretability than another model, if it's decisions are easier to comprehend for a human than decisions from the second model.
I will be using both the terms interpretable and explainable equally.
Like @miller2017explanation, I believe it makes sense to distinguish between the terms interpretability/explainability and explanation.
Making a machine learning interpretable can, but does not necessarily have to imply providing a (human-style) explanation of a prediction.
See Chapter \@ref(explanation) to learn what we humans see as a good explanation.

## The Importance of Interpretability {#interpretability.importance}
If a machine learning model performs well, **why not just trust** it and ignore why it made a certain decision?

Let's dive deeper into the reasons why interpretability is so important.
Machine learning has come to a state where you have to make a trade-off:
Do you simply want to know **what** is predicted happen?
For example if a client will churn or if a medication will work well for a patient.
Or do you want to know **why** something is predicted to happen and paying for the interpretability with accuracy?
In some cases you don't care why a decision was made, only the assurance that the predictive performance was good on a test dataset is enough.
But in other cases knowing the 'why' can help you understand more about the problem, the data and it can also tell you why a model might fail.
Some problems might not need explanations, because they either are low risk, meaning a mistake has no severe consequences, (e.g. a movie recommender system) or the method has already been extensively studied and evaluated (e.g. optical character recognition).
The necessity for interpretability comes from an incompleteness in the problem formalisation [@Doshi-Velez2017], meaning that for certain problems or tasks it is not enough to get the answer (the **what**), but the model also has to give an explanation how it came to the answer (the **why**), because correctly predicting is not enough to solve the problem.
The following reasons drive the demand for interpretability and explanations (@doshi-velez2017 and @miller2017explanation):

**Human curiosity and learning**: Humans have a mental model of their environment, which gets updated when something unexpected happens.
This updating is done by finding an explanation for the unexpected event.
For example, a human feels unexpectedly sick and asks himself: "Why do I feel so sick?".
He learns that he became sick every time he ate those red berries.
He updates his mental model and decides that the berries caused the sickness and therefore should be avoided.
Curiosity and learning is important for any machine learning model used in the research context, where scientific findings stay completely hidden, when the machine learning model only give predictions without explanations.
To facilitate learning and satisfy curiosity about why certain predictions or behaviours are created by machines, interpretability and explanations are crucial.
Of course, humans don't need an explanation for everything that happens.
Most people are okay with not understanding how a computer works.
The emphasis of this point is more the unexpected events, that makes us curious.
Like: Why is my computer shutting down unexpectedly?


Closely related to learning is the human desire to **find meaning in the world**.
We want to reconcile contradictions or inconsistencies between elements of our knowledge structures.
"Why did my dog bite me, even though it has never done so before?" a human might ask himself.
There is a contraction between the knowledge about the dog's past behaviour and the newly made, unpleasant experience of the bite.
The explanation of the vet reconciles the dog holders contradiction:
"The dog was under stress and did bite, dogs are animals and this can happen."
The more a machine's decision affects a human's life, the more important it will be for the machine to explain its behaviour.
When a machine learning model rejects a loan application, this could be quite unexpected to some applicant.
He can only reconcile this inconsistency in expectation and reality by having some form of explanation.
As we will learn in Chapter \@ref(explanation), the explanations don't actually have to fully explain the behaviour, but should address a main cause.
Another example are recommender systems.
Personally, I always reflect on why certain products or movies have been recommended to me algorithmically.
Often it is quite clear:
The advertising is following me on the internet because I have bought a washing machine recently, and I know that I will be followed by washing machine advertisement the next days.
Yes, it makes sense to suggest gloves, when I already have a winter hat in my shopping basket.
Ok, this movie has been recommended to me by the algorithm, because users that liked other movies that I liked, also liked the recommended movie.
Increasingly, Internet companies are adding these explanations to their recommendations.
A good example is the Amazon product recommendation based on frequently bought product combinations, see Figure \@ref(fig:amazon-recommendation).
```{r amazon-recommendation, fig.cap='Recommended products when buying some paint from [Amazon](https://www.amazon.com/Colore-Acrylic-Paint-Set-12/dp/B014UMGA5W/). Visited on December 5th 2012.',  fig.align='center', out.width='80%'}
knitr::include_graphics("images/amazon-freq-bought-together.png")
```


There is a shift in many scientific disciplines from qualitative to quantitative methods (e.g. sociology, psychology), and also towards machine learning (biology, genomics).
The **goal of science** is to gain knowledge, but many problems can only be solved with big datasets and black box machine learning models.
Interpretability allows to extract additional knowledge.


Machine learning models are taking over real world tasks, that demand **safety measurements** and testing. A self-driving car automatically detects cyclists, which is as desired. You want to beÂ´ 100% sure that the abstraction the system learned will be fail-safe, because running over cyclists is quite bad. An explanation might reveal that the most important feature learned is to recognise the two wheels of a bike and this explanation helps to think about edge cases like bikes with side bags, that partially cover the wheels.

By default most machine learning models pick up biases from the training data. This can turn your machine learning models into racists which discriminate against protected groups. Interpretability is a useful debugging tool to **detect bias** in machine learning models.

The process of integrating machines and algorithms into our daily lives demands interpretability to increase **social acceptance**. People attribute things like beliefs, desires intentions and so on to objects. In a famous experiment @heider1944experimental showed participants videos of shapes, where for example a circle opened a door to enter a room (rectangle). The participants described the actions of the shapes as they would a human agent, assuming intentions and even emotions and personality traits for the shapes. Robots are good examples, like my vacuum cleaner robot 'Doge'. When Doge gets stuck, I think "Doge wants to continue cleaning, but asks me for help because it got stuck." and later, when it is done and searches the home base to recharge I think: "Doge has the desire to recharge and intents to find the home base". Also I attribute personality traits: "Doge is a bit dumb, but in a cute way". That's what I can't help but think when I find out that it threw over some plant while dutifully cleaning the house. A machine or algorithm explaining it's prediction will receive more acceptance. See also Chapter \@ref(explanation), which argues that explanations are a social process.


Explanations are used to **manage social interactions**.
Through the creation of a shared meaning of something and the explainer influences the actions, emotions and beliefs of others.
Machines have to "persuade" us, so that we believe that they can achieve their intended goal.
In order to allow a machine to interact with us, it might need to shape our emotions and beliefs.
I would not completely accept my robot vacuum cleaner if it would not yield explanations of it's behaviour.
The vacuum cleaner creates a shared meaning of for example an "accident" (get stuck on the bathroom carpet ... again) by explaining that it got stuck, instead of simply stopping to work without comment.
Interestingly there can be a misalignment between the goal of the explaining machine, which is generating trust and the goal of the recipients, which is to understand the prediction.
Maybe the correct explanation why my vacuum cleaner got stuck (by the way, it's name is "Doge") could be that the battery was very low, additionally one of the wheels is not working properly and on top of that there is a bug that causes the robot to re-try to go to the same spot, even though there was some resistance for the robot to move.
```{r doge-stuck, fig.cap="My vacuum cleaner 'Doge' got stuck. As an explanation for the accident, Doge told me that it needs to be on a flat surface.", out.width="80%"}
knitr::include_graphics("images/doge-stuck.png")
```
These reasons (and some more) caused the robot to get stuck, but it only explained that there was something in the way, and this was enough for me to trust it's behaviour, and to get a shared meaning of that accident, which I can share with my girlfriend ("By the way, Doge got stuck again in the bathroom, we have to remove the carpets before we let it clean.").
The example of the robot getting stuck on the carpet might not even require an explanation, because I can conclude by observation, that it can't move on this carpet mess.
But there are other situations, which are less obvious, like a full dirt bag.


Only with interpretability can machine learning algorithms be **debugged and audited**.
So even in low risk environments, like movie recommendation, interpretability in the research and development stage as well as after deployment is valuable.
Because later, when some model is used in a product, things can go wrong.
Having an interpretation for a faulty classification helps to understand the cause of the fault.
It delivers a direction for how to fix the system.
Consider an example of a husky versus wolf classifier, that misclassifies some huskies as wolfs.
There can be an explanation to the classification you can see, for instance, that the misclassification happened due to the snow on the image.
The classifier learned to use snow as a feature for classifying images as wolfs, which might make sense in terms of separating features in the training data set, but not in the real world use.

If you can ensure that the machine learning model can explain decisions, the following traits can also be checked more easily [@Doshi-Velez2017]:

- Fairness: Unbiased, not discriminating against protected groups (implicit or explicit). An interpretable model can tell you why it decided a certain person is not worthy of a credit and for a human it becomes easy to judge if the decision was based on a learned demographic (e.g. racial) bias.
- Privacy: Sensitive information in the data is protected.
- Reliability or Robustness: Small changes in the input don't lead to big changes in the prediction.
- Causality: Only causal relationships are picked up. Meaning a predicted change in a decision due to arbitrary changes in the input values are also happening in reality.
- Trust: It is easier for humans to trust into a system that explains it's decisions compared to a black box.


## Scope of Interpretability
An algorithm trains a model, which produces the predictions. Each step can be evaluated in terms of transparency or interpretability.

###  Algorithm transparency
*How does the algorithm create the model?*

Algorithm transparency is about how the algorithm learns a model from the data and what kind of relationships it is capable of picking up.
If you are using convolutional neural networks for classifying images, you can explain that the algorithm learns edge detectors and filters on the lowest layers.
This is an understanding of how the algorithm works, but not of the specific model that is learned in the end and not about how single predictions are made.
For this level of transparency, only knowledge about the algorithm and not about the data or concrete learned models are required.
This book  focuses on model interpretability and not algorithm transparency.
Algorithms like the least squares method for linear models are well studied and understood.
They score high in transparency.
<!-- Deep learning approaches (pushing a gradient through a network with millions of weights) are less well understood and the inner workings are in the focus of on-going research. -->
It is not clear how they exactly work, so they are less transparent.


### Global, Holistic Model Interpretability
*How does the trained model make predictions?*

You could call a model interpretable if you can comprehend the whole model at once [@Lipton2016].
To explain the global model output, you need the trained model, knowledge about the algorithm and the data.
This level of interpretability is about understanding how the model makes the decisions, based on a holistic view of its features and each of the learned components like weights, parameters, and structures.
Which features are the important ones and what kind of interactions are happening?
Global model interpretability helps to understand the distribution of your target variable based on the features.
Arguably, global model interpretability is very hard to achieve in practice.
Any model that exceeds a handful of parameters or weights, probably won't fit an average human's brain short term memory.
I'd argue that you cannot really imagine a linear model with 5 features and draw in your head the hyperplane that was estimated in the 5-dimensional feature space.
Each feature space with more than 3 dimensions is just not imaginable for humans.
Usually when people try to comprehend a model, they look at parts of it, like the weights in linear models.

### Global Model Interpretability on a Modular Level
*How do parts of the model influence predictions?*


You might not be able to comprehend a Naive Bayes model with many hundred features, because there is no way you could
hold all the feature weights in your brain's working memory.
But you can understand a single weight easily.
Not many models are interpretable on a strict parameter level.
While global model interpretability is usually out of reach, there is a better chance to understand at least some models on a modular level.
In the case of linear models, the interpretable parts are the weights and the distribution of the features, for trees it would be splits (used feature plus the cut-off point) and leaf node predictions.
Linear models for example look like they would be perfectly interpretable on a modular level, but the interpretation of a single weight is interlocked with all of the other weights.
As you will see in Chapter \@ref(limo), the interpretation of a single weight always comes with the footnote that the other input features stay at the same value, which is not the case in many real world applications.
A linear model predicting the value of a house, which takes into account both the size of the house and the number of rooms might have a negative weight for the rooms feature, which is counter intuitive.
But it can happen, because there is already the highly correlated flat size feature and in a market where people prefer bigger rooms, a flat with less rooms might be worth more than a flat with more rooms when both have the same size. The weights only make sense in the context of the other features used in the model.
But arguably the weights in a linear model still have better interpretability than the weights of a deep neural network.


### Local Interpretability for a Single Prediction
*Why did the model make a specific decision for an instance?*

You can zoom in on a single instance and examine what kind of prediction the model makes for this input, and why it made this decision.
When you look at one example, the local distribution of the target variable might behave more nicely.
Locally it might depend only linearly or monotonic on some features rather than having a complex dependence on the features.
For example the value of an apartment might not depend linearly on the size.
But if you only look at a specific apartment of 100 square meters and check how the price changes by going up and down by 10 square meters, there is a chance that this subregion in your data space is linear. Local explanations can be more accurate compared to global explanations because of this.
This book presents methods that can make single predictions more interpretable in Chapter \@ref(agnostic).

### Local Interpretability for a Group of Prediction
*Why did the model make specific decisions for a group of instances?*

The model predictions for multiple instances can be explained by either using methods for global model interpretability (on a modular level) or single instance explanations.
The global methods can be applied by taking the group of instances, pretending it's the complete dataset, and using the global methods on this subset.
The single explanation methods can be used on each instance and listed or aggregated afterwards for the whole group.

## Evaluating interpretability
There is no real consensus on what interpretability in machine learning is.
Also it is not clear how to measure it.
But there is some first research on it and the attempt to formulate some approaches for the evaluation, as described in the following section.


### Approaches for Evaluating the Interpretability Quality
@Doshi-Velez2017 propose three major levels when evaluating interpretability:

- **Application level evaluation (real task)**: Put the explanation into the product and let the end user test it. For example, on an application level, radiologists would test fracture detection software (which includes a machine learning component to suggest where fractures might be in an x-ray image) directly in order to evaluate the model. This requires a good experimental setup and an idea of how to assess the quality. A good baseline for this is always how good a human would be at explaining the same decision.
- **Human level evaluation (simple task)** is a  simplified application level evaluation. The difference is that these experiments are not conducted with the domain experts, but with lay humans. This makes experiments less expensive (especially when the domain experts are radiologists) and it is easier to find more humans. An example would be to show a user different explanations and the human would choose the best.
- **Function level evaluation (proxy task)** does not require any humans. This works best when the class of models used was already evaluated by someone else in a human level evaluation. For example it might be known that the end users understand decision trees. In this case, a proxy for explanation quality might be the depth of the tree. Shorter trees would get a better explainability rating. It would make sense to add the constraint that the predictive performance of the tree remains good and does not drop too much compared to a larger tree.


#### More on Function Level Evaluation
Model size is an easy way to measure explanation quality, but it is too simplistic.
For example, a sparse model with features that are themselves not interpretable is still not a good explanation.

There are more dimensions to interpretability:

- Model sparsity: How many features are being used by the explanation?
- Monotonicity: Is there a monotonicity constraint? Monotonicity means that a feature has a monotonic relationship with the target. If the feature increases, the target either always increases or always decreases, but never switches between increasing and decreasing.
- Uncertainty: Is a measurement of uncertainty part of the explanation?
- Interactions: Is the explanation able to include interactions of features?
- Cognitive processing time: How long does it take to understand the explanation?
- Feature complexity: What features were used for the explanation? PCA components are harder to understand than word occurrences, for example.
- Description length of explanation.



## Human-style explanations {#explanation}

Let's dig deeper and discover what we humans accept as 'good' explanations and what the implications for interpretable machine learning are.
Research from the humanities can help us to figure that out.
@miller2017explanation did a huge survey of publications about explanations and this Chapter builds on his summary.


In this Chapter I want to convince you of the following:
As an explanation for an event, humans prefer short explanations (just 1 or 2 causes), which contrast the current situation with a situation where the event would not have happened.
Also explanations are social interactions between the explainer and the explainee (receiver of the explanation) and therefore the social context has a huge influence on the actual content of the explanation.

If you build the explanation system to get ALL the factors for a certain prediction or behaviour, you do not want a human-style explanation, but rather a complete causal attribution.
You probably want a causal attribution when you are legally required to state all influencing features or if you are debugging the machine learning model. In this case, ignore the following points.
In all other setting, where mostly lay persons are the recipients of the explanation, follow the advice here.


### What is an explanation?

An explanation is the **answer to a why-question** (@miller2017explanation).

- "Why did the treatment not work on the patient?"
- "Why was my loan rejected?"
- "Why haven't we been contacted by alien live yet?"

The first two kind of questions can be answered with an "everyday"-explanation, while the third is from the category "More general scientific phenomena and philosophical reasoning".
We focus on the "everyday"-type explanation, because this is relevant for interpretable machine learning.
Questions starting with "how" can usually be turned into "why" questions:
"How was my loan rejected?" can be answered with the same explanation as "Why was my loan rejected".


The term "explanation" means the social and cognitive process of explaining, but it's also the product of this process.
The explanation can be conducted by either a human or a machine.
If it's a machine, the following holds:

- The cognitive process is the computation of an explanations
    - Identifying possible causes
    - Selecting a few causes as explanation
    - Formulating the explanation
- The social process is the transfer of knowledge from the machine to the human.
- The product, which can be a sentence, a graphic, etc..


### What is a "good" explanation? {-}

Now that we know what an explanation is, the question arises, what a good explanation is.

Machine learning model interpretability on a per-instance level as described in Chapter \@ref(interpretability) generates explanations, but as we will learn, not necessarily "good" ones.
"Many artificial intelligence and machine learning publications and methods claim to be about "interpretable" or "explainable" methods, yet often this claim is only based on the authors intuition instead of hard facts and research." - @miller2017explanation

From @miller2017explanation's findings we can learn, what is important for an explanation to be viewed as 'good'.
The major attributes of a good explanations are: contrastive, social, selected and abnormality-focused.

**Explanations are contrastive** (@lipton1990contrastive):
Humans usually don't ask *why* a certain prediction was made, but rather why this prediction was made instead of another prediction.
We tend to think in counterfactual cases, i.e. "How would the prediction have looked like, if input X where different?".
For a house value prediction, a person might be interested in why the predicted prize was high compared to the lower prize she expected.
When my loan application is rejected, I am not interested what in general constitutes a rejection or an approval.
I am interested in the factors of my application that would need to change so that it got accepted.
I want to know the contrast between my application and the would-be-accepted version of my application.
The realisation that contrastive explanations matter, is the most important finding for explainable machine learning.
As we will see, most interpretable models allow to extract some form of explanation that implicitly contrast it to some 'fictive data instance' or an average.
Going back to the question "Why did the treatment not work on the patient?", the doctor might ask for an explanation contrastive to a patient, where the treatment worked and who was maybe similar to the non-responsive patient.
Contrastive explanations are easier to understand than complete explanations.
A complete explanation to the doctor's why question (why does the treatment not work) might include:
The patient has the disease already since 10 years, 11 genes are over-expressed making the disease more severe, the patients body is very fast in breaking down the treatment into ineffective compounds , etc..
The contrastive explanation, which answers the question compared to the other patient, for whom the drug worked, might be much simpler:
The non-responsive has a certain combination of genes, that make the medication much less effective, compared to the other patient.
The best explanation is the one that highlights the greatest difference between the object of interest and reference objects.
To explain why class A was predicted rather than class B, a good explanation consists of the difference between class A and B and not what constitutes class A (or class B) in general.
*What it means for interpretable machine learning*:
Humans don't want a complete explanation for a prediction but rather compare what the difference were to another instance's prediction (could also be a fictive one).
Making explanations contrastive is application dependent, because it requires a point of reference for comparison.
And this might depend on the data point to be explained, but also on the one receiving an explanation.
A user of a house prize prediction website might want to have an explanation of a house prize prediction contrastive to her own house or maybe to some other house on the website or maybe to an average house in the neighbourhood.
The solution for creating contrastive explanations in an automated fashion might include finding prototypes or archetypes in the data to contrast to.

**Explanations are selected**:
People don't expect expectations to cover the actual and complete cause of an event.
We are used to selecting one or two causes from a huge number of possible causes as the explanation.
For proof, switch on the television and consume some news:
"The drop in share prices is blamed on a growing backlash against problems consumers are reporting with the latest software update.",
"The Tsubasa and his team lost the match because of a weak defence: they left their opponents to much free space to play out their strategy.",
"The increased distrust in established institutions and our government are the main factors that drove voters "
The effect that an event can be explained by different causes is called the Rashomon Effect.
Rashomon is a Japanese movie in which alternative, contradictory stories (explanations) of a samurai's death are told.
For machine learning models it is good, when a good prediction can be made from different features.
Especially ensemble methods that combine multiple models with different features (different explanations) thrive on the fact that averaging over those "stories" makes the prediction more robust an accurate.
But it also means that there is no good explanation why they made the prediction.
*What it means for interpretable machine learning*:
Make the explanation very short, give only 1 to 3 reasons, even if the world is more complex.
The LIME method, described in Chapter \@ref(lime) does a good job with this.


**Explanations are social**:
They are part of a conversation or interaction between the explainer and the receiver of the explanation.
The social context determines how the content and type of explanations.
If I wanted to explain why digital cryptocurrencies are worth so much technical person I would say things like:
"The decentralised, distributed blockchain-based ledger that cannot be controlled by a central entity resonates with peoples desire to secure their wealth, which explains the high demand and high prize.".
But to my grandma I might say:
"Look Grandma: Cryptocurrencies are a bit like gold, but it is just in your computer. Old people like and pay a lot for physical gold, young people like and pay a lot for digital gold".
*What it means for interpretable machine learning*:
Keep in mind, what the social setting of your machine learning is and who the target audience is.
This will differ a lot depending on your application.
I know this advice is a bit fuzzy, but it is probably the best to ask people from the humanities (e.g. psychologists, sociologists) to help you out.

**Explanations focus on the abnormal**.
People focus more on abnormal causes (@kahneman1981simulation).
These are causes, that had a small likelihood, but happened anyways.
And removing these unlikely, abnormal causes would have changed the outcome a lot.
These kinds of "abnormal" causes are considered good explanations by humans.
Example (@vstrumbelj2011general):
We have a dataset of test situations between teachers and students.
The teachers have the option to directly let students pass after they have given a presentation or they can test the student's knowledge with additional questions, which determines if the student passes.
This means we have one feature 'teacher', which is either 0 (does not test) or 1 (teacher does test).
The students can have 6 different levels of preparation, which yield different probabilities of passing the teachers test (in case she decides to test the student).
A preparation level of 0 means 0% chance of passing, level of 1 means 20% chance up to level of 5, with which students pass for certain.
We want to predict if a student will pass the test and explain our prediction.
Scenario one:
The teacher tests the students most of the time.
A student who did not study (0 preparation) fails the test.
Why did the student fail the test?
We would say it was the students fault to not study.
*What it means for interpretable machine learning*:
If one of the input features for a prediction was abnormal in any sense (like a rare category of a categorical feature) and the feature changed the prediction, it should be included in an explanation, even if other 'normal' features have the same importance as the abnormal one.
An "abnormal" feature in our house prize predictor example might be that a rather expensive house has three balconies.
Even if some attribution method finds out that the three balconies contribute has the same effect as the somewhat above average size, the good neighbourhood and the recent renovation, the abnormal feature "three balconies" might be the best explanation why the house is so expensive.

**Explanations are truthful**.
If they are not true, people probably don't accept them.
But, disturbingly, this is not the most important factor for a 'good' explanation.
For example selectiveness is more important than truthfulness.
An explanation that selects only one or two possible causes can never cover the complete causes in reality. Selectivity omits part of the truth.
It's not true that only one or the other factor caused a market crash for example, but the truth is that there are millions of causes that influence millions of people to act in a way that caused a stock crash in the end.
*What it means for interpretable machine learning*:
The explanation should explain the event as truthfully as possible, which is sometimes called **fidelity** in the context of machine learning.
So when we say that three balconies increase the prize of a house, it should hold true for other houses as well (or at least for similar houses).
To humans, fidelity is not as important for a good explanations as selectivity, contrast and the social aspect.

**Good explanations are coherent with prior beliefs of the explainee**.
Humans tend to ignore information that is not coherent with their prior beliefs .
This effect is known as confirmation bias (@nickerson1998confirmation).
Explanations are not spared from this type of bias:
People will tend to devalue or ignore explanations that do not cohere with their beliefs.
This of course differs individually, but there are also group-based prior beliefs like political opinions.
*What it means for interpretable machine learning*:
Good explanations are consistent with prior beliefs.
This one is hard to infuse into machine learning and would probably drastically compromise accuracy.
An example would be negative effects of house size for the predicted prize of the house for a few special houses, which let's say improve accuracy (because of some complex interactions), but strongly contradicts prior beliefs.
One thing you can do is to enforce monotonicity constraints (a feature can affect the outcome only into one direction) or use something like a linear model that has this property.

**Good explanations are general and probable**.
A cause that can explain a lot of events is very general and makes for a good explanation.
Note that this contradicts the fact that this contradicts that people explain things with abnormal causes. Abnormal causes are more important.
But in the absence of abnormal causes, causes that are more probable are better.  But keep in mind that people tend to wrongly judge probability of joint events.
(Joe is a librarian. Is it more likely that he is shy or that he is a shy person that loves reading books?).
A good example is the size of a house, which is a very general, good explanation why houses are expensive or cheap.
Note that it contradicts the claim that "abnormal" causes make good explanations. As I see it, abnormal causes beat general causes.
Abnormal causes are, by definition, rare. So in the absence of some abnormal event, a general explanation is judged to be good by humans.
*What it means for interpretable machine learning*:
Generality is easily measured by its support, which is the number of instances for which the explanation applies over the total number of instances.
