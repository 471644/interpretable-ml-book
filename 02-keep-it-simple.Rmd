# Keep it simple {#simple}

The most straight forward way to achieve explainable machine learning algorithms is to use only a subset algorithms that yield an understandable model structure.

These are:

* linear models
* trees
* rules

In the following chapters these we will talk about the algorithm with it's variants. Not in detail, only the basics, because there are already a ton of books, videos, tutorials, papers and so on about them. We will focus on how to interpret the models and why they are explainable.
The chapter covers linear models, decision trees, decision rules, neighbour methods and graphical models. 

## Linear models

Linear models have been and are still used by statistician, computer scientists and other people with quantitative problems.  

Linear models can be used to model the dependency of a regression variable (here Y) on K covariates. As the name says, the learned relationships are linear in the form of

$$y_{i} = \beta_{0} + \beta_{1} \cdot x_{i,1} + \ldots + \beta_{K} x_{i,K} + \epsilon_{i}$$

The i-th observation's outcome is a weighted sum of it's K features. The $\beta_{k}$ represent the learned feature weights or coefficients. The $\epsilon_{i}$ is the error we are still making, the difference between the predicted and actual outcome.

The biggest advantage is the linearity: It makes the estimation procedure straight forward and most importantly these linear equations have an easy to understand interpretation. That is one of the main reasons why the linear model and all it's descendants are so widespread in academic fields like medicine, sociology, psychology and many more quantitative research fields. In this areas it is important to not only predict e.g. the clinical outcome of a patient, but also quantify the influence of the medication while at the same time accounting for things like sex, age and other variables.

### Interpretation
The interpretation of the coefficients:

- Continuous regression variable: For an increase of one point of the variable $x_{j}$ the estimated outcome changes by $\beta_{j}$
- Binary categorial variables: One of the variables is the reference level (in some languages the one that was coded in 0). A change of the variable $x_{i}$ the reference level to the other category changes the estimated outcome by $\beta_{i}$
- Categorial variables with many levels: One solution to deal with many variables is to one-hot-encode them, meaning each level gets it's own column. From a categorial variable with L levels, you only need L-1 columsn, otherwise it is over parameterized. The interpretation for each level is then according to the binary variables. Some language like R allow to
- Intercept $\beta_{0}$: The interpretation is: Given all continuous variables are zero and the categorial variables are on the reference level, the estimated outcome of $y_{i}$ is $\beta_{0}$. The interpretation of $\beta_{0}$ is usually not relevant.



#### Coding categorial variables:
There are several ways to represent a categorial variable, which has an influence on the interpretation: http://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/ and
http://heidiseibold.github.io/page7/

Described above is the dummy coding, which is usually sufficient.
Using different codings boils down to creating different matrices from your one column with the categorial variable.
An overview:

- Dummy coding compares each level to the reference level. The intercept is the mean of the reference group
- Simple coding also compares each level to the reference, but the intercept is the overall mean.
- Deviation coding: Intercept is overall mean, and levels are compared to the mean
- Split coding: compare each level to the previous level. Usually used when there is an order in the levels.
- Helmert coding: ...




### Explaining a single observation:

Let's look at a concrete example:

You are trying to predict the house price using the notorious Boston house prize data set. So y is the prize of the house and your X are: ...

The fitted linear model for the prize comes down to the following equation:

EQUATION



### Linear models beyond gaussian regression


## Decision trees
## Decision rules
## Decision tables
## Nearest neighbours
## Bayesian network classifiers
## Naive Bayes Classifier and other graphical models
### Naive bayes
Good in case you want interpretation of all features. Bad: No interaction, but even strong assumption of independence of features given the class. But if you indeed have independent features, then it might be the best model.

### More general bayesian network approache(s)
#### Bayesian Network-Augmented Na√Øve Bayes (BAN) illustrated
#### General Bayesian Network
