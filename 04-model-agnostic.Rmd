# Model-agnostic explanations
Separating the explanations from the machine learning model (= model-agnostic explanations) gives some benefits.
The big advantage of model-agnostic vs model-specific explanation algorithm is the flexibility. When the explanation system is independently applicable even when the underlying model is switched, it frees the practitioner to use different machine learning models without restrictions. It is also more efficient to build interfaces on top of model-agnostic systems, because  this has to be done only once and not for each model-specific explanation system. Usually not one but many types of machine learning models are tested in development time and if you want to compare the models in terms of interpretability this is easier with model-agnostic explanations because the system is the same for both models that are being compared [@Ribeiro2016b].


The alternatives are either using only interpretable models as introduced in Chapter 2, which has the big disadvantage to usually loose accuracy compared to other approaches. The other alternative is to use more flexible model classes that come with built in explanations. The drawback here is that it ties you to this one algorithm and it will be hard to switch to something else.

Desirable aspects of a model-agnostic explanation system [@Ribeiro2016b]:
- Model flexibility: Not being tied to an underlying particular machine learning model. The method should work for random forests as well as convolutional neural networks
- Explanation flexibility: Not being tied to a certain form of explanation. In some cases it might be useful to have a linear formula in other cases some decision rules
- Representation flexibility: The explanation system should not have to use the same feature representation as the model that is being explained. So when a text classifier uses abstract word embedding vectors, it might be preferable to use the presence of single words for the explanation.

## Global: Explain the behaviour of a model


### Global surrogate models
A surrogate model is a simple, explainable model that explains another complex machine learning model. Models in [#simple] are viable candidates.

UNDER CONSTRUCTION

### Partial dependence plot {#pdp}
The partial dependence plot shows the marginal effect of a variable on the target (regression / classification) [@friedman2001greedy]. A partial dependence plot can show if the relationship between target and feature is linear, monotonic or something else.
In linear regression, those plots will always show a linear relationship.

The partial dependence function for regression is defined as:
$$f_S = E_{x_C}[f(x_S, x_C)] = \int f(x_S, x_C) dP(x_C)$$
The $x_S$ is the set of variables for which the partial dependence should be depicted and $x_C$ are the other variables that were used in the machine learning model. Partial dependence works by averaging out the other variables, so that the remaining function shows the relationsship between the $x_S$, in which we are interested, and the target. $x_S$ is fixed and $x_C$ is varying.

The integral is estimated by calculating averages in the training data, which looks like this for regression:
$$ \hat{f}(x_S) = \frac{1}{n} \sum_{i=1}^n f(x_S, x_{Ci}) $$
In  this formula, $x$ is the variable for which to calculate the partial dependence, $x_{iC}$ is the other variables and $n$ the number of instances in the data set.

For classification it is the logits:
$$ f(x) = \log p_k(x) - \frac{1}{K} \sum_{j=1}^K \log p_j(x) $$


Partial dependence plots are only partially global: They are global because they take into account all instances, but it is local in the feature, because partial dependence plots only examine one variable, as the name suggests.


#### Examples
In practice $x_S$ usually only contains one variable or a maximum of two, because one variable produces 2D plots and two variables produce 3D plots. Everything beyond that is quite tricky. Even 3D on a 2D paper or monitor is already challenging. This example here shows an artificial dataset with two x variables on which a Random Forest was trained.



```{r dpd-cervical, fig.cap = 'Partial dependence plot of cancer probability and the age of the women. On average, the cancer probability is low before 45, spikes between age 45 and 55 and plateaus after that.'}
source(sprintf('%s/create-cervical-cancer-data.R', src_dir))

mod = mlr::train(mlr::makeLearner(cl = 'classif.randomForest', id = 'cervical-rf', predict.type = 'prob'), cervical_task)
pd1 = mlr::generatePartialDependenceData(mod, cervical_task, 'Age')
mlr::plotPartialDependence(pd1) + my_theme()+ scale_x_continuous(limits = c(0, NA))


#pd2 = mlr::generatePartialDependenceData(mod, cervical_task, 'Smokes..years.')
#mlr::plotPartialDependence(pd2) + my_theme() + scale_x_continuous(limits = c(0, NA))
```

\Sexpr{set_parent('Makepaper.Rnw')}
\section{The effect of a feature}
\label{sec:feature-effects}

\subsection{Average Marginal Effects}

Average marginal effects were originally developed for generalized linear models with special emphasis on the logistic regression model (CITATION).
Logistic regression models try to model the probability of a binary response using a set of features.
The main idea in generalized linear models is to model the expected value of the target feature using a linear combination (also called linear predictor) of the features by weighting them with the coefficients $\beta$ and a so-called response function $h$, i.e.
$$E(Y|X) = h(x^\top \beta).$$
In any case the linear predictor takes values between minus and plus infinity, i.e., for a logistic regression model those values need to be translated into probabilities which is typically done by appling a sigmoid function (e.g., using a logistic function for $h$) to the linear predictor to make sure that the values are between 0 and 1.
Although the logistic regression model is claimed to be an interpretable model (CITATION), the interpretations of the estimated coefficients (feature effects) are not directly related to the probability of the binary response due to the non-linear response function $h$.
Instead, the interpretations are done w.r.t. log-odds, e.g. if $x$ increases by one, the log-odds ($log \frac{P(Y = 1)}{P(Y = 0)}$) will increase by $\beta$.
Researchers of applied sciences, however, are often interested in making direct interpretations of how changes in $x$ affect the probability $P(Y=1)$. For this purpose one can use average marginal effects.



### Feature importance

#### Permutation feature importance

The permutation importance measurement was orginially introduced for RandomForests [@breiman2001random]. It is calculated on the out-of-bag instances and works by
estimating the original model performance and checking what happens with the model performance when you permute each feature. A big loss in performance
means a big feature importance. The idea of permutation of features is per se model-agnostic, only the OOB-scheme is specific for ensemble methods.
In can be used for any model when a hold-out dataset is used, instead of OOB samples. Of course you could also use the training data, but you risk
getting variable importance measures that overfit your training data, since the model was already trained on it.

Algorithm [@breiman2001random]:

Input: Trained model $\hat{f}$, hold-out dataset $D$, number of permutations $n_{perm}$

1. Estimate performance $Perf$ of $\hat{f}$ with $D$ (e.g. MSE for regression or accuracy for classification)
2. For each feature $j \in 1, \ldots, J$ do:
  - For $i \in 1,\ldots , n_{perm}$
    - Get $D_{j_{perm}}$ by permuting feature $X_j$ in data $D$. This breaks the association between $X_j$ and $Y$.
    - Estimate performance $Perf_{i,j_{perm}}$ of $\hat{f}$ with $D_{j_{perm}}$
    - Calculate permutation variable importance $VI_i(X_j) = Perf_{i,j_{perm}} - Perf$
  - Calculate mean variable importance: $VI(X_j) = \frac{1}{n_{perm}}\sum_{i=1}^{n_{perm}} VI_i(X_j)$
  - Optional: Calculate p-value $p = \frac{I(Perf_{j_{perm}} > Perf)}{n_{perm}}$
3. Sort variables by descending $VI$.

The feature with the highest $VI$ measure is the most important globally in your model.
With the p-value you can additionally check if a feature importance is significantly different from 0.
You might want to adjust your $\alpha$ confidence level for multiple testing.


You can also find the algorithm in more detail in  [@Strobl2008].
The authors additionally suggest a conditional feature importance measurement, which is not (yet) covered in this book.
The standard permutation feature importance only works with marginal feature improvements and cannot distinguish between correlation and spurious correlation.
[@Strobl2008] suggest to condition the importance measure also on other features, which makes it possible to account for correlation among the features.

#### Model dependent feature importance

Some model classes already come with built in feature importance measurements.
A few examples:
- RandomForest: Permutation based feature importance
- CART and boosting: mean decrease in Gini impurity index
- Linear Model: (absolute value of) t-test statistic for each feature


## Local: Explain a single decisions
Instead of trying to explain predictions or mechanisms of the model globally, local methods focus on explaining single predictions.

### Individual Conditional Expectation (ICE) plot

The partial dependence plot for visualizing the averaged effect of a feature is a global method, because it does not focus on the partial dependence of a specific instance, but on an average over all.
The equivalent to a PDP for local expectations is called individiual conditional expectation (ICE) plot [@goldstein2015peeking].
An ICE plot visualizes the dependence of each instance's predicted response on a feature.
They are event simpler than PDPs, since no averaging is needed.
Instead of drawing one line for a feature, each instance in the dataset gets it's own line.
The values for a line can be computed easily, by leaving all other features the same, but creating variants of the instance of interest and letting the black box make the predictions or classifications.
The result is a set of points for a varying feature, for one specific instance.
The lines for the instances can look quite differently (if the black box allows interactions between features), because the course of the line depends on the specific values of each instance.
For drawing each line, the $x_C$ are fixed for this one instance, and the $x_C$ is varied on a grid and the $\hat(y)$ calculated with $\hat(f)$.

So, what do you gain by looking at individual expectations, instead of partial dependencies?
This averaged display can obfuscate a heterogeneous relationship that comes from interactions.
ICE plots \citep{goldstein2015peeking} solve this problem by plotting the relationship between feature and predicted response for individual observations.
It can be seen as an extension to the standard PDP.
PDP can show you how the average relationship between feature $x_S$ and $\hat(y)$ looks like.
This works only well in cases where the interactions between $x_S$ and the remaining $x_C$ are weak.
If there are interactions, a ICE plot will give a lot more insight.

A more formal definition:
In ICE plots, for each observation in $\{(x_{S_i}, x_{C_i})\}_{i=1}^N$ the curve $\hat{f}_S^{(i)}$ is plotted against $x_{S_i}$, while $x_{C_i}$ is kept fixed.
#### Example
Let's go back to the dataset about risk factors for cervical cancers and see how each instance's prediction is associated with the feature 'Age'.
In the partial dependence plot chapter {@pdp} we have seen that the probability increases around the age of 50, but does this hold true for each woman in the dataset?
The ICE plot reveals that the most women's predicted probability follows the average pattern of increase at 50, but there are a few exceptions:
In a few cases, the prediction of cancer probability does not change much with the age, and that is for women that have a high predicted probability.
```{r ice-cervical, fig.cap="Individual conditional expectation plot of cervical cancer probability and age. Most women with a low cancer probability in younger years see an increase in predicted cancer probability, given all other feature values. Interestingly for a few women that have a high estimated cancer probability > 0.4, the estimated probability does not change much with higher age. "}
set.seed(42)
cervical_subset_index = sample(1:nrow(cervical), size = 130)
cervical_subset = cervical[cervical_subset_index, ]
pd1 = mlr::generatePartialDependenceData(mod, cervical_subset, 'Age', individual = TRUE)
mlr::plotPartialDependence(pd1) + my_theme()
```


#### Centered ICE plot
There is one issue with the ICE plot:
It can be hard to see if the individual conditionl expectations curve differ between individuals when they start at different $\hat{f^{(i)}}.
An easy fix is to center the curves at a certain point in $x_S$ and only display the difference in predited response.
The resulting plot is called centered ICE plot (c-ICE).
It is a kind of anchoring, and doing this at the lower end of $x_S$ is a good choice.
The new curves are defined as: $$\hat{f}_{cent}^{(i)} = \hat{f}_i - 1\hat{f}(x^{\text{*}}, x_{C_i}), $$
where $1$ is a vector of 1's with the appropriate dimensions (usually one- or two-dimensional), and $\hat{f}$ the fitted model.

#### Example
Taking Figure \@ref{fig:ice-cervical} and centering the lines at the youngest observed age yields Figure \@ref{fig:ice-cervical-centered}.
It is easier to see now, how the relative change of the curves from the youngest age is.
This can be useful when we are not interested in seing the absolute change of a predicted value, but rather the difference in prediction compared to a fixed point of the feature range. 
```{r ice-cervical-centered, fig.cap = "Centered ICE plot for predicted cervical cancer risk probability and age. Compared to age 18, the most predictions for most instances stay the same and see an increase up to +20%. A few cases show the opposite behaviour: The predicted probability decreases with increasing age. "}
pd1 = mlr::generatePartialDependenceData(mod, cervical_subset, 'Age', individual = TRUE, center = list(Age=18))
mlr::plotPartialDependence(pd1) + my_theme()
# pd1 = mlr::generatePartialDependenceData(mod, cervical_subset, 'Smokes..years.', individual = TRUE, center = list(Smokes..years.=0), grid=20)
# mlr::plotPartialDependence(pd1) + my_theme()

```

#### Derivative ICE plot
Another way to make it it visually easier to spot heterogenity is to look at the individual derivatives of $\hat(f)$ with respect to $\x_S$ instead of the predicted response $\hat(f)$.
The resulting plot is called derivative ICE plot (d-ICE).
The derivatives of a function (or curve) tells you in which direction changes occur and if any occur at all.
With the derivative ICE plot it is easy to spot value ranges in a feature where the black box's predicted value changes for (at least some) instances. 
If there is no interaction between $x_S$ and $x_C$, then $\hat{f}$ can be expressed as:
$$\hat{f}(x) = \hat(f)(x_S, x_C) = g(x_S) + h(x_C), \text{ so that } \frac{\delta\hat{f}(x)}{\delta x_S} = g'(x_S)$$
Without interactions, the individual partial derivatives should be the same for all observations.
If they differ, it's because of interactions and it will become visible in the d-ICE plot.
In addition to displaying the individual curves for derivative $\hat{f}$, showing the standard deviation of derivative $\hat{f}$ helps to highlight regions in $x_S$ with heterogeneity in the estimated derivatives.

[@goldstein2015peeking]

#### Example
As we have seen, the most changes in estimated cancer probability happen around age 45.
This is confirmed by the derivative ICE plot in Figure \@ref{fig:ice-cervical-derivative}.
```{r ice-cervical-derivative, fig.cap = "Derivatice ICE plot of predicted cancer probability and age. Between age 14 and the early forties, a few instance see changes in prediction both upwards and downards, but the majorities derivatives are near zero. Between age 45 and 50, most women's prediction curves have a positive derivative, indicating an increase in predicted cancer probability."}
# takes to much time ... maybe precompute and save results.
pd1 = mlr::generatePartialDependenceData(mod, cervical_subset, 'Age', individual = TRUE, derivative = TRUE, method = 'simple')
mlr::plotPartialDependence(pd1) + my_theme()
```

### Local surrogate models (LIME)
Local interpretable model-agnostic explanations (LIME) is a method for fitting local, interpretable models that can explain single predictions or classifications of any black-box machine learning model. LIME explanations are local surrogate models. Instead of trying to fit a global surrogate model, LIME focuses on a prediction done by a black-box algorithm and explains it's outcome.


The idea is quite simple, really. First of all, forget about the training data and imagine you only have the black box model where you can input data points and get the models outcome. You can probe the box as often as you want. Your goal is to understand why the machine learning model gave the outcome it produced. LIME tests out what happens to the model's predictions when you put some variations of your data point of interest into the machine learning model. This basically generates a new dataset consisting of the perturbed samples and the associated model's outcome. On this dataset LIME then trains a simple model weighted by the proximity of the sampled instances to the instance of interest. The simple mode  can basically be any from Section [#simple], for example LASSO or a short tree. The learned model should be a good approximation of the machine learning model locally, but it does not have to be so globally. This kind of accuracy is also called local fidelity.

The recipe:

- Choose your instance of interest for which you want to have an explanation of it's black box outcome
- Make some variations of the instances and check what the black box predicts in the neighbourhood of the instance of interest.
- Fit a local, interpretable model on the dataset with the variations
- Explain prediction by interpreting the local simple model.

In the current implementation, only LASSO can be chosen as a simple model. Upfront you have to choose K, the number of features that you want to have in your simple model. The lower the K, the simpler the model is to understand, higher K potentially creates models with higher fidelity. There are different methods for how to fit models with exactly K features. The most natural with LASSO is the lasso path. Starting from a model with a very high regularisation parameter $\lambda$ yields a model with only the intercept. By refitting the LASSO models with slowly decreasing $\lambda$ one after each other the features are getting weight estimates different from zero. When K features are in the model, you reached the desired number of features. Other strategies are forward or backward selection of features. This means you either start with the full model (=containing all features) or with a model with only the intercept and then testing which feature would create the biggest improvement when added or removed, until a model with K features are reached. Other simple models like decision trees are currently not implemented.

As always, the devil is in the details. In a high-dimensional space, defining a neighbourhood is not trivial. Distance measures are quite arbitrary and distances in different dimensions (aka features) might not be comparable at all. How big should the neighbourhood be you look into? If it is too small, then there might be no difference in the predictions of the machine learning model at all.
The other question is: How do you get the variations of the data? This differs depending on the type of data, which can be either text, an image or tabular data. For text and image the solution is turning off and on single words or superpixels. In the case of tabular data LIME creates new samples by pertubing each feature individually, by drawing from a normal distribution with mean and standard deviation from the feature.


#### LIME for tabular data
Tabular data means any data that comes in tables, where each row represents an instance and each column a feature.
Sampling is not done around the point, but from the training data's mass center. Has it's problems. But it increases the likelihood that the outcome for some of the sampled points predictions differ from the data point of interest and that LIME can learn at least some explanation.

Figure \@ref(fig:lime-fitting) explains how the sampling and local model fitting works.


```{r lime-fitting, fig.cap='How LIME sampling works: A) The trainig data has two classes. The most data points have class 0, and the ones with class 1 are grouped in an upside-down V-shape. The plot displays the decision boundaries learned by a machine learning model. In this case it was a Random Forest, but it does not matter, because LIME is model-agnostic and we only care about the decision boundaries. B) The yellow is the instance of interest, for which an explanation is desired. The black dots are data sampled from a normal distribution around the means of the features in the training sample. This has only to be done once and can be reused for other explanations. C) Introducing locality by giving points near the instance of interest a higher weights. D) The colors and signs of the grid display the classifications of the locally learned model form the weighted samples. The white line marks the decision boundary (P(class) = 0.5) at which the classification changes.', fig.height=9, fig.width=9}
source(sprintf('%s/lime.R', src_dir))

colors = c('#132B43', '#56B1F7')
# Data with some noise
p_data = ggplot(lime_training_df) +
  geom_point(aes(x=x1,y=x2,fill=y_noisy, color=y_noisy), alpha =0.3, shape=21) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  my_theme(legend.position = 'none')

# The decision boundaries of the learned black box classifier
p_boundaries = ggplot(grid_df) +
  geom_raster(aes(x=x1,y=x2,fill=predicted), alpha = 0.3, interpolate=TRUE) +
  my_theme(legend.position='none') +
  ggtitle('A')


# Drawing some samples
p_samples = p_boundaries +
  geom_point(data = df_sample, aes(x=x1, y=x2)) +
  scale_x_continuous(limits = c(-2, 2)) +
  scale_y_continuous(limits = c(-2, 1))
# The point to be explained
p_explain = p_samples +
  geom_point(data = df_explain, aes(x=x1,y=x2), fill = 'yellow', shape = 21, size=4) +
  ggtitle('B')



p_weighted = p_boundaries +
  geom_point(data = df_sample, aes(x=x1, y=x2, size=weights)) +
  scale_x_continuous(limits = c(-2, 2)) +
  scale_y_continuous(limits = c(-2, 1)) +
  geom_point(data = df_explain, aes(x=x1,y=x2), fill = 'yellow', shape = 21, size=4) +
  ggtitle('C')

p_boundaries_lime = ggplot(grid_df)  +
  geom_raster(aes(x=x1,y=x2,fill=predicted), alpha = 0.3, interpolate=TRUE) +
  geom_point(aes(x=x1, y=x2, color=explained), size = 2, data = grid_df_small[grid_df_small$explained_class==1,], shape=3) +
  geom_point(aes(x=x1, y=x2, color=explained), size = 2, data = grid_df_small[grid_df_small$explained_class==0,], shape=95) +
  geom_point(data = df_explain, aes(x=x1,y=x2), fill = 'yellow', shape = 21, size=4) +
  geom_line(aes(x=x1, y=x2), data =logistic_boundary_df, color = 'white') +
  my_theme(legend.position='none') + ggtitle('D')


gridExtra::grid.arrange(p_boundaries, p_explain, p_weighted, p_boundaries_lime, ncol=2)

```




##### Example
Let's look at a concrete example. We go back to the bike rental and turn the prediction problem into a classification: After accounting for the trend that the bike rental get's more popular over time we want to know on a given day if the number of rented bikes will be above or below the trend line. You can also interpret 'above' as being above the mean bike counts, but adjusted for the trend. 

```{r lime-tabular-example-train-black-box, cache = TRUE}
ntree = 100

bike.train.resid = factor(resid(lm(cnt ~ days_since_2010, data = bike.train)) > 0, levels = c(FALSE, TRUE), labels = c('below', 'above'))
bike.train.x = bike.train[names(bike.train) != 'cnt']

model <- caret::train(bike.train.x,
               bike.train.resid, 
               method = 'rf', ntree=ntree, maximise = FALSE)


instance_indices = c(295, 285)

probs = predict(model, newdata = bike.train[instance_indices, ], type='prob')


# Accuracy on test set
# test_predict = predict(model, test_speed_dating_classification[features_of_interest])
# caret::confusionMatrix(test_predict, test_speed_dating_classification[,target_var_class])

```
First we train a Random Forest on the classification task. Given seasonal and wheather information, on which day will the number of bike rentals be above the trend-free average? The Random Forest has `r ntree` trees.
```{r lime-tabular-example-explain-plot-1, fig.cap=sprintf('Explanations for two instances. The first instance got a probability of 0.13 for a match by the Random Forest. The correlation of interests between the participant and the partner is low which reduces the probability of a match for this participant. Also the importance for race of the partner is higher than 6, which lowers the probability of a match.  In the second case, the probability for a match is quite high with 0.46. The reasons are the high correlation (>0.43) and that the partner is younger then 25.', instance_indices[1], instance_indices[2])}

# Create explanation function
set.seed(44)
explain <- lime::lime(bike.train.x, model, bin_continuous=FALSE)


n_features_lime = 3
n_labels = 2


# Explain new observation
explanation <- explain(bike.train.x[instance_indices, ],
                       n_labels=n_labels, n_features = n_features_lime, n_permutations = 3000)


explanation = filter(explanation, label == 'above')
lime::plot_features(explanation, ncol=1)
```
The continuous features are categorised into bins by quantiles for the explanation models. The explanations are set to contain `r n_features_lime` features.  Figure  \@ref{fig:lime-tabular-example-explain-plot-1} shows the results of the sparse local linear model that was fitted for two instances with different predicted classes. It becomes clear from the figure, that it is easier to interpret categorical features than continuous features. Figure  \@ref{fig:lime-tabular-example-explain-plot-2} shows a variant where the continuous features are turned into categorial features by putting them into bins along the quantiles. 



```{r lime-tabular-example-explain-plot-2, fig.cap=sprintf('Explanations for two instances. This time continuous features were turned into categorial features by binning them.')}
# An can be visualised directly
# Create explanation function
set.seed(44)
explain <- lime::lime(bike.train.x, model, bin_continuous=TRUE)

# Explain new observation
explanation <- explain(bike.train.x[instance_indices, ], n_labels=n_labels, n_features = n_features_lime, n_permutations = 3000)


explanation = filter(explanation, label == 'above')
lime::plot_features(explanation, ncol=1)
```

#### LIME for images
For images the sampling procedure works differently. Instead of sampling single pixels, LIME create variations of the image by turning off superpixel.

#### LIME for text
LIME for text works a bit differently than for tabular data. Variation of the point to be explained are created differently: Starting from the original text, new texts are created by randomly removing words from it.



##### Example
In this example we classify spam vs. ham of YouTube comments. The dataset is described in [#TubeSpam].

The black box model is a decision tree on the document word matrix. Each comment is one document (= one row) and each column is a the number of occurences of a specific word. A decision tree was trained on this data. As discussed in Section [#simple], decision trees are easy to understand, but in this case the tree is very deep. Also in the place of this tree there could have been a recurrent neural network or a support vector machine that was trained on the embeddings from word2vec. The machine learning model was trained on 80% of the approximately 2000 comments. From the remaining comments two were selected for showing the explanations.

```{r load-text-classification-lime}
source(sprintf('%s/create_text_classifier.R', src_dir))
```
Let's look at tow comments of this dataset and the corresponding classes:


```{r example-comments}
example_indices = c(1,28)

DT::datatable(test[example_indices, c('CONTENT', 'CLASS')], options = list(DT::JS('{"scrollX": true}')))
texts = test$CONTENT[example_indices]
```


In the next step we create some variations of the datasets, which are used in a local model. For
example some variations of one of the comments.
```{r lime-text-variations, results='asis'}
tokenized = tokenize(texts[2])
variations = create_variations(texts[2], predict_fun, prob=0.8, n_variations = 5, class='spam')
colnames(variations) = c(tokenized, 'prob', 'weight')

DT::datatable(variations, options = list(DT::JS('{"scrollX": true}')))

example_sentence = paste(colnames(variations)[variations[2, ] == 1], collapse = ' ')
```
Each column corresponds to one word in the sentence. Each row is a variation. 1 indicates that
the word is part of this variation. 0 indicates that the word was removed.  The corresponding sentence for the first variation is "```r example_sentence```".



```{r lime-text-explanations}
explanations  = data.table::rbindlist(lapply(seq_along(texts), function(i) {
 explain_text(texts[i], predict_fun, class='spam', case=i, prob = 0.5)
 })
)
lime::plot_features(data.frame(explanations), ncol=1)
```
##### Problems with LIME

- LIME does not work if the classification is very unbalanced (one class is very common) and the black box only predicts one class
- Defining the neighbourhood is tricky. 
- 


## Model-agnostic: Why not use them on the data itself?
Well, nothing stops you from using it on the data itself.

## Explanation types

UNDER CONSTRUCTION

### Structured output
Regression tables, decision tree plots.
There are overlaps with visual explanations (Section [#viz-explanation]).

### Visualization {#viz-explanation}
Easy to understand Visualizations. First choice in image classification tasks.

### Natural language (narratives)
That's what humans usually do.

### Examples and prototypes

# References
