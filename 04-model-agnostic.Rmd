# Model-agnostic explanations
Separating the explanations from the machine learning model (= model-agnostic explanations) gives some benefits.
The big advantage of model-agnostic vs model-specific explanation algorithm is the flexibility. When the explanation system is independently applicable even when the underlying model is switched, it frees the practitioner to use different machine learning models without restrictions. It is also more efficient to build interfaces on top of model-agnostic systems, because  this has to be done only once and not for each model-specific explanation system. Usually not one but many types of machine learning models are tested in development time and if you want to compare the models in terms of interpretability this is easier with model-agnostic explanations because the system is the same for both models that are being compared [@Ribeiro2016b].


The alternatives are either using only interpretable models as introduced in Chapter 2, which has the big disadvantage to usually loose accuracy compared to other approaches. The other alternative is to use more flexible model classes that come with built in explanations. The drawback here is that it ties you to this one algorithm and it will be hard to switch to something else.

Desirable aspects of a model-agnostic explanation system [@Ribeiro2016b]:
- Model flexibility: Not being tied to an underlying particular machine learning model. The method should work for random forests as well as convolutional neural networks
- Explanation flexibility: Not being tied to a certain form of explanation. In some cases it might be useful to have a linear formula in other cases some decision rules
- Representation flexibility: The explanation system should not have to use the same feature representation as the model that is being explained. So when a text classifier uses abstract word embedding vectors, it might be preferable to use the presence of single words for the explanation.

## Justification narrative structure for classification
See also Paper.
Idea: Per feature only use effect and importance. This method is per se model-agnostic, but you need a method for computing effect and importance, which is different for each model class.  

The **effect** of a feature is how much the feature contributed towards (or against) a classification to a certain category for an instance. In case of a linear model it is simply the j-th weight times the feature value for observation i: $\beta_{j} x_{ij}$. For classification it is class specific (class k):  $eff_{ji} = \beta_{kj} x_{ij}$

The **importance** of a feature is defined as the overall strength of a feature within the model. So it is the expected effect of feature j for a particular class. The formula for importance of feature j towards class k is: $imp_{ji} = \beta_{ji} \frac{\sum_{x \in X^j} x_{i}}{|X^j|}$, where $X^j$ is the set of all instances which have class j. Note that the polarity of a feature ($=sign(\beta_{j})$) might be different from the importance, for example when the weight is negative and also the associated feature is negative for most cases in class k.


**Narrative role** of a feature for the classification of an instance depends on effect and importance.

Step 1: Decide what magnitude of importance can be seen as high and separate into low and high. This can be done by applying a fixed threshold or keeping a fixed number of features or some kind of 'ellbow criterium'.  The absolute magnitude has to be considered because importance comes both from features that count towards and against a class.   


| Importance \ Effect | High positive | Low |  High negative |
|:--------------|:--------------------|:------------------------|:----------------------------|
|High positive |Normal evidence     |Missing evidence        |Contrarian counter-evidence |
|Low           |Exceptional evidence|Negligible              |Exceptional counter-evidence|
|High negative |Contrarian evidence |Missing counter-evidence|Normal counter-evidence     |


Contrarian evidene and contrarian counter-evidence is only possible with negative features.


You should mean center the features, otherwise the importance and the effects will very much look the same (unless the means between the classes vary greatly). The importance and effects are dependent on the scale of your features, but it should not matter whether the a feature is measured in meters or in inch (you should use meter of course) or if it is visits per hour or per minute.

Textual template: TODO

### Example justification narratives with the vehicle data set
The Vehicle dastaset contains the silhoutte descriptions of four types of vehicles. Different features are extracted from the silhouettes from different angles. The four classes are bus, opel, saab and van, but for the purpose of illustration we only focus on the task classifying bus vs. not bus given the silhoutte features. The dataset contains 846 cars with 18 silhoutte features.
```{r echo=FALSE,  message=FALSE}
library('ggplot2')
library('dplyr')
library('tidyr')

## Create data set for classification and train classifier
data('Vehicle')
X = Vehicle
normalize = function(x){(x - mean(x)) / sd(x)}
X = X %>% mutate_each(funs(normalize), -Class)
## Task: Bus vs. not bus
X$Class = ifelse(X$Class == 'bus', 1, 0)
classifier = glm(Class ~ ., data = X,  family = binomial())
y = as.factor(X$Class)
X$Class = NULL

feature_descriptions = c('Compactness', 'Circularity', 'Distance Circularity', 	'Radius ratio',
'pr.axis aspect ratio', 'max.length aspect ratio', 'scatter ratio', 'elongatedness', 'pr.axis rectangularity', 'max.length rectangularity', 'scaled variance along major axis', 'scaled variance along minor axis', 'scaled radius of gyration', 'skewness about major axis', 'skewness about minor axis', 	'kurtosis about minor axis', '	kurtosis about major axis', 	'hollows ratio')

#' Calculate importance of features
#'
#' See also  Justification Narratives for Individual Classifications
#'
#' @param X The data.frame with the features. Have to be named the same as in model
#' @param mod The logistic regression model of class 'glm'
#' @param y The vector with the class variable
#' @param class The name of the class for which the importance will be computed
importance_binom = function(X, mod, y, class){
  feature_names = names(X)
  coefs = coefficients(mod)[feature_names]
  X_class = X[y == class, ]
  coefs * (colSums(X_class[feature_names])/nrow(X_class))
}

imps = importance_binom(X, classifier,  y, 1)
imps = data.frame(imps)
names(imps) = 'importance'
imps$coef = rownames(imps)

imps_print = imps
imps_print$feature_descriptions = feature_descriptions
imps_print = rename(imps_print, Feature=coef, Description=feature_descriptions, Importance=importance)
kable(imps_print[c('Feature', 'Description', 'Importance')], caption = 'Feature importances for the class "bus"', row.names = FALSE)
## ggplot(imps) + geom_segment(aes(x = coef, xend = coef, y = 0, yend = importance)) + ggtitle('Importance per variable')

important_features = imps[abs(imps$importance) > 1,]

#' Calculate effect of features for single instance
#'
#' See also  Justification Narratives for Individual Classifications
#'
#' @param X The data.frame with the features. Have to be named the same as in model
#' @param mod The logistic regression model of class 'glm'
#' @param i The index of the observation for which to compute the effect
effects_binom = function(X, i, mod){
  instance = X[i,]
  feature_names = names(X)
  coefs = mod$coefficients[feature_names]
  coefs * instance
}

## Compute effects for all instances
effects = lapply(1:nrow(X),  function(x){effects_binom(X, x, classifier)})
effects_r = data.table::rbindlist(effects)

## Choose exemplary instances
instance_indices = c(1, 5)

instance_index1 = 1
instance_index2 = 5

pdata = gather(effects_r[instance_index1,])
pdata$type = 'effect'

pdata_imps = imps %>% rename(key=coef, value=importance) %>%
  mutate(type = 'importance')

ggplot() +
  geom_segment(aes(y = key, yend = key, x = 0, xend = value), data = pdata_imps) +
  geom_point(aes(y = key, x = value), data = pdata) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + guides(lty = guide_legend())


ggplot() +  geom_bar(aes(x = key,  y= value, fill=type, lty=type), data = rbind(pdata_imps, pdata), stat='identity', alpha = 0.3, position= position_identity()) + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```



## Global: Explain behaviour of system with data
### Global surrogate models
A surrogate model is a simple, explainable model that explains a complex machine learning model.
### Partial dependency plots
Partial dependency plots show the relationship between the target and one or more features by averaging out all the other features. A dependency plot can show if the relationship between target and feature is linear, monotonic or something else.
Are only partially global: It is global because it takes into account all instances, but it is local in the feature, because partial dependency plots only examine one variable, as the name suggests.
### Individual conditional expectation (ICE) plot
### Variable importance
### LOCO (Leave-One-Covariate-Out)
### Interactions
### Residual analysis
A residual value is the difference of the models prediction and the actual value.
### Confusion matrix
### Sensitivity analysis of predictions
Testing the stability of the model predictions/classifications using simulated data. It can help to trust the model to not be instable in certain settings.



## Local: Explain single decisions
### Local surrogate models (LIME)
LIME is also a surrogate model, but it is a local one.
### Model explanation system
From paper: [@Turner2015]
Classifier $f$, that takes feature vector $ x \in X = \R^D$. Let's call the explanation 'Ex'. Applied only on binary classification.
Properties:
- Eglibility: An explanation is called eligible if $P(f(x) = 1 | Ex(x) = 1) \geq P(f(x) = 1)$
- Generality (or recall): Probability that the explanation is true $P(Ex(x) = 1 | f(x) = 1)$
- Accuracy (or precision) of explanation: Probability the classifier is correct, given the explanation is correct $P(f(x) = 1 | Ex(x) = 1)$
- Validity of explanation: An explanation is valid at x if it is eliglbe and true at x $(E(x)=1)$
## Maximum activation analysis
### LOCO (Leave-One-Covariate-Out) also local
### Max points lost
Compare individual prediction with 'ideal' case (maximum probability) in terms of points lost per feature. Only works with monotonicity. And ideal case candidate maxes out each feature regarding highest probability of interest. The feature in which the instance is farthest away from the ideal case is the most negative point, why it should not be in class of interest. Feature with point closest to ideal is the least negative reason.
