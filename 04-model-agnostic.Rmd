# Model-agnostic explanations

## Justification narrative structure for classification
See also Paper.
Idea: Per feature only use effect and importance. This method is per se model-agnostic, but you need a method for computing effect and importance, which is different for each model class.  

The **effect** of a feature is how much the feature contributed towards (or against) a classification to a certain category for an instance. In case of a linear model it is simply the j-th weight times the feature value for observation i: $\beta_{j} x_{ij}$. For classification it is class specific (class k):  $eff_{ji} = \beta_{kj} x_{ij}$

The **importance** of a feature is defined as the overall strength of a feature within the model. So it is the expected effect of feature j for a particular class. The formula for importance of feature j towards class k is: $imp_{ji} = \beta_{ji} \frac{\sum_{x \in X^j} x_{i}}{|X^j|}$, where $X^j$ is the set of all instances which have class j. Note that the polarity of a feature ($=sign(\beta_{j})$) might be different from the importance, for example when the weight is negative and also the associated feature is negative for most cases in class k.


**Narrative role** of a feature for the classification of an instance depends on effect and importance.

Step 1: Decide what magnitude of importance can be seen as high and separate into low and high. This can be done by applying a fixed threshold or keeping a fixed number of features or some kind of 'ellbow criterium'.  The absolute magnitude has to be considered because importance comes both from features that count towards and against a class.   


| Importance \ Effect | High positive | Low |  High negative |
|:--------------|:--------------------|:------------------------|:----------------------------|
|High positive |Normal evidence     |Missing evidence        |Contrarian counter-evidence |
|Low           |Exceptional evidence|Negligible              |Exceptional counter-evidence|
|High negative |Contrarian evidence |Missing counter-evidence|Normal counter-evidence     |


Contrarian evidene and contrarian counter-evidence is only possible with negative features.


You should mean center the features, otherwise the importance and the effects will very much look the same (unless the means between the classes vary greatly). The importance and effects are dependent on the scale of your features, but it should not matter whether the a feature is measured in meters or in inch (you should use meter of course) or if it is visits per hour or per minute. 

Textual template: TODO

### Example justification narratives with the vehicle data set
The Vehicle dastaset contains the silhoutte descriptions of four types of vehicles. Different features are extracted from the silhouettes from different angles. The four classes are bus, opel, saab and van, but for the purpose of illustration we only focus on the task classifying bus vs. not bus given the silhoutte features. The dataset contains 846 cars with 18 silhoutte features. 
```{r echo=FALSE,  message=FALSE}
library('ggplot2')
library('dplyr')
library('tidyr')

## Create data set for classification and train classifier
data('Vehicle')
X = Vehicle
normalize = function(x){(x - mean(x)) / sd(x)}
X = X %>% mutate_each(funs(normalize), -Class)
## Task: Bus vs. not bus
X$Class = ifelse(X$Class == 'bus', 1, 0)
classifier = glm(Class ~ ., data = X,  family = binomial())
y = as.factor(X$Class)
X$Class = NULL

feature_descriptions = c('Compactness', 'Circularity', 'Distance Circularity', 	'Radius ratio',
'pr.axis aspect ratio', 'max.length aspect ratio', 'scatter ratio', 'elongatedness', 'pr.axis rectangularity', 'max.length rectangularity', 'scaled variance along major axis', 'scaled variance along minor axis', 'scaled radius of gyration', 'skewness about major axis', 'skewness about minor axis', 	'kurtosis about minor axis', '	kurtosis about major axis', 	'hollows ratio')

#' Calculate importance of features
#'
#' See also  Justification Narratives for Individual Classifications 
#' 
#' @param X The data.frame with the features. Have to be named the same as in model
#' @param mod The logistic regression model of class 'glm'
#' @param y The vector with the class variable
#' @param class The name of the class for which the importance will be computed
importance_binom = function(X, mod, y, class){
  feature_names = names(X)
  coefs = coefficients(mod)[feature_names]
  X_class = X[y == class, ]
  coefs * (colSums(X_class[feature_names])/nrow(X_class))
}

imps = importance_binom(X, classifier,  y, 1)
imps = data.frame(imps)
names(imps) = 'importance'
imps$coef = rownames(imps)

imps_print = imps
imps_print$feature_descriptions = feature_descriptions
imps_print = rename(imps_print, Feature=coef, Description=feature_descriptions, Importance=importance)
kable(imps_print[c('Feature', 'Description', 'Importance')], caption = 'Feature importances for the class "bus"', row.names = FALSE)
## ggplot(imps) + geom_segment(aes(x = coef, xend = coef, y = 0, yend = importance)) + ggtitle('Importance per variable')

important_features = imps[abs(imps$importance) > 1,]

#' Calculate effect of features for single instance
#'
#' See also  Justification Narratives for Individual Classifications 
#' 
#' @param X The data.frame with the features. Have to be named the same as in model
#' @param mod The logistic regression model of class 'glm'
#' @param i The index of the observation for which to compute the effect
effects_binom = function(X, i, mod){
  instance = X[i,]
  feature_names = names(X)
  coefs = mod$coefficients[feature_names]
  coefs * instance
}

## Compute effects for all instances
effects = lapply(1:nrow(X),  function(x){effects_binom(X, x, classifier)})
effects_r = data.table::rbindlist(effects)

## Choose exemplary instances
instance_indices = c(1, 5)

instance_index1 = 1
instance_index2 = 5

pdata = gather(effects_r[instance_index1,])
pdata$type = 'effect'

pdata_imps = imps %>% rename(key=coef, value=importance) %>%
  mutate(type = 'importance')

ggplot() + 
  geom_segment(aes(y = key, yend = key, x = 0, xend = value), data = pdata_imps) + 
  geom_point(aes(y = key, x = value), data = pdata) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + guides(lty = guide_legend())


ggplot() +  geom_bar(aes(x = key,  y= value, fill=type, lty=type), data = rbind(pdata_imps, pdata), stat='identity', alpha = 0.3, position= position_identity()) + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```



## Global: Explain behaviour of system with data
### Global surrogate models
A surrogate model is a simple, explainable model that explains a complex machine learning model.
### Partial dependency plots
Partial dependency plots show the relationship between the target and one or more features by averaging out all the other features. A dependency plot can show if the relationship between target and feature is linear, monotonic or something else.
Are only partially global: It is global because it takes into account all instances, but it is local in the feature, because partial dependency plots only examine one variable, as the name suggests.
### Individual conditional expectation (ICE) plot
### Variable importance
### LOCO (Leave-One-Covariate-Out)
### Interactions
### Residual analysis
A residual value is the difference of the models prediction and the actual value.
### Confusion matrix
### Sensitivity analysis of predictions
Testing the stability of the model predictions/classifications using simulated data. It can help to trust the model to not be instable in certain settings.



## Local: Explain single decisions
### Local surrogate models (LIME)
LIME is also a surrogate model, but it is a local one.
## Maximum activation analysis
### LOCO (Leave-One-Covariate-Out) also local
### Max points lost
Compare individual prediction with 'ideal' case (maximum probability) in terms of points lost per feature. Only works with monotonicity. And ideal case candidate maxes out each feature regarding highest probability of interest. The feature in which the instance is farthest away from the ideal case is the most negative point, why it should not be in class of interest. Feature with point closest to ideal is the least negative reason.
