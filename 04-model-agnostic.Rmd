# Model-agnostic explanations
Separating the explanations from the machine learning model (= model-agnostic explanations) gives some benefits.
The big advantage of model-agnostic vs model-specific explanation algorithm is the flexibility. When the explanation system is independently applicable even when the underlying model is switched, it frees the practitioner to use different machine learning models without restrictions. It is also more efficient to build interfaces on top of model-agnostic systems, because  this has to be done only once and not for each model-specific explanation system. Usually not one but many types of machine learning models are tested in development time and if you want to compare the models in terms of interpretability this is easier with model-agnostic explanations because the system is the same for both models that are being compared [@Ribeiro2016b].


The alternatives are either using only interpretable models as introduced in Chapter 2, which has the big disadvantage to usually loose accuracy compared to other approaches. The other alternative is to use more flexible model classes that come with built in explanations. The drawback here is that it ties you to this one algorithm and it will be hard to switch to something else.

Desirable aspects of a model-agnostic explanation system [@Ribeiro2016b]:
- Model flexibility: Not being tied to an underlying particular machine learning model. The method should work for random forests as well as convolutional neural networks
- Explanation flexibility: Not being tied to a certain form of explanation. In some cases it might be useful to have a linear formula in other cases some decision rules
- Representation flexibility: The explanation system should not have to use the same feature representation as the model that is being explained. So when a text classifier uses abstract word embedding vectors, it might be preferable to use the presence of single words for the explanation.

## Global: Explain the behaviour of a model


### Global surrogate models
A surrogate model is a simple, explainable model that explains another complex machine learning model. Models in [#simple] are viable candidates.

UNDER CONSTRUCTION

### Partial dependency plots
Partial dependency plots show the relationship between the target and one or more features by averaging out all the other features. A dependency plot can show if the relationship between target and feature is linear, monotonic or something else.
Are only partially global: It is global because it takes into account all instances, but it is local in the feature, because partial dependency plots only examine one variable, as the name suggests.

UNDER CONSTRUCTION

### Variable importance

UNDER CONSTRUCTION


## Local: Explain a single decisions

### Local surrogate models (LIME)
LIME is also a surrogate model, but it is a local one.


UNDER CONSTRUCTION


## Explanation types

UNDER CONSTRUCTION

### Structured output
Regression tables, decision tree plots.
There are overlaps with visual explanations (Section [#viz-explanation]).

### Visualization {#viz-explanation}
Easy to understand Visualizations. First choice in image classification tasks.

### Natural language (narratives)
That's what humans usually do.

### Examples and prototypes

# References
