# Model-agnostic explanations
Separating the explanations from the machine learning model (= model-agnostic explanations) gives some benefits.
The big advantage of model-agnostic vs model-specific explanation algorithm is the flexibility. When the explanation system is independently applicable even when the underlying model is switched, it frees the practitioner to use different machine learning models without restrictions. It is also more efficient to build interfaces on top of model-agnostic systems, because  this has to be done only once and not for each model-specific explanation system. Usually not one but many types of machine learning models are tested in development time and if you want to compare the models in terms of interpretability this is easier with model-agnostic explanations because the system is the same for both models that are being compared [@Ribeiro2016b].


The alternatives are either using only interpretable models as introduced in Chapter 2, which has the big disadvantage to usually loose accuracy compared to other approaches. The other alternative is to use more flexible model classes that come with built in explanations. The drawback here is that it ties you to this one algorithm and it will be hard to switch to something else.

Desirable aspects of a model-agnostic explanation system [@Ribeiro2016b]:
- Model flexibility: Not being tied to an underlying particular machine learning model. The method should work for random forests as well as convolutional neural networks
- Explanation flexibility: Not being tied to a certain form of explanation. In some cases it might be useful to have a linear formula in other cases some decision rules
- Representation flexibility: The explanation system should not have to use the same feature representation as the model that is being explained. So when a text classifier uses abstract word embedding vectors, it might be preferable to use the presence of single words for the explanation.

## Global: Explain the behaviour of a model


### Global surrogate models
A surrogate model is a simple, explainable model that explains another complex machine learning model. Models in [#simple] are viable candidates.

UNDER CONSTRUCTION

### Partial dependency plots
Partial dependency plots show the relationship between the target and one or more features by averaging out all the other features. A dependency plot can show if the relationship between target and feature is linear, monotonic or something else.
Are only partially global: It is global because it takes into account all instances, but it is local in the feature, because partial dependency plots only examine one variable, as the name suggests.

UNDER CONSTRUCTION

### Variable importance

UNDER CONSTRUCTION


## Local: Explain a single decisions
Instead of trying to explain predictions or mechanisms of the model globally, local methods focus on explaining single predictions.

### Local surrogate models (LIME)
Local interpretable model-agnostic explanations (LIME) is a method for fitting local, interpretable models that can explain single predictions or classifications of any black-box machine learning model. LIME explanations are local surrogate models. Instead of trying to fit a global surrogate model, LIME focuses on a prediction done by a black-box algorithm and explains it's outcome.


The idea is quite simple, really. First of all, forget about the training data and imagine you only have the black box model where you can input  data points and get the models outcome. You can probe the box as often as you want. Your goal is to understand why the machine learning model gave the outcome it produced. LIME tests out what happens to the model's predictions when you put some variations of your data point of interest into the machine learning model. This basically generates a new dataset consisting of the perturbed samples and the associated model's outcome. On this dataset LIME then trains a simple model weighted by the proximity of the sampled instances to the instance of interest. The simple mode  can basically be any from Section [#simple], for example LASSO or a short tree. The learned model should  be a good approximation of the machine learning model locally, but it does not have to be so globally. 

The recipe:

- Choose your instance of interest for which you want to have an explanation of it's black box outcome
- Make some variations of the instances and check what the black box predicts in the neighbourhood of the instance of interest. 
- Fit a local, interpretable model on the dataset with the variations
- Explain prediction by interpreting the local simple model. 

As always, the devil is in the details. In a high-dimensional space, defining a neighbourhood is not trivial. Distance measures are quite arbitrary and distances in different dimensions (aka features) might not be comparable at all. How big should the neighbourhood be you look into? If it is too small, then there might be no difference in the predictions of the machine learning model at all. 
The other question is: How do you get the variations of the data? This differs depending on the type of data, which can be either text, an image or tabular data. For text and image the solution is turning off and on single words or superpixels. In the case of tabular data LIME creates new samples by pertubing each feature individually, by drawing from a normal distribution with mean and standard deviation from the feature. 


#### LIME for tabular data
Tabular data means any data that comes in tables, where each row represents an instance and each column a feature.
Sampling is not done around the point, but from the training data's mass center. Has it's problems. But it increases the likelihood that the outcome for some of the sampled points predictions differ from the data point of interest and that LIME can learn at least some explanation. 

Figure \@ref(fig:lime-fitting) explains how the sampling and local model fitting works. 


```{r lime-fitting, fig.cap='How LIME sampling works: A) The trainig data has two classes. The most data points have class 0, and the ones with class 1 are grouped in an upside-down V-shape. The plot displays the decision boundaries learned by a machine learning model. In this case it was a Random Forest, but it does not matter, because LIME is model-agnostic and we only care about the decision boundaries. B) The yellow is the instance of interest, for which an explanation is desired. The black dots are data sampled from a normal distribution around the means of the features in the training sample. This has only to be done once and can be reused for other explanations. C) Introducing locality by giving points near the instance of interest a higher weights. D) The colors and signs of the grid display the classifications of the locally learned model form the weighted samples. The white line marks the decision boundary (P(class) = 0.5) at which the classification changes.', fig.height=9, fig.width=9}
source(sprintf('%s/lime.R', src_dir))

colors = c('#132B43', '#56B1F7')
# Data with some noise
p_data = ggplot(lime_training_df) + 
  geom_point(aes(x=x1,y=x2,fill=y_noisy, color=y_noisy), alpha =0.3, shape=21) + 
  scale_fill_manual(values = colors) + 
  scale_color_manual(values = colors) + 
  my_theme(legend.position = 'none') 

# The decision boundaries of the learned black box classifier
p_boundaries = ggplot(grid_df) + 
  geom_raster(aes(x=x1,y=x2,fill=predicted), alpha = 0.3, interpolate=TRUE) + 
  my_theme(legend.position='none') + 
  ggtitle('A')


# Drawing some samples
p_samples = p_boundaries + 
  geom_point(data = df_sample, aes(x=x1, y=x2)) + 
  scale_x_continuous(limits = c(-2, 2)) + 
  scale_y_continuous(limits = c(-2, 1))
# The point to be explained
p_explain = p_samples +
  geom_point(data = df_explain, aes(x=x1,y=x2), fill = 'yellow', shape = 21, size=4) + 
  ggtitle('B')



p_weighted = p_boundaries + 
  geom_point(data = df_sample, aes(x=x1, y=x2, size=weights)) + 
  scale_x_continuous(limits = c(-2, 2)) + 
  scale_y_continuous(limits = c(-2, 1)) +
  geom_point(data = df_explain, aes(x=x1,y=x2), fill = 'yellow', shape = 21, size=4) + 
  ggtitle('C')

p_boundaries_lime = ggplot(grid_df)  + 
  geom_raster(aes(x=x1,y=x2,fill=predicted), alpha = 0.3, interpolate=TRUE) + 
  geom_point(aes(x=x1, y=x2, color=explained), size = 2, data = grid_df_small[grid_df_small$explained_class==1,], shape=3) + 
  geom_point(aes(x=x1, y=x2, color=explained), size = 2, data = grid_df_small[grid_df_small$explained_class==0,], shape=95) + 
  geom_point(data = df_explain, aes(x=x1,y=x2), fill = 'yellow', shape = 21, size=4) +
  geom_line(aes(x=x1, y=x2), data =logistic_boundary_df, color = 'white') + 
  my_theme(legend.position='none') + ggtitle('D')


gridExtra::grid.arrange(p_boundaries, p_explain, p_weighted, p_boundaries_lime, ncol=2)

```




##### Example
Let's look at a concrete example. We go back to the speed dating dataset and predict the probability that the participant will want to see the partner for a second date.

```{r lime-tabular-example-train-black-box, cache = TRUE}
ntree = 30

model <- caret::train(train_speed_dating_classification[features_of_interest], 
               train_speed_dating_classification$match, 
               method = 'rf', ntree=ntree, maximise = FALSE)


# Accuracy on test set
# test_predict = predict(model, test_speed_dating_classification[features_of_interest])
# caret::confusionMatrix(test_predict, test_speed_dating_classification[,target_var_class])

```
First we train a Random Forest on the classification task. Given correlation of interest, age of partner, field of study and so on, what is the probability that the participant will choose the partner for a second date?
The Random Forest has `r ntree` trees. 
```{r lime-tabular-example-explain}

# Create explanation function
set.seed(44)
explain <- lime::lime(train_speed_dating_classification, model)


n_features_lime = 2
n_labels = 2
instance_indices = c(1, 33)


# Explain new observation
explanation <- explain(test_speed_dating_classification[instance_indices, features_of_interest],  
                       n_labels=n_labels, n_features = n_features_lime)

explanation = filter(explanation, label == 'yes')
```
The continuous features are categorised into bins by quantiles for the explanation models. The explanations are set to contain `r n_features_lime` features. 



```{r lime-tabular-example-explain-plot, fig.cap=sprintf('Explanations for two instances in the test data. The first instance got a probability of 0.13 for a match by the Random Forest. The correlation of interests between the participant and the partner is low which reduces the probability of a match for this participant. Also the importance for race of the partner is higher than 6, which lowers the probability of a match.  In the second case, the probability for a match is quite high with 0.46. The reasons are the high correlation (>0.43) and that the partner is younger then 25.', instance_indices[1], instance_indices[2])}
# An can be visualised directly
lime::plot_features(explanation, ncol=1)
```

#### LIME for images
For images the sampling procedure works differently. Instead of sampling single pixels, LIME create variations of the image by turning off superpixel. 

#### LIME for text



## Explanation types

UNDER CONSTRUCTION

### Structured output
Regression tables, decision tree plots.
There are overlaps with visual explanations (Section [#viz-explanation]).

### Visualization {#viz-explanation}
Easy to understand Visualizations. First choice in image classification tasks.

### Natural language (narratives)
That's what humans usually do.

### Examples and prototypes

# References
