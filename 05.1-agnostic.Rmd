# Model-agnostic tools for interpretability {#agnostic}
Separating the explanations from the machine learning model (= model-agnostic explanations) gives some benefits.
The big advantage of model-agnostic vs model-specific explanation algorithm is the flexibility. When the explanation system is independently applicable even when the underlying model is switched, it frees the practitioner to use different machine learning models without restrictions. It is also more efficient to build interfaces on top of model-agnostic systems, because  this has to be done only once and not for each model-specific explanation system. Usually not one but many types of machine learning models are tested in development time and if you want to compare the models in terms of interpretability this is easier with model-agnostic explanations because the system is the same for both models that are being compared [@Ribeiro2016b].

The alternatives are either using only interpretable models as introduced in Chapter 2, which has the big disadvantage to usually loose accuracy compared to other approaches. The other alternative is to use more flexible model classes that come with built in explanations. The drawback here is that it ties you to this one algorithm and it will be hard to switch to something else.

Desirable aspects of a model-agnostic explanation system [@Ribeiro2016b]:
- Model flexibility: Not being tied to an underlying particular machine learning model. The method should work for random forests as well as convolutional neural networks
- Explanation flexibility: Not being tied to a certain form of explanation. In some cases it might be useful to have a linear formula in other cases some decision rules
- Representation flexibility: The explanation system should not have to use the same feature representation as the model that is being explained. So when a text classifier uses abstract word embedding vectors, it might be preferable to use the presence of single words for the explanation.


## The bigger picture

Let's take a look from further away. What do we want to explain, and what kind of 'layers' are inbetween? The infographic displays the concepts, see Figure \@ref(fig:bigpicture). The bottom layer is the 'World'. This could literally be nature itself, like the biology of the human body and how it reacts to medication, but also human behaviour like if people payed back their loans. The 'World'-layer contains everything that can be observed and is of interest. Ultimately we want to learn something about the 'World' and interact with it.

The second layer is the 'Data'-layer. We have to digitalise the 'World' in to make it processable for computers and also to store information. The 'Data'-layer contains anything from images, texts, tabular data and so on.

With machine learning on top of the 'Data'-layer we get to the 'Black Box Model'-layer. Machine learning algorithms learns with data from the real world to make predictions / classifications or finds structures.

Now with the 'Interpretable models'-layer we come the part that this book is concerned with. On top of the 'Black-Box-Layer' we want to have something that helps us deal with the opaqueness of machine learning models. What were the important attributes for a particular diagnosis? Why was a financial transaction classified as fraud?

On top of that, there is the 'Explanations'-layer. I put it as a layer separate from 'Interpretable models', since the simple models deal with capturing associations and it is useful to think of the explanation as independent. There are different ways to present the results of a linear regression model for example, it could be a coefficient table, a coefficient plot with confidence intervals, a coloured bar chart, a few sentences, ...
It depends on the target audience what representation which explanation to choose.

The last layer is 'Human'. Look this one is waiving at you because you are reading this book and you are helping to provide better explanations for black box models! Humans are the consumers of the explanations ultimately.

```{r bigpicture, fig.cap="The big picture of explainable machine learning. The real world goes through many layers before it reaches the human in forms of explanations.", out.width="80%"}
knitr::include_graphics("images/big-picture.png")
```

This layered abstraction also helps in understanding what the difference between statisticians and machine learning practitioners is. Statistician are concerned with the 'Data' layer, like planning clinical trials or designing surveys. The they skip the 'Black Box Model'-layer and go right to the 'Interpretable Models' and from there to the explanations for our human. Machine learning specialists are also concerned with the 'Data'-layer, like collecting labeled samples of skin cancer images or crawling Wikipedia. Then comes the machine learning model. 'Interpretable models' and 'Explanations' are skipped and the human deals directly with the 'Black Box Model'. It's a nice thing, that in explainable machine learning, the work of a statistician and a machine learner fuses and becomes something better.

Of course this graphic does not capture everything: Data could come from simulations. Black box models also output predictions that might not even reach humans, but only feed other machines and so on. But overall it is a useful abstraction for understanding what explainable machine learning is.
