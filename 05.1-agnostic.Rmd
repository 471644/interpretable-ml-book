# Model-agnostic tools for interpretability {#agnostic}
Separating the explanations from the machine learning model (= model-agnostic explanations) gives some benefits.
The big advantage of model-agnostic vs model-specific explanation algorithm is the flexibility.
When the explanation system is independently applicable even when the underlying model is switched, it frees the practitioner to use different machine learning models without restrictions.
It is also more efficient to build interfaces on top of model-agnostic systems, because this has to be done only once and not for each model-specific explanation system.
Usually not one but many types of machine learning models are tested in development time and if you want to compare the models in terms of interpretability this is easier with model-agnostic explanations because the system is the same for both models that are being compared [@Ribeiro2016b].

The alternatives are either using only interpretable models as introduced in Chapter 2, which has the big disadvantage to usually loose accuracy compared to other approaches.
The other alternative is to use more flexible model classes that come with built in explanations.
The drawback here is that it ties you to this one algorithm and it will be hard to switch to something else.

Desirable aspects of a model-agnostic explanation system [@Ribeiro2016b]:
- Model flexibility: Not being tied to an underlying particular machine learning model. The method should work for random forests as well as convolutional neural networks
- Explanation flexibility: Not being tied to a certain form of explanation. In some cases it might be useful to have a linear formula in other cases some decision rules
- Representation flexibility: The explanation system should not have to use the same feature representation as the model that is being explained. So when a text classifier uses abstract word embedding vectors, it might be preferable to use the presence of single words for the explanation.


\textbf{The bigger picture}

Let's take a high level view on model-agnostic interpretability.
Figure \@ref(fig:bigpicture) shows how we first abstract the world by capturing it by collecting data and abstract it further by learning the essence of the data (for the task) with a machine learning model.
Interpretability is just another layer on top, that helps humans understand.

- The bottom layer is the 'World'. This could literally be nature itself, like the biology of the human body and how it reacts to medication, but also more abstract things like the real estate market. The 'World'-layer contains everything that can be observed and is of interest. Ultimately we want to learn something about the 'World' and interact with it.
- The second layer is the 'Data'-layer. We have to digitalise the 'World' in order to make it processable for computers and also to store information. The 'Data'-layer contains anything from images, texts, tabular data and so on.
- By fitting machine learning models on top of the 'Data'-layer we get the 'Black Box Model'-layer. Machine learning algorithms learns with data from the real world to make predictions or find structures.
- On top of the 'Black-Box-Layer' is the 'Interpretable Methods' layer that helps us deal with the opaqueness of machine learning models. What were the important attributes for a particular diagnosis? Why was a financial transaction classified as fraud?
- The last layer is occupied by a 'Human'. Look! This one is waving at you because you are reading this book and you are helping to provide better explanations for black box models! Humans are the consumers of the explanations ultimately.

```{r bigpicture, fig.cap="The big picture of explainable machine learning. The real world goes through many layers before it reaches the human in forms of explanations.", out.width="80%"}
knitr::include_graphics("images/big-picture.png")
```

This layered abstraction also helps in understanding what the differences in approaches between statisticians and machine learning practitioners is.
Statistician are concerned with the 'Data' layer, like planning clinical trials or designing surveys.
The they skip the 'Black Box Model'-layer and go right to the 'Interpretable Methods' and from there to the explanations for our human.
Machine learning specialists are also concerned with the 'Data'-layer, like collecting labeled samples of skin cancer images or crawling Wikipedia.
Then comes the machine learning model.
'Interpretable models' and 'Explanations' are skipped and the human deals directly with the 'Black Box Model'.
It's a nice thing, that in interpretable machine learning, the work of a statistician and a machine learner fuses and becomes something better.

Of course this graphic does not capture everything: Data could come from simulations. Black box models also output predictions that might not even reach humans, but only feed other machines and so on.
But overall it is a useful abstraction for understanding how (model-agnostic) interpretability becomes this new layer on top of machine learning models.
