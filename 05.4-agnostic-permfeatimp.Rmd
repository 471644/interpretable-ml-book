## Permutation feature importance

The permutation importance measurement was orginially introduced for RandomForests [@breiman2001random]. It is calculated on the out-of-bag instances and works by
estimating the original model performance and checking what happens with the model performance when you permute each feature. A big loss in performance
means a big feature importance. The idea of permutation of features is per se model-agnostic, only the OOB-scheme is specific for ensemble methods.
In can be used for any model when a hold-out dataset is used, instead of OOB samples. Of course you could also use the training data, but you risk
getting variable importance measures that overfit your training data, since the model was already trained on it.

Algorithm [@breiman2001random]:

Input: Trained model $\hat{f}$, hold-out dataset $D$, number of permutations $n_{perm}$

1. Estimate performance $Perf$ of $\hat{f}$ with $D$ (e.g. MSE for regression or accuracy for classification)
2. For each feature $j \in 1, \ldots, J$ do:
  - For $i \in 1,\ldots , n_{perm}$
    - Get $D_{j_{perm}}$ by permuting feature $X_j$ in data $D$. This breaks the association between $X_j$ and $Y$.
    - Estimate performance $Perf_{i,j_{perm}}$ of $\hat{f}$ with $D_{j_{perm}}$
    - Calculate permutation variable importance $VI_i(X_j) = Perf_{i,j_{perm}} - Perf$
  - Calculate mean variable importance: $VI(X_j) = \frac{1}{n_{perm}}\sum_{i=1}^{n_{perm}} VI_i(X_j)$
  - Optional: Calculate p-value $p = \frac{I(Perf_{j_{perm}} > Perf)}{n_{perm}}$
3. Sort variables by descending $VI$.

The feature with the highest $VI$ measure is the most important globally in your model.
With the p-value you can additionally check if a feature importance is significantly different from 0.
You might want to adjust your $\alpha$ confidence level for multiple testing.


You can also find the algorithm in more detail in  [@Strobl2008].
The authors additionally suggest a conditional feature importance measurement, which is not (yet) covered in this book.
The standard permutation feature importance only works with marginal feature improvements and cannot distinguish between correlation and spurious correlation.
[@Strobl2008] suggest to condition the importance measure also on other features, which makes it possible to account for correlation among the features.


### Model dependent feature importance

Some model classes already come with built in feature importance measurements.
A few examples:
- RandomForest: Permutation based feature importance
- CART and boosting: mean decrease in Gini impurity index
- Linear Model: (absolute value of) t-test statistic for each feature
