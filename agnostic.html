<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable." />
  <meta name="github-repo" content="christophM/interpretable-ml-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable." />
  

<meta name="author" content="Christoph Molnar">


<meta name="date" content="2018-01-02">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="other-interpretable-models.html">
<link rel="next" href="pdp.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>
<link href="libs/dt-core-bootstrap-1.10.12/css/dataTables.bootstrap.min.css" rel="stylesheet" />
<link href="libs/dt-core-bootstrap-1.10.12/css/dataTables.bootstrap.extra.css" rel="stylesheet" />
<script src="libs/dt-core-bootstrap-1.10.12/js/jquery.dataTables.min.js"></script>
<script src="libs/dt-core-bootstrap-1.10.12/js/dataTables.bootstrap.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110543840-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110543840-1');
</script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="what-to-expect-from-this-book.html"><a href="what-to-expect-from-this-book.html"><i class="fa fa-check"></i><b>1.1</b> What to Expect from This Book</a></li>
<li class="chapter" data-level="1.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>1.2</b> What Is Machine Learning?</a></li>
<li class="chapter" data-level="1.3" data-path="definitions.html"><a href="definitions.html"><i class="fa fa-check"></i><b>1.3</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> Interpretability</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>2.1</b> The Importance of Interpretability</a></li>
<li class="chapter" data-level="2.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html"><i class="fa fa-check"></i><b>2.2</b> Scope of Interpretability</a><ul>
<li class="chapter" data-level="2.2.1" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#algorithm-transparency"><i class="fa fa-check"></i><b>2.2.1</b> Algorithm transparency</a></li>
<li class="chapter" data-level="2.2.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-holistic-model-interpretability"><i class="fa fa-check"></i><b>2.2.2</b> Global, Holistic Model Interpretability</a></li>
<li class="chapter" data-level="2.2.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-model-interpretability-on-a-modular-level"><i class="fa fa-check"></i><b>2.2.3</b> Global Model Interpretability on a Modular Level</a></li>
<li class="chapter" data-level="2.2.4" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-single-prediction"><i class="fa fa-check"></i><b>2.2.4</b> Local Interpretability for a Single Prediction</a></li>
<li class="chapter" data-level="2.2.5" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-group-of-prediction"><i class="fa fa-check"></i><b>2.2.5</b> Local Interpretability for a Group of Prediction</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="evaluating-interpretability.html"><a href="evaluating-interpretability.html"><i class="fa fa-check"></i><b>2.3</b> Evaluating Interpretability</a><ul>
<li class="chapter" data-level="2.3.1" data-path="evaluating-interpretability.html"><a href="evaluating-interpretability.html#approaches-for-evaluating-the-interpretability-quality"><i class="fa fa-check"></i><b>2.3.1</b> Approaches for Evaluating the Interpretability Quality</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>2.4</b> Human-style Explanations</a><ul>
<li class="chapter" data-level="2.4.1" data-path="explanation.html"><a href="explanation.html#what-is-an-explanation"><i class="fa fa-check"></i><b>2.4.1</b> What is an explanation?</a></li>
<li class="chapter" data-level="2.4.2" data-path="explanation.html"><a href="explanation.html#good-explanation"><i class="fa fa-check"></i><b>2.4.2</b> What is a “good” explanation?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Datasets</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> Bike Sharing Counts (Regression)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> YouTube Spam Comments (Text Classification)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical-data.html"><a href="cervical-data.html"><i class="fa fa-check"></i><b>3.3</b> Risk Factors for Cervical Cancer (Classification)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>4</b> Interpretable Models</a><ul>
<li class="chapter" data-level="4.1" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>4.1</b> Terminology</a></li>
<li class="chapter" data-level="4.2" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>4.2</b> Linear Model</a><ul>
<li class="chapter" data-level="4.2.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>4.2.1</b> Interpretation</a></li>
<li class="chapter" data-level="4.2.2" data-path="limo.html"><a href="limo.html#interpretation-example"><i class="fa fa-check"></i><b>4.2.2</b> Interpretation Example</a></li>
<li class="chapter" data-level="4.2.3" data-path="limo.html"><a href="limo.html#interpretation-templates"><i class="fa fa-check"></i><b>4.2.3</b> Interpretation templates</a></li>
<li class="chapter" data-level="4.2.4" data-path="limo.html"><a href="limo.html#visual-parameter-interpretation"><i class="fa fa-check"></i><b>4.2.4</b> Visual parameter interpretation</a></li>
<li class="chapter" data-level="4.2.5" data-path="limo.html"><a href="limo.html#explaining-single-predictions"><i class="fa fa-check"></i><b>4.2.5</b> Explaining Single Predictions</a></li>
<li class="chapter" data-level="4.2.6" data-path="limo.html"><a href="limo.html#cat.code"><i class="fa fa-check"></i><b>4.2.6</b> Coding Categorical Features</a></li>
<li class="chapter" data-level="4.2.7" data-path="limo.html"><a href="limo.html#the-disadvantages-of-linear-models"><i class="fa fa-check"></i><b>4.2.7</b> The disadvantages of linear models</a></li>
<li class="chapter" data-level="4.2.8" data-path="limo.html"><a href="limo.html#do-linear-models-create-good-explanations"><i class="fa fa-check"></i><b>4.2.8</b> Do linear models create good explanations?</a></li>
<li class="chapter" data-level="4.2.9" data-path="limo.html"><a href="limo.html#extending-linear-models"><i class="fa fa-check"></i><b>4.2.9</b> Extending Linear Models</a></li>
<li class="chapter" data-level="4.2.10" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>4.2.10</b> Sparse linear models</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>4.3</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.3.1" data-path="logistic.html"><a href="logistic.html#whats-wrong-with-linear-regression-models-for-classification"><i class="fa fa-check"></i><b>4.3.1</b> What’s Wrong with Linear Regression Models for Classification?</a></li>
<li class="chapter" data-level="4.3.2" data-path="logistic.html"><a href="logistic.html#logistic-regression"><i class="fa fa-check"></i><b>4.3.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="logistic.html"><a href="logistic.html#interpretation-1"><i class="fa fa-check"></i><b>4.3.3</b> Interpretation</a></li>
<li class="chapter" data-level="4.3.4" data-path="logistic.html"><a href="logistic.html#example"><i class="fa fa-check"></i><b>4.3.4</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>4.4</b> Decision Tree</a><ul>
<li class="chapter" data-level="4.4.1" data-path="tree.html"><a href="tree.html#interpretation-2"><i class="fa fa-check"></i><b>4.4.1</b> Interpretation</a></li>
<li class="chapter" data-level="4.4.2" data-path="tree.html"><a href="tree.html#interpretation-example-1"><i class="fa fa-check"></i><b>4.4.2</b> Interpretation Example</a></li>
<li class="chapter" data-level="4.4.3" data-path="tree.html"><a href="tree.html#advantages"><i class="fa fa-check"></i><b>4.4.3</b> Advantages</a></li>
<li class="chapter" data-level="4.4.4" data-path="tree.html"><a href="tree.html#disadvantages"><i class="fa fa-check"></i><b>4.4.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>4.5</b> RuleFit</a><ul>
<li class="chapter" data-level="4.5.1" data-path="rulefit.html"><a href="rulefit.html#interpretation-and-example"><i class="fa fa-check"></i><b>4.5.1</b> Interpretation and Example</a></li>
<li class="chapter" data-level="4.5.2" data-path="rulefit.html"><a href="rulefit.html#guidelines"><i class="fa fa-check"></i><b>4.5.2</b> Guidelines</a></li>
<li class="chapter" data-level="4.5.3" data-path="rulefit.html"><a href="rulefit.html#theory"><i class="fa fa-check"></i><b>4.5.3</b> Theory</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="other-interpretable-models.html"><a href="other-interpretable-models.html"><i class="fa fa-check"></i><b>4.6</b> Other Interpretable Models</a><ul>
<li class="chapter" data-level="4.6.1" data-path="other-interpretable-models.html"><a href="other-interpretable-models.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>4.6.1</b> Naive Bayes classifier</a></li>
<li class="chapter" data-level="4.6.2" data-path="other-interpretable-models.html"><a href="other-interpretable-models.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>4.6.2</b> K-Nearest Neighbours</a></li>
<li class="chapter" data-level="4.6.3" data-path="other-interpretable-models.html"><a href="other-interpretable-models.html#and-so-many-more"><i class="fa fa-check"></i><b>4.6.3</b> And so many more …</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>5</b> Model-Agnostic Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>5.1</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="5.1.1" data-path="pdp.html"><a href="pdp.html#examples"><i class="fa fa-check"></i><b>5.1.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="individual-conditional-expectation-ice-plot.html"><a href="individual-conditional-expectation-ice-plot.html"><i class="fa fa-check"></i><b>5.2</b> Individual Conditional Expectation (ICE) Plot</a><ul>
<li class="chapter" data-level="5.2.1" data-path="individual-conditional-expectation-ice-plot.html"><a href="individual-conditional-expectation-ice-plot.html#example-1"><i class="fa fa-check"></i><b>5.2.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="permutation-feature-importance.html"><a href="permutation-feature-importance.html"><i class="fa fa-check"></i><b>5.3</b> Permutation Feature Importance</a></li>
<li class="chapter" data-level="5.4" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>5.4</b> Local Surrogate Models (LIME)</a><ul>
<li class="chapter" data-level="5.4.1" data-path="lime.html"><a href="lime.html#lime-for-tabular-data"><i class="fa fa-check"></i><b>5.4.1</b> LIME for Tabular Data</a></li>
<li class="chapter" data-level="5.4.2" data-path="lime.html"><a href="lime.html#lime-for-text"><i class="fa fa-check"></i><b>5.4.2</b> LIME for Text</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>6</b> Contribute</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="agnostic" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Model-Agnostic Methods</h1>
<p>Separating the explanations from the machine learning model (= model-agnostic interpretability methods) has some benefits <span class="citation">(Ribeiro, Singh, and Guestrin <a href="#ref-Ribeiro2016b">2016</a>)</span>. The big advantage of model-agnostic interpretability methods over model-specific ones is their flexibility. The machine learning developer is free to use any machine learning model she likes, when the interpretability methods can be applied to any model. Anything that is build on top of an interpretation of a machine learning model, like a graphic or some user interface, also becomes independent of the underlying machine learning model. Usually not one but many types of machine learning models are evaluated for solving a task and if you compare the models in terms of interpretability, it is easier to do with model-agnostic explanations, because the same method can be used for any type of model.</p>
<p>An alternative to model-agnostic interpretability methods is using only interpretable models as introduced in Chapter <a href="simple.html#simple">4</a>, which often has the big disadvantage to usually loose accuracy compared to other machine learning models and locking you into one type of model and interpretability method. The other alternative is to use model-specific interpretability methods. The drawback here is also that it ties you to this one algorithm and it will be hard to switch to something else.</p>
<p>Desirable aspects of a model-agnostic explanation system are <span class="citation">(Ribeiro, Singh, and Guestrin <a href="#ref-Ribeiro2016b">2016</a>)</span>:</p>
<ul>
<li><strong>Model flexibility:</strong> Not being tied to an underlying particular machine learning model. The method should work for random forests as well as deep neural networks.</li>
<li><strong>Explanation flexibility:</strong> Not being tied to a certain form of explanation. In some cases it might be useful to have a linear formula in other cases a decision tree or a graphic with feature importances.</li>
<li><strong>Representation flexibility:</strong> The explanation system should not have to use the same feature representation as the model that is being explained. For a text classifier that uses abstract word embedding vectors it might be preferable to use the presence of single words for the explanation.</li>
</ul>

<p>Let’s take a high level view on model-agnostic interpretability. Figure <a href="agnostic.html#fig:bigpicture">5.1</a> shows how we first abstract the world by capturing it by collecting data and abstract it further by learning the essence of the data (for the task) with a machine learning model. Interpretability is just another layer on top, that helps humans understand.</p>
<ul>
<li>The bottom layer is the ‘World’. This could literally be nature itself, like the biology of the human body and how it reacts to medication, but also more abstract things like the real estate market. The ‘World’-layer contains everything that can be observed and is of interest. Ultimately we want to learn something about the ‘World’ and interact with it.</li>
<li>The second layer is the ‘Data’-layer. We have to digitalise the ‘World’ in order to make it processable for computers and also to store information. The ‘Data’-layer contains anything from images, texts, tabular data and so on.</li>
<li>By fitting machine learning models on top of the ‘Data’-layer we get the ‘Black Box Model’-layer. Machine learning algorithms learn with data from the real world to make predictions or find structures.</li>
<li>On top of the ‘Black Box Model’-layer is the ‘Interpretability Methods’-layer that helps us deal with the opaqueness of machine learning models. What were the important features for a particular diagnosis? Why was a financial transaction classified as fraud?</li>
<li>The last layer is occupied by a ‘Human’. Look! This one is waving at you because you are reading this book and you are helping to provide better explanations for black box models! Humans are the consumers of the explanations, ultimately.</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:bigpicture"></span>
<img src="images/big-picture.png" alt="The big picture of explainable machine learning. The real world goes through many layers before it reaches the human in forms of explanations." width="80%" />
<p class="caption">
FIGURE 5.1: The big picture of explainable machine learning. The real world goes through many layers before it reaches the human in forms of explanations.
</p>
</div>
<p>This layered abstraction also helps in understanding what the differences in approaches between statisticians and machine learning practitioners are. Statistician are concerned with the ‘Data’ layer, like planning clinical trials or designing surveys. They skip the ‘Black Box Model’-layer and go right to the ‘Interpretability Methods’-layer. Machine learning specialists are also concerned with the ‘Data’-layer, like collecting labeled samples of skin cancer images or crawling Wikipedia. Then comes the machine learning model. ‘Interpretability Methods’ is skipped and the human deals directly with the black box models prediction. It’s a nice thing, that in interpretable machine learning, the work of a statistician and a machine learner fuses and becomes something better.</p>
<p>Of course this graphic does not capture everything: Data could come from simulations. Black box models also output predictions that might not even reach humans, but only feed other machines and so on. But overall it is a useful abstraction for understanding how (model-agnostic) interpretability becomes this new layer on top of machine learning models.</p>
<!--chapter:end:chapters/05.1-agnostic.Rmd--> 
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Ribeiro2016b">
<p>Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. “Model-Agnostic Interpretability of Machine Learning.” <em>ICML Workshop on Human Interpretability in Machine Learning</em>, no. Whi.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="other-interpretable-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pdp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/chapters/05.2-agnostic-pdp.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
