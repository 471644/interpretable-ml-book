@article{friedman2008predictive,
  title={Predictive learning via rule ensembles},
  author={Friedman, Jerome H and Popescu, Bogdan E},
  journal={The Annals of Applied Statistics},
  pages={916--954},
  year={2008},
  publisher={JSTOR}
}

@article{bike2013,
year={2013}, 
issn={2192-6352}, 
journal={Progress in Artificial Intelligence}, 
doi={10.1007/s13748-013-0040-3}, 
title={Event labeling combining ensemble detectors and background knowledge}, 
url={[Web Link]}, 
publisher={Springer Berlin Heidelberg}, 
keywords={Event labeling; Event detection; Ensemble learning; Background knowledge}, 
author={Fanaee-T, Hadi and Gama, Joao}, 
pages={1-15} 
}

@inproceedings{fernandes2017transfer,
  title={Transfer Learning with Partial Observability Applied to Cervical Cancer Screening},
  author={Fernandes, Kelwin and Cardoso, Jaime S and Fernandes, Jessica},
  booktitle={Iberian Conference on Pattern Recognition and Image Analysis},
  pages={243--250},
  year={2017},
  organization={Springer}
}

@article{Strobl2008,
abstract = {BACKGROUND: Random forests are becoming increasingly popular in many scientific fields because they can cope with "small n large p" problems, complex interactions and even highly correlated predictor variables. Their variable importance measures have recently been suggested as screening tools for, e.g., gene expression studies. However, these variable importance measures show a bias towards correlated predictor variables. RESULTS: We identify two mechanisms responsible for this finding: (i) A preference for the selection of correlated predictors in the tree building process and (ii) an additional advantage for correlated predictor variables induced by the unconditional permutation scheme that is employed in the computation of the variable importance measure. Based on these considerations we develop a new, conditional permutation scheme for the computation of the variable importance measure. CONCLUSION: The resulting conditional variable importance reflects the true impact of each predictor variable more reliably than the original marginal approach.},
author = {Strobl, Carolin and Boulesteix, Anne-Laure and Kneib, Thomas and Augustin, Thomas and Zeileis, Achim},
doi = {10.1186/1471-2105-9-307},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Strobl et al. - 2008 - Conditional variable importance for random forests.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Amino Acid Sequence,Binding Sites,Biometry,Biometry: methods,Computational Biology,Computational Biology: methods,Decision Trees,Factor Analysis,Major Histocompatibility Complex,Major Histocompatibility Complex: genetics,Nonparametric,Regression Analysis,Research Design,Statistical,Statistics,xai-book},
mendeley-groups = {Master Thesis},
mendeley-tags = {xai-book},
month = {jan},
pages = {307},
pmid = {18620558},
title = {{Conditional variable importance for random forests.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2491635{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {9},
year = {2008}
}

@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@article{goldstein2015peeking,
  title={Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation},
  author={Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
  journal={Journal of Computational and Graphical Statistics},
  volume={24},
  number={1},
  pages={44--65},
  year={2015},
  publisher={Taylor \& Francis}
}

@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}

@inproceedings{alberto2015tubespam,
  title={Tubespam: Comment spam filtering on youtube},
  author={Alberto, T{\'u}lio C and Lochter, Johannes V and Almeida, Tiago A},
  booktitle={Machine Learning and Applications (ICMLA), 2015 IEEE 14th International Conference on},
  pages={138--143},
  year={2015},
  organization={IEEE}
}


@article{Lipton2016,
abstract = {Supervised machine learning models boast re-markable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but inter-pretable. And yet the task of interpretation ap-pears underspecified. Papers provide diverse and sometimes non-overlapping motivations for in-terpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim inter-pretability axiomatically, absent further explana-tion. In this paper, we seek to refine the dis-course on interpretability. First, we examine the motivations underlying interest in interpretabil-ity, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of dif-ferent notions, and question the oft-made asser-tions that linear models are interpretable and that deep neural networks are not.},
archivePrefix = {arXiv},
arxivId = {arXiv:1606.03490v1},
author = {Lipton, Zachary C},
eprint = {arXiv:1606.03490v1},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Lipton - 2016 - The Mythos of Model Interpretability.pdf:pdf},
journal = {ICML Workshop on Human Interpretability in Machine Learning},
keywords = {Black Box,Deep Learning,Interpretability,Machine Learning,Supervised Learning,xai-book},
mendeley-tags = {xai-book},
number = {Whi},
title = {{The Mythos of Model Interpretability}},
year = {2016}
}

@book{Hastie2009,
author = {Hastie, T and Tibshirani, R and Friedman, J},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Hastie, Tibshirani, Friedman - 2009 - The elements of statistical learning.pdf:pdf},
title = {{The elements of statistical learning}},
url = {http://link.springer.com/content/pdf/10.1007/978-0-387-84858-7.pdf},
year = {2009}
}



@article{Ribeiro2016b,
abstract = {Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found re-newed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to inter-pretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, com-parison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.},
archivePrefix = {arXiv},
arxivId = {1606.05386},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
eprint = {1606.05386},
file = {:Users/chris/Downloads/1606.05386.pdf:pdf},
journal = {ICML Workshop on Human Interpretability in Machine Learning},
keywords = {comprehensibil,interpretability, machine learning, comprehensibil,machine learning,model-agnostic,xai-book},
mendeley-tags = {model-agnostic,xai-book},
number = {Whi},
title = {{Model-Agnostic Interpretability of Machine Learning}},
year = {2016}
}

@article{Doshi-Velez2017,
abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
archivePrefix = {arXiv},
arxivId = {1702.08608},
author = {Doshi-Velez, Finale and Kim, Been},
eprint = {1702.08608},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Doshi-Velez, Kim - 2017 - Towards A Rigorous Science of Interpretable Machine Learning.pdf:pdf},
keywords = {xai-book},
mendeley-tags = {xai-book},
number = {Ml},
pages = {1--13},
title = {{Towards A Rigorous Science of Interpretable Machine Learning}},
url = {http://arxiv.org/abs/1702.08608},
year = {2017}
}

@misc{algorithm,
  title = {Definition of Algorithm},
  howpublished = {\url{https://www.merriam-webster.com/dictionary/algorithm}},
  note = {Accessed: 2017-02-12}
}


@article{Turner2015,
abstract = {We propose a general model explanation system (MES) for “explaining” the output of black box classifiers. In this introduction we use the motivating example of a classifier trained to detect fraud in a credit card transaction history. The key aspect is that we provide explanations applicable to a single prediction, rather than provide an interpretable set of parameters. The labels in the provided examples are usually negative. Hence, we focus on explaining positive predictions (alerts).},
annote = {From Duplicate 2 (A Model Explanation System - Turner, Ryan)

Extensions to multi-class should be easy, because if you only look at one class of interest it is again the same as the binary case.

How to extend this to regression?},
archivePrefix = {arXiv},
arxivId = {1606.09517},
author = {Turner, Ryan},
eprint = {1606.09517},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Turner - Unknown - A Model Explanation System.pdf:pdf;:Users/chris/Downloads/1606.09517.pdf:pdf},
isbn = {9781509007462},
journal = {NIPS Workshop},
keywords = {,xai-book},
mendeley-tags = {xai-book},
pages = {1--5},
title = {{A Model Explanation System}},
url = {http://www.blackboxworkshop.org/pdf/Turner2015{\_}MES.pdf},
volume = {0},
year = {2015}
}
