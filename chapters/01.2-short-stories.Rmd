## Storytime {#storytime}

A few short stories inspired by Netflix's Black Mirror and Jack Clark's [Import AI Newsletter](https://jack-clark.net/).
Each story is an exaggerated call for interpretable machine learning - of course ;-)
If you are in a hurry, skip them. 
If you want to be entertained and (de-)motivated, read!


**2050, A subway station in Singapore**
  
With her mind she was already at work, while she rushed to Bishan subway station. 
The tests for the new neural architecture should have finished by now. 
She lead the re-design of the governments "tax affinity prediction system for individual entities", which predicts if an individual will hide money from the tax office. 
Her team came up with an elegant piece of engineering.
If successful, the system would not only serve the tax office, but also feed into other systems, like the terrorist prediction and the trade registry. 
One day, the government might even integrate it into the civic trust score. 
The trust system estimates how trustworthy an individual is and the result affects any part of the daily life, like getting a loan or how long you wait in line when getting a new passport.
Descending the escalator, she imagined how an integration into the current trust score system could look like.

Routinely, she wiped her hand over the RFID reader without reducing her walking speed. 
Her mind was occupied, yet a dissonance of sensory expectations reality caused alarm in her brain. 
Too late. 
Nose first she ran into the subway entrance gate and fell, bottom first, onto the floor. 
The door was supposed to open, ... but it didn't.
Baffled, she stood up and looked at the gates screen. 
"Please try again some other time." it said in friendly colors. 
A person walked by, and, ignoring her, wiped his had over the reader. 
The door opened and he walked through. 
The doors close again. 
She wiped her nose. 
It hurt, but at least it wasn't bleeding.
She tried to open the door again, but got rejected.
That is weird, she thought. 
Maybe her public transport account did not have sufficient tokens, which would be unusual, because funding should happen automatically 
She raised her watch to check the account balance.

"Login denied. Please contact your Citizens Advice Bureau!", the projection informed her.

A feeling of nausea hit her like a fist into the stomach.  
With trembling hands she started the mobile game "Sniper guild", an ego-shooter. 
After a few seconds, the loading screen shut down. 
She felt dizzy and dropped down on the floor. 

There was only one possible explanation: 
Her trust score must have dropped. Substantially. 
A small trust drop meant that you couldn't get first class flights for example and was de-prioritized in government queueing systems. 
A really low trust score was rare and meant that you were classified as a potiential threat to society. 
The process of dealing with those people was to keep them from any public place like the subway.
The government restricted financial transactions. 
The authorities started active monitoring of your behaviour on social media, even going so far as to restrict certain media, like violent games. 
It became exponentially more difficult to increase your own trust score, the lower it is. 
People with a very low trust score usually never recovered. 

There was no reason, why her score should have dropped. 
The score was completely implemented via machine learning. 
It worked like a well oiled machine, stabilising society.
Its performance of the score was always closely monitored. 
Machine learning had become A LOT better since the beginning of the century. 
It had become so efficient, that decisions made by the trust score system could not be disputed. 
An infallible system.
She laughed hysterically. 
Infallible system. 
If only. 
The system failed rarely.
But it did fail in edge cases. 
She was an edge case. 
An error of the system. 
From now on, an outcast.
Nobody dared to question the system, it had become to integrated into the government, into society itself to be questioned. 
The same as you are forbidden to form an anti-democratic party in a democratic system, you were not allowed to undermine the trust system. 
The algorithmic trust was the very fabric the societal order was made of. 
For the greater good, rare incorrect trust scorings were accepted silently. 
Hundreds of other prediction systems and databases were feeding into the score, so it was impossible to know what triggered the drop in her score. 
Wild emotions twisted her, most of all terror. 
She vomitted on the floor. 

Her tax affinity system was eventually integrated into the trust system, but she never got to know.



**2030: A medical lab in Switzerland**

"It's definitely not the worst way to die!", Tom trying to find something positive in the tragedy. 
He was removing the machine from the intravenous pole.
"Just the wrong age and reason of death.", Lena added.
"And certainly the with the wrong morphium pump! Just creating more work for us!" Tom complained, while he unscrewed the pumps back side.
After he removed all screws he lifted and put aside the plate and plugged a cable into the diagnostic port. 
"You didn't just complain about having a job, did you?", Lena gave him a mocking smile. 
"Of course not. Never!", he exclaimed with an ironic subnote. 

He booted the pump. 
Lena plugged the other side of the cable into her tablet. 
"Alright, diagnostics are running.", she announced, "I am really curious what went happened."
"It certainly shot our John Doe to the stars. This high concentration of this morphium stuff. I mean ... that's the first mistake of this kind. Usually a faulty pump gives too little of the sweet stuff or sometimes nothing. But never, you know, like the most golden shot on earth.", Tom explained.
"I know. You don't have to convince me ... Hey, look at that.", Lena held up the tablet. 
"See this peek here? That's the painkiller coktails potency, look here is the reference level. The machine pumped the painkiller mix into the poor guys blood system to reach a factor above 17. An here ...", she swiped, "Here you see the moment of the patients demise."
"So, any first ideas what happened Lena?", Tom asked his supervisor. 
"Hm ... The sensory functions seem to be okay. Heart rate, oxygen levels, glucose, ..., the data was collected as expected. Some missing values in the blood oxygen data, but that's not unusual. See here: It also picked up the slowing heart rate of the patient and the extremely low cortisol levels caused by the morphium derivate".
She continued swiping through the diagnostics. 
Tom stared in a daze at the screen. 
It was his first bigger device failure to investigate. 

"Ok, here is our first piece of the puzzle, the system failed to send a warning to the hospital communication channel. The warning was triggered, but rejected on the protocol level. Might be our fault, but could be also the hospitals fault. Please send the logs over to the IT team.", Tom nodded, eyes locked on the screen. Lena continued: "It's weird. The warning should also have triggered the shutdown of the pump. But it obviously failed to do so. That must be a bug. Something the quality team missed. Something really bad. Maybe connected to the protocol issue."
"So, the pumps emergency systems did not work, but why did it pump up our guy in the first place?", Tom wondered. 
"Good question. You are right. Protocol emergency failure aside, the pump shouldn't have administered this amount of medication in the first place. The algorithm should have stopped much earlier on it's own, given this sensory input.", Lena explained. 
"Maybe some bad luck, like a one in a milion thing, like being hit by a lightning?", Tom asked Lena. 
"No Tom.  If you read the documentation I sent you, you should know that the pump was first trained on animal trials, later on humans to learn by itself the perfect amount of pain killer, given the sensory input. The pumps algorithm might be opaque and complex, but it is not random. That means the pump would show the same behaviour given the same input. Our patient would die again. Some combination of sensory inputs must have triggered the erratical behaviour of the pump. That's why we have to dig deep what happened here.", Lena explained. 

"I see ...", Tom responded thoughtfully, "Wasn't the patient going to die soon anyway? Because of cancer or something?". 
Lena nodded while reading the analysis report. 
Tom got up, and walked to the window. 
He looked outside, eyes fixating on some point in the distance. 
"Maybe the machine did him a favor, you know, like free him from the pain. No suffering. Maybe it just did the right thing. Like a lightning, but, you know, a good one. I mean like the lottery, but not random. For a reason.". 
She looked up from her tablet, looking at him. 
He continued watching something outside. 
Nobody said anything for a minute or two. 
Lena lowered her head and continued the analysis. 
"No Tom. It's a bug... Just a goddam bug".




