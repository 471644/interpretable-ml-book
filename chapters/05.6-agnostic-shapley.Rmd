## Explain Predictions with the Shapley Value

Predictions can be explained by assuming that each feature is a 'player' in a game where the prediction is the payout.
The Shapley value is a way how to fairly distribute the 'payout' among the features.


### The general idea

Someone gives you a data point and a machine learning model.
Your task is to find out how much each feature value contributed to the prediction.

For example you get a Random Forest model trained to predict the value of apartments.
For a certain apartment it predicts 300.000 Euros and you need to explain this prediction.
The flat has a size of $50 m^2$, is located on the 2nd floor, with a park nearby and keeping cats in the flat is forbidden (see Figure \@ref(fig:shapley-instance)).
```{r shapley-instance, fig.cap = "The predicted value for a flat is 300.00 Euros. It's a 50 square meter flat on the second floor. There is a park nearby and cats are not allowed. Our goal is to explain how each of these features contributed towards the predicted value of 300k Euros."}
knitr::include_graphics("./images/shapley-instance.png")
```
The average prediction over all apartments is 310.000 Euros.
How much did each feature value contribute to the prediction compared to the average prediction?


The answer is easy for linear regression models:
The effect of each feature is the weight of the feature times the feature value minus the average effect of all apartments:
See also Chapter \@ref(limo).
This works only because of the linearity of the effects and missing feature interactions.

For more complex model we need a different solution.
LIME (see Chapter \@ref(lime)) suggests local models to estimate effects.

Another solution comes from cooperative game theory: the Shapley value (@shapley1953value).
The Shapley value is a method for assigning payouts to players depending on their contribution towards the total payout.
Players cooperate in a coalition and obtain a certain gain from that cooperation.
Players?
Game?
Payout?
"What's the connection to machine learning prediction and interpretability?", you might ask.
The game is the prediction task for a single instance of the dataset.
The gain is the actual prediction for this instance minus the average of all instance's predictions.
The players are the feature values of the instance, which collaborate to receive the gain (predict a certain value).
In our flat example from Figure \@ref(fig:shapley-instance), the feature values park-allowed, cat-forbidden, area-$50m^2$ and floor-2nd worked together to receive a 'payout' of the 300.000 Euro prediction.
Our goal is to explain the difference of the actual prediction (300.000) and the average prediction (310.000):
So we want to explain the -10k difference.
The answer might be:
The great neighbourhood contributed 30k higher value, the size of the flat ($50 m^2$) contributed 10k and the fact that cat's are not allowed contributed -50k.
The contributions add up to the final prediction (= payout) of -10k.
This example shows that the Shapley value can also be negative.

How the hell do we calculate the Shapley value for one feature?

The Shapley value is the average marginal contribution of a feature value over all possible coalitions.
Clear now?
Have a look at Figure \@ref(fig:shapley-instance-intervened) to understand how to compute the contribution of a feature value to one coalition.
```{r shapley-instance-intervened, fig.cap = "We want to assess the contribution of the cat-forbidden feature value when added to a coalition of park-nearby, size-$50m^2$. By randomly drawing the value for the floor feature, we simulate that only the park, cat and size feature are in a coalition. Then we predict the value of the flat with this combination (301.000 Euros). In a second step we remove the cat-forbidden feature value from the coalition and replace it with a random value for the cat allowed/forbidden feature from another flat. Here it was cat-allowed, but note that it could have been cat-allowed again. Again we predict the flat value for this coalition of park-nearby and size-$50m^2$ (320.000 Euros). The contribution of the cat-disallowed feature value was 301.000 Euros - 320.000 Euros = -19.000 Euros. A negative effect. Of course this estimation depends on the sample of the non-participating features, so we should repeat this procedure to get a better estimate. This figure shows the computation of the marginal contribution for one coalition. The Shapley value is the average marginal contribution over all such coalition of feature value."}
knitr::include_graphics("./images/shapley-instance-intervention.png")
```
We repeat this computation for all possible coalition.
When we have many variables it becomes exponentially large, so we have to sample from all possible coalitions.
The Shapley value is the average over all the marginal contributions.
Figure \@ref(fig:shapley-coalitions) shows all possible coalitions for which we compute the marginal contribution of adding cat-forbidden feature value.
```{r shapley-coalitions, fig.cap = "All possible coalitions of feature values that are needed to assess the Shapley value of the cat-forbidden feature value. The first row is the coalition without any feature value. For each of this coalitions we compute the predicted apartment value with and without the cat-forbidden feature value and calculate the difference to get the marginal contribution. The Shapley value is the (weighted) average marginal contribution. We replace the feature values of features not in a coalition with random values from the apartment dataset to get a prediction from the machine learning model."}
knitr::include_graphics("./images/shapley-coalitions.png")
```




### Interpretation and example

The interpretation of the Shapley value is: the feature $x_j$ contributed $\phi_j$ towards the prediction for instance $i$ compared to the average prediction for the dataset.

We use the Shapley value to analyse the predictions of a Random Forest model on the cervical cancer dataset.
Table \@ref{fig:shapley-cervical-instance} shows the feature values for the cancer risk factors of a woman in the dataset.
Figure \@ref(fig:shapley-cervical-plot) visualizes the Shapley values, that are also displayed in Table \@ref(fig:shapley-cervical-table).


```{r shapley-cervical-prepare}
ntree = 30
cervical.x = cervical.data[names(cervical.data) != 'Biopsy']

model <- caret::train(cervical.x,
               cervical.data$Biopsy,
               method = 'rf', ntree=ntree, maximise = FALSE)


instance_indices = 326
x.interest = cervical.x[instance_indices,]

avg.prediction = mean(predict(model, type = 'prob')[,'Cancer'])
actual.prediction = predict(model, newdata = x.interest, type = 'prob')['Cancer']
diff.prediction = actual.prediction - avg.prediction
```


```{r shapley-cervical-instance, fig.cap = sprintf("Instance (day) for which to explain the predicted outcome (%.0f bikes)", actual.prediction)}
kable(t(x.interest))
```

```{r shapley-cervical-plot, fig.cap = sprintf("Feature value contributions for instance %i. With a predicted %.2f, this womans cancer probability is %.2f above average prediction of %.0f. The feature values that increased the probability the most was the number of diagnosed STDs. The feature contributions sum up to the difference of actual and average prediction (%.0f).", instance_indices, actual.prediction,diff.prediction, avg.prediction, diff.prediction)}
# shapley1 = shapley(model, bike.train.x, x.interest = bike.train.x[instance_indices[1],], class = 2) 
shapley2 = shapley(model, cervical.x, x.interest = x.interest, class = 1) 
plot(shapley2) + geom_vline(xintercept=0) + scale_x_continuous("Feature value contribution") +   
  ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction)) 
```
```{r shapley-cervical-table, fig.cap = sprintf("The Shapley values for data point %i from the cervical cancer dataset.", instance_indices)}
kable(shapley2$data())
```



The Shapley value works for both classification (if we get the probabilities) and regression. 
For the bike rental dataset we can also train a Random Forest to predict the number of rented bikes on a given day with its weather conditions and calendaric information.
Figure \@ref(fig:shapley-bike-plot) shows the explanations created for the prediction of a RandomForest on the bike rental dataset for an instance (values displayed in Table \@ref{fig:shapley-bike-instance}). 
The Shapley values can also be summarized in a table, see Table \@ref(fig:shapley-bike-table)


```{r shapley-bike-prepare}
ntree = 30
bike.train.x = bike.data[names(bike.data) != 'cnt']

model <- caret::train(bike.train.x,
               bike.data$cnt,
               method = 'rf', ntree=ntree, maximise = FALSE)


instance_indices = c(295, 285)

avg.prediction = mean(predict(model))
actual.prediction = predict(model, newdata = bike.train.x[instance_indices[2],])
diff.prediction = actual.prediction - avg.prediction
x.interest = bike.train.x[instance_indices[2],]
```


```{r shapley-bike-instance, fig.cap = sprintf("Instance (day) for which to explain the predicted outcome (%.0f bikes)", actual.prediction)}
kable(t(x.interest))
```

```{r shapley-bike-plot, fig.cap = sprintf("Feature value contributions for instance %i. With a predicted %.0f number of rented bikes, this day is %.0f below the average prediction of %.0f. The feature values that had the most negative effects were the weather situation, humidity and the time trend (years since 2011). The Temperature on that day had a positive effect compared to the average prediction. The feature contributions sum up to the difference of actual and average prediction (%.0f).", instance_indices[2], actual.prediction, diff.prediction, avg.prediction, diff.prediction)}
shapley2 = shapley(model, bike.train.x, x.interest = x.interest) 
plot(shapley2) + geom_vline(xintercept=0) + scale_x_continuous("Feature value contribution") +   
  ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction)) 
```

```{r shapley-bike-table, fig.cap = sprintf("The Shapley values for data point %i from the cervical cancer dataset.", instance_indices[2])}
kable(shapley2$data())
```
Ways to misinterpret the Shapley value:
The Shapley value is the average contribution of a feature towards the prediction in different alliances with features.
The Shapley value is NOT the difference in prediction when we would drop the feature from the model.



### In Detail

This Section goes deeper into the definition and computation of the Shapley value for the curious reader.
Skip to Advantages and Disadvantages if you are not interested in the details.

We motivate the Shapley value from a machine learning (or statistics) perspective. 
We want to know the effect each feature has for a prediction of a data point. 
In a linear model, this is easy to answer. 
A linear model $\hat{f}$ has following form:
$$\hat{f}(x_{i\cdot}) = \hat{f}(x_{i1}, \ldots, x_{ip}) = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip} $$
where $x_{i\cdot} is the instance for which we want to compute the feature effects. 
Each $x_{ij}$ is a feature value, with $j \in \{1, \ldots p\}$. 
The $\beta_j$ are the coefficients corresponding to $x_{ij}$. 

The effect $\phi_j$ of feature $x_{ij}$ is on the prediction $\hat{f}(x_{i\cdot})$ is:
$$ \phi_{ij}(\hat{f}) = \beta_j x_{ij} - E(\beta_j X_{\cdot j}) = \beta_j x_{ij} - \beta_j E(X_{\cdot j})$$
where $E_X(\beta_jX_{\cdot j})$ is the mean estimate for feature $X_{\cdot j}$
This is nice!
Now we know how much each feature contributed towards the prediction. 
If we sum up all the feature effects over all features for one instance, the result is: 
$$\sum_{j=1}^p \phi_{ij}(\hat{f}) = \sum_{j=1}^p (\beta_j x_{ij} - E(\beta_j X_{\cdot j})) =   (\beta_0 +\sum_{j=1}^p \beta_j x_{ij}) - (\beta_0 + \sum_{j=1}^p E(\beta_j X_{\cdot j})) = \hat{f}(x_{i\cdot}) - E(\hat{f}(X))$$
This is the predicted value minus the predicted value on average in the dataset. 
This is nice, because we can now explain the difference between the predicted value and the average prediction by contributing it among the features. 
Note that the feature effects $\phi_{ij}$ can also be negative.

Now, can we do the same for other models?
It would be great to have this for any kind of machine learning model, as a model-agnostic tool. 
Since we don't have the $\beta$ from a linear equation in other model types, we need a different solution. 

Help comes from unexpected places: cooperative game theory. 
The Shapley value is a solution for computing feature effects $\phi_{ij}(\hat{f})$ for single predictions for any model $\hat{f}$. 


#### Excursion: The Shapley Value
The original Shapley value is defined via a value function over players in S ($val(S)$). 

The Shapley value of a feature value $x_{i,j}$ is the difference it makes in the payed outcome summed over all possible feature value combinations.
$$ \phi_{ij} (val) = \sum_{S \subseteq \{x_{i1}, \ldots, x_{ip}\} \setminus \{x_{ij}\}} \frac{|S|!\left(p-|S| - 1\right)!}{p!} \left(val\left(S \cup \{x_{ij}\}\right) - val(S)\right)$$

The Shapley value is the only attribution method that satisfies the following properties (which can be seen as a definition of a fair payout):
1. Eficiency: $\sum_{j=1}^p \phi_{ij} = \hat{f}(x_i) - E_X(\hat{f}(X))$. 
The feature effects have to sum up to the difference in prediction between prediction for $x_i$ and the models mean prediction.
1.   Symmetry: If $val(S \cup \{x_{ij}\}) = val(S \cup \{x_{ik}\})$ for all $S \subseteq \{x_{i1}, \ldots, x_{ip}\} \setminus \{x_{ij},x_{ik}\}$, then $\phi_{ij} = \phi_{ik}$. 
$val(S)$ is the prediction for feature values in its argument (marginalised over the other features):
The contribution for two features should be the same if they contribute the same to all possible coalitions.
1.  Dummy: If $val(S \cup \{x_{ij}\}) = val(S)$ for all $S \subseteq \{x_{i1}, \ldots, x_{ip}\}$, then $\phi_{ij} = 0$. 
A feature which does not change the predicted value no matter to which coalition of features it is added should get assigned a contribution of 0.
1.  Additivity: For a game with combined values $v + v'$ the respective Shapley values are $\phi_{ij} + \phi_{ij}'$. 
The additivity axiom has no relevance in the context of feature effects.


An intuitive way to understand the Shapley value is  the following illustration:
The features enter a room in random order.
All features in the room participate in the game (= contribute to the prediction).
The Shapley value $\phi_{ij}$ is the average additional contribution of feature $j$ by joining whatever features already entered the room before, i.e. 
$$\phi_{ij} = \sum_{\text{All orderings}} val(\{\text{features before j}\} \cup x_{ij}) -  val(\{\text{features before j}\})$$


The Shapley value for feature $j$ can be computed with: 
$$ \phi_{ij} (\hat{f}) = \sum_{S \subseteq \{x_{i1}, \ldots, x_{ip}\} \setminus \{x_{ij}\}} \frac{|S|!\left(p-|S| - 1\right)!}{p!} \left(val_{x_i}\left(S \cup \{x_{ij}\}\right) - val_{x_i}(S)\right)$$
where $S$ is a subset of the features used in the model, $x_{i\cdot}$ all feature values of observation $i$ and $p$ the number of features. 
$val_{x_i}(S)$ is the prediction for feature values in set $S$ (marginalised over features not in $S$):
$$val_{x_i}(S) = \int \hat{f}(x_{i1}, \ldots, x_{ip})d\mathbb{P}_{X_{i\cdot} \notin S}   - E_X(\hat{f}(X)) $$
You actually do multiple integrations, for each feature not in $S$. 
One concrete example: 
The machine learning model works on 4 features $\{x_{i1}, x_{i2}, x_{i3}, x_{i4}\}$ and we evaluate $\hat{f}$ for the coalition $S$ consisting of features $x_{i1}$ and $x_{i3}$:
$$val_{x_i}(S) = val_{x_i}(\{x_{i1}, x_{i3}\}) = \int_{\mathbb{R}}  \int_{\mathbb{R}} \hat{f}(x_{i1}, X_{2}, x_{i3}, X_{4})d\mathbb{P}_{X_2,X_4}  - E_X(\hat{f}(X))$$



#### Estimating the Shapley value

The approximate solution for the Shapley value is: 
$$ \hat{\phi}_{ij} =  \frac{1}{m} \sum_{m=1}^M \left(\hat{f}(x^{*+j}) - \hat{f}(x^{*-j})\right) $$
where $x^{* + j}$ is the prediction for $x_{i\cdot}$, but with a random number of features replaced from a random sample from your data $X$, excluding the feature value $x_{ij}$
The x-vector  $x^{* - j}$ is the same as  $x^{* + j}$, with the difference that the value $x_{ij}$ is also replaced by the sampled $x$. 
Each is a kind of Frankenstein-instances, pieced together from two instances. 

The value function $val_{x_i}(S) $ (parameterized by the instance of interest $x_i$) for a coalition of features $S \subseteq \{x^{(i)}_1, \ldots, x^{(i)}_p\}$ is the expected prediction (minus expected value of model) when the values for features in $S$ are taken from instance $x_i$ and for features not in $S$ the values come from the distribution of $X$.

$$val_{x_i}(S) = E\left(\hat{f}|\text{values of features in S taken from }x_i \right) - E\left(\hat{f}\right)$$

The value can be estimated via Monte Carlo sampling by sampling $M$-times from the data on which the machine learning model was trained on from a holdout dataset and averaging the results:
$$\hat{val}_{x_i}(S) = \sum_{i=1}^M \hat{f}(x_{S}^{(i)})- E\left(\hat{f}\right)$$

The $x_{S}^{(i)}$ is defined as an x value where the values for features in $S$ come from a random sample from $X$ and for features not in $S$ from  $x_i$.

The idea to use the Shapley value to measure feature contributions was first presented for classification cases in \citep{kononenko2010efficient} and later generalized for regression, extended for global analysis and some sampling assumptions loosened in \citep{vstrumbelj2011general}.
\citep{vstrumbelj2011general} suggest a sample based strategy to calculate each features contribution towards the prediction of a chosen instance.
The contribution of a feature can be negative and they all sum up to the prediction (minus the overall expected value of the prediction), due to the efficiency property of the Shapley value.

For calculating the Shapley value for one feature, all the possible coalitions (sets) of features have to be evaluated, with and without the feature of interest.
For more than a few features the exact solution to this problem becomes intractable, because the number of possible coalitions increases exponentially by adding more features.
\citep{vstrumbelj2011general} suggest an approximate approach for the calculation, as described in Algorithm \ref{algo:shapley}.
First select an instance of interest $i$ and a feature $j$ and select the number of samples $M$.
For each sample a random instance from the data is chosen and the order of the features is mixed.
From this instance, two new instances are created, by using values from the instance of interest $x$.
The first instance $x_{+j}^*$ has all values in order before and including feature $j$ replaced by features from the instance of interest.
The second instance $x_{-j}^*$ has all the values in order before, but excluding feature $j$, replaced by features from the instance of interest.
The difference in prediction from the black box is computed: $f(x_{+j}^*) - f(x_{-j}^*)$.
All these differences are averaged and result in $\phi_{ij} = \sum_{k=1}^m f(x_{+j}^*) - f(x_{-j}^*)$
Averaging like this automatically weighs samples by the probability distribution of x.

Algorithm: Estimating the Shapley value, the contribution of feature j's contribution towards the difference $\hat{f}(x_{i\cdot}) - \mathbb{E}(\hat{f})$ for instance $x_{i\cdot} \in X$.

- Require: Number of iterations $m$, instance of interest $x$, data $X$, and machine learning model $\hat{f}$
- For all $k \in \{1, \ldots, m\}$:
	- draw random instance $z$ from $X$
	- choose a random permutation of feature $o \in \pi(S)$
	- order instance x: $x_{o} = (x_{o_1}, \ldots, x_{o_j}, \ldots, x_{o_p})$
	- order instance z: $z_{o} = (z_{o_1}, \ldots, z_{o_j}, \ldots, z_{o_p})$
	- construct two new instances
		- $x_{+j}^*= (x_{o_1}, \ldots x_{o_{j-1}}, x_{o_j}, z_{o_{j+1}}, \ldots, z_{o_p})$
		- $x_{-j}^*= (x_{o_1}, \ldots x_{o_{j-1}}, z_{o_j}, z_{o_{j+1}}, \ldots, z_{o_p})$
	- $\phi_j^{(k)} = \hat{f}(x_{+j}^*) - \hat{f}(x_{-j}^*)$
- $\phi_j(x) = \sum_{k=1}^m\frac{\phi_j^{(k)}}{m}$



### Advantages
- The difference between the prediction and the average prediction is fairly distributed among the features values of the instance (efficiency property). 
This property sets the Shapley value also apart from other methods like LIME (Chapter \@ref(lime)). 
LIME does not guarantee to perfectly distribute the effects. 
I see the effeciency property as main selling point for compliance. 
It might make the Shapley value the only method to deliver a full explanation. 
Shapley offers a full causal attribution.
In situations that demand explainability by law (e.g. EU's "right to explanations") the Shapley value might actually be the only compliant method. 
I am not a lawyer, so this reflects only my intuition about the requirements.
- The Shapley value allows contrastive explanations:
Instead of comparing the prediction of the instance of interest with the average prediction, once can compare it to a subsets prediction or even to a single datapoint. 
I can compare the predictions of two data points. 
- The Shapley value is the only explanation method with a solid theory backing it up. 
The theory behind is well developed and the axioms (efficiency, symmetry, dummy, additivity) give the explanation a reasonable foundation.
Methods like LIME assume nice behaviour of the machine learning model locally but there is no theory why this should work or not. 
LIME comes with serious problems, like defining neighbourhood and sampling. 
- Framing prediction as a game played by the instances feature values is also a nice analogy to understand explanations. 


### Disadvantages
- It need a lot of computational power to be computed. 
That means only the approximate solution is feasible in 99.9% of the real world problems.
An accurate computation of the Shapley value is potentially computational expensive, because there are $2^k$ possible coalitions of features and the 'absence' of a feature has to be simulated by drawing random samples, which increases the variance for the estimate $\phi_j$.
The exponentiality of the coalitions is handled by sampling coalitions and fixing the number of samples $m$  \citep{vstrumbelj2011general}.
Decreasing $m$ reduces computation time, but increases the variance of $\phi_j$.
It is unclear how to automatically choose a sensitive $m$.
- The interpretation of the Shapley value can go wrong easily:
The Shapley value $\phi_j$ of a feature $j$ is not the difference in predicted value after the removal of feature $j$.
Removing a feature from a machine learning model is equal to removing its main effect and all interaction effects with other features that include feature $j$.
If $\phi_j$ would be equal to the sum of the main feature effect and all possible interactions, then interactions involving feature $j4$ would be fully attributed to each $\phi_j$.
For example, the interaction effect between feature $j$ and feature $k$ would be attributed to both $\phi_j$ and $\phi_k$.
This would violate the efficiency axiom which says that all effects sum up to the total prediction, because interactions would be summed multiple times.
The interpretation of the Shapley value is rather:
Given the current set of features, the total contribution of the feature $j$ to the difference in the actual prediction  and the mean prediction is $\phi_j$.
Feature interaction effects contributing to the prediction are split up evenly among the features.
- The Shapley value is the wrong explanation method if you seek sparse explanations. 
Humans prefer selective explanations \cite{miller2017explanation}, like LIME produces, so especially for explanations facing lay-persons, LIME might be the better choice for feature effects computation.
- The Shapley value returns a simple value per feature, and not a prediction model like LIME.
This means it can't be used to make statements about changes in the prediction for changes in the input like:
"If I would earn 300 Euro more per year, my credit score should go up by 5 points."




