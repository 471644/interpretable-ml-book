## Shapley value

Single predictions can be explained by assuming that each feature is a 'player' in a game where the prediction is the payout. 
The Shapley value is a way how to fairly distribute the 'payout' among the features. 


### The general idea

We are given a data point, it's prediction from a machine learning model and the task to say how much each feature contributed to the prediction. 
For example someone gives you a Random Forest model trained to predict the value of a flat, which predicts 300.000 Euros for a flat in a great neighbourhood with 50 square meters and having cats in the flat is forbidden. 
How much did each feature contribute to the prediction?

Cooperative game theory provides an answer to this type of question. 
a coalition of players cooperate and obtain a certain gain from that cooperation. 
Cooperative game theory is concerned with how to fairly distribute the gain. 
There are different ways how to define fair. 
The Shapley value is the unique solution how to fairly calculate each players share of the gain (depending on a certain definition of 'fair'). 
The definition of fairness involves the following axioms: 
Assume we have a given set of players and can form different teams and measure their gain from collaboration:
The gain of a team is the payout for this cooperation. 
The gain of a team with zero players is zero. 
The fairness axioms are:
1. The individual payouts for each player have to sum up to the total gain (efficiency)
1. If we add player A or player B to a team and the gain is always the same, no matter which players are in the team, the payout for A and B should be the same. 
1. If adding player A to any team never changes the gain of the team, A's payout should be 0. 
1. When we play to different games and sum up their gain, each players gain of this combined game should be the sum of payouts. 


Players?
Game?
Payout?
"What's the connection to machine learning prediction and interpretability?", you might ask. 
The game is the prediction task for a single instance of the dataset. 
The gain is the actual prediction for this instance minus the average of all instance's predictions. 
The players are the features, which collaborate to receive the gain. 
In our flat example, the square meter feature, the neighbourhood feature and the balcony feature collaborated, resulting in a 'payout' of 300.000 Euro prediction. 
The Shapley value is the value each feature should be given, when we fairly distribute the 300.000 Euro among them. 
Actually we don't distribute the 300.000 Euro, but the difference between the actual prediction (300k) and the average predicted value for all flats in the dataset (let's say 310k). 
So we want to explain the -10k difference. 
The answer might be: 
The great neighbourhood contributed 30k higher value, the size of the flat ($50 m^2$) contributed 10k and the fact that cat's are not allowed contributed -50k. 
The contributions add up to the final prediction (= payout) of -10k. 
What personally took me a while to understand is that the Shapley value for a feature can also be negative.
But that's important, because a feature can also negatively contribute to a prediction, meaning that its presence reduces the predicted value. 
For example a not-so-good neighbourhood might reduce the predicted value of a house.  

So how the hell do we calculate this Shapley value for one feature?

The answer: We form a coalitions of a subset of the features, in one version with the feature of interest and in the other without. 
The we take the difference in both predictions and get the contribution to the gain of the feature of interest for this specific coalition of features. 
Then we do this for all possible combinations of coalitions of features and average the results to get the Shapley value of the feature. 


The first big question I had was how you even calculate the prediction where one or more of the features are not part of the team. 
One answer is: Refit the model with only the feature that are 'playing'. 
Has the big disadvantage to having to refit the model. 
Also it is problematic, since the model will be a different model if you completely remove it. 
That's not what you want. 
The other and preferred option is to marginalise the feature(s), meaning we integrate over all possible values. 
That's what we do for calculating the gain aka prediction of the model for a subset of features: 
For the features that are not in the subset, we average over all possible values. 
We can draw as many samples as we want. 
The more we sample, the better the approximation will be. 
You could also only sample one, then it has a high variance. 

If we have five features and want to calculate the gain of a coalition of feature 1, 4 and 5, we leave the values for those features the same, but draw random values for features 2 and 3 from our dataset. 
Then we average the prediction and see what our model would predict when only the features 1,4 and 5 of this instance would have "collaborated". 

There are a lot of possible coalitions of features, namely 2 to the power of number of features. 
For 2 features that's 4, for 10 features that's 1024 and for 100 features that's a number with thirty zeroes. 
It becomes infeasible to compute all combinations. 
The solution: 
We only sample some of the possible combinations to estimate the Shapley value for a feature. 


There are now two reasons why we sample: 
First, we have to approximate that a feature is not in the coalition by marginalising it, which we do by sampling values from our data. 
Second, we usually can't go over all possible feature combinations, so we sample from possible coalitions. 

For each sampled coalition, we calculate the difference in gain of the coalition with and without the feature of interest. 
When we have done this a few times, we can average the results and get the Shapley value. 


It's also possible to define the Shapley value not compared to the average prediction in the dataset, but with a subset or even with single point. 

### Interpretation and example

The interpretation of the shapley value is: the feature $x_j$ contributed $sh(x_j)$ towards the prediction for instance $i$ compared to the average prediction for the dataset.  

We use the Shapley value to analyse the predictions of a Random Forest model on the cervical cancer dataset. 
Table XXX shows the Shapley values for the instance XXX. 
Figure XXX visualizes the same information. 
TODO: Example of Shapley value for cervical. 

The Shapley values for all features sum up to the difference in predicted value of the instance and the average prediction for the whole dastaset. 


TODO: Example for Shapley on toy data?
TODO: Example for Shapley on bike data
TODO: Example for Shapely on cervical data. 
TODO: Implement Shapley visualisation






### In Detail


TODO: The 4 axioms
TODO: Mathematical formulation of Shapley value

TODO: Estimation algorithm for Shapley value from the paper. 
TODO: Cite the 3 papers (two lundberg + SHAP). 

