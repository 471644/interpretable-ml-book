<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>exML: Explainable machine learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more explainable.">
  <meta name="generator" content="bookdown 0.3.10 and GitBook 2.6.7">

  <meta property="og:title" content="exML: Explainable machine learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more explainable." />
  <meta name="github-repo" content="christophM/exML-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="exML: Explainable machine learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more explainable." />
  

<meta name="author" content="Christoph Molnar">


<meta name="date" content="2017-03-19">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="simple.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-machine-learning-and-why-is-it-important"><i class="fa fa-check"></i><b>1.1</b> What is machine learning and why is it important?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#why-is-explainability-important"><i class="fa fa-check"></i><b>1.2</b> Why is explainability important?</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#who-this-book-is-for"><i class="fa fa-check"></i><b>1.3</b> Who this book is for</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#outline-of-the-book"><i class="fa fa-check"></i><b>1.4</b> Outline of the book</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#is-good-user-experience-enough"><i class="fa fa-check"></i><b>1.5</b> Is good user experience enough?</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#scope-of-explainability"><i class="fa fa-check"></i><b>1.6</b> Scope of explainability</a><ul>
<li class="chapter" data-level="1.6.1" data-path="intro.html"><a href="intro.html#algorithm"><i class="fa fa-check"></i><b>1.6.1</b> Algorithm</a></li>
<li class="chapter" data-level="1.6.2" data-path="intro.html"><a href="intro.html#global-model-explainability"><i class="fa fa-check"></i><b>1.6.2</b> Global model explainability</a></li>
<li class="chapter" data-level="1.6.3" data-path="intro.html"><a href="intro.html#explain-a-single-observation"><i class="fa fa-check"></i><b>1.6.3</b> Explain a single observation</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#measuring-explainability-comprehensibility-interpretability"><i class="fa fa-check"></i><b>1.7</b> Measuring explainability / comprehensibility / interpretability</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#definitions"><i class="fa fa-check"></i><b>1.8</b> Definitions</a></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#terminology"><i class="fa fa-check"></i><b>1.9</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>2</b> Keep it simple</a><ul>
<li class="chapter" data-level="2.1" data-path="simple.html"><a href="simple.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="simple.html"><a href="simple.html#linear-models"><i class="fa fa-check"></i><b>2.2</b> Linear models</a><ul>
<li class="chapter" data-level="2.2.1" data-path="simple.html"><a href="simple.html#interpretation"><i class="fa fa-check"></i><b>2.2.1</b> Interpretation</a></li>
<li class="chapter" data-level="2.2.2" data-path="simple.html"><a href="simple.html#interpretation-example"><i class="fa fa-check"></i><b>2.2.2</b> Interpretation example</a></li>
<li class="chapter" data-level="2.2.3" data-path="simple.html"><a href="simple.html#coding-categorial-variables"><i class="fa fa-check"></i><b>2.2.3</b> Coding categorial variables:</a></li>
<li class="chapter" data-level="2.2.4" data-path="simple.html"><a href="simple.html#explaining-a-single-observation"><i class="fa fa-check"></i><b>2.2.4</b> Explaining a single observation:</a></li>
<li class="chapter" data-level="2.2.5" data-path="simple.html"><a href="simple.html#the-disadvantages-of-linear-models"><i class="fa fa-check"></i><b>2.2.5</b> The disadvantages of linear models</a></li>
<li class="chapter" data-level="2.2.6" data-path="simple.html"><a href="simple.html#linear-models-beyond-gaussian-regression"><i class="fa fa-check"></i><b>2.2.6</b> Linear models beyond gaussian regression</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="simple.html"><a href="simple.html#decision-trees"><i class="fa fa-check"></i><b>2.3</b> Decision trees</a><ul>
<li class="chapter" data-level="2.3.1" data-path="simple.html"><a href="simple.html#explaining-treeinterpreter"><i class="fa fa-check"></i><b>2.3.1</b> Explaining: Treeinterpreter</a></li>
<li class="chapter" data-level="2.3.2" data-path="simple.html"><a href="simple.html#more-general-bayesian-network-approaches"><i class="fa fa-check"></i><b>2.3.2</b> More general bayesian network approache(s)</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="simple.html"><a href="simple.html#interpretation-overview"><i class="fa fa-check"></i><b>2.4</b> Interpretation overview</a></li>
<li class="chapter" data-level="2.5" data-path="simple.html"><a href="simple.html#monotonicity-constraints-between-features"><i class="fa fa-check"></i><b>2.5</b> Monotonicity constraints between features</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Explain the data</a><ul>
<li class="chapter" data-level="3.1" data-path="data.html"><a href="data.html#visualizations"><i class="fa fa-check"></i><b>3.1</b> visualizations</a></li>
<li class="chapter" data-level="3.2" data-path="data.html"><a href="data.html#pca"><i class="fa fa-check"></i><b>3.2</b> PCA</a><ul>
<li class="chapter" data-level="3.2.1" data-path="data.html"><a href="data.html#mds"><i class="fa fa-check"></i><b>3.2.1</b> MDS</a></li>
<li class="chapter" data-level="3.2.2" data-path="data.html"><a href="data.html#t-sne"><i class="fa fa-check"></i><b>3.2.2</b> t-SNE</a></li>
<li class="chapter" data-level="3.2.3" data-path="data.html"><a href="data.html#glyphs"><i class="fa fa-check"></i><b>3.2.3</b> Glyphs</a></li>
<li class="chapter" data-level="3.2.4" data-path="data.html"><a href="data.html#correlation-graphs"><i class="fa fa-check"></i><b>3.2.4</b> Correlation Graphs</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="data.html"><a href="data.html#biases.-source-httpsen.m.wikipedia.orgwikibias_statistics"><i class="fa fa-check"></i><b>3.3</b> Biases. Source: <a href="https://en.m.wikipedia.org/wiki/Bias_(statistics)" class="uri">https://en.m.wikipedia.org/wiki/Bias_(statistics)</a></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html"><i class="fa fa-check"></i><b>4</b> Model-agnostic explanations</a><ul>
<li class="chapter" data-level="4.1" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#global-explain-behaviour-of-system-with-data"><i class="fa fa-check"></i><b>4.1</b> Global: Explain behaviour of system with data</a><ul>
<li class="chapter" data-level="4.1.1" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#global-surrogate-models"><i class="fa fa-check"></i><b>4.1.1</b> Global surrogate models</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#local-explain-single-decisions"><i class="fa fa-check"></i><b>4.2</b> Local: Explain single decisions</a><ul>
<li class="chapter" data-level="4.2.1" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#local-surrogate-models-lime"><i class="fa fa-check"></i><b>4.2.1</b> Local surrogate models (LIME)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="specific.html"><a href="specific.html"><i class="fa fa-check"></i><b>5</b> Model-specific explanations for complex models</a><ul>
<li class="chapter" data-level="5.1" data-path="specific.html"><a href="specific.html#global-explanations"><i class="fa fa-check"></i><b>5.1</b> Global explanations</a><ul>
<li class="chapter" data-level="5.1.1" data-path="specific.html"><a href="specific.html#random-forests-treeinterpreter"><i class="fa fa-check"></i><b>5.1.1</b> Random Forests: Treeinterpreter</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">exML: Explainable machine learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intro" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>Start with motivational examples. Things that went wrong.</p>
<div id="what-is-machine-learning-and-why-is-it-important" class="section level2">
<h2><span class="header-section-number">1.1</span> What is machine learning and why is it important?</h2>
<p>Black box</p>
</div>
<div id="why-is-explainability-important" class="section level2">
<h2><span class="header-section-number">1.2</span> Why is explainability important?</h2>
<p>Machine learning has come to a state where you have to make a trade-off: Do you simply want to know <strong>what</strong> will happen? For example if a client will churn or if medication will work well for a patient. Or do you want to know <strong>why</strong> something will happen and paying for the explainability with accuracy? In some cases you will not care why a decision was made, only the assurance that the accuracy was good on some test set is enough. But in other cases knowing the ‘why’ can help you understand more about the problem, the data and also know why a model might fail.</p>
</div>
<div id="who-this-book-is-for" class="section level2">
<h2><span class="header-section-number">1.3</span> Who this book is for</h2>
<p>This book is for everyone who wants to learn how to make machine learning models more explainable. It is a recommended reading for machine learning practitioners, statisticians, data scientists and everyone who has contact with machine learning applications. It contains one or the other formula, but it’s kept at a manageable level of math. This book is not for people who are trying to learn machine learning from scratch. If you want to learn machine learning, there are loads of books and other resources for learning the basics.</p>
</div>
<div id="outline-of-the-book" class="section level2">
<h2><span class="header-section-number">1.4</span> Outline of the book</h2>
<p>Definition of interpretability / explainability</p>
<p>System is interpretable if it falls into certain class of models, where authors claim that this class is interpretable and the authors present algorithms to optimize within that class. [Towards A Rigorous Science of Interpretable Machine Learning]</p>
<p>There is no good definition of interpretability yet?</p>
<p>Classic statistics usually aims at having interpretable models. That’s why doctors, sociologists and banks tend to go to statisticians with research questions instead of computer scientists.</p>
<p>Some systems need more interpretability. The more impactful the decision, the more explainable it should be. A possibly incorrect product recommendation might is not as bad as a wrong diagnose and recommendation of an inappropriate treatment.</p>
<p>Need for explainability arises when something goes wrong. Because having an explanation for a faulty classification helps to understand the cause of the fault. It delivers a direction for how to fix the system. Consider an example of a husky versus wolf classifier, that missclassifies some huskys as wolfs. If there is an explanation to the classification you can see, that the missclassification happend due to the snow on the image. The classifier learned to use snow as a feature for classifying images as wolfs, which might make sense in terms of separating features in the training data set, but not in the real world use. [TODO: Add image from Ribeiro + ask for permission]</p>
<p>As a machine learning practitioner, your main goal is to drive down the loss function while also keeping the learned model generalizable to other data sets.</p>
</div>
<div id="is-good-user-experience-enough" class="section level2">
<h2><span class="header-section-number">1.5</span> Is good user experience enough?</h2>
<p>Maybe it is enough to show that the algorithm works and does what it is supposed to. The recommended movies are all reasonable and my self-driving car never had an accident.</p>
<p>We have to distinguish between between low and high stakes scenario and individual risks and systematic biases.</p>
<p>For low-risk applications (e.g. product recommender systems) there is not so much damage if something goes wrong. No one will die because they got a product recommendation on Amazon for something they are not interested in. Biggest risk is for the company deploying those algorithms that they do not work and loose customers to competitors. But there is the risk for a systematic bias and while for each individual the impact might be negligible on a group or society level it is quite problematic. Social bubbles through newsfeeds? An example is a restaurant recommender system that would never recommend restaurants to a certain minority, because they are from that minority.</p>
<p>High risk applications are self-driving cars, AI doctors etc. Here it is quite important to have explainability. Only with explainability can you ‘debug’ why a car accident happend by a self-driving car or decide if you want to trust the diagnose of a machine learning algorithm.</p>
</div>
<div id="scope-of-explainability" class="section level2">
<h2><span class="header-section-number">1.6</span> Scope of explainability</h2>
<p>There are basically</p>
<div id="algorithm" class="section level3">
<h3><span class="header-section-number">1.6.1</span> Algorithm</h3>
<p>On this level of explainability, you are only asking how the algorithm learns, what kind of relationships it is capable of picking up. This level of explainability is what each good machine learning practitioner today usually has. If you are using convolutional neural networks for classifying images, you can explain that the algorithm learns edge detectors and filters on the lowest layers. This is an understanding of how the algorithm works, but not of the specific model that is learned in the end and not about how single decisions are reached. For this level of explainability only knowledge about the algorithm and the data is required. This book will talk a little about algorithmic explainability, but will focuse more on global and local explainability.</p>
</div>
<div id="global-model-explainability" class="section level3">
<h3><span class="header-section-number">1.6.2</span> Global model explainability</h3>
<p>To explain the global model output, you need the trained model, knowledge about the algorithm and the data. This level of explainability is about understanding how the model makes the decisions, based on the features. Which features are the important ones and what kind of interactions are happening? Global model explainability helps to understand the distribution of your target variable based on the features.</p>
</div>
<div id="explain-a-single-observation" class="section level3">
<h3><span class="header-section-number">1.6.3</span> Explain a single observation</h3>
<p>You can go all the way down to a single observation and examine what kind of classification or decision the model gives for this input, and why it gives this input. When you zoom in into one example, the conditional distribution of the target variable might behave more nicely. Locally it might depend only linearly or monotonic on some variables rather than having a complex dependency. For example the rent of an appartment might not depend linearly on the size, but if you only look at a specific appartment of 100 square meter and check how the prize changes going up plus and minus 10 square meters there is a chance that this sub region in your data space is linear. Local explanations can be more accurate compared to global explanations because of this.</p>
</div>
</div>
<div id="measuring-explainability-comprehensibility-interpretability" class="section level2">
<h2><span class="header-section-number">1.7</span> Measuring explainability / comprehensibility / interpretability</h2>
<p>Model size is an easy way to measure, but might be too simplistic.</p>
</div>
<div id="definitions" class="section level2">
<h2><span class="header-section-number">1.8</span> Definitions</h2>
<ul>
<li>An <strong>Algorithm</strong> is a set of rules that a machine follows to achieve a particular goal [1]</li>
<li><strong>Machine learning algorithm</strong> is an set of rules that a machine follows to learn how to a achieve a particular goal. The output of a machine learning algorithm is a machine learning model.</li>
<li><strong>(Machine learning) Model</strong> is the outcome of a machine learning algorithm. This can be a set of weights for a linear model or neural network plus the architecture.</li>
<li><strong>Features</strong> are the variables/information used for prediction/classification/clustering.</li>
<li><strong>(machine learning) Task</strong> can be classification, regression, survival analysis, clustering, outlier detection</li>
<li><strong>Instance</strong> One row in the dataset.</li>
</ul>
</div>
<div id="terminology" class="section level2">
<h2><span class="header-section-number">1.9</span> Terminology</h2>
<p>Y is the target variable in supervised settings. X are the features or covariates. w are the weights. $% are regression weights.</p>
<p>[1] <a href="https://www.merriam-webster.com/dictionary/algorithm" class="uri">https://www.merriam-webster.com/dictionary/algorithm</a>, accessed on Feb. 12th</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simple.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
