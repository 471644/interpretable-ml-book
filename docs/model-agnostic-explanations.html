<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>XAI: Explainable artificial intelligence</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more explainable.">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="XAI: Explainable artificial intelligence" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more explainable." />
  <meta name="github-repo" content="christophM/xai-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="XAI: Explainable artificial intelligence" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more explainable." />
  

<meta name="author" content="Christoph Molnar">


<meta name="date" content="2017-04-28">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="data.html">
<link rel="next" href="specific.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#who-is-this-book-for"><i class="fa fa-check"></i><b>1.1</b> Who is this book for?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#what-is-machine-learning-and-why-is-it-important"><i class="fa fa-check"></i><b>1.2</b> What is machine learning and why is it important?</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#why-is-explainability-important"><i class="fa fa-check"></i><b>1.3</b> Why is explainability important?</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#is-a-good-user-experience-enough"><i class="fa fa-check"></i><b>1.4</b> Is a good user experience enough?</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#outline-of-the-book"><i class="fa fa-check"></i><b>1.5</b> Outline of the book</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#scope-of-explainability"><i class="fa fa-check"></i><b>1.6</b> Scope of explainability</a><ul>
<li class="chapter" data-level="1.6.1" data-path="intro.html"><a href="intro.html#algorithm-transparency"><i class="fa fa-check"></i><b>1.6.1</b> Algorithm transparency</a></li>
<li class="chapter" data-level="1.6.2" data-path="intro.html"><a href="intro.html#global-holistic-model-explainability"><i class="fa fa-check"></i><b>1.6.2</b> Global, holistic model explainability</a></li>
<li class="chapter" data-level="1.6.3" data-path="intro.html"><a href="intro.html#global-model-explainability-on-a-modular-level"><i class="fa fa-check"></i><b>1.6.3</b> Global model explainability on a modular level</a></li>
<li class="chapter" data-level="1.6.4" data-path="intro.html"><a href="intro.html#explain-the-decision-for-a-single-instance"><i class="fa fa-check"></i><b>1.6.4</b> Explain the decision for a single instance</a></li>
<li class="chapter" data-level="1.6.5" data-path="intro.html"><a href="intro.html#explain-the-decisions-for-a-group-of-instances"><i class="fa fa-check"></i><b>1.6.5</b> Explain the decisions for a group of instances</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#tool-vs.product"><i class="fa fa-check"></i><b>1.7</b> Tool vs.Â product</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#evaluating-explainability"><i class="fa fa-check"></i><b>1.8</b> Evaluating explainability</a><ul>
<li class="chapter" data-level="1.8.1" data-path="intro.html"><a href="intro.html#approaches-for-evaluation-of-the-explanation-quality"><i class="fa fa-check"></i><b>1.8.1</b> Approaches for evaluation of the explanation quality</a></li>
<li class="chapter" data-level="1.8.2" data-path="intro.html"><a href="intro.html#function-level-evaluation"><i class="fa fa-check"></i><b>1.8.2</b> Function level evaluation</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#definitions"><i class="fa fa-check"></i><b>1.9</b> Definitions</a></li>
<li class="chapter" data-level="1.10" data-path="intro.html"><a href="intro.html#terminology"><i class="fa fa-check"></i><b>1.10</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>2</b> Keep it simple</a><ul>
<li class="chapter" data-level="2.1" data-path="simple.html"><a href="simple.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="simple.html"><a href="simple.html#limo"><i class="fa fa-check"></i><b>2.2</b> Linear models</a><ul>
<li class="chapter" data-level="2.2.1" data-path="simple.html"><a href="simple.html#interpretation"><i class="fa fa-check"></i><b>2.2.1</b> Interpretation</a></li>
<li class="chapter" data-level="2.2.2" data-path="simple.html"><a href="simple.html#interpretation-example"><i class="fa fa-check"></i><b>2.2.2</b> Interpretation example</a></li>
<li class="chapter" data-level="2.2.3" data-path="simple.html"><a href="simple.html#visual-parameter-interpretation"><i class="fa fa-check"></i><b>2.2.3</b> Visual parameter interpretation</a></li>
<li class="chapter" data-level="2.2.4" data-path="simple.html"><a href="simple.html#explaining-single-predictions"><i class="fa fa-check"></i><b>2.2.4</b> Explaining single predictions</a></li>
<li class="chapter" data-level="2.2.5" data-path="simple.html"><a href="simple.html#interpretation-templates"><i class="fa fa-check"></i><b>2.2.5</b> Interpretation templates</a></li>
<li class="chapter" data-level="2.2.6" data-path="simple.html"><a href="simple.html#coding-categorical-variables"><i class="fa fa-check"></i><b>2.2.6</b> Coding categorical variables:</a></li>
<li class="chapter" data-level="2.2.7" data-path="simple.html"><a href="simple.html#assuring-sparsity-in-linear-models"><i class="fa fa-check"></i><b>2.2.7</b> Assuring sparsity in linear models</a></li>
<li class="chapter" data-level="2.2.8" data-path="simple.html"><a href="simple.html#the-disadvantages-of-linear-models"><i class="fa fa-check"></i><b>2.2.8</b> The disadvantages of linear models</a></li>
<li class="chapter" data-level="2.2.9" data-path="simple.html"><a href="simple.html#towards-complexer-relationships-within-linear-model-class"><i class="fa fa-check"></i><b>2.2.9</b> Towards complexer relationships within linear model class</a></li>
<li class="chapter" data-level="2.2.10" data-path="simple.html"><a href="simple.html#linear-models-beyond-gaussian-regression"><i class="fa fa-check"></i><b>2.2.10</b> Linear models beyond gaussian regression</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="simple.html"><a href="simple.html#decision-trees"><i class="fa fa-check"></i><b>2.3</b> Decision trees</a><ul>
<li class="chapter" data-level="2.3.1" data-path="simple.html"><a href="simple.html#interpretation-1"><i class="fa fa-check"></i><b>2.3.1</b> Interpretation</a></li>
<li class="chapter" data-level="2.3.2" data-path="simple.html"><a href="simple.html#interpretation-example-1"><i class="fa fa-check"></i><b>2.3.2</b> Interpretation example</a></li>
<li class="chapter" data-level="2.3.3" data-path="simple.html"><a href="simple.html#advantages"><i class="fa fa-check"></i><b>2.3.3</b> Advantages</a></li>
<li class="chapter" data-level="2.3.4" data-path="simple.html"><a href="simple.html#disadvantages"><i class="fa fa-check"></i><b>2.3.4</b> Disadvantages</a></li>
<li class="chapter" data-level="2.3.5" data-path="simple.html"><a href="simple.html#explaining-treeinterpreter"><i class="fa fa-check"></i><b>2.3.5</b> Explaining: Treeinterpreter</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="simple.html"><a href="simple.html#rulefit"><i class="fa fa-check"></i><b>2.4</b> RuleFit</a></li>
<li class="chapter" data-level="2.5" data-path="simple.html"><a href="simple.html#decision-rules"><i class="fa fa-check"></i><b>2.5</b> Decision rules</a><ul>
<li class="chapter" data-level="2.5.1" data-path="simple.html"><a href="simple.html#association-rule-mining"><i class="fa fa-check"></i><b>2.5.1</b> Association rule mining</a></li>
<li class="chapter" data-level="2.5.2" data-path="simple.html"><a href="simple.html#rule-induction"><i class="fa fa-check"></i><b>2.5.2</b> Rule induction</a></li>
<li class="chapter" data-level="2.5.3" data-path="simple.html"><a href="simple.html#other-algorithms"><i class="fa fa-check"></i><b>2.5.3</b> Other algorithms</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="simple.html"><a href="simple.html#interpretation-overview"><i class="fa fa-check"></i><b>2.6</b> Interpretation overview</a></li>
<li class="chapter" data-level="2.7" data-path="simple.html"><a href="simple.html#monotonicity-constraints-between-features"><i class="fa fa-check"></i><b>2.7</b> Monotonicity constraints between features</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Explain the data</a><ul>
<li class="chapter" data-level="3.1" data-path="data.html"><a href="data.html#visualizations"><i class="fa fa-check"></i><b>3.1</b> visualizations</a></li>
<li class="chapter" data-level="3.2" data-path="data.html"><a href="data.html#pca"><i class="fa fa-check"></i><b>3.2</b> PCA</a><ul>
<li class="chapter" data-level="3.2.1" data-path="data.html"><a href="data.html#mds"><i class="fa fa-check"></i><b>3.2.1</b> MDS</a></li>
<li class="chapter" data-level="3.2.2" data-path="data.html"><a href="data.html#t-sne"><i class="fa fa-check"></i><b>3.2.2</b> t-SNE</a></li>
<li class="chapter" data-level="3.2.3" data-path="data.html"><a href="data.html#glyphs"><i class="fa fa-check"></i><b>3.2.3</b> Glyphs</a></li>
<li class="chapter" data-level="3.2.4" data-path="data.html"><a href="data.html#correlation-graphs"><i class="fa fa-check"></i><b>3.2.4</b> Correlation Graphs</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="data.html"><a href="data.html#biases.-source-httpsen.m.wikipedia.orgwikibias_statistics"><i class="fa fa-check"></i><b>3.3</b> Biases. Source: <a href="https://en.m.wikipedia.org/wiki/Bias_(statistics)" class="uri">https://en.m.wikipedia.org/wiki/Bias_(statistics)</a></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html"><i class="fa fa-check"></i><b>4</b> Model-agnostic explanations</a><ul>
<li class="chapter" data-level="4.1" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#justification-narrative-structure-for-classification"><i class="fa fa-check"></i><b>4.1</b> Justification narrative structure for classification</a><ul>
<li class="chapter" data-level="4.1.1" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#example-justification-narratives-with-the-vehicle-data-set"><i class="fa fa-check"></i><b>4.1.1</b> Example justification narratives with the vehicle data set</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#global-explain-the-behaviour-of-a-model"><i class="fa fa-check"></i><b>4.2</b> Global: Explain the behaviour of a model</a><ul>
<li class="chapter" data-level="4.2.1" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#global-surrogate-models"><i class="fa fa-check"></i><b>4.2.1</b> Global surrogate models</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#local-explain-a-single-decisions"><i class="fa fa-check"></i><b>4.3</b> Local: Explain a single decisions</a><ul>
<li class="chapter" data-level="4.3.1" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#local-surrogate-models-lime"><i class="fa fa-check"></i><b>4.3.1</b> Local surrogate models (LIME)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#explanation-types"><i class="fa fa-check"></i><b>4.4</b> Explanation types</a><ul>
<li class="chapter" data-level="4.4.1" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#structured-output"><i class="fa fa-check"></i><b>4.4.1</b> Structured output</a></li>
<li class="chapter" data-level="4.4.2" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#viz-explanation"><i class="fa fa-check"></i><b>4.4.2</b> Visualization</a></li>
<li class="chapter" data-level="4.4.3" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#natural-language-narratives"><i class="fa fa-check"></i><b>4.4.3</b> Natural language (narratives)</a></li>
<li class="chapter" data-level="4.4.4" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#examples-and-prototypes"><i class="fa fa-check"></i><b>4.4.4</b> Examples and prototypes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="specific.html"><a href="specific.html"><i class="fa fa-check"></i><b>5</b> Model-specific explanations for complex models</a><ul>
<li class="chapter" data-level="5.1" data-path="specific.html"><a href="specific.html#global-explanations"><i class="fa fa-check"></i><b>5.1</b> Global explanations</a><ul>
<li class="chapter" data-level="5.1.1" data-path="specific.html"><a href="specific.html#random-forests-treeinterpreter"><i class="fa fa-check"></i><b>5.1.1</b> Random Forests: Treeinterpreter</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">XAI: Explainable artificial intelligence</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="model-agnostic-explanations" class="section level1">
<h1><span class="header-section-number">4</span> Model-agnostic explanations</h1>
<p>Separating the explanations from the machine learning model (= model-agnostic explanations) gives some benefits. The big advantage of model-agnostic vs model-specific explanation algorithm is the flexibility. When the explanation system is independently applicable even when the underlying model is switched, it frees the practitioner to use different machine learning models without restrictions. It is also more efficient to build interfaces on top of model-agnostic systems, because this has to be done only once and not for each model-specific explanation system. Usually not one but many types of machine learning models are tested in development time and if you want to compare the models in terms of interpretability this is easier with model-agnostic explanations because the system is the same for both models that are being compared <span class="citation">(Ribeiro, Singh, and Guestrin <a href="#ref-Ribeiro2016b">2016</a>)</span>.</p>
<p>The alternatives are either using only interpretable models as introduced in Chapter 2, which has the big disadvantage to usually loose accuracy compared to other approaches. The other alternative is to use more flexible model classes that come with built in explanations. The drawback here is that it ties you to this one algorithm and it will be hard to switch to something else.</p>
<p>Desirable aspects of a model-agnostic explanation system <span class="citation">(Ribeiro, Singh, and Guestrin <a href="#ref-Ribeiro2016b">2016</a>)</span>: - Model flexibility: Not being tied to an underlying particular machine learning model. The method should work for random forests as well as convolutional neural networks - Explanation flexibility: Not being tied to a certain form of explanation. In some cases it might be useful to have a linear formula in other cases some decision rules - Representation flexibility: The explanation system should not have to use the same feature representation as the model that is being explained. So when a text classifier uses abstract word embedding vectors, it might be preferable to use the presence of single words for the explanation.</p>
<div id="justification-narrative-structure-for-classification" class="section level2">
<h2><span class="header-section-number">4.1</span> Justification narrative structure for classification</h2>
<p>See also Paper. Idea: Per feature only use effect and importance. This method is per se model-agnostic, but you need a method for computing effect and importance, which is different for each model class.</p>
<p>The <strong>effect</strong> of a feature is how much the feature contributed towards (or against) a classification to a certain category for an instance. In case of a linear model it is simply the j-th weight times the feature value for observation i: <span class="math inline">\(\beta_{j} x_{ij}\)</span>. For classification it is class specific (class k): <span class="math inline">\(eff_{ji} = \beta_{kj} x_{ij}\)</span></p>
<p>The <strong>importance</strong> of a feature is defined as the overall strength of a feature within the model. So it is the expected effect of feature j for a particular class. The formula for importance of feature j towards class k is: <span class="math inline">\(imp_{ji} = \beta_{ji} \frac{\sum_{x \in X^j} x_{i}}{|X^j|}\)</span>, where <span class="math inline">\(X^j\)</span> is the set of all instances which have class j. Note that the polarity of a feature (<span class="math inline">\(=sign(\beta_{j})\)</span>) might be different from the importance, for example when the weight is negative and also the associated feature is negative for most cases in class k.</p>
<p><strong>Narrative role</strong> of a feature for the classification of an instance depends on effect and importance.</p>
<p>Step 1: Decide what magnitude of importance can be seen as high and separate into low and high. This can be done by applying a fixed threshold or keeping a fixed number of features or some kind of âellbow criteriumâ. The absolute magnitude has to be considered because importance comes both from features that count towards and against a class.</p>
<table>
<colgroup>
<col width="17%" />
<col width="23%" />
<col width="27%" />
<col width="31%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Importance Â Effect</th>
<th align="left">High positive</th>
<th align="left">Low</th>
<th align="left">High negative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">High positive</td>
<td align="left">Normal evidence</td>
<td align="left">Missing evidence</td>
<td align="left">Contrarian counter-evidence</td>
</tr>
<tr class="even">
<td align="left">Low</td>
<td align="left">Exceptional evidence</td>
<td align="left">Negligible</td>
<td align="left">Exceptional counter-evidence</td>
</tr>
<tr class="odd">
<td align="left">High negative</td>
<td align="left">Contrarian evidence</td>
<td align="left">Missing counter-evidence</td>
<td align="left">Normal counter-evidence</td>
</tr>
</tbody>
</table>
<p>Contrarian evidene and contrarian counter-evidence is only possible with negative features.</p>
<p>You should mean center the features, otherwise the importance and the effects will very much look the same (unless the means between the classes vary greatly). The importance and effects are dependent on the scale of your features, but it should not matter whether the a feature is measured in meters or in inch (you should use meter of course) or if it is visits per hour or per minute.</p>
<p>Textual template: TODO</p>
<div id="example-justification-narratives-with-the-vehicle-data-set" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Example justification narratives with the vehicle data set</h3>
<p>The Vehicle dastaset contains the silhoutte descriptions of four types of vehicles. Different features are extracted from the silhouettes from different angles. The four classes are bus, opel, saab and van, but for the purpose of illustration we only focus on the task classifying bus vs.Â not bus given the silhoutte features. The dataset contains 846 cars with 18 silhoutte features.</p>
<table>
<caption><span id="tab:unnamed-chunk-6">Table 4.1: </span>Feature importances for the class âbusâ</caption>
<thead>
<tr class="header">
<th align="left">Feature</th>
<th align="left">Description</th>
<th align="right">Importance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Comp</td>
<td align="left">Compactness</td>
<td align="right">0.3641786</td>
</tr>
<tr class="even">
<td align="left">Circ</td>
<td align="left">Circularity</td>
<td align="right">-0.0168379</td>
</tr>
<tr class="odd">
<td align="left">D.Circ</td>
<td align="left">Distance Circularity</td>
<td align="right">1.3298154</td>
</tr>
<tr class="even">
<td align="left">Rad.Ra</td>
<td align="left">Radius ratio</td>
<td align="right">1.9065983</td>
</tr>
<tr class="odd">
<td align="left">Pr.Axis.Ra</td>
<td align="left">pr.axis aspect ratio</td>
<td align="right">3.1085208</td>
</tr>
<tr class="even">
<td align="left">Max.L.Ra</td>
<td align="left">max.length aspect ratio</td>
<td align="right">0.5426764</td>
</tr>
<tr class="odd">
<td align="left">Scat.Ra</td>
<td align="left">scatter ratio</td>
<td align="right">-0.2893669</td>
</tr>
<tr class="even">
<td align="left">Elong</td>
<td align="left">elongatedness</td>
<td align="right">1.6277211</td>
</tr>
<tr class="odd">
<td align="left">Pr.Axis.Rect</td>
<td align="left">pr.axis rectangularity</td>
<td align="right">0.0158149</td>
</tr>
<tr class="even">
<td align="left">Max.L.Rect</td>
<td align="left">max.length rectangularity</td>
<td align="right">-0.1105358</td>
</tr>
<tr class="odd">
<td align="left">Sc.Var.Maxis</td>
<td align="left">scaled variance along major axis</td>
<td align="right">0.4457976</td>
</tr>
<tr class="even">
<td align="left">Sc.Var.maxis</td>
<td align="left">scaled variance along minor axis</td>
<td align="right">0.6233814</td>
</tr>
<tr class="odd">
<td align="left">Ra.Gyr</td>
<td align="left">scaled radius of gyration</td>
<td align="right">0.1950721</td>
</tr>
<tr class="even">
<td align="left">Skew.Maxis</td>
<td align="left">skewness about major axis</td>
<td align="right">-1.2873672</td>
</tr>
<tr class="odd">
<td align="left">Skew.maxis</td>
<td align="left">skewness about minor axis</td>
<td align="right">0.4024018</td>
</tr>
<tr class="even">
<td align="left">Kurt.maxis</td>
<td align="left">kurtosis about minor axis</td>
<td align="right">-0.4525923</td>
</tr>
<tr class="odd">
<td align="left">Kurt.Maxis</td>
<td align="left">kurtosis about major axis</td>
<td align="right">-2.0461912</td>
</tr>
<tr class="even">
<td align="left">Holl.Ra</td>
<td align="left">hollows ratio</td>
<td align="right">5.9133060</td>
</tr>
</tbody>
</table>
<p><img src="_main_files/figure-html/unnamed-chunk-6-1.png" width="672" /><img src="_main_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
</div>
</div>
<div id="global-explain-the-behaviour-of-a-model" class="section level2">
<h2><span class="header-section-number">4.2</span> Global: Explain the behaviour of a model</h2>
<div id="global-surrogate-models" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Global surrogate models</h3>
<p>A surrogate model is a simple, explainable model that explains another complex machine learning model. Models in [#simple] are viable candidates. ### Partial dependency plots Partial dependency plots show the relationship between the target and one or more features by averaging out all the other features. A dependency plot can show if the relationship between target and feature is linear, monotonic or something else. Are only partially global: It is global because it takes into account all instances, but it is local in the feature, because partial dependency plots only examine one variable, as the name suggests. ### Individual conditional expectation (ICE) plot ### Variable importance ### LOCO (Leave-One-Covariate-Out) ### Interactions ### Residual analysis A residual value is the difference of the models prediction and the actual value. ### Confusion matrix ### Sensitivity analysis of predictions Testing the stability of the model predictions/classifications using simulated data. It can help to trust the model to not be instable in certain settings.</p>
</div>
</div>
<div id="local-explain-a-single-decisions" class="section level2">
<h2><span class="header-section-number">4.3</span> Local: Explain a single decisions</h2>
<div id="local-surrogate-models-lime" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Local surrogate models (LIME)</h3>
<p>LIME is also a surrogate model, but it is a local one. ### Model explanation system From paper: <span class="citation">(Turner <a href="#ref-Turner2015">2015</a>)</span> Classifier <span class="math inline">\(f\)</span>, that takes feature vector $ x X = ^D$. Letâs call the explanation âExâ. Applied only on binary classification. Properties: - Eglibility: An explanation is called eligible if <span class="math inline">\(P(f(x) = 1 | Ex(x) = 1) \geq P(f(x) = 1)\)</span> - Generality (or recall): Probability that the explanation is true <span class="math inline">\(P(Ex(x) = 1 | f(x) = 1)\)</span> - Accuracy (or precision) of explanation: Probability the classifier is correct, given the explanation is correct <span class="math inline">\(P(f(x) = 1 | Ex(x) = 1)\)</span> - Validity of explanation: An explanation is valid at x if it is eliglbe and true at x <span class="math inline">\((E(x)=1)\)</span> ## Maximum activation analysis ### LOCO (Leave-One-Covariate-Out) also local ### Max points lost Compare individual prediction with âidealâ case (maximum probability) in terms of points lost per feature. Only works with monotonicity. And ideal case candidate maxes out each feature regarding highest probability of interest. The feature in which the instance is farthest away from the ideal case is the most negative point, why it should not be in class of interest. Feature with point closest to ideal is the least negative reason.</p>
</div>
</div>
<div id="explanation-types" class="section level2">
<h2><span class="header-section-number">4.4</span> Explanation types</h2>
<div id="structured-output" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Structured output</h3>
<p>Regression tables, decision tree plots. There are overlaps with []#viz-explanation]</p>
</div>
<div id="viz-explanation" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Visualization</h3>
<p>Easy to understand Visualizations. First choice in image classification tasks.</p>
</div>
<div id="natural-language-narratives" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Natural language (narratives)</h3>
<p>Thatâs what humans usually do.</p>
</div>
<div id="examples-and-prototypes" class="section level3">
<h3><span class="header-section-number">4.4.4</span> Examples and prototypes</h3>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Ribeiro2016b">
<p>Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. âModel-Agnostic Interpretability of Machine Learning.â <em>ICML Workshop on Human Interpretability in Machine Learning</em>, no. Whi.</p>
</div>
<div id="ref-Turner2015">
<p>Turner, Ryan. 2015. âA Model Explanation System.â <em>NIPS Workshop</em> 0: 1â5. <a href="http://www.blackboxworkshop.org/pdf/Turner2015{\_}MES.pdf" class="uri">http://www.blackboxworkshop.org/pdf/Turner2015{\_}MES.pdf</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="specific.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
