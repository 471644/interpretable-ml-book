<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>XAI: Explainable artificial intelligence</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more explainable.">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="XAI: Explainable artificial intelligence" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more explainable." />
  <meta name="github-repo" content="christophM/xai-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="XAI: Explainable artificial intelligence" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more explainable." />
  

<meta name="author" content="Christoph Molnar">


<meta name="date" content="2017-05-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="the-bigger-picture.html">
<link rel="next" href="evaluating-explainability.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>



</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="who-should-read-this-book.html"><a href="who-should-read-this-book.html"><i class="fa fa-check"></i><b>1.1</b> Who should read this book</a></li>
<li class="chapter" data-level="1.2" data-path="outline.html"><a href="outline.html"><i class="fa fa-check"></i><b>1.2</b> Outline</a></li>
<li class="chapter" data-level="1.3" data-path="what-is-machine-learning-and-why-is-it-important.html"><a href="what-is-machine-learning-and-why-is-it-important.html"><i class="fa fa-check"></i><b>1.3</b> What is machine learning and why is it important?</a></li>
<li class="chapter" data-level="1.4" data-path="definitions.html"><a href="definitions.html"><i class="fa fa-check"></i><b>1.4</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="explainability.html"><a href="explainability.html"><i class="fa fa-check"></i><b>2</b> Explainability</a><ul>
<li class="chapter" data-level="2.1" data-path="what-is-explainability.html"><a href="what-is-explainability.html"><i class="fa fa-check"></i><b>2.1</b> What is explainability</a></li>
<li class="chapter" data-level="2.2" data-path="when-is-explainability-important.html"><a href="when-is-explainability-important.html"><i class="fa fa-check"></i><b>2.2</b> When is explainability important?</a></li>
<li class="chapter" data-level="2.3" data-path="the-bigger-picture.html"><a href="the-bigger-picture.html"><i class="fa fa-check"></i><b>2.3</b> The bigger picture</a></li>
<li class="chapter" data-level="2.4" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html"><i class="fa fa-check"></i><b>2.4</b> Scope of explainability</a><ul>
<li class="chapter" data-level="2.4.1" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#algorithm-transparency"><i class="fa fa-check"></i><b>2.4.1</b> Algorithm transparency</a></li>
<li class="chapter" data-level="2.4.2" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#global-holistic-model-explainability"><i class="fa fa-check"></i><b>2.4.2</b> Global, holistic model explainability</a></li>
<li class="chapter" data-level="2.4.3" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#global-model-explainability-on-a-modular-level"><i class="fa fa-check"></i><b>2.4.3</b> Global model explainability on a modular level</a></li>
<li class="chapter" data-level="2.4.4" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#explain-the-decision-for-a-single-instance"><i class="fa fa-check"></i><b>2.4.4</b> Explain the decision for a single instance</a></li>
<li class="chapter" data-level="2.4.5" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#explain-the-decisions-for-a-group-of-instances"><i class="fa fa-check"></i><b>2.4.5</b> Explain the decisions for a group of instances</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="evaluating-explainability.html"><a href="evaluating-explainability.html"><i class="fa fa-check"></i><b>2.5</b> Evaluating explainability</a><ul>
<li class="chapter" data-level="2.5.1" data-path="evaluating-explainability.html"><a href="evaluating-explainability.html#approaches-for-evaluation-of-the-explanation-quality"><i class="fa fa-check"></i><b>2.5.1</b> Approaches for evaluation of the explanation quality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>3</b> Simple, interpretable models</a><ul>
<li class="chapter" data-level="3.1" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>3.1</b> Terminology</a></li>
<li class="chapter" data-level="3.2" data-path="the-dataset-speed-dating.html"><a href="the-dataset-speed-dating.html"><i class="fa fa-check"></i><b>3.2</b> The dataset: speed dating</a></li>
<li class="chapter" data-level="3.3" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>3.3</b> Overview</a></li>
<li class="chapter" data-level="3.4" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>3.4</b> Linear models</a><ul>
<li class="chapter" data-level="3.4.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>3.4.1</b> Interpretation</a></li>
<li class="chapter" data-level="3.4.2" data-path="limo.html"><a href="limo.html#interpretation-example"><i class="fa fa-check"></i><b>3.4.2</b> Interpretation example</a></li>
<li class="chapter" data-level="3.4.3" data-path="limo.html"><a href="limo.html#interpretation-templates"><i class="fa fa-check"></i><b>3.4.3</b> Interpretation templates</a></li>
<li class="chapter" data-level="3.4.4" data-path="limo.html"><a href="limo.html#visual-parameter-interpretation"><i class="fa fa-check"></i><b>3.4.4</b> Visual parameter interpretation</a></li>
<li class="chapter" data-level="3.4.5" data-path="limo.html"><a href="limo.html#explaining-single-predictions"><i class="fa fa-check"></i><b>3.4.5</b> Explaining single predictions</a></li>
<li class="chapter" data-level="3.4.6" data-path="limo.html"><a href="limo.html#coding-categorical-variables"><i class="fa fa-check"></i><b>3.4.6</b> Coding categorical variables:</a></li>
<li class="chapter" data-level="3.4.7" data-path="limo.html"><a href="limo.html#assuring-sparsity-in-linear-models"><i class="fa fa-check"></i><b>3.4.7</b> Assuring sparsity in linear models</a></li>
<li class="chapter" data-level="3.4.8" data-path="limo.html"><a href="limo.html#the-disadvantages-of-linear-models"><i class="fa fa-check"></i><b>3.4.8</b> The disadvantages of linear models</a></li>
<li class="chapter" data-level="3.4.9" data-path="limo.html"><a href="limo.html#towards-complexer-relationships-within-linear-model-class"><i class="fa fa-check"></i><b>3.4.9</b> Towards complexer relationships within linear model class</a></li>
<li class="chapter" data-level="3.4.10" data-path="limo.html"><a href="limo.html#linear-models-beyond-gaussian-regression"><i class="fa fa-check"></i><b>3.4.10</b> Linear models beyond gaussian regression</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>3.5</b> Decision trees</a><ul>
<li class="chapter" data-level="3.5.1" data-path="decision-trees.html"><a href="decision-trees.html#interpretation-1"><i class="fa fa-check"></i><b>3.5.1</b> Interpretation</a></li>
<li class="chapter" data-level="3.5.2" data-path="decision-trees.html"><a href="decision-trees.html#interpretation-example-1"><i class="fa fa-check"></i><b>3.5.2</b> Interpretation example</a></li>
<li class="chapter" data-level="3.5.3" data-path="decision-trees.html"><a href="decision-trees.html#advantages"><i class="fa fa-check"></i><b>3.5.3</b> Advantages</a></li>
<li class="chapter" data-level="3.5.4" data-path="decision-trees.html"><a href="decision-trees.html#disadvantages"><i class="fa fa-check"></i><b>3.5.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="other-simple-explainable-models.html"><a href="other-simple-explainable-models.html"><i class="fa fa-check"></i><b>3.6</b> Other simple, explainable models</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html"><i class="fa fa-check"></i><b>4</b> Model-agnostic explanations</a><ul>
<li class="chapter" data-level="4.1" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html"><i class="fa fa-check"></i><b>4.1</b> Global: Explain the behaviour of a model</a><ul>
<li class="chapter" data-level="4.1.1" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html#global-surrogate-models"><i class="fa fa-check"></i><b>4.1.1</b> Global surrogate models</a></li>
<li class="chapter" data-level="4.1.2" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html#partial-dependency-plots"><i class="fa fa-check"></i><b>4.1.2</b> Partial dependency plots</a></li>
<li class="chapter" data-level="4.1.3" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html#variable-importance"><i class="fa fa-check"></i><b>4.1.3</b> Variable importance</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="local-explain-a-single-decisions.html"><a href="local-explain-a-single-decisions.html"><i class="fa fa-check"></i><b>4.2</b> Local: Explain a single decisions</a><ul>
<li class="chapter" data-level="4.2.1" data-path="local-explain-a-single-decisions.html"><a href="local-explain-a-single-decisions.html#local-surrogate-models-lime"><i class="fa fa-check"></i><b>4.2.1</b> Local surrogate models (LIME)</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="explanation-types.html"><a href="explanation-types.html"><i class="fa fa-check"></i><b>4.3</b> Explanation types</a><ul>
<li class="chapter" data-level="4.3.1" data-path="explanation-types.html"><a href="explanation-types.html#structured-output"><i class="fa fa-check"></i><b>4.3.1</b> Structured output</a></li>
<li class="chapter" data-level="4.3.2" data-path="explanation-types.html"><a href="explanation-types.html#viz-explanation"><i class="fa fa-check"></i><b>4.3.2</b> Visualization</a></li>
<li class="chapter" data-level="4.3.3" data-path="explanation-types.html"><a href="explanation-types.html#natural-language-narratives"><i class="fa fa-check"></i><b>4.3.3</b> Natural language (narratives)</a></li>
<li class="chapter" data-level="4.3.4" data-path="explanation-types.html"><a href="explanation-types.html#examples-and-prototypes"><i class="fa fa-check"></i><b>4.3.4</b> Examples and prototypes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>5</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">XAI: Explainable artificial intelligence</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="scope-of-explainability" class="section level2">
<h2><span class="header-section-number">2.4</span> Scope of explainability</h2>
<p>An algorithm trains a model, which produces the predictions. Each step can be evaluated in terms of transparency or explainability.</p>
<div id="algorithm-transparency" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Algorithm transparency</h3>
<p>How does the algorithm create the model?</p>
<p>Algorithm transparency is about how the algorithm learns a model from data and what kind of relationships it is capable of picking up. If you are using convolutional neural networks for classifying images, you can explain that the algorithm learns edge detectors and filters on the lowest layers. This is an understanding of how the algorithm works, but not of the specific model that is learned in the end and not about how single predictions are made. For this level of transparency only knowledge about the algorithm and not about the data or concrete learned models are required. This book focuses on model explainability. Algorithms like the least squares method for linear models are well studied and understood. They score high in transparency. Deep learning approaches (pushing a gradient through a network with millions of weights) are less understood and the inner workings are in the focus on-going research. It is not clear how they exactly work, so they are less transparent.</p>
</div>
<div id="global-holistic-model-explainability" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Global, holistic model explainability</h3>
<p>How does the trained model make predictions?</p>
<p>You could call a model explainable if you can comprehend the whole model at once <span class="citation">(Lipton <a href="#ref-Lipton2016">2016</a>)</span>. To explain the global model output, you need the trained model, knowledge about the algorithm and the data. This level of explainability is about understanding how the model makes the decisions, based on a holistic few on it’s features and each learned components like weights, parameters and structures. Which features are the important ones and what kind of interactions are happening? Global model explainability helps to understand the distribution of your target variable based on the features. Arguably, global model explainability is very hard to achieve in practice. Any model that exceeds a handful of parameters or weights, probably won’t fit an average human’s brain capacity. I’d argue that you cannot really imagine a linear model with 5 features and draw in your head the hyperplane that the was estimated in the 5-dimensional feature space. Each feature space with more than 3 dimensions is just not imaginable for humans. Usually when people try to comprehend a model, they look at parts of it, like the weights in linear models.</p>
</div>
<div id="global-model-explainability-on-a-modular-level" class="section level3">
<h3><span class="header-section-number">2.4.3</span> Global model explainability on a modular level</h3>
<p>How do parts of the model influence predictions?</p>
<p>You might not be able to comprehend a naive bayes model with many hundred features, because there is no way you could hold all the feature weights in your brain’s working memory. But you can understand a single weight easily. Not many models are explainable on a strict parameter level.While global model explainability is usually out of reach, there is a better chance to understand at least some models on a modular level. In the case of linear models parts to understand are the weights and the distribution of the features, for trees it would be splits (used feature and cut-off point) and leaf node predictions. Linear models for example look like they would be, but the interpretation of a single weight is interlocked with all of the other weights. As you will see in Chapter [#limo], the interpretation of a single weight always comes with the footnote that the other input features stay at the same way value, which is not the case in many real world applications. A linear model predicting the rent of a flat, which takes into account both the size of the flat and the number of rooms might have a negative weight for the rooms feature, which is counter intuitive. But it can happen, because there is already the highly correlated flat size feature and in a market where people prefer bigger rooms, a flat with more rooms might be worth less than a flat with more rooms and same square meters. The weights only make sense in the light of the other features used in the model. But arguably a linear models weights still have better explainability than the weights of a deep neural network.</p>
</div>
<div id="explain-the-decision-for-a-single-instance" class="section level3">
<h3><span class="header-section-number">2.4.4</span> Explain the decision for a single instance</h3>
<p>Why did the model make a specific decision for an instance?</p>
<p>You can go all the way down to a single observation and examine what kind of classification or decision the model gives for this input, and why it made this decision. When you look at one example, the local distribution of the target variable might behave more nicely. Locally it might depend only linearly or monotonic on some variables rather than having a complex dependency on the features. For example the rent of an apartment might not depend linearly on the size, but if you only look at a specific apartment of 100 square meter and check how the prize changes going up plus and minus 10 square meters there is a chance that this sub region in your data space is linear. Local explanations can be more accurate compared to global explanations because of this.</p>
</div>
<div id="explain-the-decisions-for-a-group-of-instances" class="section level3">
<h3><span class="header-section-number">2.4.5</span> Explain the decisions for a group of instances</h3>
<p>Why did the model make specific decisions for a group of instances?</p>
<p>The model output for multiple instances can be explained by using methods for global model explainability and single instance explanations. The global methods can be applied by taking the group of observations pretending it’s the complete dataset and using the global methods on this subset. The single explanation methods can be used on each instance and listed or aggregated afterwards for the whole group.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Lipton2016">
<p>Lipton, Zachary C. 2016. “The Mythos of Model Interpretability.” <em>ICML Workshop on Human Interpretability in Machine Learning</em>, no. Whi.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-bigger-picture.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="evaluating-explainability.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
