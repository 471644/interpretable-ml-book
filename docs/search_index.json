[
["index.html", "exML: Explaining decisions of machine learning algorithm Preface", " exML: Explaining decisions of machine learning algorithm Christoph Molnar 2017-03-18 Preface In this book, we will talk about explainable machine learning. What is explainable machine learning and how can you build explainability in algorithms. This books aims at machine learning practitioners, but also at stakeholders deciding on the use of machine learning and intelligent algorithm. "],
["intro.html", "1 Introduction 1.1 What is machine learning and why is it important? 1.2 Why is explainability important? 1.3 Who this book is for 1.4 Outline of the book 1.5 Is good user experience enough? 1.6 Scope of explainability 1.7 Measuring explainability / comprehensibility / interpretability 1.8 Definitions 1.9 Terminology", " 1 Introduction Start with motivational examples. Things that went wrong. 1.1 What is machine learning and why is it important? Black box 1.2 Why is explainability important? Machine learning has come to a state where you have to make a trade-off: Do you simply want to know what will happen? For example if a client will churn or if medication will work well for a patient. Or do you want to know why something will happen and paying for the explainability with accuracy? In some cases you will not care why a decision was made, only the assurance that the accuracy was good on some test set is enough. But in other cases knowing the ‘why’ can help you understand more about the problem, the data and also know why a model might fail. 1.3 Who this book is for This book is for everyone who wants to learn how to make machine learning models more explainable. It is a recommended reading for machine learning practitioners, statisticians, data scientists and everyone who has contact with machine learning applications. It contains one or the other formula, but it’s kept at a manageable level of math. This book is not for people who are trying to learn machine learning from scratch. If you want to learn machine learning, there are loads of books and other resources for learning the basics. 1.4 Outline of the book Definition of interpretability / explainability System is interpretable if it falls into certain class of models, where authors claim that this class is interpretable and the authors present algorithms to optimize within that class. [Towards A Rigorous Science of Interpretable Machine Learning] There is no good definition of interpretability yet? Classic statistics usually aims at having interpretable models. That’s why doctors, sociologists and banks tend to go to statisticians with research questions instead of computer scientists. Some systems need more interpretability. The more impactful the decision, the more explainable it should be. A possibly incorrect product recommendation might is not as bad as a wrong diagnose and recommendation of an inappropriate treatment. Need for explainability arises when something goes wrong. Because having an explanation for a faulty classification helps to understand the cause of the fault. It delivers a direction for how to fix the system. Consider an example of a husky versus wolf classifier, that missclassifies some huskys as wolfs. If there is an explanation to the classification you can see, that the missclassification happend due to the snow on the image. The classifier learned to use snow as a feature for classifying images as wolfs, which might make sense in terms of separating features in the training data set, but not in the real world use. [TODO: Add image from Ribeiro + ask for permission] As a machine learning practitioner, your main goal is to drive down the loss function while also keeping the learned model generalizable to other data sets. 1.5 Is good user experience enough? Maybe it is enough to show that the algorithm works and does what it is supposed to. The recommended movies are all reasonable and my self-driving car never had an accident. We have to distinguish between between low and high stakes scenario and individual risks and systematic biases. For low-risk applications (e.g. product recommender systems) there is not so much damage if something goes wrong. No one will die because they got a product recommendation on Amazon for something they are not interested in. Biggest risk is for the company deploying those algorithms that they do not work and loose customers to competitors. But there is the risk for a systematic bias and while for each individual the impact might be negligible on a group or society level it is quite problematic. Social bubbles through newsfeeds? An example is a restaurant recommender system that would never recommend restaurants to a certain minority, because they are from that minority. High risk applications are self-driving cars, AI doctors etc. Here it is quite important to have explainability. Only with explainability can you ‘debug’ why a car accident happend by a self-driving car or decide if you want to trust the diagnose of a machine learning algorithm. 1.6 Scope of explainability There are basically 1.6.1 Algorithm On this level of explainability, you are only asking how the algorithm learns, what kind of relationships it is capable of picking up. This level of explainability is what each good machine learning practitioner today usually has. If you are using convolutional neural networks for classifying images, you can explain that the algorithm learns edge detectors and filters on the lowest layers. This is an understanding of how the algorithm works, but not of the specific model that is learned in the end and not about how single decisions are reached. For this level of explainability only knowledge about the algorithm and the data is required. This book will talk a little about algorithmic explainability, but will focuse more on global and local explainability. 1.6.2 Global model explainability To explain the global model output, you need the trained model, knowledge about the algorithm and the data. This level of explainability is about understanding how the model makes the decisions, based on the features. Which features are the important ones and what kind of interactions are happening? Global model explainability helps to understand the distribution of your target variable based on the features. 1.6.3 Explain a single observation You can go all the way down to a single observation and examine what kind of classification or decision the model gives for this input, and why it gives this input. When you zoom in into one example, the conditional distribution of the target variable might behave more nicely. Locally it might depend only linearly or monotonic on some variables rather than having a complex dependency. For example the rent of an appartment might not depend linearly on the size, but if you only look at a specific appartment of 100 square meter and check how the prize changes going up plus and minus 10 square meters there is a chance that this sub region in your data space is linear. Local explanations can be more accurate compared to global explanations because of this. 1.7 Measuring explainability / comprehensibility / interpretability Model size is an easy way to measure, but might be too simplistic. 1.8 Definitions An Algorithm is a set of rules that a machine follows to achieve a particular goal [1] Machine learning algorithm is an set of rules that a machine follows to learn how to a achieve a particular goal. The output of a machine learning algorithm is a machine learning model. (Machine learning) Model is the outcome of a machine learning algorithm. This can be a set of weights for a linear model or neural network plus the architecture. Features are the variables/information used for prediction/classification/clustering. (machine learning) Task can be classification, regression, survival analysis, clustering, outlier detection Instance One row in the dataset. 1.9 Terminology Y is the target variable in supervised settings. X are the features or covariates. w are the weights. $% are regression weights. [1] https://www.merriam-webster.com/dictionary/algorithm, accessed on Feb. 12th "],
["simple.html", "2 Keep it simple 2.1 Overview 2.2 Linear models 2.3 Decision trees 2.4 Interpretation overview 2.5 Monotonicity constraints between features", " 2 Keep it simple The most straight forward way to achieve explainable machine learning algorithms is to use only a subset algorithms that yield an understandable model structure. These are: linear models trees rules In the following chapters these we will talk about the algorithm with it’s variants. Not in detail, only the basics, because there are already a ton of books, videos, tutorials, papers and so on about them. We will focus on how to interpret the models and why they are explainable. The chapter covers linear models, decision trees, decision rules, neighbour methods and graphical models. 2.1 Overview Algorithm Linear Monotonicity Interaction built-in Linear models Yes Yes No Decision trees No Not by default Yes Decision rules No Not by default Yes Naive bayes Yes Yes No Nearest neighbours No No No 2.2 Linear models Linear models have been and are still used by statistician, computer scientists and other people with quantitative problems. They learn straightforward linear (and monotonic) relationships between the target and the features. The target changes by a learned weight depending on the feature. Monotonicity makes the interpretation easy. Linear models can be used to model the dependency of a regression variable (here Y) on K covariates. As the name says, the learned relationships are linear in the form of \\[y_{i} = \\beta_{0} + \\beta_{1} \\cdot x_{i,1} + \\ldots + \\beta_{K} x_{i,K} + \\epsilon_{i}\\] The i-th observation’s outcome is a weighted sum of it’s K features. The \\(\\beta_{k}\\) represent the learned feature weights or coefficients. The \\(\\epsilon_{i}\\) is the error we are still making, the difference between the predicted and actual outcome. The biggest advantage is the linearity: It makes the estimation procedure straight forward and most importantly these linear equations have an easy to understand interpretation. That is one of the main reasons why the linear model and all it’s descendants are so widespread in academic fields like medicine, sociology, psychology and many more quantitative research fields. In this areas it is important to not only predict e.g. the clinical outcome of a patient, but also quantify the influence of the medication while at the same time accounting for things like sex, age and other variables. 2.2.1 Interpretation The interpretation of the coefficients: Continuous regression variable: For an increase of one point of the variable \\(x_{j}\\) the estimated outcome changes by \\(\\beta_{j}\\) Binary categorial variables: One of the variables is the reference level (in some languages the one that was coded in 0). A change of the variable \\(x_{i}\\) the reference level to the other category changes the estimated outcome by \\(\\beta_{i}\\) Categorial variables with many levels: One solution to deal with many variables is to one-hot-encode them, meaning each level gets it’s own column. From a categorial variable with L levels, you only need L-1 columsn, otherwise it is over parameterized. The interpretation for each level is then according to the binary variables. Some language like R allow to Intercept \\(\\beta_{0}\\): The interpretation is: Given all continuous variables are zero and the categorial variables are on the reference level, the estimated outcome of \\(y_{i}\\) is \\(\\beta_{0}\\). The interpretation of \\(\\beta_{0}\\) is usually not relevant. 2.2.2 Interpretation example library(&#39;mlbench&#39;) library(&#39;knitr&#39;) data(&#39;BostonHousing&#39;) lm_summary = summary(lm(medv ~ ., data = BostonHousing))$coefficients[,c(&#39;Estimate&#39;, &#39;Std. Error&#39;)] name_matches = c(&#39;crim&#39;=&#39;per capita crime rate&#39;, &#39;zn&#39;=&#39;proportion of residential land zoned for lots over 25,000 sq.ft&#39;, &#39;indus&#39;=&#39;proportion of non-retail business acres per town&#39;, &#39;chas1&#39;=&#39;Charles River dummy variable (= 1 if tract bounds river; 0 otherwise&#39;, &#39;nox&#39;=&#39;nitric oxides concentration (parts per 10 million)&#39;, &#39;rm&#39;=&#39;average number of rooms per dwelling&#39;, &#39;age&#39;=&#39;proportion of owner-occupied units built prior to 1940&#39;, &#39;dis&#39;=&#39;weighted distances to five Boston employment centres&#39;, &#39;rad&#39;=&#39;index of accessibility to radial highways&#39;, &#39;tax&#39;=&#39;full-value property-tax rate per USD 10,000&#39;, &#39;ptratio&#39;=&#39;pupil-teacher ratio by town&#39;, &#39;b&#39;=&#39;1000(B - 0.63)^2 where B is the proportion of blacks by town&#39;, &#39;lstat&#39;=&#39;percentage of lower status of the population&#39;, &#39;medv&#39;=&#39;median value of owner-occupied homes in USD 1000&#39;, &#39;(Intercept)&#39;=&#39;(Intercept)&#39;) rownames(lm_summary) = name_matches[rownames(lm_summary)] kable(lm_summary) Estimate Std. Error (Intercept) 36.4594884 5.1034588 per capita crime rate -0.1080114 0.0328650 proportion of residential land zoned for lots over 25,000 sq.ft 0.0464205 0.0137275 proportion of non-retail business acres per town 0.0205586 0.0614957 Charles River dummy variable (= 1 if tract bounds river; 0 otherwise 2.6867338 0.8615798 nitric oxides concentration (parts per 10 million) -17.7666112 3.8197437 average number of rooms per dwelling 3.8098652 0.4179253 proportion of owner-occupied units built prior to 1940 0.0006922 0.0132098 weighted distances to five Boston employment centres -1.4755668 0.1994547 index of accessibility to radial highways 0.3060495 0.0663464 full-value property-tax rate per USD 10,000 -0.0123346 0.0037605 pupil-teacher ratio by town -0.9527472 0.1308268 1000(B - 0.63)^2 where B is the proportion of blacks by town 0.0093117 0.0026860 percentage of lower status of the population -0.5247584 0.0507153 2.2.3 Coding categorial variables: There are several ways to represent a categorial variable, which has an influence on the interpretation: http://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/ and http://heidiseibold.github.io/page7/ Described above is the dummy coding, which is usually sufficient. Using different codings boils down to creating different matrices from your one column with the categorial variable. An overview: Dummy coding compares each level to the reference level. The intercept is the mean of the reference group Simple coding also compares each level to the reference, but the intercept is the overall mean. Deviation coding: Intercept is overall mean, and levels are compared to the mean Split coding: compare each level to the previous level. Usually used when there is an order in the levels. Helmert coding: … 2.2.4 Explaining a single observation: Let’s look at a concrete example: You are trying to predict the house price using the notorious Boston house prize data set. So y is the prize of the house and your X are: … The fitted linear model for the prize comes down to the following equation: EQUATION 2.2.5 The disadvantages of linear models They can only represent linear relationships as the name suggests. Each non-linearity or interaction has to be hand-crafted and explicitly given to the model as an input feature. Because of possible high correlation between features, it is possible that a feature that is positively correlated with the outcome might get a negative weight in a linear model, because in the high dimensional space it is negatively correlated. An example: You have a model to predict the rent price and have features like number of rooms and size of the flat. Of course flat size and room number are highly correlated, the bigger a flat the more rooms it has. If you now take both variables into a linear model it might happen, that the flat size is the better predictor and get’s a large positive weight. The room number might end up getting a negative weight, because given that a flat has the same size, increasing the number of rooms could make it less valuable. ### Towards complexer relationships within linear model class - Adding interactions - Adding non-linear terms like polynomials - Stratifying data by variable and fitting linear models on subsets 2.2.6 Linear models beyond gaussian regression Logistic regression Lasso, Ridge, elasticnet GAMs Quantile regression 2.3 Decision trees 2.3.1 Explaining: Treeinterpreter https://github.com/andosa/treeinterpreter ## RuleFit ## Decision rules ## Decision tables ## Nearest neighbours ### Learning deep k-nearest neighbour representations ## Bayesian network classifiers ## Naive Bayes Classifier and other graphical models ### Naive bayes Good in case you want interpretation of all features. Bad: No interaction, but even strong assumption of independence of features given the class. But if you indeed have independent features, then it might be the best model. 2.3.2 More general bayesian network approache(s) 2.3.2.1 Bayesian Network-Augmented Naïve Bayes (BAN) illustrated 2.3.2.2 General Bayesian Network 2.4 Interpretation overview INSERT HERE: One example of classification task and one for regression task plus for each a table with the interpretations per model (maybe only example for one variable). Plus verbal template for each interpretation. 2.5 Monotonicity constraints between features "],
["data.html", "3 Explain the data 3.1 visualizations 3.2 PCA 3.3 Biases. Source: https://en.m.wikipedia.org/wiki/Bias_(statistics)", " 3 Explain the data See also here: https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning Humans can understand visualizations in 1, 2 and 3 dimensions, but beyond that it is not possible. Of course you can also add dimensions by mapping some information on the colors used in a graphic or the size of some symbols, but the dimensionality that can be shown together is very limited. 3.1 visualizations 3.2 PCA 3.2.1 MDS 3.2.2 t-SNE 3.2.3 Glyphs 3.2.4 Correlation Graphs 3.3 Biases. Source: https://en.m.wikipedia.org/wiki/Bias_(statistics) Guidelines for assessing bias ## Compare sample statistics with target population ## Archetype analysis? ## Clustering "],
["model-agnostic-explanations.html", "4 Model-agnostic explanations 4.1 Global: Explain behaviour of system with data 4.2 Local: Explain single decisions", " 4 Model-agnostic explanations 4.1 Global: Explain behaviour of system with data 4.1.1 Global surrogate models A surrogate model is a simple, explainable model that explains a complex machine learning model. ### Partial dependency plots Partial dependency plots show the relationship between the target and one or more features by averaging out all the other features. A dependency plot can show if the relationship between target and feature is linear, monotonic or something else. Are only partially global: It is global because it takes into account all instances, but it is local in the feature, because partial dependency plots only examine one variable, as the name suggests. ### Individual conditional expectation (ICE) plot ### Variable importance ### LOCO (Leave-One-Covariate-Out) ### Interactions ### Residual analysis A residual value is the difference of the models prediction and the actual value. ### Confusion matrix ### Sensitivity analysis of predictions Testing the stability of the model predictions/classifications using simulated data. It can help to trust the model to not be instable in certain settings. 4.2 Local: Explain single decisions 4.2.1 Local surrogate models (LIME) LIME is also a surrogate model, but it is a local one. ## Maximum activation analysis ### LOCO (Leave-One-Covariate-Out) also local ### Max points lost Compare individual prediction with ‘ideal’ case (maximum probability) in terms of points lost per feature. Only works with monotonicity. And ideal case candidate maxes out each feature regarding highest probability of interest. The feature in which the instance is farthest away from the ideal case is the most negative point, why it should not be in class of interest. Feature with point closest to ideal is the least negative reason. "],
["specific.html", "5 Model-specific explanations for complex models 5.1 Global explanations", " 5 Model-specific explanations for complex models 5.1 Global explanations 5.1.1 Random Forests: Treeinterpreter https://github.com/andosa/treeinterpreter ## Local explanations "]
]
