[
["index.html", "XAI: Explainable artificial intelligence Preface", " XAI: Explainable artificial intelligence Christoph Molnar 2017-04-25 Preface This book is about explainable artificial intelligence, focusing on machine learning models. Machine learning is being built into many products and processes of our daily lives, yet decisions made by machines don’t automatically come any explanation. An explanation increases the trust in the decision and in the machine learning model. As the programmer of an algorithm you want to know whether you can trust the learned model. Did it learn generalizable features? Or are there some odd artifacts in the training data which the algorithm picked up? This book will give an overview over techniques that can be used to make black boxes as transparent as possible and explain decisions. In the first chapter algorithms that produce simple, explainable models are introduced together with instructions how to interpret the output. The later chapters focus on analyzing complex models and their decisions. In an ideal future, machines will be able to explain their decisions and make a transition into an algorithmic age more human. This books is recommended for machine learning practitioners, data scientists, statisticians and also for stakeholders deciding on the use of machine learning and intelligent algorithms. The online version of this book (currently the only available version) is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["intro.html", "1 Introduction 1.1 Who is this book for? 1.2 What is machine learning and why is it important? 1.3 Why is explainability important? 1.4 Is a good user experience enough? 1.5 Outline of the book 1.6 Scope of explainability 1.7 Tool vs. product 1.8 Evaluating explainability 1.9 Definitions 1.10 Terminology", " 1 Introduction 1.1 Who is this book for? This book is for everyone who wants to learn how to make machine learning models more explainable. It is a recommended reading for machine learning practitioners, statisticians, data scientists, scientists and everyone who has contact with machine learning applications. It contains one or the other formula, but it’s kept at a manageable level of math. This book is not for people who are trying to learn machine learning from scratch. If you want to learn machine learning, there are loads of books and other resources for learning the basics. 1.2 What is machine learning and why is it important? Recommendation of products, identifying street signs, counting people on the street, assessing a person’s credit worthiness, detecting fraud: All these examples have in common that they can, and are increasingly, realized with machine learning algorithms. The tasks are different, but the approach is the same: Step 1 is to collect data. This can be images with and without street signs plus the information which sign is visible or the personal data from loan applicants together with the information if they repaid their loan or not. Step 2: Feed this information into a machine learning algorithm, which produces a sign detector model or a credit worthiness model. This model can then be used in Step 3: Integrate the model into the product or process, like an self-driving car or a loan application process. There are a lot of tasks in which machines exceed humans. Even if the machine is as good as a human at a task, or slightly worse, there remains big advantages, and that is speed, reproducibility and scale. A machine learning model that has been implemented once, can do a task much faster than humans, will reliably produce the same results from the same input and can be copied endlessly. 1.3 Why is explainability important? AI explainability is the ability of a machine learning system to explain or to present a decision in an understandable way for humans. Machine learning has come to a state where you have to make a trade-off: Do you simply want to know what will happen? For example if a client will churn or if medication will work well for a patient. Or do you want to know why something will happen and paying for the explainability with accuracy? In some cases you will not care why a decision was made, only the assurance that the accuracy was good on some test set is enough. But in other cases knowing the ‘why’ can help you understand more about the problem, the data and also know why a model might fail. Two sorts of problems might not need explanations, because they either are low risk (e.g. movie recommender system) or the method is already extensively studied and evaluated (e.g. optical character recognition). The necessity for explainability comes from an incompleteness in the problem formalization (Doshi-Velez and Kim 2017), meaning that for certain problems/tasks it is not enough to get the answer (the “what”), but the model also has to give an explanation (the *why**) There is a shift in many scientific disciplines Add example disciplines--> from qualitative to quantitative methods (e.g. sociology, psychology), and also towards machine learning (biology, genomics). The goal of science is to gain knowledge, but many problems can only be solved with big datasets and black box machine learning models. Explainability allows to extract additional knowledge. Machine learning models are taking over real world tasks, that demand safety measurements and testing. A self-driving car automatically detects cyclists, which is as desired. You want to bet 100% sure that the abstraction the system learned will be fail-safe, because running over cyclists is quite bad. An explanation might reveal that the most important feature learned is to recognize the two wheels of a bike and this explanation helps to think about edge cases like bikes with side bags, that partially cover the wheels. By default most machine learning models pick up biases from the training data. This can turn your machine learning models into racists (see Microsoft failed experiment: Tay) or discriminate in against other demographic, protected groups. Explainability helps you to get the ethics of your machine learning model right. From the data it might not be possible to see the biases, so it might also not be enough to implement protections against known biases. An example: The goal is to train a machine learning model to pre-sort applications for jobs in a big company. The company had some troubles in the past because they preferred to hire men over women, having the same qualifications. So for the model training the data scientist decides to remove all features that indicate the gender. But it turns out that the model picks up another, a priori unknown, bias favoring younger people over older people. This could only be uncovered because the model is explainable. Sometimes it is not possible to optimize a goal directly, but only a proxy. Consider a marketing where a e-commerce company sends some gift to some customers in order to bind them to the business. It has already done similar campaigns, so there is data to learn a model to select the best customers to send the gifts to (sending it to all would be far to expensive). Ideally the company wants to optimize the likelihood that the customers with the gift are still their customers in five years from now, but there is no data for analysing this, because the first of theses kinds of campaigns was two years ago, meaning there is no suitable training data for this objective. So the company opts for using a proxy: The amount of money the customers will be spending in the next 6 months at your company. An explainable model can help to find problems that might arise due to using a proxy objective (possibly mismatched objectives). In the e-commerce case it might turn out that the customers selected by the algorithm are only international students that might spent heavily in a short time, even more after receiving a gift, but usually leave the country and therefore the company as a customer within a few years. In volatile environments a machine learning model might be outdated at some point because of a data shift. A data shift is a change in the relationships in the data. Explanation make it easier to detect when that happens or if one model is more prone than another to face problems soon. Spam filter for mails use works in a mail to detect if they should reach your inbox or be moved into the spam folder. ‘Viagra’ is one of the words that increase the likelihood that a mail is spam. When ‘viagra’ is suddenly used in a different context, for some obscure reason, suddenly non-spam mails will land in the spam folder (like ‘Random forests are basically a bunch of decision trees on viagra’). An investigation of this problem might reveal very quickly that the data shift (‘viagra’-occurrence / spam relationship) is caused by the new use of the former spam related word. An explainable model could highlight in the text which sections caused the spam classification. Sometimes training of machine learning models happens with training data that comes from a different distribution as the live data with which the model will be confronted with in production. The machine learning team basically trains the model on the dedicated training dataset, crosses their fingers and put it into production. The hope is, that the learned associations between the features and the output has a good generalization from training to production. The training data for a fracture detection machine learning model might come from labeled radiographs that were done with a handful of different x-ray devices. But the company hopes to use their fracture detection algorithm in their software that they sell to hundreds of hospitals that likely use different machines. Explainable machine learning models can give the engineers the confidence that the model only picked up relationships that will generalize well for new datasets. Even if the problem formalization is complete, there might be an unknown trade-off between objectives like privacy and predictive ability. Besides these scenarios with an incompleteness to of the problem formalization, there is also the task of debugging a machine learning model, which is crucial and difficult. Explainability is a useful debugging tool for black box algorithm. So even in low risk environments (e.g. movie recommenders) explainability in the research and development stage is valuable. Also later when some model is used in a product, things can go wrong. And needed for explainability arises when something goes wrong. Because having an explanation for a faulty classification helps to understand the cause of the fault. It delivers a direction for how to fix the system. Consider an example of a husky versus wolf classifier, that missclassifies some huskys as wolfs. If there is an explanation to the classification you can see, that the missclassification happened due to the snow on the image. The classifier learned to use snow as a feature for classifying images as wolfs, which might make sense in terms of separating features in the training data set, but not in the real world use. 1.4 Is a good user experience enough? Maybe it is enough to show that the algorithm works and does what it is supposed to. The recommended movies are all reasonable and my self-driving car never had an accident. We have to distinguish between between low and high stakes scenario and individual risks and systematic biases. For low-risk applications (e.g. product recommender systems) there is not so much damage if something goes wrong. No one will die because they got a product recommendation on Amazon for something they are not interested in. Biggest risk is for the company deploying those algorithms that they do not work and loose customers to competitors. But there is the risk for a systematic bias and while for each individual the impact might be negligible on a group or society level it is quite problematic. Social bubbles through newsfeeds? An example is a restaurant recommender system that would never recommend restaurants to a certain minority, because they are from that minority. High risk applications are self-driving cars, AI doctors etc. Here it is quite important to have explainability. Only with explainability can you ‘debug’ why a car accident happened by a self-driving car or decide if you want to trust the diagnose of a machine learning algorithm. 1.5 Outline of the book This book starts out with framing the problem: What aspects does explainability have? It goes on and lays out the ‘simple’ machine learning models (linear models, decision trees, decision rules) that come with interpretability. The following chapter explains different methods for explaining and understanding the data better, followed by the core chapter about methods for explaining the output of any black box model. Later methods that are specific for certain model classes are shown. The book concludes with an outlook on the future. 1.6 Scope of explainability 1.6.1 Algorithm On this level of explainability, you are only asking how the algorithm learns, what kind of relationships it is capable of picking up. This level of explainability is what each good machine learning practitioner today usually has. If you are using convolutional neural networks for classifying images, you can explain that the algorithm learns edge detectors and filters on the lowest layers. This is an understanding of how the algorithm works, but not of the specific model that is learned in the end and not about how single decisions are reached. For this level of explainability only knowledge about the algorithm and the data is required. This book will talk a little about algorithmic explainability, but will focuse more on global and local explainability. 1.6.2 Global model explainability To explain the global model output, you need the trained model, knowledge about the algorithm and the data. This level of explainability is about understanding how the model makes the decisions, based on the features. Which features are the important ones and what kind of interactions are happening? Global model explainability helps to understand the distribution of your target variable based on the features. 1.6.3 Explain the decision for a single instance You can go all the way down to a single observation and examine what kind of classification or decision the model gives for this input, and why it gives this input. When you zoom in into one example, the conditional distribution of the target variable might behave more nicely. Locally it might depend only linearly or monotonic on some variables rather than having a complex dependency. For example the rent of an apartment might not depend linearly on the size, but if you only look at a specific apartment of 100 square meter and check how the prize changes going up plus and minus 10 square meters there is a chance that this sub region in your data space is linear. Local explanations can be more accurate compared to global explanations because of this. 1.6.4 Explain the decisions for a group of instances The model output for multiple instances can be explained by using methods for global model explainability and single instance explanations. The global methods can be applied by taking the group of observations pretending it’s the complete dataset and using the global methods on this subset. The single explanation methods can be used on each instance and listed or aggregated afterwards for the whole group. 1.7 Tool vs. product Explainability can be either a tool to analyze the machine learning model or a part of the final product. 1.8 Evaluating explainability There is no real consensus what explainability in machine learning is. Also it is not clear how to measure it. Model size is an easy way to measure, but might be too simplistic. Dimensions of interpretability: - Model sparsity: How many features are being used by the explanation? - Monotonicity: Is there a Monotonicity constraint? - Uncertainty: Is a measurement of uncertainty part of the explanation? - Interactions: Is the explanation able to include interaction of features? - Cognitive processing time: How long does it take to understand the explanation. - Feature complexity: What features were used for the explanation? PCA components are harder to understand than word occurrences for example. - Description length of explanation If you can ensure that the machine learning model can explain decisions, following traits can also be checked more easily (Doshi-Velez and Kim 2017). - Fairness: Unbiased, not discrimating against protected groups (implicit or explicit). An interpretable model can tell you why it decided it decided a certain person is not worthy of a credit and for a human it becomes easy to decide if the decision was based on a learned demographic (e.g. racial) bias. - Privacy: sensitive information in the data is protected. - Reliability/Robustness: Small changes in the input don’t lead to big changes in the ouput/decision. - Causality: Only causal relationships are picked up. So a predicted change in a decision due to arbitrary changes in the input values, are also happening in reality. - Usability: - Trust: It is easier for humans to trust into a system that explains it’s decisions compared to a black box 1.8.1 Approaches for evaluation of the explanation quality Application level evaluation (real task): Put the explanation into the product and let the end user test it. On an application level the radiologists would test the fracture detection software in order to evaluate the model. This requires a good experimental setup and an idea of how to assess the quality. A good baseline for this is always how good a human would be at explaining the same decision. Human level evaluation (simple task) is a simplified application level evaluation. The difference is that these experiments are not conducted with the domain experts, but with lay humans. This makes experiments less expensive (especially when the domain experts are radiologists) and it is easier to find more humans. An example would be to show a user different explanations and the human would choose the best . Function level evaluation (proxy task) does not require any humans. This works best when the class of models used is already evaluated by someone else in a human level evaluation. 1.9 Definitions An Algorithm is a set of rules that a machine follows to achieve a particular goal (“Definition of Algorithm,” n.d.) Machine learning algorithm is an set of rules that a machine follows to learn how to a achieve a particular goal. The output of a machine learning algorithm is a machine learning model. (Machine learning) Model is the outcome of a machine learning algorithm. This can be a set of weights for a linear model or neural network plus the architecture. Features are the variables/information used for prediction/classification/clustering. (machine learning) Task can be classification, regression, survival analysis, clustering, outlier detection Instance One row in the dataset. 1.10 Terminology Y is the target variable in supervised settings. X are the features or covariates. w are the weights. $% are regression weights. References "],
["simple.html", "2 Keep it simple 2.1 Overview 2.2 Linear models 2.3 Decision trees 2.4 RuleFit 2.5 Decision rules 2.6 Interpretation overview 2.7 Monotonicity constraints between features", " 2 Keep it simple The most straight forward way to achieve explainable machine learning algorithms is to use only a subset algorithms that yield an understandable model structure. These are: Linear models (sparse) Decision trees Decision rules In the following chapters these we will talk about the algorithm with it’s variants. Not in detail, only the basics, because there are already a ton of books, videos, tutorials, papers and so on about them. We will focus on how to interpret the models and why they are explainable. The chapter covers linear models, decision trees, decision rules, neighbour methods and graphical models. 2.1 Overview Algorithm Linear Monotonicity Interaction built-in Linear models Yes Yes No Decision trees No Not by default Yes Decision rules No Not by default Yes Naive bayes Yes Yes No Nearest neighbours No No No 2.2 Linear models Linear models have been and are still used by statistician, computer scientists and other people with quantitative problems. They learn straightforward linear (and monotonic) relationships between the target and the features. The target changes by a learned weight depending on the feature. Monotonicity makes the interpretation easy. Linear models can be used to model the dependency of a regression variable (here Y) on K covariates. As the name says, the learned relationships are linear in the form of \\[y_{i} = \\beta_{0} + \\beta_{1} \\cdot x_{i,1} + \\ldots + \\beta_{K} x_{i,K} + \\epsilon_{i}\\] The i-th observation’s outcome is a weighted sum of it’s K features. The \\(\\beta_{k}\\) represent the learned feature weights or coefficients. The \\(\\epsilon_{i}\\) is the error we are still making, the difference between the predicted and actual outcome. The biggest advantage is the linearity: It makes the estimation procedure straight forward and most importantly these linear equations have an easy to understand interpretation. That is one of the main reasons why the linear model and all it’s descendants are so widespread in academic fields like medicine, sociology, psychology and many more quantitative research fields. In this areas it is important to not only predict e.g. the clinical outcome of a patient, but also quantify the influence of the medication while at the same time accounting for things like sex, age and other variables. 2.2.1 Interpretation The interpretation of the coefficients: Continuous regression variable: For an increase of one point of the variable \\(x_{j}\\) the estimated outcome changes by \\(\\beta_{j}\\) Binary categorical variables: One of the variables is the reference level (in some languages the one that was coded in 0). A change of the variable \\(x_{i}\\) the reference level to the other category changes the estimated outcome by \\(\\beta_{i}\\) categorical variables with many levels: One solution to deal with many variables is to one-hot-encode them, meaning each level gets it’s own column. From a categorical variable with L levels, you only need L-1 columsn, otherwise it is over parameterized. The interpretation for each level is then according to the binary variables. Some language like R allow to Intercept \\(\\beta_{0}\\): The interpretation is: Given all continuous variables are zero and the categorical variables are on the reference level, the estimated outcome of \\(y_{i}\\) is \\(\\beta_{0}\\). The interpretation of \\(\\beta_{0}\\) is usually not relevant. 2.2.2 Interpretation example Estimate Std. Error (Intercept) 36.4594884 5.1034588 per capita crime rate -0.1080114 0.0328650 proportion of residential land zoned for lots over 25,000 sq.ft 0.0464205 0.0137275 proportion of non-retail business acres per town 0.0205586 0.0614957 Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) 2.6867338 0.8615798 nitric oxides concentration (parts per 10 million) -17.7666112 3.8197437 average number of rooms per dwelling 3.8098652 0.4179253 proportion of owner-occupied units built prior to 1940 0.0006922 0.0132098 weighted distances to five Boston employment centres -1.4755668 0.1994547 index of accessibility to radial highways 0.3060495 0.0663464 full-value property-tax rate per USD 10,000 -0.0123346 0.0037605 pupil-teacher ratio by town -0.9527472 0.1308268 1000(B - 0.63)^2 where B is the proportion of blacks by town 0.0093117 0.0026860 percentage of lower status of the population -0.5247584 0.0507153 Interpretation of a numerical variable (‘average number of rooms per dwelling’): An increase of the average number of rooms by 1 increases the median value of houses in this suburb by \\(3800\\$ (= 3.8 \\cdot 1000)\\), given all other features stay the same. Interpretation of a categorical variable (‘Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)’): The median value of houses in suburbs that bound the Charles River is \\(2690\\$ (= 2.69 \\cdot 1000\\$)\\) compared to houses that do not, given all other features stay the same. As you can see in the interpretation examples, the interpretations are always coming with the clause that ‘all other features stay the same’. That’s because of the nature of linear models: All features are input linearly into the function with no interactions (unless explicitly specified). The good side is, that is isolates the interpretation. If you think of the features as turn-switches that you can turn up or down, it is nice to see what happens when you would just turn the switch for one feature, for example the average number of rooms. But this might not reflect the structure of the data, because the features are often correlated. It might not be meaningful to see the effect of turning up average room sizes because it might be strongly correlated with the proportion of owner-occupied units built prior to 1940. 2.2.3 Visual parameter interpretation 2.2.3.1 Weight plot The information of the coefficient table can also be put into a visualization, which makes the weights and the uncertainty about them can be made understandable on one glance. The weight is displayed as a point and the 95% confidence interval around the point with a line. The 95% confidence interval means that if the linear model was repeated 100 times on coef_plot = function(mod, alpha = 0.05, remove_intercept = TRUE){ lm_summary = summary(mod)$coefficients df = data.frame(Features = rownames(lm_summary), Estimate = lm_summary[,&#39;Estimate&#39;], std_error = lm_summary[,&#39;Std. Error&#39;]) df$lower = df$Estimate - qnorm(alpha/2) * df$std_error df$upper = df$Estimate + qnorm(alpha/2) * df$std_error if(remove_intercept){ df = df[!(df$Features == &#39;(Intercept)&#39;),] } ggplot(df) + geom_point(aes(x=Estimate, y=Features)) + geom_segment(aes(y=Features, yend=Features, x=lower, xend=upper), arrow = arrow(angle=90, ends=&#39;both&#39;, length = unit(0.1, &#39;cm&#39;))) + my_theme } coef_plot(mod) TODO: Add interpetation 2.2.3.2 Effect plot The weights of the linear model only have meaning, when combined with the actual features. The weights depend on the scale of the features and will be different if you have a features measuring some height and you switch from inches to centemeters. The weight will change, but the actual relationships in your data will not. Also it is important to know the distribution of your feature in the data, because if you have a very low variance, it means that almost all instances will get a similar contribution from this feature. The effect plot can help to understand how much the combination of a weight and a feature contributes to the predictions in your data. Start with the computation of the effects, which is the weight per feature times the feature of an instance: \\(eff_{i,k} = w_{k} \\cdot x_{i,k}\\). The resulting effects are visualized with boxplots: The box contains the effect range for half of your data (25% to 75% effect quantiles). The line in the box is the median effect, so 50% of the instances have a lower and the other half a higher effect on the prediction than the median value. The whiskers are \\(+/i 1.58 IQR / \\sqrt{n}\\), with IQR being the inter quartile range ($q_{0.75} - q_{0.25}). The points are outlier to the whiskers. get_effects = function(mod){ X = model.matrix(mod) X = data.frame(predict(mod, type = &#39;terms&#39;)) means = apply(mod$x, 2, mean) * mod$coefficients means = means[names(means) != &quot;(Intercept)&quot;] # predict with type=&#39;terms&#39; centers the results, so we have to add the mean again means_X = matrix(rep(means, times = nrow(X)), nrow=nrow(X), byrow=TRUE) X = X + means_X X } effect_plot = function(mod, feature_names=NULL){ X = get_effects(mod) if(!missing(feature_names)){ rownames(X) = feature_names } X = gather(X) ggplot(X) + geom_boxplot(aes(x=key, y=value, group=key)) + coord_flip() + my_theme } effect_plot(mod) TODO: Add interpetation 2.2.4 Explaining single predictions Why did a certain instance get the prediction it got from the linear model? This can again be answered by bringing together the weights and features and computing the effect. Now the effect will tell you how much each feature contributed towards the sum of the prediction. This is only meaningful if you compare the instance specific effects with the mean effects. See also the chapter about ‘Justification Narratives’. i = 1 effects = get_effects(mod) effects_i = gather(effects[i, ]) predictions = predict(mod) predictions_mean = mean(predictions) pred_i = predictions[i] effect_plot(mod) + geom_point(aes(x=key, y=value), color = &#39;red&#39;, data = effects_i) + ggtitle(sprintf(&#39;Predicted value for instance: %.2f\\n Average predicted value: %.2f&#39;, predictions_mean, pred_i)) 2.2.5 Interpretation templates Interpretation of a numerical feature: An increase of \\(x_{k}\\) by one unit increases the expectation for \\(y\\) by \\(\\beta_x{k}\\) units if all other features X stay the same. Interpretation of a categorical feature: The category coded with 1 of \\(x_{k}\\) increases the expectation for \\(y\\) by \\(\\beta_{k}\\) compared to the reference category (coded with 0). 2.2.6 Coding categorical variables: There are several ways to represent a categorical variable, which has an influence on the interpretation: http://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/ and http://heidiseibold.github.io/page7/ Described above is the treatment coding, which is usually sufficient. Using different codings boils down to creating different matrices from your one column with the categorical feature. I present three different codings, but there are many more. The example has six instances and one categorical feature with 3 levels. The first two instances are in category A, instances three and four are in category B and the last two instances are in category C. Treatment coding compares each level to the reference level. The intercept is the mean of the reference group. The first column is the intercept, which is always 1. Column two is an indicator whether instance \\(i\\) is in category B, columns three is an indicator for category C. There is no need for a column for category A, because than the system would be over specified. Knowing that an instance is neither in category B or C is enough. \\[ \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\] Effect coding compares each level to the overall mean of \\(y\\). The first column is again the intercept. The weight \\(\\beta_{0}\\) which is associated to the intercept represents the overall mean and \\(\\beta_{1}\\), the weight for column two is the difference between the overall mean and category B. The overall effect of category B is \\(\\beta_{0}\\) + _{1}$. Interpretation for category C is equivalent. For the reference category A, \\(-(\\beta_{1} + \\beta_{2})\\) is the difference of the category C to the overall mean and \\(\\beta_{0} -(\\beta_{1} + \\beta_{2})\\) the overall effect of category C. \\[ \\begin{pmatrix} 1 &amp; -1 &amp; -1 \\\\ 1 &amp; -1 &amp; -1 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\] Dummy coding compares each level to the level mean of \\(y\\). If all level are have the same frequency the resulting coefficients will be the same as in effect coding. Note that the intercept was dropped here. \\[ \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\] 2.2.7 Assuring sparsity in linear models Lasso, Ridge, elasticnet, forward/backward variable selection, dimensionality reduction, … UNDER CONSTRUCTION 2.2.8 The disadvantages of linear models They can only represent linear relationships as the name suggests. Each non-linearity or interaction has to be hand-crafted and explicitly given to the model as an input feature. Because of possible high correlation between features, it is possible that a feature that is positively correlated with the outcome might get a negative weight in a linear model, because in the high dimensional space it is negatively correlated. An example: You have a model to predict the rent price and have features like number of rooms and size of the flat. Of course flat size and room number are highly correlated, the bigger a flat the more rooms it has. If you now take both variables into a linear model it might happen, that the flat size is the better predictor and get’s a large positive weight. The room number might end up getting a negative weight, because given that a flat has the same size, increasing the number of rooms could make it less valuable. 2.2.9 Towards complexer relationships within linear model class Adding interactions Adding non-linear terms like polynomials Stratifying data by variable and fitting linear models on subsets 2.2.10 Linear models beyond gaussian regression Logistic regression GAMs Quantile regression 2.3 Decision trees Linear models fail in situation where the relationship is non-linear and/or where the features are interacting with each other. Time to shine for the decision trees! Tree-based models partition the data along the features into rectangles. For predicting the outcome in each rectangle it fits a simple model (for example the average of the outcome of the instances that fall into this rectangle). Trees have an intuitive structure starting from a root and splitting into nodes, according to cutoff values of the features. After each split, the instances fall into one of the new nodes. At the end of the training all the instances from the training data set are assigned into one of the leaf nodes. There are a lot of different tree algorithms. They differ in structure (number of splits per node), criteria for how to find the splits, when to stop splitting and how to estimate the simple models within the leaf nodes. Classification and regression trees (CART) is one of the more popular. This book will only talk about CART, because in the interpretation they are all the same. If you know of some tree algorithm with a different interpretation, I would welcome your feedback. Each of these rectangles is associated with a simple model of the outcome of the interest. This is usually estimated by taking the mean of outcomes from all training instances that fall into a rectangle. I recommend the book ‘The elements of statistical learning’ (Hastie, Tibshirani, and Friedman 2009) for a more detailed introduction. library(&#39;party&#39;) set.seed(42) n = 100 dat = data.frame(feature_x1 = rep(c(1,1,2,2), times = n), feature_x2 = rep(c(2,3,3,3), times = n), y = rep(c(1, 2, 3, 4), times = n)) dat = dat[sample(1:nrow(dat), size = 0.9 * nrow(dat)), ] dat$y = dat$y + rnorm(nrow(dat), sd = 0.2) ct = ctree(y ~ feature_x1 + feature_x2, dat) plot(ct, inner_panel = node_inner(ct, pval = FALSE)) The following formula describes relationship of y and x (in which rectangle does x fall?) \\[\\hat{y}_i = \\hat{f}(x_i) = \\sum_{m = 1}^M c_m I\\{x_i \\in R_m\\}\\] An instance \\(x_i\\) falls into one leaf node (=rectangle), so \\(I\\{x_i \\in R_m\\}\\) is only 1 for the this single leaf node (\\(I\\) is the identity function which is 1 if \\(x_i \\in R_m\\) and else 0). If \\(x_i\\) falls into leaf node \\(R_l\\), the predicted outcome \\(hat{y} = c_l\\), where \\(c_l\\) is the mean of all the training instances in leaf node \\(R_l\\). But where do the rectangles come from? This is quite simple: The algorithm takes a feature and tries which cut-off point minimizes the sum of squares if it is a regression task or the Gini index in classification tasks. It’s the cut-off point that makes the two resulting subsets as different as possible in terms of the outcome variable of interest. After this was done for each feature, the algorithm looks for the feature with the best cut-off and chooses this to split the node into two new nodes. The algorithm continues doing this in both new nodes until the stopping criteria is reached. Possible criteria are: A minimum number of observations that have to be in a node before the split, the minimum number of instances that have to be in a terminal node. A common strategy is to grow a tree fully and then cut it back to optimize it’s complexity measure \\(cp\\). 2.3.1 Interpretation It’s easy: Starting from the root node you go to the next nodes and the edges tell you which subsets you are looking at. Once you reach the leaf node, the node tells you the predicted outcome. All the edges are connected by ‘AND’. Template: If feature x is [smaller/bigger] than threshold c AND …, then the predicted value is \\(\\hat{y}\\). 2.3.2 Interpretation example TODO 2.3.3 Advantages The tree structure is perfectly suited to cover interactions between features in the data. The interpretation is pretty straight forward. 2.3.4 Disadvantages Handling of real linear relationships, that’s what trees suck at. Any real linear relationship between an input feature and the outcome has to be approximated by hard splits, which produces a step function. This is not efficient. This goes hand in hand with lack of smoothness. Slight changes in the input feature can have a big impact on the predicted outcome, which might not be desirable. Imagine a tree that predicts the worth of a house and the tree splits in the square meters multiple times. One of the splits is at 100.5 square meters. When a user measure his house and arrives at 99 square meters, types it into some nice web interface and get’s 200 000 Euro. The user notices that she forgot to measure a small storeroom with 2 square meters. The storeroom has a skewed wall, so she is not sure if she can count it fully towards the whole flat area or only half of the space. So she decides to try both 100.0 and 101.0 square meters. The results: 200 000 Euro and 205 000 Euro, which is quite unintuitive. Trees are also quite unstable, so a few changes in the training data set might create a completely different tree. That’s because each splits depends on the parent split. It does not generate trust if the structure flips so easily. 2.3.5 Explaining: Treeinterpreter https://github.com/andosa/treeinterpreter 2.4 RuleFit 2.5 Decision rules Decision rules are a class of machine learning model that consists of if X then Y statements. Within the if statement there are sets of features and conditions on them (like X &gt; 4) and Y is a statement about the outcome of interest. One decision rule from a model predicting rents could be: If a flat is bigger than 100 square meters and has a garden the average rent is 2000 Euro per month. Let’s split that up: - ‘Flat is bigger than 100 square meters’ is the first part of the IF statement - ‘Flat has a garden’ is a second conditions in the IF. - Both are connected with an ‘AND’, so both have to be true for the rule to apply - Predicted outcome (THEN) is that the average rent is 2000 Euro per month. RULE: IF CONDITIONS THEN TARGET Arguably decision rules are the most simple classifier in terms of understandability. They are very close to natural language and the way we think. Of course it relies on using sensible, understandable features and the number of conditions within the if statement. Usually there is more then one rule to describe the associations of the features with the outcome. In our example we only have one rule covering big appartments with flats, but what about a smaller apartment? Rules can be induced from trees. Each path through the tree is one rule. Rules made of trees have the nice There are many ways how to induce those kind of rules from a training dataset. The induction algorithms differ in how they create the rules: - Do they allow overlapping rules and how are conflicts handled? - Do they cover the whole training set? 2.5.1 Association rule mining Concepts: - Support - Confidence - Lift 2.5.2 Rule induction 2.5.2.1 The Bayesian Machine List [BML] Target: Multi-class classification with labels 1, …, L and training data \\({(x_i, y_i)}_{i=1}^n\\), where \\(x_i \\in \\R\\) are the features for observation \\(i\\) and \\(y_i \\in {1, ..., L}\\) are the labels. Algorithm: 1. Generate a list of rules $ r = 1, …, R$ with a rule-mining Algorithm 2. Sample permutation of rules using \\(\\pi\\) from prior \\(P(p, C)\\) 3. Using this ordering, select the first rule that applies, in that it matches the observed feature \\(x_i\\). Call the rule \\(\\tilde{r}_i\\) 4. Draw a label \\(y_i\\) from a Dirichlet-Multinomial distribution \\(\\Theta^{\\tilde{r}_i}\\), with Dirichlet parameters \\(\\alpha_1, ..., \\alpha_L\\) and counts $n_{_i 1}, …, \\(n_{\\tilde{r}_i L}\\) for rule \\(\\tilde{r}_i\\) chosen in the previous step. [BML] Letham, B., Rudin, C., Mccormick, T. H., &amp; Madigan, D. (2010). An Interpretable Stroke Prediction Model Using Rules and Bayesian Analysis. AAAI Technical Report WS-13-17, (609), 65–67. 2.5.3 Other algorithms Decision tables Nearest neighbours Learning deep k-nearest neighbour representations Bayesian network classifiers Naive Bayes Classifier and other graphical models Naive bayes Good in case you want interpretation of all features. Bad: No interaction, but even strong assumption of independence of features given the class. But if you indeed have independent features, then it might be the best model. More general bayesian network approache(s) Bayesian Network-Augmented Naïve Bayes (BAN) illustrated General Bayesian Network Prototypes 2.6 Interpretation overview INSERT HERE: One example of classification task and one for regression task plus for each a table with the interpretations per model (maybe only example for one variable). Plus verbal template for each interpretation. 2.7 Monotonicity constraints between features References "],
["data.html", "3 Explain the data 3.1 visualizations 3.2 PCA 3.3 Biases. Source: https://en.m.wikipedia.org/wiki/Bias_(statistics)", " 3 Explain the data See also here: https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning Humans can understand visualizations in 1, 2 and 3 dimensions, but beyond that it is not possible. Of course you can also add dimensions by mapping some information on the colors used in a graphic or the size of some symbols, but the dimensionality that can be shown together is very limited. 3.1 visualizations 3.2 PCA 3.2.1 MDS 3.2.2 t-SNE 3.2.3 Glyphs 3.2.4 Correlation Graphs 3.3 Biases. Source: https://en.m.wikipedia.org/wiki/Bias_(statistics) Guidelines for assessing bias ## Compare sample statistics with target population ## Archetype analysis? ## Clustering "],
["model-agnostic-explanations.html", "4 Model-agnostic explanations 4.1 Justification narrative structure for classification 4.2 Global: Explain behaviour of system with data 4.3 Local: Explain single decisions", " 4 Model-agnostic explanations Separating the explanations from the machine learning model (= model-agnostic explanations) gives some benefits. The big advantage of model-agnostic vs model-specific explanation algorithm is the flexibility. When the explanation system is independently applicable even when the underlying model is switched, it frees the practitioner to use different machine learning models without restrictions. It is also more efficient to build interfaces on top of model-agnostic systems, because this has to be done only once and not for each model-specific explanation system. Usually not one but many types of machine learning models are tested in development time and if you want to compare the models in terms of interpretability this is easier with model-agnostic explanations because the system is the same for both models that are being compared (Ribeiro, Singh, and Guestrin 2016). The alternatives are either using only interpretable models as introduced in Chapter 2, which has the big disadvantage to usually loose accuracy compared to other approaches. The other alternative is to use more flexible model classes that come with built in explanations. The drawback here is that it ties you to this one algorithm and it will be hard to switch to something else. Desirable aspects of a model-agnostic explanation system (Ribeiro, Singh, and Guestrin 2016): - Model flexibility: Not being tied to an underlying particular machine learning model. The method should work for random forests as well as convolutional neural networks - Explanation flexibility: Not being tied to a certain form of explanation. In some cases it might be useful to have a linear formula in other cases some decision rules - Representation flexibility: The explanation system should not have to use the same feature representation as the model that is being explained. So when a text classifier uses abstract word embedding vectors, it might be preferable to use the presence of single words for the explanation. 4.1 Justification narrative structure for classification See also Paper. Idea: Per feature only use effect and importance. This method is per se model-agnostic, but you need a method for computing effect and importance, which is different for each model class. The effect of a feature is how much the feature contributed towards (or against) a classification to a certain category for an instance. In case of a linear model it is simply the j-th weight times the feature value for observation i: \\(\\beta_{j} x_{ij}\\). For classification it is class specific (class k): \\(eff_{ji} = \\beta_{kj} x_{ij}\\) The importance of a feature is defined as the overall strength of a feature within the model. So it is the expected effect of feature j for a particular class. The formula for importance of feature j towards class k is: \\(imp_{ji} = \\beta_{ji} \\frac{\\sum_{x \\in X^j} x_{i}}{|X^j|}\\), where \\(X^j\\) is the set of all instances which have class j. Note that the polarity of a feature (\\(=sign(\\beta_{j})\\)) might be different from the importance, for example when the weight is negative and also the associated feature is negative for most cases in class k. Narrative role of a feature for the classification of an instance depends on effect and importance. Step 1: Decide what magnitude of importance can be seen as high and separate into low and high. This can be done by applying a fixed threshold or keeping a fixed number of features or some kind of ‘ellbow criterium’. The absolute magnitude has to be considered because importance comes both from features that count towards and against a class. Importance Effect High positive Low High negative High positive Normal evidence Missing evidence Contrarian counter-evidence Low Exceptional evidence Negligible Exceptional counter-evidence High negative Contrarian evidence Missing counter-evidence Normal counter-evidence Contrarian evidene and contrarian counter-evidence is only possible with negative features. You should mean center the features, otherwise the importance and the effects will very much look the same (unless the means between the classes vary greatly). The importance and effects are dependent on the scale of your features, but it should not matter whether the a feature is measured in meters or in inch (you should use meter of course) or if it is visits per hour or per minute. Textual template: TODO 4.1.1 Example justification narratives with the vehicle data set The Vehicle dastaset contains the silhoutte descriptions of four types of vehicles. Different features are extracted from the silhouettes from different angles. The four classes are bus, opel, saab and van, but for the purpose of illustration we only focus on the task classifying bus vs. not bus given the silhoutte features. The dataset contains 846 cars with 18 silhoutte features. Table 4.1: Feature importances for the class “bus” Feature Description Importance Comp Compactness 0.3641786 Circ Circularity -0.0168379 D.Circ Distance Circularity 1.3298154 Rad.Ra Radius ratio 1.9065983 Pr.Axis.Ra pr.axis aspect ratio 3.1085208 Max.L.Ra max.length aspect ratio 0.5426764 Scat.Ra scatter ratio -0.2893669 Elong elongatedness 1.6277211 Pr.Axis.Rect pr.axis rectangularity 0.0158149 Max.L.Rect max.length rectangularity -0.1105358 Sc.Var.Maxis scaled variance along major axis 0.4457976 Sc.Var.maxis scaled variance along minor axis 0.6233814 Ra.Gyr scaled radius of gyration 0.1950721 Skew.Maxis skewness about major axis -1.2873672 Skew.maxis skewness about minor axis 0.4024018 Kurt.maxis kurtosis about minor axis -0.4525923 Kurt.Maxis kurtosis about major axis -2.0461912 Holl.Ra hollows ratio 5.9133060 4.2 Global: Explain behaviour of system with data 4.2.1 Global surrogate models A surrogate model is a simple, explainable model that explains a complex machine learning model. ### Partial dependency plots Partial dependency plots show the relationship between the target and one or more features by averaging out all the other features. A dependency plot can show if the relationship between target and feature is linear, monotonic or something else. Are only partially global: It is global because it takes into account all instances, but it is local in the feature, because partial dependency plots only examine one variable, as the name suggests. ### Individual conditional expectation (ICE) plot ### Variable importance ### LOCO (Leave-One-Covariate-Out) ### Interactions ### Residual analysis A residual value is the difference of the models prediction and the actual value. ### Confusion matrix ### Sensitivity analysis of predictions Testing the stability of the model predictions/classifications using simulated data. It can help to trust the model to not be instable in certain settings. 4.3 Local: Explain single decisions 4.3.1 Local surrogate models (LIME) LIME is also a surrogate model, but it is a local one. ### Model explanation system From paper: (Turner 2015) Classifier \\(f\\), that takes feature vector $ x X = ^D$. Let’s call the explanation ‘Ex’. Applied only on binary classification. Properties: - Eglibility: An explanation is called eligible if \\(P(f(x) = 1 | Ex(x) = 1) \\geq P(f(x) = 1)\\) - Generality (or recall): Probability that the explanation is true \\(P(Ex(x) = 1 | f(x) = 1)\\) - Accuracy (or precision) of explanation: Probability the classifier is correct, given the explanation is correct \\(P(f(x) = 1 | Ex(x) = 1)\\) - Validity of explanation: An explanation is valid at x if it is eliglbe and true at x \\((E(x)=1)\\) ## Maximum activation analysis ### LOCO (Leave-One-Covariate-Out) also local ### Max points lost Compare individual prediction with ‘ideal’ case (maximum probability) in terms of points lost per feature. Only works with monotonicity. And ideal case candidate maxes out each feature regarding highest probability of interest. The feature in which the instance is farthest away from the ideal case is the most negative point, why it should not be in class of interest. Feature with point closest to ideal is the least negative reason. References "],
["specific.html", "5 Model-specific explanations for complex models 5.1 Global explanations", " 5 Model-specific explanations for complex models 5.1 Global explanations 5.1.1 Random Forests: Treeinterpreter https://github.com/andosa/treeinterpreter ## Local explanations "]
]
