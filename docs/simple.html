<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>XAI: Explainable artificial intelligence</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more explainable.">
  <meta name="generator" content="bookdown 0.3.10 and GitBook 2.6.7">

  <meta property="og:title" content="XAI: Explainable artificial intelligence" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more explainable." />
  <meta name="github-repo" content="christophM/xai-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="XAI: Explainable artificial intelligence" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more explainable." />
  

<meta name="author" content="Christoph Molnar">


<meta name="date" content="2017-04-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="intro.html">
<link rel="next" href="data.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#who-is-this-book-for"><i class="fa fa-check"></i><b>1.1</b> Who is this book for?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#what-is-machine-learning-and-why-is-it-important"><i class="fa fa-check"></i><b>1.2</b> What is machine learning and why is it important?</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#why-is-explainability-important"><i class="fa fa-check"></i><b>1.3</b> Why is explainability important?</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#is-a-good-user-experience-enough"><i class="fa fa-check"></i><b>1.4</b> Is a good user experience enough?</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#outline-of-the-book"><i class="fa fa-check"></i><b>1.5</b> Outline of the book</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#scope-of-explainability"><i class="fa fa-check"></i><b>1.6</b> Scope of explainability</a><ul>
<li class="chapter" data-level="1.6.1" data-path="intro.html"><a href="intro.html#algorithm"><i class="fa fa-check"></i><b>1.6.1</b> Algorithm</a></li>
<li class="chapter" data-level="1.6.2" data-path="intro.html"><a href="intro.html#global-model-explainability"><i class="fa fa-check"></i><b>1.6.2</b> Global model explainability</a></li>
<li class="chapter" data-level="1.6.3" data-path="intro.html"><a href="intro.html#explain-the-decision-for-a-single-instance"><i class="fa fa-check"></i><b>1.6.3</b> Explain the decision for a single instance</a></li>
<li class="chapter" data-level="1.6.4" data-path="intro.html"><a href="intro.html#explain-the-decisions-for-a-group-of-instances"><i class="fa fa-check"></i><b>1.6.4</b> Explain the decisions for a group of instances</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#tool-vs.product"><i class="fa fa-check"></i><b>1.7</b> Tool vs.Â product</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#evaluating-explainability"><i class="fa fa-check"></i><b>1.8</b> Evaluating explainability</a><ul>
<li class="chapter" data-level="1.8.1" data-path="intro.html"><a href="intro.html#approaches-for-evaluation-of-the-explanation-quality"><i class="fa fa-check"></i><b>1.8.1</b> Approaches for evaluation of the explanation quality</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#definitions"><i class="fa fa-check"></i><b>1.9</b> Definitions</a></li>
<li class="chapter" data-level="1.10" data-path="intro.html"><a href="intro.html#terminology"><i class="fa fa-check"></i><b>1.10</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>2</b> Keep it simple</a><ul>
<li class="chapter" data-level="2.1" data-path="simple.html"><a href="simple.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="simple.html"><a href="simple.html#linear-models"><i class="fa fa-check"></i><b>2.2</b> Linear models</a><ul>
<li class="chapter" data-level="2.2.1" data-path="simple.html"><a href="simple.html#interpretation"><i class="fa fa-check"></i><b>2.2.1</b> Interpretation</a></li>
<li class="chapter" data-level="2.2.2" data-path="simple.html"><a href="simple.html#interpretation-example"><i class="fa fa-check"></i><b>2.2.2</b> Interpretation example</a></li>
<li class="chapter" data-level="2.2.3" data-path="simple.html"><a href="simple.html#visual-parameter-interpretation"><i class="fa fa-check"></i><b>2.2.3</b> Visual parameter interpretation</a></li>
<li class="chapter" data-level="2.2.4" data-path="simple.html"><a href="simple.html#explaining-single-predictions"><i class="fa fa-check"></i><b>2.2.4</b> Explaining single predictions</a></li>
<li class="chapter" data-level="2.2.5" data-path="simple.html"><a href="simple.html#interpretation-templates"><i class="fa fa-check"></i><b>2.2.5</b> Interpretation templates</a></li>
<li class="chapter" data-level="2.2.6" data-path="simple.html"><a href="simple.html#coding-categorical-variables"><i class="fa fa-check"></i><b>2.2.6</b> Coding categorical variables:</a></li>
<li class="chapter" data-level="2.2.7" data-path="simple.html"><a href="simple.html#assuring-sparsity-in-linear-models"><i class="fa fa-check"></i><b>2.2.7</b> Assuring sparsity in linear models</a></li>
<li class="chapter" data-level="2.2.8" data-path="simple.html"><a href="simple.html#the-disadvantages-of-linear-models"><i class="fa fa-check"></i><b>2.2.8</b> The disadvantages of linear models</a></li>
<li class="chapter" data-level="2.2.9" data-path="simple.html"><a href="simple.html#towards-complexer-relationships-within-linear-model-class"><i class="fa fa-check"></i><b>2.2.9</b> Towards complexer relationships within linear model class</a></li>
<li class="chapter" data-level="2.2.10" data-path="simple.html"><a href="simple.html#linear-models-beyond-gaussian-regression"><i class="fa fa-check"></i><b>2.2.10</b> Linear models beyond gaussian regression</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="simple.html"><a href="simple.html#decision-trees"><i class="fa fa-check"></i><b>2.3</b> Decision trees</a><ul>
<li class="chapter" data-level="2.3.1" data-path="simple.html"><a href="simple.html#explaining-treeinterpreter"><i class="fa fa-check"></i><b>2.3.1</b> Explaining: Treeinterpreter</a></li>
<li class="chapter" data-level="2.3.2" data-path="simple.html"><a href="simple.html#association-rule-mining"><i class="fa fa-check"></i><b>2.3.2</b> Association rule mining</a></li>
<li class="chapter" data-level="2.3.3" data-path="simple.html"><a href="simple.html#rule-induction"><i class="fa fa-check"></i><b>2.3.3</b> Rule induction</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="simple.html"><a href="simple.html#decision-tables"><i class="fa fa-check"></i><b>2.4</b> Decision tables</a></li>
<li class="chapter" data-level="2.5" data-path="simple.html"><a href="simple.html#nearest-neighbours"><i class="fa fa-check"></i><b>2.5</b> Nearest neighbours</a><ul>
<li class="chapter" data-level="2.5.1" data-path="simple.html"><a href="simple.html#learning-deep-k-nearest-neighbour-representations"><i class="fa fa-check"></i><b>2.5.1</b> Learning deep k-nearest neighbour representations</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="simple.html"><a href="simple.html#bayesian-network-classifiers"><i class="fa fa-check"></i><b>2.6</b> Bayesian network classifiers</a></li>
<li class="chapter" data-level="2.7" data-path="simple.html"><a href="simple.html#naive-bayes-classifier-and-other-graphical-models"><i class="fa fa-check"></i><b>2.7</b> Naive Bayes Classifier and other graphical models</a><ul>
<li class="chapter" data-level="2.7.1" data-path="simple.html"><a href="simple.html#naive-bayes"><i class="fa fa-check"></i><b>2.7.1</b> Naive bayes</a></li>
<li class="chapter" data-level="2.7.2" data-path="simple.html"><a href="simple.html#more-general-bayesian-network-approaches"><i class="fa fa-check"></i><b>2.7.2</b> More general bayesian network approache(s)</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="simple.html"><a href="simple.html#prototypes"><i class="fa fa-check"></i><b>2.8</b> Prototypes</a></li>
<li class="chapter" data-level="2.9" data-path="simple.html"><a href="simple.html#interpretation-overview"><i class="fa fa-check"></i><b>2.9</b> Interpretation overview</a></li>
<li class="chapter" data-level="2.10" data-path="simple.html"><a href="simple.html#monotonicity-constraints-between-features"><i class="fa fa-check"></i><b>2.10</b> Monotonicity constraints between features</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Explain the data</a><ul>
<li class="chapter" data-level="3.1" data-path="data.html"><a href="data.html#visualizations"><i class="fa fa-check"></i><b>3.1</b> visualizations</a></li>
<li class="chapter" data-level="3.2" data-path="data.html"><a href="data.html#pca"><i class="fa fa-check"></i><b>3.2</b> PCA</a><ul>
<li class="chapter" data-level="3.2.1" data-path="data.html"><a href="data.html#mds"><i class="fa fa-check"></i><b>3.2.1</b> MDS</a></li>
<li class="chapter" data-level="3.2.2" data-path="data.html"><a href="data.html#t-sne"><i class="fa fa-check"></i><b>3.2.2</b> t-SNE</a></li>
<li class="chapter" data-level="3.2.3" data-path="data.html"><a href="data.html#glyphs"><i class="fa fa-check"></i><b>3.2.3</b> Glyphs</a></li>
<li class="chapter" data-level="3.2.4" data-path="data.html"><a href="data.html#correlation-graphs"><i class="fa fa-check"></i><b>3.2.4</b> Correlation Graphs</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="data.html"><a href="data.html#biases.-source-httpsen.m.wikipedia.orgwikibias_statistics"><i class="fa fa-check"></i><b>3.3</b> Biases. Source: <a href="https://en.m.wikipedia.org/wiki/Bias_(statistics)" class="uri">https://en.m.wikipedia.org/wiki/Bias_(statistics)</a></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html"><i class="fa fa-check"></i><b>4</b> Model-agnostic explanations</a><ul>
<li class="chapter" data-level="4.0.1" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#example-justification-narratives-with-the-vehicle-data-set"><i class="fa fa-check"></i><b>4.0.1</b> Example justification narratives with the vehicle data set</a></li>
<li class="chapter" data-level="4.1" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#global-explain-behaviour-of-system-with-data"><i class="fa fa-check"></i><b>4.1</b> Global: Explain behaviour of system with data</a><ul>
<li class="chapter" data-level="4.1.1" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#global-surrogate-models"><i class="fa fa-check"></i><b>4.1.1</b> Global surrogate models</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#local-explain-single-decisions"><i class="fa fa-check"></i><b>4.2</b> Local: Explain single decisions</a><ul>
<li class="chapter" data-level="4.2.1" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html#local-surrogate-models-lime"><i class="fa fa-check"></i><b>4.2.1</b> Local surrogate models (LIME)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="specific.html"><a href="specific.html"><i class="fa fa-check"></i><b>5</b> Model-specific explanations for complex models</a><ul>
<li class="chapter" data-level="5.1" data-path="specific.html"><a href="specific.html#global-explanations"><i class="fa fa-check"></i><b>5.1</b> Global explanations</a><ul>
<li class="chapter" data-level="5.1.1" data-path="specific.html"><a href="specific.html#random-forests-treeinterpreter"><i class="fa fa-check"></i><b>5.1.1</b> Random Forests: Treeinterpreter</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">XAI: Explainable artificial intelligence</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple" class="section level1">
<h1><span class="header-section-number">2</span> Keep it simple</h1>
<p>The most straight forward way to achieve explainable machine learning algorithms is to use only a subset algorithms that yield an understandable model structure.</p>
<p>These are:</p>
<ul>
<li>Linear models (sparse)</li>
<li>Decision trees</li>
<li>Decision rules</li>
</ul>
<p>In the following chapters these we will talk about the algorithm with itâs variants. Not in detail, only the basics, because there are already a ton of books, videos, tutorials, papers and so on about them. We will focus on how to interpret the models and why they are explainable. The chapter covers linear models, decision trees, decision rules, neighbour methods and graphical models.</p>
<div id="overview" class="section level2">
<h2><span class="header-section-number">2.1</span> Overview</h2>
<table>
<thead>
<tr class="header">
<th align="left">Algorithm</th>
<th align="left">Linear</th>
<th align="left">Monotonicity</th>
<th align="left">Interaction built-in</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Linear models</td>
<td align="left">Yes</td>
<td align="left">Yes</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">Decision trees</td>
<td align="left">No</td>
<td align="left">Not by default</td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">Decision rules</td>
<td align="left">No</td>
<td align="left">Not by default</td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">Naive bayes</td>
<td align="left">Yes</td>
<td align="left">Yes</td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left">Nearest neighbours</td>
<td align="left">No</td>
<td align="left">No</td>
<td align="left">No</td>
</tr>
</tbody>
</table>
</div>
<div id="linear-models" class="section level2">
<h2><span class="header-section-number">2.2</span> Linear models</h2>
<p>Linear models have been and are still used by statistician, computer scientists and other people with quantitative problems. They learn straightforward linear (and monotonic) relationships between the target and the features. The target changes by a learned weight depending on the feature. Monotonicity makes the interpretation easy.</p>
<p>Linear models can be used to model the dependency of a regression variable (here Y) on K covariates. As the name says, the learned relationships are linear in the form of</p>
<p><span class="math display">\[y_{i} = \beta_{0} + \beta_{1} \cdot x_{i,1} + \ldots + \beta_{K} x_{i,K} + \epsilon_{i}\]</span></p>
<p>The i-th observationâs outcome is a weighted sum of itâs K features. The <span class="math inline">\(\beta_{k}\)</span> represent the learned feature weights or coefficients. The <span class="math inline">\(\epsilon_{i}\)</span> is the error we are still making, the difference between the predicted and actual outcome.</p>
<p>The biggest advantage is the linearity: It makes the estimation procedure straight forward and most importantly these linear equations have an easy to understand interpretation. That is one of the main reasons why the linear model and all itâs descendants are so widespread in academic fields like medicine, sociology, psychology and many more quantitative research fields. In this areas it is important to not only predict e.g.Â the clinical outcome of a patient, but also quantify the influence of the medication while at the same time accounting for things like sex, age and other variables.</p>
<div id="interpretation" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Interpretation</h3>
<p>The interpretation of the coefficients:</p>
<ul>
<li>Continuous regression variable: For an increase of one point of the variable <span class="math inline">\(x_{j}\)</span> the estimated outcome changes by <span class="math inline">\(\beta_{j}\)</span></li>
<li>Binary categorical variables: One of the variables is the reference level (in some languages the one that was coded in 0). A change of the variable <span class="math inline">\(x_{i}\)</span> the reference level to the other category changes the estimated outcome by <span class="math inline">\(\beta_{i}\)</span></li>
<li>categorical variables with many levels: One solution to deal with many variables is to one-hot-encode them, meaning each level gets itâs own column. From a categorical variable with L levels, you only need L-1 columsn, otherwise it is over parameterized. The interpretation for each level is then according to the binary variables. Some language like R allow to</li>
<li>Intercept <span class="math inline">\(\beta_{0}\)</span>: The interpretation is: Given all continuous variables are zero and the categorical variables are on the reference level, the estimated outcome of <span class="math inline">\(y_{i}\)</span> is <span class="math inline">\(\beta_{0}\)</span>. The interpretation of <span class="math inline">\(\beta_{0}\)</span> is usually not relevant.</li>
</ul>
</div>
<div id="interpretation-example" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Interpretation example</h3>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">Estimate</th>
<th align="right">Std. Error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td align="right">36.4594884</td>
<td align="right">5.1034588</td>
</tr>
<tr class="even">
<td>per capita crime rate</td>
<td align="right">-0.1080114</td>
<td align="right">0.0328650</td>
</tr>
<tr class="odd">
<td>proportion of residential land zoned for lots over 25,000 sq.ft</td>
<td align="right">0.0464205</td>
<td align="right">0.0137275</td>
</tr>
<tr class="even">
<td>proportion of non-retail business acres per town</td>
<td align="right">0.0205586</td>
<td align="right">0.0614957</td>
</tr>
<tr class="odd">
<td>Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</td>
<td align="right">2.6867338</td>
<td align="right">0.8615798</td>
</tr>
<tr class="even">
<td>nitric oxides concentration (parts per 10 million)</td>
<td align="right">-17.7666112</td>
<td align="right">3.8197437</td>
</tr>
<tr class="odd">
<td>average number of rooms per dwelling</td>
<td align="right">3.8098652</td>
<td align="right">0.4179253</td>
</tr>
<tr class="even">
<td>proportion of owner-occupied units built prior to 1940</td>
<td align="right">0.0006922</td>
<td align="right">0.0132098</td>
</tr>
<tr class="odd">
<td>weighted distances to five Boston employment centres</td>
<td align="right">-1.4755668</td>
<td align="right">0.1994547</td>
</tr>
<tr class="even">
<td>index of accessibility to radial highways</td>
<td align="right">0.3060495</td>
<td align="right">0.0663464</td>
</tr>
<tr class="odd">
<td>full-value property-tax rate per USD 10,000</td>
<td align="right">-0.0123346</td>
<td align="right">0.0037605</td>
</tr>
<tr class="even">
<td>pupil-teacher ratio by town</td>
<td align="right">-0.9527472</td>
<td align="right">0.1308268</td>
</tr>
<tr class="odd">
<td>1000(B - 0.63)^2 where B is the proportion of blacks by town</td>
<td align="right">0.0093117</td>
<td align="right">0.0026860</td>
</tr>
<tr class="even">
<td>percentage of lower status of the population</td>
<td align="right">-0.5247584</td>
<td align="right">0.0507153</td>
</tr>
</tbody>
</table>
<p>Interpretation of a numerical variable (âaverage number of rooms per dwellingâ): An increase of the average number of rooms by 1 increases the median value of houses in this suburb by <span class="math inline">\(3800\$ (= 3.8 \cdot 1000)\)</span>, given all other features stay the same.</p>
<p>Interpretation of a categorical variable (âCharles River dummy variable (= 1 if tract bounds river; 0 otherwise)â): The median value of houses in suburbs that bound the Charles River is <span class="math inline">\(2690\$ (= 2.69 \cdot 1000\$)\)</span> compared to houses that do not, given all other features stay the same.</p>
<p>As you can see in the interpretation examples, the interpretations are always coming with the clause that âall other features stay the sameâ. Thatâs because of the nature of linear models: All features are input linearly into the function with no interactions (unless explicitly specified). The good side is, that is isolates the interpretation. If you think of the features as turn-switches that you can turn up or down, it is nice to see what happens when you would just turn the switch for one feature, for example the average number of rooms. But this might not reflect the structure of the data, because the features are often correlated. It might not be meaningful to see the effect of turning up average room sizes because it might be strongly correlated with the proportion of owner-occupied units built prior to 1940.</p>
</div>
<div id="visual-parameter-interpretation" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Visual parameter interpretation</h3>
<div id="weight-plot" class="section level4">
<h4><span class="header-section-number">2.2.3.1</span> Weight plot</h4>
<p>The information of the coefficient table can also be put into a visualization, which makes the weights and the uncertainty about them can be made understandable on one glance. The weight is displayed as a point and the 95% confidence interval around the point with a line. The 95% confidence interval means that if the linear model was repeated 100 times on</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coef_plot =<span class="st"> </span><span class="cf">function</span>(mod, <span class="dt">alpha =</span> <span class="fl">0.05</span>, <span class="dt">remove_intercept =</span> <span class="ot">TRUE</span>){
  lm_summary =<span class="st"> </span><span class="kw">summary</span>(mod)<span class="op">$</span>coefficients

  df =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Features =</span> <span class="kw">rownames</span>(lm_summary),
                  <span class="dt">Estimate =</span> lm_summary[,<span class="st">&#39;Estimate&#39;</span>],
                  <span class="dt">std_error =</span> lm_summary[,<span class="st">&#39;Std. Error&#39;</span>])
  df<span class="op">$</span>lower =<span class="st"> </span>df<span class="op">$</span>Estimate <span class="op">-</span><span class="st"> </span><span class="kw">qnorm</span>(alpha<span class="op">/</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>df<span class="op">$</span>std_error
  df<span class="op">$</span>upper =<span class="st"> </span>df<span class="op">$</span>Estimate <span class="op">+</span><span class="st"> </span><span class="kw">qnorm</span>(alpha<span class="op">/</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>df<span class="op">$</span>std_error


  <span class="cf">if</span>(remove_intercept){
    df =<span class="st"> </span>df[<span class="op">!</span>(df<span class="op">$</span>Features <span class="op">==</span><span class="st"> &#39;(Intercept)&#39;</span>),]
  }
  <span class="kw">ggplot</span>(df) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x=</span>Estimate, <span class="dt">y=</span>Features)) <span class="op">+</span><span class="st">  </span>
<span class="st">    </span><span class="kw">geom_segment</span>(<span class="kw">aes</span>(<span class="dt">y=</span>Features, <span class="dt">yend=</span>Features, <span class="dt">x=</span>lower, <span class="dt">xend=</span>upper), <span class="dt">arrow =</span> <span class="kw">arrow</span>(<span class="dt">angle=</span><span class="dv">90</span>, <span class="dt">ends=</span><span class="st">&#39;both&#39;</span>, <span class="dt">length =</span> <span class="kw">unit</span>(<span class="fl">0.1</span>, <span class="st">&#39;cm&#39;</span>))) <span class="op">+</span>
<span class="st">    </span>my_theme
}

<span class="kw">coef_plot</span>(mod)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-2-1.png" width="672" /> TODO: Add interpetation</p>
</div>
<div id="effect-plot" class="section level4">
<h4><span class="header-section-number">2.2.3.2</span> Effect plot</h4>
<p>The weights of the linear model only have meaning, when combined with the actual features. The weights depend on the scale of the features and will be different if you have a features measuring some height and you switch from inches to centemeters. The weight will change, but the actual relationships in your data will not. Also it is important to know the distribution of your feature in the data, because if you have a very low variance, it means that almost all instances will get a similar contribution from this feature. The effect plot can help to understand how much the combination of a weight and a feature contributes to the predictions in your data. Start with the computation of the effects, which is the weight per feature times the feature of an instance: <span class="math inline">\(eff_{i,k} = w_{k} \cdot x_{i,k}\)</span>. The resulting effects are visualized with boxplots: The box contains the effect range for half of your data (25% to 75% effect quantiles). The line in the box is the median effect, so 50% of the instances have a lower and the other half a higher effect on the prediction than the median value. The whiskers are <span class="math inline">\(+/i 1.58 IQR / \sqrt{n}\)</span>, with IQR being the inter quartile range ($q_{0.75} - q_{0.25}). The points are outlier to the whiskers.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">get_effects =<span class="st"> </span><span class="cf">function</span>(mod){
  X =<span class="st"> </span><span class="kw">model.matrix</span>(mod)
  X =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">predict</span>(mod, <span class="dt">type =</span> <span class="st">&#39;terms&#39;</span>))

  means =<span class="st"> </span><span class="kw">apply</span>(mod<span class="op">$</span>x, <span class="dv">2</span>, mean) <span class="op">*</span><span class="st"> </span>mod<span class="op">$</span>coefficients
  means =<span class="st"> </span>means[<span class="kw">names</span>(means)  <span class="op">!=</span><span class="st"> &quot;(Intercept)&quot;</span>]
  <span class="co"># predict with type=&#39;terms&#39; centers the results, so we have to add the mean again</span>
  means_X =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(means, <span class="dt">times =</span> <span class="kw">nrow</span>(X)), <span class="dt">nrow=</span><span class="kw">nrow</span>(X), <span class="dt">byrow=</span><span class="ot">TRUE</span>)
  X =<span class="st"> </span>X <span class="op">+</span><span class="st"> </span>means_X
  X  
}

effect_plot =<span class="st"> </span><span class="cf">function</span>(mod, <span class="dt">feature_names=</span><span class="ot">NULL</span>){
  X =<span class="st"> </span><span class="kw">get_effects</span>(mod)
  <span class="cf">if</span>(<span class="op">!</span><span class="kw">missing</span>(feature_names)){
    <span class="kw">rownames</span>(X) =<span class="st"> </span>feature_names
  }
  X =<span class="st"> </span><span class="kw">gather</span>(X)
  <span class="kw">ggplot</span>(X) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_boxplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>key, <span class="dt">y=</span>value, <span class="dt">group=</span>key)) <span class="op">+</span>
<span class="st">    </span><span class="kw">coord_flip</span>() <span class="op">+</span><span class="st"> </span>my_theme
}

<span class="kw">effect_plot</span>(mod)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-3-1.png" width="672" /> TODO: Add interpetation</p>
</div>
</div>
<div id="explaining-single-predictions" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Explaining single predictions</h3>
<p>Why did a certain instance get the prediction it got from the linear model? This can again be answered by bringing together the weights and features and computing the effect. Now the effect will tell you how much each feature contributed towards the sum of the prediction. This is only meaningful if you compare the instance specific effects with the mean effects. See also the chapter about âJustification Narrativesâ.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">i =<span class="st"> </span><span class="dv">1</span>
effects =<span class="st"> </span><span class="kw">get_effects</span>(mod)
effects_i =<span class="st"> </span><span class="kw">gather</span>(effects[i, ])
predictions =<span class="st"> </span><span class="kw">predict</span>(mod)
predictions_mean =<span class="st"> </span><span class="kw">mean</span>(predictions)
pred_i =<span class="st"> </span>predictions[i]

<span class="kw">effect_plot</span>(mod) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x=</span>key, <span class="dt">y=</span>value), <span class="dt">color =</span> <span class="st">&#39;red&#39;</span>, <span class="dt">data =</span> effects_i) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="kw">sprintf</span>(<span class="st">&#39;Predicted value for instance: %.2f</span><span class="ch">\n</span><span class="st"> Average predicted value: %.2f&#39;</span>, predictions_mean, pred_i))</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<!-- TODO: Add interpetation -->
</div>
<div id="interpretation-templates" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Interpretation templates</h3>
<p><strong>Interpretation of a numerical feature</strong>:</p>
<p>An increase of <span class="math inline">\(x_{k}\)</span> by one unit increases the expectation for <span class="math inline">\(y\)</span> by <span class="math inline">\(\beta_x{k}\)</span> units if all other features X stay the same.</p>
<p><strong>Interpretation of a categorical feature</strong>:</p>
<p>The category coded with 1 of <span class="math inline">\(x_{k}\)</span> increases the expectation for <span class="math inline">\(y\)</span> by <span class="math inline">\(\beta_{k}\)</span> compared to the reference category (coded with 0).</p>
</div>
<div id="coding-categorical-variables" class="section level3">
<h3><span class="header-section-number">2.2.6</span> Coding categorical variables:</h3>
<p>There are several ways to represent a categorical variable, which has an influence on the interpretation: <a href="http://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/" class="uri">http://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/</a> and <a href="http://heidiseibold.github.io/page7/" class="uri">http://heidiseibold.github.io/page7/</a></p>
<p>Described above is the treatment coding, which is usually sufficient. Using different codings boils down to creating different matrices from your one column with the categorical feature. I present three different codings, but there are many more. The example has six instances and one categorical feature with 3 levels. The first two instances are in category A, instances three and four are in category B and the last two instances are in category C.</p>
<ul>
<li><strong>Treatment coding</strong> compares each level to the reference level. The intercept is the mean of the reference group. The first column is the intercept, which is always 1. Column two is an indicator whether instance <span class="math inline">\(i\)</span> is in category B, columns three is an indicator for category C. There is no need for a column for category A, because than the system would be over specified. Knowing that an instance is neither in category B or C is enough. <span class="math display">\[
\begin{pmatrix}
1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 \\
1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
\end{pmatrix}
\]</span></li>
<li><strong>Effect coding</strong> compares each level to the overall mean of <span class="math inline">\(y\)</span>. The first column is again the intercept. The weight <span class="math inline">\(\beta_{0}\)</span> which is associated to the intercept represents the overall mean and <span class="math inline">\(\beta_{1}\)</span>, the weight for column two is the difference between the overall mean and category B. The overall effect of category B is <span class="math inline">\(\beta_{0}\)</span> + _{1}$. Interpretation for category C is equivalent. For the reference category A, <span class="math inline">\(-(\beta_{1} + \beta_{2})\)</span> is the difference of the category C to the overall mean and <span class="math inline">\(\beta_{0} -(\beta_{1} + \beta_{2})\)</span> the overall effect of category C. <span class="math display">\[
\begin{pmatrix}
1 &amp; -1 &amp; -1 \\
1 &amp; -1 &amp; -1 \\
1 &amp; 1 &amp; 0 \\
1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
\end{pmatrix}
\]</span></li>
<li><strong>Dummy coding</strong> compares each level to the level mean of <span class="math inline">\(y\)</span>. If all level are have the same frequency the resulting coefficients will be the same as in effect coding. Note that the intercept was dropped here. <span class="math display">\[
\begin{pmatrix}
1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 \\
\end{pmatrix}
\]</span></li>
</ul>
</div>
<div id="assuring-sparsity-in-linear-models" class="section level3">
<h3><span class="header-section-number">2.2.7</span> Assuring sparsity in linear models</h3>
<p>Lasso, Ridge, elasticnet, forward/backward variable selection, dimensionality reduction, â¦ UNDER CONSTRUCTION</p>
</div>
<div id="the-disadvantages-of-linear-models" class="section level3">
<h3><span class="header-section-number">2.2.8</span> The disadvantages of linear models</h3>
<p>They can only represent linear relationships as the name suggests. Each non-linearity or interaction has to be hand-crafted and explicitly given to the model as an input feature. Because of possible high correlation between features, it is possible that a feature that is positively correlated with the outcome might get a negative weight in a linear model, because in the high dimensional space it is negatively correlated. An example: You have a model to predict the rent price and have features like number of rooms and size of the flat. Of course flat size and room number are highly correlated, the bigger a flat the more rooms it has. If you now take both variables into a linear model it might happen, that the flat size is the better predictor and getâs a large positive weight. The room number might end up getting a negative weight, because given that a flat has the same size, increasing the number of rooms could make it less valuable.</p>
</div>
<div id="towards-complexer-relationships-within-linear-model-class" class="section level3">
<h3><span class="header-section-number">2.2.9</span> Towards complexer relationships within linear model class</h3>
<ul>
<li>Adding interactions</li>
<li>Adding non-linear terms like polynomials</li>
<li>Stratifying data by variable and fitting linear models on subsets</li>
</ul>
</div>
<div id="linear-models-beyond-gaussian-regression" class="section level3">
<h3><span class="header-section-number">2.2.10</span> Linear models beyond gaussian regression</h3>
<ul>
<li>Logistic regression</li>
<li>GAMs</li>
<li>Quantile regression</li>
</ul>
</div>
</div>
<div id="decision-trees" class="section level2">
<h2><span class="header-section-number">2.3</span> Decision trees</h2>
<div id="explaining-treeinterpreter" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Explaining: Treeinterpreter</h3>
<p><a href="https://github.com/andosa/treeinterpreter" class="uri">https://github.com/andosa/treeinterpreter</a> ## RuleFit ## Decision rules Decision rules are a class of machine learning model that consists of if X then Y statements. Within the if statement there are sets of features and conditions on them (like X &gt; 4) and Y is a statement about the outcome of interest. One decision rule from a model predicting rents could be:</p>
<p>If a flat is bigger than 100 square meters and has a garden the average rent is 2000 Euro per month.</p>
<p>Letâs split that up: - âFlat is bigger than 100 square metersâ is the first part of the IF statement - âFlat has a gardenâ is a second conditions in the IF. - Both are connected with an âANDâ, so both have to be true for the rule to apply - Predicted outcome (THEN) is that the average rent is 2000 Euro per month.</p>
<p>RULE: IF CONDITIONS THEN TARGET</p>
<p>Arguably decision rules are the most simple classifier in terms of understandability. They are very close to natural language and the way we think. Of course it relies on using sensible, understandable features and the number of conditions within the if statement.</p>
<p>Usually there is more then one rule to describe the associations of the features with the outcome. In our example we only have one rule covering big appartments with flats, but what about a smaller apartment?</p>
<p>Rules can be induced from trees. Each path through the tree is one rule. Rules made of trees have the nice</p>
<p>There are many ways how to induce those kind of rules from a training dataset. The induction algorithms differ in how they create the rules: - Do they allow overlapping rules and how are conflicts handled? - Do they cover the whole training set?</p>
</div>
<div id="association-rule-mining" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Association rule mining</h3>
<p>Concepts: - <strong><em>Support</em></strong> - <strong><em>Confidence</em></strong> - <strong><em>Lift</em></strong></p>
</div>
<div id="rule-induction" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Rule induction</h3>
<div id="the-bayesian-machine-list" class="section level4">
<h4><span class="header-section-number">2.3.3.1</span> The Bayesian Machine List</h4>
<p>[BML]</p>
<p>Target: Multi-class classification with labels 1, â¦, L and training data <span class="math inline">\({(x_i, y_i)}_{i=1}^n\)</span>, where <span class="math inline">\(x_i \in \R\)</span> are the features for observation <span class="math inline">\(i\)</span> and <span class="math inline">\(y_i \in {1, ..., L}\)</span> are the labels.<br />
Algorithm: 1. Generate a list of rules $ r = 1, â¦, R$ with a rule-mining Algorithm 2. Sample permutation of rules using <span class="math inline">\(\pi\)</span> from prior <span class="math inline">\(P(p, C)\)</span> 3. Using this ordering, select the first rule that applies, in that it matches the observed feature <span class="math inline">\(x_i\)</span>. Call the rule <span class="math inline">\(\tilde{r}_i\)</span> 4. Draw a label <span class="math inline">\(y_i\)</span> from a Dirichlet-Multinomial distribution <span class="math inline">\(\Theta^{\tilde{r}_i}\)</span>, with Dirichlet parameters <span class="math inline">\(\alpha_1, ..., \alpha_L\)</span> and counts $n_{_i 1}, â¦, <span class="math inline">\(n_{\tilde{r}_i L}\)</span> for rule <span class="math inline">\(\tilde{r}_i\)</span> chosen in the previous step.</p>
<p>[BML] Letham, B., Rudin, C., Mccormick, T. H., &amp; Madigan, D. (2010). An Interpretable Stroke Prediction Model Using Rules and Bayesian Analysis. AAAI Technical Report WS-13-17, (609), 65â67.</p>
</div>
</div>
</div>
<div id="decision-tables" class="section level2">
<h2><span class="header-section-number">2.4</span> Decision tables</h2>
</div>
<div id="nearest-neighbours" class="section level2">
<h2><span class="header-section-number">2.5</span> Nearest neighbours</h2>
<div id="learning-deep-k-nearest-neighbour-representations" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Learning deep k-nearest neighbour representations</h3>
</div>
</div>
<div id="bayesian-network-classifiers" class="section level2">
<h2><span class="header-section-number">2.6</span> Bayesian network classifiers</h2>
</div>
<div id="naive-bayes-classifier-and-other-graphical-models" class="section level2">
<h2><span class="header-section-number">2.7</span> Naive Bayes Classifier and other graphical models</h2>
<div id="naive-bayes" class="section level3">
<h3><span class="header-section-number">2.7.1</span> Naive bayes</h3>
<p>Good in case you want interpretation of all features. Bad: No interaction, but even strong assumption of independence of features given the class. But if you indeed have independent features, then it might be the best model.</p>
</div>
<div id="more-general-bayesian-network-approaches" class="section level3">
<h3><span class="header-section-number">2.7.2</span> More general bayesian network approache(s)</h3>
<div id="bayesian-network-augmented-naive-bayes-ban-illustrated" class="section level4">
<h4><span class="header-section-number">2.7.2.1</span> Bayesian Network-Augmented NaÃ¯ve Bayes (BAN) illustrated</h4>
</div>
<div id="general-bayesian-network" class="section level4">
<h4><span class="header-section-number">2.7.2.2</span> General Bayesian Network</h4>
</div>
</div>
</div>
<div id="prototypes" class="section level2">
<h2><span class="header-section-number">2.8</span> Prototypes</h2>
</div>
<div id="interpretation-overview" class="section level2">
<h2><span class="header-section-number">2.9</span> Interpretation overview</h2>
<p>INSERT HERE: One example of classification task and one for regression task plus for each a table with the interpretations per model (maybe only example for one variable). Plus verbal template for each interpretation.</p>
</div>
<div id="monotonicity-constraints-between-features" class="section level2">
<h2><span class="header-section-number">2.10</span> Monotonicity constraints between features</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
