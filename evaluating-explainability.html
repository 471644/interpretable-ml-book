<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Interpretable machine learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Interpretable machine learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable." />
  <meta name="github-repo" content="christophM/xai-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Interpretable machine learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable." />
  

<meta name="author" content="Christoph Molnar">


<meta name="date" content="2017-08-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="scope-of-explainability.html">
<link rel="next" href="datasets.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Explainable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="who-should-read-this-book.html"><a href="who-should-read-this-book.html"><i class="fa fa-check"></i><b>1.1</b> Who should read this book</a></li>
<li class="chapter" data-level="1.2" data-path="outline.html"><a href="outline.html"><i class="fa fa-check"></i><b>1.2</b> Outline</a></li>
<li class="chapter" data-level="1.3" data-path="what-is-machine-learning-and-why-is-it-important.html"><a href="what-is-machine-learning-and-why-is-it-important.html"><i class="fa fa-check"></i><b>1.3</b> What is machine learning and why is it important?</a></li>
<li class="chapter" data-level="1.4" data-path="definitions.html"><a href="definitions.html"><i class="fa fa-check"></i><b>1.4</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> Interpretability</a><ul>
<li class="chapter" data-level="2.1" data-path="what-is-interpretability.html"><a href="what-is-interpretability.html"><i class="fa fa-check"></i><b>2.1</b> What is interpretability</a></li>
<li class="chapter" data-level="2.2" data-path="when-is-interpretability-important.html"><a href="when-is-interpretability-important.html"><i class="fa fa-check"></i><b>2.2</b> When is interpretability important?</a></li>
<li class="chapter" data-level="2.3" data-path="the-bigger-picture.html"><a href="the-bigger-picture.html"><i class="fa fa-check"></i><b>2.3</b> The bigger picture</a></li>
<li class="chapter" data-level="2.4" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html"><i class="fa fa-check"></i><b>2.4</b> Scope of explainability</a><ul>
<li class="chapter" data-level="2.4.1" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#algorithm-transparency"><i class="fa fa-check"></i><b>2.4.1</b> Algorithm transparency</a></li>
<li class="chapter" data-level="2.4.2" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#global-holistic-model-explainability"><i class="fa fa-check"></i><b>2.4.2</b> Global, holistic model explainability</a></li>
<li class="chapter" data-level="2.4.3" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#global-model-explainability-on-a-modular-level"><i class="fa fa-check"></i><b>2.4.3</b> Global model explainability on a modular level</a></li>
<li class="chapter" data-level="2.4.4" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#explain-the-decision-for-a-single-instance"><i class="fa fa-check"></i><b>2.4.4</b> Explain the decision for a single instance</a></li>
<li class="chapter" data-level="2.4.5" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#explain-the-decisions-for-a-group-of-instances"><i class="fa fa-check"></i><b>2.4.5</b> Explain the decisions for a group of instances</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="evaluating-explainability.html"><a href="evaluating-explainability.html"><i class="fa fa-check"></i><b>2.5</b> Evaluating explainability</a><ul>
<li class="chapter" data-level="2.5.1" data-path="evaluating-explainability.html"><a href="evaluating-explainability.html#approaches-for-evaluation-of-the-explanation-quality"><i class="fa fa-check"></i><b>2.5.1</b> Approaches for evaluation of the explanation quality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="datasets.html"><a href="datasets.html"><i class="fa fa-check"></i><b>3</b> Datasets</a><ul>
<li class="chapter" data-level="3.1" data-path="regression-dataset-bike-sharing-counts.html"><a href="regression-dataset-bike-sharing-counts.html"><i class="fa fa-check"></i><b>3.1</b> Regression dataset: Bike sharing counts</a></li>
<li class="chapter" data-level="3.2" data-path="the-dataset-speed-dating.html"><a href="the-dataset-speed-dating.html"><i class="fa fa-check"></i><b>3.2</b> The dataset: speed dating</a></li>
<li class="chapter" data-level="3.3" data-path="TubeSpam.html"><a href="TubeSpam.html"><i class="fa fa-check"></i><b>3.3</b> TubeSpam dataset: Spam classification on YouTube comments</a></li>
<li class="chapter" data-level="3.4" data-path="risk-factors-for-cervical-cancer.html"><a href="risk-factors-for-cervical-cancer.html"><i class="fa fa-check"></i><b>3.4</b> Risk factors for cervical cancer</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>4</b> Simple, interpretable models</a><ul>
<li class="chapter" data-level="4.1" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>4.1</b> Terminology</a></li>
<li class="chapter" data-level="4.2" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>4.2</b> Overview</a></li>
<li class="chapter" data-level="4.3" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>4.3</b> Linear models</a><ul>
<li class="chapter" data-level="4.3.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>4.3.1</b> Interpretation</a></li>
<li class="chapter" data-level="4.3.2" data-path="limo.html"><a href="limo.html#interpretation-example"><i class="fa fa-check"></i><b>4.3.2</b> Interpretation example</a></li>
<li class="chapter" data-level="4.3.3" data-path="limo.html"><a href="limo.html#interpretation-templates"><i class="fa fa-check"></i><b>4.3.3</b> Interpretation templates</a></li>
<li class="chapter" data-level="4.3.4" data-path="limo.html"><a href="limo.html#visual-parameter-interpretation"><i class="fa fa-check"></i><b>4.3.4</b> Visual parameter interpretation</a></li>
<li class="chapter" data-level="4.3.5" data-path="limo.html"><a href="limo.html#explaining-single-predictions"><i class="fa fa-check"></i><b>4.3.5</b> Explaining single predictions</a></li>
<li class="chapter" data-level="4.3.6" data-path="limo.html"><a href="limo.html#coding-categorical-variables"><i class="fa fa-check"></i><b>4.3.6</b> Coding categorical variables:</a></li>
<li class="chapter" data-level="4.3.7" data-path="limo.html"><a href="limo.html#assuring-sparsity-in-linear-models"><i class="fa fa-check"></i><b>4.3.7</b> Assuring sparsity in linear models</a></li>
<li class="chapter" data-level="4.3.8" data-path="limo.html"><a href="limo.html#the-disadvantages-of-linear-models"><i class="fa fa-check"></i><b>4.3.8</b> The disadvantages of linear models</a></li>
<li class="chapter" data-level="4.3.9" data-path="limo.html"><a href="limo.html#towards-complexer-relationships-within-linear-model-class"><i class="fa fa-check"></i><b>4.3.9</b> Towards complexer relationships within linear model class</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html"><i class="fa fa-check"></i><b>4.4</b> Logistic regression: a linear model for classification</a><ul>
<li class="chapter" data-level="4.4.1" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html#whats-wrong-with-linear-regression-for-classification"><i class="fa fa-check"></i><b>4.4.1</b> What’s wrong with linear regression for classification?</a></li>
<li class="chapter" data-level="4.4.2" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html#logistic-regression"><i class="fa fa-check"></i><b>4.4.2</b> Logistic regression</a></li>
<li class="chapter" data-level="4.4.3" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html#interpretation-1"><i class="fa fa-check"></i><b>4.4.3</b> Interpretation</a></li>
<li class="chapter" data-level="4.4.4" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html#example"><i class="fa fa-check"></i><b>4.4.4</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>4.5</b> Decision trees</a><ul>
<li class="chapter" data-level="4.5.1" data-path="decision-trees.html"><a href="decision-trees.html#interpretation-2"><i class="fa fa-check"></i><b>4.5.1</b> Interpretation</a></li>
<li class="chapter" data-level="4.5.2" data-path="decision-trees.html"><a href="decision-trees.html#interpretation-example-1"><i class="fa fa-check"></i><b>4.5.2</b> Interpretation example</a></li>
<li class="chapter" data-level="4.5.3" data-path="decision-trees.html"><a href="decision-trees.html#advantages"><i class="fa fa-check"></i><b>4.5.3</b> Advantages</a></li>
<li class="chapter" data-level="4.5.4" data-path="decision-trees.html"><a href="decision-trees.html#disadvantages"><i class="fa fa-check"></i><b>4.5.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="other-simple-explainable-models.html"><a href="other-simple-explainable-models.html"><i class="fa fa-check"></i><b>4.6</b> Other simple, explainable models</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html"><i class="fa fa-check"></i><b>5</b> Model-agnostic explanations</a><ul>
<li class="chapter" data-level="5.1" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html"><i class="fa fa-check"></i><b>5.1</b> Global: Explain the behaviour of a model</a><ul>
<li class="chapter" data-level="5.1.1" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html#pdp"><i class="fa fa-check"></i><b>5.1.1</b> Partial dependence plot</a></li>
<li class="chapter" data-level="5.1.2" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html#average-marginal-effects"><i class="fa fa-check"></i><b>5.1.2</b> Average Marginal Effects}</a></li>
<li class="chapter" data-level="5.1.3" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html#feature-importance"><i class="fa fa-check"></i><b>5.1.3</b> Feature importance</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="local-explain-a-single-decisions.html"><a href="local-explain-a-single-decisions.html"><i class="fa fa-check"></i><b>5.2</b> Local: Explain a single decisions</a><ul>
<li class="chapter" data-level="5.2.1" data-path="local-explain-a-single-decisions.html"><a href="local-explain-a-single-decisions.html#individual-conditional-expectation-ice-plot"><i class="fa fa-check"></i><b>5.2.1</b> Individual Conditional Expectation (ICE) plot</a></li>
<li class="chapter" data-level="5.2.2" data-path="local-explain-a-single-decisions.html"><a href="local-explain-a-single-decisions.html#local-surrogate-models-lime"><i class="fa fa-check"></i><b>5.2.2</b> Local surrogate models (LIME)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="model-agnostic-why-not-use-them-on-the-data-itself.html"><a href="model-agnostic-why-not-use-them-on-the-data-itself.html"><i class="fa fa-check"></i><b>5.3</b> Model-agnostic: Why not use them on the data itself?</a></li>
<li class="chapter" data-level="5.4" data-path="explanation-types.html"><a href="explanation-types.html"><i class="fa fa-check"></i><b>5.4</b> Explanation types</a><ul>
<li class="chapter" data-level="5.4.1" data-path="explanation-types.html"><a href="explanation-types.html#structured-output"><i class="fa fa-check"></i><b>5.4.1</b> Structured output</a></li>
<li class="chapter" data-level="5.4.2" data-path="explanation-types.html"><a href="explanation-types.html#viz-explanation"><i class="fa fa-check"></i><b>5.4.2</b> Visualization</a></li>
<li class="chapter" data-level="5.4.3" data-path="explanation-types.html"><a href="explanation-types.html#natural-language-narratives"><i class="fa fa-check"></i><b>5.4.3</b> Natural language (narratives)</a></li>
<li class="chapter" data-level="5.4.4" data-path="explanation-types.html"><a href="explanation-types.html#examples-and-prototypes"><i class="fa fa-check"></i><b>5.4.4</b> Examples and prototypes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>6</b> References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable machine learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="evaluating-explainability" class="section level2">
<h2><span class="header-section-number">2.5</span> Evaluating explainability</h2>
<p>There is no real consensus what explainability in machine learning is. Also it is not clear how to measure it.</p>
<div id="approaches-for-evaluation-of-the-explanation-quality" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Approaches for evaluation of the explanation quality</h3>
<p><span class="citation">(Doshi-Velez and Kim <a href="#ref-Doshi-Velez2017">2017</a>)</span> proposes 3 major levels of evaluating explainability. - Application level evaluation (real task): Put the explanation into the product and let the end user test it. On an application level the radiologists would test the fracture detection software in order to evaluate the model. This requires a good experimental setup and an idea of how to assess the quality. A good baseline for this is always how good a human would be at explaining the same decision. - Human level evaluation (simple task) is a simplified application level evaluation. The difference is that these experiments are not conducted with the domain experts, but with lay humans. This makes experiments less expensive (especially when the domain experts are radiologists) and it is easier to find more humans. An example would be to show a user different explanations and the human would choose the best . - Function level evaluation (proxy task) does not require any humans. This works best when the class of models used is already evaluated by someone else in a human level evaluation.</p>
<div id="function-level-evaluation" class="section level4">
<h4><span class="header-section-number">2.5.1.1</span> Function level evaluation</h4>
<p>Model size is an easy way to measure, but might be too simplistic.</p>
<p>Dimensions of interpretability:</p>
<ul>
<li>Model sparsity: How many features are being used by the explanation?</li>
<li>Monotonicity: Is there a Monotonicity constraint?</li>
<li>Uncertainty: Is a measurement of uncertainty part of the explanation?</li>
<li>Interactions: Is the explanation able to include interaction of features?</li>
<li>Cognitive processing time: How long does it take to understand the explanation.</li>
<li>Feature complexity: What features were used for the explanation? PCA components are harder to understand than word occurrences for example.</li>
<li>Description length of explanation</li>
</ul>
<p>If you can ensure that the machine learning model can explain decisions, following traits can also be checked more easily <span class="citation">(Doshi-Velez and Kim <a href="#ref-Doshi-Velez2017">2017</a>)</span>.</p>
<ul>
<li>Fairness: Unbiased, not discrimating against protected groups (implicit or explicit). An interpretable model can tell you why it decided it decided a certain person is not worthy of a credit and for a human it becomes easy to decide if the decision was based on a learned demographic (e.g. racial) bias.</li>
<li>Privacy: sensitive information in the data is protected.</li>
<li>Reliability/Robustness: Small changes in the input don’t lead to big changes in the ouput/decision.</li>
<li>Causality: Only causal relationships are picked up. So a predicted change in a decision due to arbitrary changes in the input values, are also happening in reality.</li>
<li>Usability:</li>
<li>Trust: It is easier for humans to trust into a system that explains it’s decisions compared to a black box</li>
</ul>

</div>
</div>
</div>
<!-- </div> -->
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Doshi-Velez2017">
<p>Doshi-Velez, Finale, and Been Kim. 2017. “Towards A Rigorous Science of Interpretable Machine Learning,” no. Ml: 1–13. <a href="http://arxiv.org/abs/1702.08608" class="uri">http://arxiv.org/abs/1702.08608</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="scope-of-explainability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="datasets.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/xai-book/edit/master/02-interpretability.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
