<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Explainable machine learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable.">
  <meta name="generator" content="bookdown 0.4 and GitBook 2.6.7">

  <meta property="og:title" content="Explainable machine learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable." />
  <meta name="github-repo" content="christophM/xai-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Explainable machine learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable." />
  

<meta name="author" content="Christoph Molnar">


<meta name="date" content="2017-08-04">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="model-agnostic-explanations.html">
<link rel="next" href="local-explain-a-single-decisions.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Explainable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="who-should-read-this-book.html"><a href="who-should-read-this-book.html"><i class="fa fa-check"></i><b>1.1</b> Who should read this book</a></li>
<li class="chapter" data-level="1.2" data-path="outline.html"><a href="outline.html"><i class="fa fa-check"></i><b>1.2</b> Outline</a></li>
<li class="chapter" data-level="1.3" data-path="what-is-machine-learning-and-why-is-it-important.html"><a href="what-is-machine-learning-and-why-is-it-important.html"><i class="fa fa-check"></i><b>1.3</b> What is machine learning and why is it important?</a></li>
<li class="chapter" data-level="1.4" data-path="definitions.html"><a href="definitions.html"><i class="fa fa-check"></i><b>1.4</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="explainability.html"><a href="explainability.html"><i class="fa fa-check"></i><b>2</b> Explainability</a><ul>
<li class="chapter" data-level="2.1" data-path="what-is-explainability.html"><a href="what-is-explainability.html"><i class="fa fa-check"></i><b>2.1</b> What is explainability</a></li>
<li class="chapter" data-level="2.2" data-path="when-is-explainability-important.html"><a href="when-is-explainability-important.html"><i class="fa fa-check"></i><b>2.2</b> When is explainability important?</a></li>
<li class="chapter" data-level="2.3" data-path="the-bigger-picture.html"><a href="the-bigger-picture.html"><i class="fa fa-check"></i><b>2.3</b> The bigger picture</a></li>
<li class="chapter" data-level="2.4" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html"><i class="fa fa-check"></i><b>2.4</b> Scope of explainability</a><ul>
<li class="chapter" data-level="2.4.1" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#algorithm-transparency"><i class="fa fa-check"></i><b>2.4.1</b> Algorithm transparency</a></li>
<li class="chapter" data-level="2.4.2" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#global-holistic-model-explainability"><i class="fa fa-check"></i><b>2.4.2</b> Global, holistic model explainability</a></li>
<li class="chapter" data-level="2.4.3" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#global-model-explainability-on-a-modular-level"><i class="fa fa-check"></i><b>2.4.3</b> Global model explainability on a modular level</a></li>
<li class="chapter" data-level="2.4.4" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#explain-the-decision-for-a-single-instance"><i class="fa fa-check"></i><b>2.4.4</b> Explain the decision for a single instance</a></li>
<li class="chapter" data-level="2.4.5" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#explain-the-decisions-for-a-group-of-instances"><i class="fa fa-check"></i><b>2.4.5</b> Explain the decisions for a group of instances</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="evaluating-explainability.html"><a href="evaluating-explainability.html"><i class="fa fa-check"></i><b>2.5</b> Evaluating explainability</a><ul>
<li class="chapter" data-level="2.5.1" data-path="evaluating-explainability.html"><a href="evaluating-explainability.html#approaches-for-evaluation-of-the-explanation-quality"><i class="fa fa-check"></i><b>2.5.1</b> Approaches for evaluation of the explanation quality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>3</b> Simple, interpretable models</a><ul>
<li class="chapter" data-level="3.1" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>3.1</b> Terminology</a></li>
<li class="chapter" data-level="3.2" data-path="regression-dataset-bike-sharing-counts.html"><a href="regression-dataset-bike-sharing-counts.html"><i class="fa fa-check"></i><b>3.2</b> Regression dataset: Bike sharing counts</a></li>
<li class="chapter" data-level="3.3" data-path="the-dataset-speed-dating.html"><a href="the-dataset-speed-dating.html"><i class="fa fa-check"></i><b>3.3</b> The dataset: speed dating</a></li>
<li class="chapter" data-level="3.4" data-path="TubeSpam.html"><a href="TubeSpam.html"><i class="fa fa-check"></i><b>3.4</b> TubeSpam dataset: Spam classification on YouTube comments</a></li>
<li class="chapter" data-level="3.5" data-path="risk-factors-for-cervical-cancer.html"><a href="risk-factors-for-cervical-cancer.html"><i class="fa fa-check"></i><b>3.5</b> Risk factors for cervical cancer</a></li>
<li class="chapter" data-level="3.6" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>3.6</b> Overview</a></li>
<li class="chapter" data-level="3.7" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>3.7</b> Linear models</a><ul>
<li class="chapter" data-level="3.7.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>3.7.1</b> Interpretation</a></li>
<li class="chapter" data-level="3.7.2" data-path="limo.html"><a href="limo.html#interpretation-example"><i class="fa fa-check"></i><b>3.7.2</b> Interpretation example</a></li>
<li class="chapter" data-level="3.7.3" data-path="limo.html"><a href="limo.html#interpretation-templates"><i class="fa fa-check"></i><b>3.7.3</b> Interpretation templates</a></li>
<li class="chapter" data-level="3.7.4" data-path="limo.html"><a href="limo.html#visual-parameter-interpretation"><i class="fa fa-check"></i><b>3.7.4</b> Visual parameter interpretation</a></li>
<li class="chapter" data-level="3.7.5" data-path="limo.html"><a href="limo.html#explaining-single-predictions"><i class="fa fa-check"></i><b>3.7.5</b> Explaining single predictions</a></li>
<li class="chapter" data-level="3.7.6" data-path="limo.html"><a href="limo.html#coding-categorical-variables"><i class="fa fa-check"></i><b>3.7.6</b> Coding categorical variables:</a></li>
<li class="chapter" data-level="3.7.7" data-path="limo.html"><a href="limo.html#assuring-sparsity-in-linear-models"><i class="fa fa-check"></i><b>3.7.7</b> Assuring sparsity in linear models</a></li>
<li class="chapter" data-level="3.7.8" data-path="limo.html"><a href="limo.html#the-disadvantages-of-linear-models"><i class="fa fa-check"></i><b>3.7.8</b> The disadvantages of linear models</a></li>
<li class="chapter" data-level="3.7.9" data-path="limo.html"><a href="limo.html#towards-complexer-relationships-within-linear-model-class"><i class="fa fa-check"></i><b>3.7.9</b> Towards complexer relationships within linear model class</a></li>
<li class="chapter" data-level="3.7.10" data-path="limo.html"><a href="limo.html#linear-models-beyond-gaussian-regression"><i class="fa fa-check"></i><b>3.7.10</b> Linear models beyond gaussian regression</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>3.8</b> Decision trees</a><ul>
<li class="chapter" data-level="3.8.1" data-path="decision-trees.html"><a href="decision-trees.html#interpretation-1"><i class="fa fa-check"></i><b>3.8.1</b> Interpretation</a></li>
<li class="chapter" data-level="3.8.2" data-path="decision-trees.html"><a href="decision-trees.html#interpretation-example-1"><i class="fa fa-check"></i><b>3.8.2</b> Interpretation example</a></li>
<li class="chapter" data-level="3.8.3" data-path="decision-trees.html"><a href="decision-trees.html#advantages"><i class="fa fa-check"></i><b>3.8.3</b> Advantages</a></li>
<li class="chapter" data-level="3.8.4" data-path="decision-trees.html"><a href="decision-trees.html#disadvantages"><i class="fa fa-check"></i><b>3.8.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="other-simple-explainable-models.html"><a href="other-simple-explainable-models.html"><i class="fa fa-check"></i><b>3.9</b> Other simple, explainable models</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html"><i class="fa fa-check"></i><b>4</b> Model-agnostic explanations</a><ul>
<li class="chapter" data-level="4.1" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html"><i class="fa fa-check"></i><b>4.1</b> Global: Explain the behaviour of a model</a><ul>
<li class="chapter" data-level="4.1.1" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html#global-surrogate-models"><i class="fa fa-check"></i><b>4.1.1</b> Global surrogate models</a></li>
<li class="chapter" data-level="4.1.2" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html#pdp"><i class="fa fa-check"></i><b>4.1.2</b> Partial dependence plot</a></li>
<li class="chapter" data-level="4.1.3" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html#average-marginal-effects"><i class="fa fa-check"></i><b>4.1.3</b> Average Marginal Effects}</a></li>
<li class="chapter" data-level="4.1.4" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html#feature-importance"><i class="fa fa-check"></i><b>4.1.4</b> Feature importance</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="local-explain-a-single-decisions.html"><a href="local-explain-a-single-decisions.html"><i class="fa fa-check"></i><b>4.2</b> Local: Explain a single decisions</a><ul>
<li class="chapter" data-level="4.2.1" data-path="local-explain-a-single-decisions.html"><a href="local-explain-a-single-decisions.html#individual-conditional-expectation-ice-plot"><i class="fa fa-check"></i><b>4.2.1</b> Individual Conditional Expectation (ICE) plot</a></li>
<li class="chapter" data-level="4.2.2" data-path="local-explain-a-single-decisions.html"><a href="local-explain-a-single-decisions.html#local-surrogate-models-lime"><i class="fa fa-check"></i><b>4.2.2</b> Local surrogate models (LIME)</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-agnostic-why-not-use-them-on-the-data-itself.html"><a href="model-agnostic-why-not-use-them-on-the-data-itself.html"><i class="fa fa-check"></i><b>4.3</b> Model-agnostic: Why not use them on the data itself?</a></li>
<li class="chapter" data-level="4.4" data-path="explanation-types.html"><a href="explanation-types.html"><i class="fa fa-check"></i><b>4.4</b> Explanation types</a><ul>
<li class="chapter" data-level="4.4.1" data-path="explanation-types.html"><a href="explanation-types.html#structured-output"><i class="fa fa-check"></i><b>4.4.1</b> Structured output</a></li>
<li class="chapter" data-level="4.4.2" data-path="explanation-types.html"><a href="explanation-types.html#viz-explanation"><i class="fa fa-check"></i><b>4.4.2</b> Visualization</a></li>
<li class="chapter" data-level="4.4.3" data-path="explanation-types.html"><a href="explanation-types.html#natural-language-narratives"><i class="fa fa-check"></i><b>4.4.3</b> Natural language (narratives)</a></li>
<li class="chapter" data-level="4.4.4" data-path="explanation-types.html"><a href="explanation-types.html#examples-and-prototypes"><i class="fa fa-check"></i><b>4.4.4</b> Examples and prototypes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>5</b> References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Explainable machine learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="global-explain-the-behaviour-of-a-model" class="section level2">
<h2><span class="header-section-number">4.1</span> Global: Explain the behaviour of a model</h2>
<div id="global-surrogate-models" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Global surrogate models</h3>
<p>A surrogate model is a simple, explainable model that explains another complex machine learning model. Models in [#simple] are viable candidates.</p>
<p>UNDER CONSTRUCTION</p>
</div>
<div id="pdp" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Partial dependence plot</h3>
<p>The partial dependence plot shows the marginal effect of a variable on the target (regression / classification) <span class="citation">(J. H. Friedman <a href="#ref-friedman2001greedy">2001</a>)</span>. A partial dependence plot can show if the relationship between target and feature is linear, monotonic or something else. In linear regression, those plots will always show a linear relationship.</p>
<p>The partial dependence function for regression is defined as: <span class="math display">\[f_S = E_{x_C}[f(x_S, x_C)] = \int f(x_S, x_C) dP(x_C)\]</span> The <span class="math inline">\(x_S\)</span> is the set of variables for which the partial dependence should be depicted and <span class="math inline">\(x_C\)</span> are the other variables that were used in the machine learning model. Partial dependence works by averaging out the other variables, so that the remaining function shows the relationsship between the <span class="math inline">\(x_S\)</span>, in which we are interested, and the target. <span class="math inline">\(x_S\)</span> is fixed and <span class="math inline">\(x_C\)</span> is varying.</p>
<p>The integral is estimated by calculating averages in the training data, which looks like this for regression: <span class="math display">\[ \hat{f}(x_S) = \frac{1}{n} \sum_{i=1}^n f(x_S, x_{Ci}) \]</span> In this formula, <span class="math inline">\(x\)</span> is the variable for which to calculate the partial dependence, <span class="math inline">\(x_{iC}\)</span> is the other variables and <span class="math inline">\(n\)</span> the number of instances in the data set.</p>
<p>For classification it is the logits: <span class="math display">\[ f(x) = \log p_k(x) - \frac{1}{K} \sum_{j=1}^K \log p_j(x) \]</span></p>
<p>Partial dependence plots are only partially global: They are global because they take into account all instances, but it is local in the feature, because partial dependence plots only examine one variable, as the name suggests.</p>
<div id="examples" class="section level4">
<h4><span class="header-section-number">4.1.2.1</span> Examples</h4>
<p>In practice <span class="math inline">\(x_S\)</span> usually only contains one variable or a maximum of two, because one variable produces 2D plots and two variables produce 3D plots. Everything beyond that is quite tricky. Even 3D on a 2D paper or monitor is already challenging. This example here shows an artificial dataset with two x variables on which a Random Forest was trained.</p>
<div class="figure"><span id="fig:dpd-cervical"></span>
<img src="xai-book_files/figure-html/dpd-cervical-1.svg" alt="Partial dependence plot of cancer probability and different factors. For the age feature, the models partial dependence shows that on average, the cancer probability is low before 45, spikes between age 45 and 55 and plateaus after that." width="672" />
<p class="caption">
FIGURE 4.1: Partial dependence plot of cancer probability and different factors. For the age feature, the models partial dependence shows that on average, the cancer probability is low before 45, spikes between age 45 and 55 and plateaus after that.
</p>
</div>
<div class="figure"><span id="fig:dpd-cervical-2d"></span>
<img src="xai-book_files/figure-html/dpd-cervical-2d-1.svg" alt="Partial dependence plot of cancer probability and the interaction of number of years on hormonal contraceptives and number of sexual partners. Interestingly, there is some odd interaction between the two variables when the number of sexual partners is 1 and the years of on hormonal contraceptives larger than 12. There are actually only two women in that group, who both happen to have cancer. So my best guess is that this was random and the model did overfit on those two women, but only more data could solve this question. " width="672" />
<p class="caption">
FIGURE 4.2: Partial dependence plot of cancer probability and the interaction of number of years on hormonal contraceptives and number of sexual partners. Interestingly, there is some odd interaction between the two variables when the number of sexual partners is 1 and the years of on hormonal contraceptives larger than 12. There are actually only two women in that group, who both happen to have cancer. So my best guess is that this was random and the model did overfit on those two women, but only more data could solve this question.
</p>
</div>
<p>Let’s turn to the regression example with the bike counts again and have a look at how the weather effects look like. @ref{fig:dpd-bike} shows the average influence of the weather features on the predicted bike counts. Warm, but not too hot weather makes the model predict a high number of bikes rentals. The potential bikers are increasingly inhibited in engaging in cycling when humidity reaches above 60%. Also the more wind the less people like to bike, which personally I can understand. Interestingly the predicted bike counts don’t drop between 25 and 35 km/h, but maybe there is just not enough training data. At least intuitively I would expect the bike rentals to drop with each increase in windspeed, especially when the windspeed is very high.</p>
<div class="figure"><span id="fig:dpd-bike"></span>
<img src="xai-book_files/figure-html/dpd-bike-1.svg" alt="Partial dependence plot of rental bike count and different weather measurements (Temperature, Humidity, Windspeed). The biggest differences can be seen in different temperatures: With rising temperatures, on average the bike rentals rise, until 20C degrees, where it stays the same also for hotter temperatures and drops a bit again towards 30C degrees." width="672" />
<p class="caption">
FIGURE 4.3: Partial dependence plot of rental bike count and different weather measurements (Temperature, Humidity, Windspeed). The biggest differences can be seen in different temperatures: With rising temperatures, on average the bike rentals rise, until 20C degrees, where it stays the same also for hotter temperatures and drops a bit again towards 30C degrees.
</p>
</div>
</div>
</div>
<div id="average-marginal-effects" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Average Marginal Effects}</h3>
<p>Average marginal effects were originally developed for generalized linear models with special emphasis on the logistic regression model (CITATION). Logistic regression models try to model the probability of a binary response using a set of features. The main idea in generalized linear models is to model the expected value of the target feature using a linear combination (also called linear predictor) of the features by weighting them with the coefficients <span class="math inline">\(\beta\)</span> and a so-called response function <span class="math inline">\(h\)</span>, i.e. <span class="math display">\[E(Y|X) = h(x^\top \beta).\]</span> In any case the linear predictor takes values between minus and plus infinity, i.e., for a logistic regression model those values need to be translated into probabilities which is typically done by appling a sigmoid function (e.g., using a logistic function for <span class="math inline">\(h\)</span>) to the linear predictor to make sure that the values are between 0 and 1. Although the logistic regression model is claimed to be an interpretable model (CITATION), the interpretations of the estimated coefficients (feature effects) are not directly related to the probability of the binary response due to the non-linear response function <span class="math inline">\(h\)</span>. Instead, the interpretations are done w.r.t. log-odds, e.g. if <span class="math inline">\(x\)</span> increases by one, the log-odds (<span class="math inline">\(log \frac{P(Y = 1)}{P(Y = 0)}\)</span>) will increase by <span class="math inline">\(\beta\)</span>. Researchers of applied sciences, however, are often interested in making direct interpretations of how changes in <span class="math inline">\(x\)</span> affect the probability <span class="math inline">\(P(Y=1)\)</span>. For this purpose one can use average marginal effects.</p>
</div>
<div id="feature-importance" class="section level3">
<h3><span class="header-section-number">4.1.4</span> Feature importance</h3>
<div id="permutation-feature-importance" class="section level4">
<h4><span class="header-section-number">4.1.4.1</span> Permutation feature importance</h4>
<p>The permutation importance measurement was orginially introduced for RandomForests <span class="citation">(Breiman <a href="#ref-breiman2001random">2001</a>)</span>. It is calculated on the out-of-bag instances and works by estimating the original model performance and checking what happens with the model performance when you permute each feature. A big loss in performance means a big feature importance. The idea of permutation of features is per se model-agnostic, only the OOB-scheme is specific for ensemble methods. In can be used for any model when a hold-out dataset is used, instead of OOB samples. Of course you could also use the training data, but you risk getting variable importance measures that overfit your training data, since the model was already trained on it.</p>
<p>Algorithm <span class="citation">(Breiman <a href="#ref-breiman2001random">2001</a>)</span>:</p>
<p>Input: Trained model <span class="math inline">\(\hat{f}\)</span>, hold-out dataset <span class="math inline">\(D\)</span>, number of permutations <span class="math inline">\(n_{perm}\)</span></p>
<ol style="list-style-type: decimal">
<li>Estimate performance <span class="math inline">\(Perf\)</span> of <span class="math inline">\(\hat{f}\)</span> with <span class="math inline">\(D\)</span> (e.g. MSE for regression or accuracy for classification)</li>
<li>For each feature <span class="math inline">\(j \in 1, \ldots, J\)</span> do:</li>
</ol>
<ul>
<li>For <span class="math inline">\(i \in 1,\ldots , n_{perm}\)</span>
<ul>
<li>Get <span class="math inline">\(D_{j_{perm}}\)</span> by permuting feature <span class="math inline">\(X_j\)</span> in data <span class="math inline">\(D\)</span>. This breaks the association between <span class="math inline">\(X_j\)</span> and <span class="math inline">\(Y\)</span>.</li>
<li>Estimate performance <span class="math inline">\(Perf_{i,j_{perm}}\)</span> of <span class="math inline">\(\hat{f}\)</span> with <span class="math inline">\(D_{j_{perm}}\)</span></li>
<li>Calculate permutation variable importance <span class="math inline">\(VI_i(X_j) = Perf_{i,j_{perm}} - Perf\)</span></li>
</ul></li>
<li>Calculate mean variable importance: <span class="math inline">\(VI(X_j) = \frac{1}{n_{perm}}\sum_{i=1}^{n_{perm}} VI_i(X_j)\)</span></li>
<li>Optional: Calculate p-value <span class="math inline">\(p = \frac{I(Perf_{j_{perm}} &gt; Perf)}{n_{perm}}\)</span></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Sort variables by descending <span class="math inline">\(VI\)</span>.</li>
</ol>
<p>The feature with the highest <span class="math inline">\(VI\)</span> measure is the most important globally in your model. With the p-value you can additionally check if a feature importance is significantly different from 0. You might want to adjust your <span class="math inline">\(\alpha\)</span> confidence level for multiple testing.</p>
<p>You can also find the algorithm in more detail in <span class="citation">(Strobl et al. <a href="#ref-Strobl2008">2008</a>)</span>. The authors additionally suggest a conditional feature importance measurement, which is not (yet) covered in this book. The standard permutation feature importance only works with marginal feature improvements and cannot distinguish between correlation and spurious correlation. <span class="citation">(Strobl et al. <a href="#ref-Strobl2008">2008</a>)</span> suggest to condition the importance measure also on other features, which makes it possible to account for correlation among the features.</p>
</div>
<div id="model-dependent-feature-importance" class="section level4">
<h4><span class="header-section-number">4.1.4.2</span> Model dependent feature importance</h4>
<p>Some model classes already come with built in feature importance measurements. A few examples: - RandomForest: Permutation based feature importance - CART and boosting: mean decrease in Gini impurity index - Linear Model: (absolute value of) t-test statistic for each feature</p>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-friedman2001greedy">
<p>Friedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” <em>Annals of Statistics</em>. JSTOR, 1189–1232.</p>
</div>
<div id="ref-breiman2001random">
<p>Breiman, Leo. 2001. “Random Forests.” <em>Machine Learning</em> 45 (1). Springer: 5–32.</p>
</div>
<div id="ref-Strobl2008">
<p>Strobl, Carolin, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin, and Achim Zeileis. 2008. “Conditional variable importance for random forests.” <em>BMC Bioinformatics</em> 9 (January): 307. doi:<a href="https://doi.org/10.1186/1471-2105-9-307">10.1186/1471-2105-9-307</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="model-agnostic-explanations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="local-explain-a-single-decisions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/xai-book/edit/master/04-model-agnostic.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
