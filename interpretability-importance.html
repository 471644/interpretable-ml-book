<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Interpretable machine learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Interpretable machine learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable." />
  <meta name="github-repo" content="christophM/xai-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Interpretable machine learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable." />
  

<meta name="author" content="Christoph Molnar">


<meta name="date" content="2017-11-16">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="interpretability.html">
<link rel="next" href="scope-of-interpretability.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="what-to-expect-from-this-book.html"><a href="what-to-expect-from-this-book.html"><i class="fa fa-check"></i><b>1.1</b> What to expect from this book</a></li>
<li class="chapter" data-level="1.2" data-path="what-is-machine-learning-and-why-is-it-important.html"><a href="what-is-machine-learning-and-why-is-it-important.html"><i class="fa fa-check"></i><b>1.2</b> What is machine learning and why is it important?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> Interpretability</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>2.1</b> The importance of machine learning interpretability</a></li>
<li class="chapter" data-level="2.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html"><i class="fa fa-check"></i><b>2.2</b> Scope of interpretability</a><ul>
<li class="chapter" data-level="2.2.1" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#algorithm-transparency"><i class="fa fa-check"></i><b>2.2.1</b> Algorithm transparency</a></li>
<li class="chapter" data-level="2.2.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-holistic-model-interpretability"><i class="fa fa-check"></i><b>2.2.2</b> Global, holistic model interpretability</a></li>
<li class="chapter" data-level="2.2.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-model-interpretability-on-a-modular-level"><i class="fa fa-check"></i><b>2.2.3</b> Global model interpretability on a modular level</a></li>
<li class="chapter" data-level="2.2.4" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#explain-the-prediction-for-a-single-instance"><i class="fa fa-check"></i><b>2.2.4</b> Explain the prediction for a single instance</a></li>
<li class="chapter" data-level="2.2.5" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#explain-the-predictions-for-a-group-of-instances"><i class="fa fa-check"></i><b>2.2.5</b> Explain the predictions for a group of instances</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="evaluating-interpretability.html"><a href="evaluating-interpretability.html"><i class="fa fa-check"></i><b>2.3</b> Evaluating interpretability</a><ul>
<li class="chapter" data-level="2.3.1" data-path="evaluating-interpretability.html"><a href="evaluating-interpretability.html#approaches-for-evaluating-the-explanation-quality"><i class="fa fa-check"></i><b>2.3.1</b> Approaches for evaluating the explanation quality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Datasets</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> Bike sharing counts (regression)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> Youtube spam comments (text classification)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical-data.html"><a href="cervical-data.html"><i class="fa fa-check"></i><b>3.3</b> Risk factors for cervical cancer (classification)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="definitions.html"><a href="definitions.html"><i class="fa fa-check"></i><b>4</b> Definitions</a></li>
<li class="chapter" data-level="5" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>5</b> Interpretable models</a><ul>
<li class="chapter" data-level="5.1" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>5.1</b> Terminology</a></li>
<li class="chapter" data-level="5.2" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>5.2</b> Overview</a></li>
<li class="chapter" data-level="5.3" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>5.3</b> Linear models</a><ul>
<li class="chapter" data-level="5.3.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>5.3.1</b> Interpretation</a></li>
<li class="chapter" data-level="5.3.2" data-path="limo.html"><a href="limo.html#interpretation-example"><i class="fa fa-check"></i><b>5.3.2</b> Interpretation example</a></li>
<li class="chapter" data-level="5.3.3" data-path="limo.html"><a href="limo.html#interpretation-templates"><i class="fa fa-check"></i><b>5.3.3</b> Interpretation templates</a></li>
<li class="chapter" data-level="5.3.4" data-path="limo.html"><a href="limo.html#visual-parameter-interpretation"><i class="fa fa-check"></i><b>5.3.4</b> Visual parameter interpretation</a></li>
<li class="chapter" data-level="5.3.5" data-path="limo.html"><a href="limo.html#explaining-single-predictions"><i class="fa fa-check"></i><b>5.3.5</b> Explaining single predictions</a></li>
<li class="chapter" data-level="5.3.6" data-path="limo.html"><a href="limo.html#coding-categorical-variables"><i class="fa fa-check"></i><b>5.3.6</b> Coding categorical variables:</a></li>
<li class="chapter" data-level="5.3.7" data-path="limo.html"><a href="limo.html#the-disadvantages-of-linear-models"><i class="fa fa-check"></i><b>5.3.7</b> The disadvantages of linear models</a></li>
<li class="chapter" data-level="5.3.8" data-path="limo.html"><a href="limo.html#towards-complexer-relationships-within-linear-model-class"><i class="fa fa-check"></i><b>5.3.8</b> Towards complexer relationships within linear model class</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="sparse-linear-models.html"><a href="sparse-linear-models.html"><i class="fa fa-check"></i><b>5.4</b> Sparse linear models</a></li>
<li class="chapter" data-level="5.5" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html"><i class="fa fa-check"></i><b>5.5</b> Logistic regression: a linear model for classification</a><ul>
<li class="chapter" data-level="5.5.1" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html#whats-wrong-with-linear-regression-for-classification"><i class="fa fa-check"></i><b>5.5.1</b> What’s wrong with linear regression for classification?</a></li>
<li class="chapter" data-level="5.5.2" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html#logistic-regression"><i class="fa fa-check"></i><b>5.5.2</b> Logistic regression</a></li>
<li class="chapter" data-level="5.5.3" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html#interpretation-1"><i class="fa fa-check"></i><b>5.5.3</b> Interpretation</a></li>
<li class="chapter" data-level="5.5.4" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html#example"><i class="fa fa-check"></i><b>5.5.4</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>5.6</b> Decision trees</a><ul>
<li class="chapter" data-level="5.6.1" data-path="decision-trees.html"><a href="decision-trees.html#interpretation-2"><i class="fa fa-check"></i><b>5.6.1</b> Interpretation</a></li>
<li class="chapter" data-level="5.6.2" data-path="decision-trees.html"><a href="decision-trees.html#interpretation-example-1"><i class="fa fa-check"></i><b>5.6.2</b> Interpretation example</a></li>
<li class="chapter" data-level="5.6.3" data-path="decision-trees.html"><a href="decision-trees.html#advantages"><i class="fa fa-check"></i><b>5.6.3</b> Advantages</a></li>
<li class="chapter" data-level="5.6.4" data-path="decision-trees.html"><a href="decision-trees.html#disadvantages"><i class="fa fa-check"></i><b>5.6.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="other-simple-interpretable-models.html"><a href="other-simple-interpretable-models.html"><i class="fa fa-check"></i><b>5.7</b> Other simple, interpretable models</a><ul>
<li class="chapter" data-level="5.7.1" data-path="other-simple-interpretable-models.html"><a href="other-simple-interpretable-models.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>5.7.1</b> Naive bayes classifier</a></li>
<li class="chapter" data-level="5.7.2" data-path="other-simple-interpretable-models.html"><a href="other-simple-interpretable-models.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>5.7.2</b> k-nearest neighbours</a></li>
<li class="chapter" data-level="5.7.3" data-path="other-simple-interpretable-models.html"><a href="other-simple-interpretable-models.html#rulefit"><i class="fa fa-check"></i><b>5.7.3</b> RuleFit</a></li>
<li class="chapter" data-level="5.7.4" data-path="other-simple-interpretable-models.html"><a href="other-simple-interpretable-models.html#and-so-many-more"><i class="fa fa-check"></i><b>5.7.4</b> And so many more …</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>6</b> Model-agnostic tools for interpretability</a><ul>
<li class="chapter" data-level="6.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>6.1</b> Partial dependence plot</a></li>
<li class="chapter" data-level="6.2" data-path="individual-conditional-expectation-ice-plot.html"><a href="individual-conditional-expectation-ice-plot.html"><i class="fa fa-check"></i><b>6.2</b> Individual Conditional Expectation (ICE) plot</a></li>
<li class="chapter" data-level="6.3" data-path="permutation-feature-importance.html"><a href="permutation-feature-importance.html"><i class="fa fa-check"></i><b>6.3</b> Permutation feature importance</a><ul>
<li class="chapter" data-level="6.3.1" data-path="permutation-feature-importance.html"><a href="permutation-feature-importance.html#model-dependent-feature-importance"><i class="fa fa-check"></i><b>6.3.1</b> Model dependent feature importance</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="local-surrogate-models-lime.html"><a href="local-surrogate-models-lime.html"><i class="fa fa-check"></i><b>6.4</b> Local surrogate models (LIME)</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable machine learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="interpretability.importance" class="section level2">
<h2><span class="header-section-number">2.1</span> The importance of machine learning interpretability</h2>
<p>If a machine learning model performs well, <strong>why not just trust</strong> it and ignore why it made a certain decision?</p>
<p>Let’s dive deeper into the reasons why interpretability is so important. Machine learning has come to a state where you have to make a trade-off: Do you simply want to know <strong>what</strong> is predicted happen? For example if a client will churn or if a medication will work well for a patient. Or do you want to know <strong>why</strong> something is predicted to happen and paying for the interpretability with accuracy? In some cases you don’t care why a decision was made, only the assurance that the predictive performance was good on a test dataset is enough. But in other cases knowing the ‘why’ can help you understand more about the problem, the data and it can also tell you why a model might fail. Some problems might not need explanations, because they either are low risk, meaning a mistake has no severe consequences, (e.g. a movie recommender system) or the method has already been extensively studied and evaluated (e.g. optical character recognition). The necessity for interpretability comes from an incompleteness in the problem formalisation <span class="citation">(Doshi-Velez and Kim <a href="#ref-Doshi-Velez2017">2017</a>)</span>, meaning that for certain problems or tasks it is not enough to get the answer (the <strong>what</strong>), but the model also has to give an explanation how it came to the answer (the <strong>why</strong>), because correctly predicting is not enough to solve the problem. Following reasons drive the demand for interpretability.</p>
<ul>
<li>There is a shift in many scientific disciplines from qualitative to quantitative methods (e.g. sociology, psychology), and also towards machine learning (biology, genomics). The <strong>goal of science</strong> is to gain knowledge, but many problems can only be solved with big datasets and black box machine learning models. Interpretability allows to extract additional knowledge.</li>
<li>It is <strong>human nature</strong> wanting to understand things and to have some form of control.</li>
<li>Machine learning models are taking over real world tasks, that demand <strong>safety measurements</strong> and testing. A self-driving car automatically detects cyclists, which is as desired. You want to bet 100% sure that the abstraction the system learned will be fail-safe, because running over cyclists is quite bad. An explanation might reveal that the most important feature learned is to recognise the two wheels of a bike and this explanation helps to think about edge cases like bikes with side bags, that partially cover the wheels.</li>
<li>By default most machine learning models pick up biases from the training data. This can turn your machine learning models into racists which discriminate against protected groups. Interpretability is a useful debugging tool to <strong>detect bias</strong> in machine learning models.</li>
</ul>
<p>Even in low risk environments, like movie recommendation, interpretability in the research and development stage is valuable. Also later when some model is used in a product, things can go wrong. And need for interpretability arises when your model fucks up. Because having an explanation for a faulty classification helps to understand the cause of the fault. It delivers a direction for how to fix the system. Consider an example of a husky versus wolf classifier, that misclassifies some huskies as wolfs. If there is an explanation to the classification you can see, that the misclassification happened due to the snow on the image. The classifier learned to use snow as a feature for classifying images as wolfs, which might make sense in terms of separating features in the training data set, but not in the real world use.</p>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Doshi-Velez2017">
<p>Doshi-Velez, Finale, and Been Kim. 2017. “Towards A Rigorous Science of Interpretable Machine Learning,” no. Ml: 1–13. <a href="http://arxiv.org/abs/1702.08608" class="uri">http://arxiv.org/abs/1702.08608</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="interpretability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="scope-of-interpretability.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/xai-book/edit/master/02-interpretability.Rmd",
"text": "Edit"
},
"download": ["xai-book.pdf"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

<!-- </html> -->
