<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Interpretable machine learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Interpretable machine learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable." />
  <meta name="github-repo" content="christophM/xai-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Interpretable machine learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable." />
  

<meta name="author" content="Christoph Molnar">


<meta name="date" content="2017-11-19">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="terminology.html">
<link rel="next" href="sparse-linear-models.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="what-to-expect-from-this-book.html"><a href="what-to-expect-from-this-book.html"><i class="fa fa-check"></i><b>1.1</b> What to expect from this book</a></li>
<li class="chapter" data-level="1.2" data-path="what-is-machine-learning-and-why-is-it-important.html"><a href="what-is-machine-learning-and-why-is-it-important.html"><i class="fa fa-check"></i><b>1.2</b> What is machine learning and why is it important?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> Interpretability</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>2.1</b> The importance of machine learning interpretability</a></li>
<li class="chapter" data-level="2.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html"><i class="fa fa-check"></i><b>2.2</b> Scope of interpretability</a><ul>
<li class="chapter" data-level="2.2.1" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#algorithm-transparency"><i class="fa fa-check"></i><b>2.2.1</b> Algorithm transparency</a></li>
<li class="chapter" data-level="2.2.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-holistic-model-interpretability"><i class="fa fa-check"></i><b>2.2.2</b> Global, holistic model interpretability</a></li>
<li class="chapter" data-level="2.2.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-model-interpretability-on-a-modular-level"><i class="fa fa-check"></i><b>2.2.3</b> Global model interpretability on a modular level</a></li>
<li class="chapter" data-level="2.2.4" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#explain-the-prediction-for-a-single-instance"><i class="fa fa-check"></i><b>2.2.4</b> Explain the prediction for a single instance</a></li>
<li class="chapter" data-level="2.2.5" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#explain-the-predictions-for-a-group-of-instances"><i class="fa fa-check"></i><b>2.2.5</b> Explain the predictions for a group of instances</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="evaluating-interpretability.html"><a href="evaluating-interpretability.html"><i class="fa fa-check"></i><b>2.3</b> Evaluating interpretability</a><ul>
<li class="chapter" data-level="2.3.1" data-path="evaluating-interpretability.html"><a href="evaluating-interpretability.html#approaches-for-evaluating-the-explanation-quality"><i class="fa fa-check"></i><b>2.3.1</b> Approaches for evaluating the explanation quality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Datasets</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> Bike sharing counts (regression)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> Youtube spam comments (text classification)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical-data.html"><a href="cervical-data.html"><i class="fa fa-check"></i><b>3.3</b> Risk factors for cervical cancer (classification)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="definitions.html"><a href="definitions.html"><i class="fa fa-check"></i><b>4</b> Definitions</a></li>
<li class="chapter" data-level="5" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>5</b> Interpretable models</a><ul>
<li class="chapter" data-level="5.1" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>5.1</b> Terminology</a></li>
<li class="chapter" data-level="5.2" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>5.2</b> Linear models</a><ul>
<li class="chapter" data-level="5.2.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>5.2.1</b> Interpretation</a></li>
<li class="chapter" data-level="5.2.2" data-path="limo.html"><a href="limo.html#interpretation-example"><i class="fa fa-check"></i><b>5.2.2</b> Interpretation example</a></li>
<li class="chapter" data-level="5.2.3" data-path="limo.html"><a href="limo.html#interpretation-templates"><i class="fa fa-check"></i><b>5.2.3</b> Interpretation templates</a></li>
<li class="chapter" data-level="5.2.4" data-path="limo.html"><a href="limo.html#visual-parameter-interpretation"><i class="fa fa-check"></i><b>5.2.4</b> Visual parameter interpretation</a></li>
<li class="chapter" data-level="5.2.5" data-path="limo.html"><a href="limo.html#explaining-single-predictions"><i class="fa fa-check"></i><b>5.2.5</b> Explaining single predictions</a></li>
<li class="chapter" data-level="5.2.6" data-path="limo.html"><a href="limo.html#coding-categorical-variables"><i class="fa fa-check"></i><b>5.2.6</b> Coding categorical variables:</a></li>
<li class="chapter" data-level="5.2.7" data-path="limo.html"><a href="limo.html#the-disadvantages-of-linear-models"><i class="fa fa-check"></i><b>5.2.7</b> The disadvantages of linear models</a></li>
<li class="chapter" data-level="5.2.8" data-path="limo.html"><a href="limo.html#towards-complexer-relationships-within-linear-model-class"><i class="fa fa-check"></i><b>5.2.8</b> Towards complexer relationships within linear model class</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="sparse-linear-models.html"><a href="sparse-linear-models.html"><i class="fa fa-check"></i><b>5.3</b> Sparse linear models</a></li>
<li class="chapter" data-level="5.4" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html"><i class="fa fa-check"></i><b>5.4</b> Logistic regression: a linear model for classification</a><ul>
<li class="chapter" data-level="5.4.1" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html#whats-wrong-with-linear-regression-for-classification"><i class="fa fa-check"></i><b>5.4.1</b> What’s wrong with linear regression for classification?</a></li>
<li class="chapter" data-level="5.4.2" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html#logistic-regression"><i class="fa fa-check"></i><b>5.4.2</b> Logistic regression</a></li>
<li class="chapter" data-level="5.4.3" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html#interpretation-1"><i class="fa fa-check"></i><b>5.4.3</b> Interpretation</a></li>
<li class="chapter" data-level="5.4.4" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html#example"><i class="fa fa-check"></i><b>5.4.4</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>5.5</b> Decision trees</a><ul>
<li class="chapter" data-level="5.5.1" data-path="decision-trees.html"><a href="decision-trees.html#interpretation-2"><i class="fa fa-check"></i><b>5.5.1</b> Interpretation</a></li>
<li class="chapter" data-level="5.5.2" data-path="decision-trees.html"><a href="decision-trees.html#interpretation-example-1"><i class="fa fa-check"></i><b>5.5.2</b> Interpretation example</a></li>
<li class="chapter" data-level="5.5.3" data-path="decision-trees.html"><a href="decision-trees.html#advantages"><i class="fa fa-check"></i><b>5.5.3</b> Advantages</a></li>
<li class="chapter" data-level="5.5.4" data-path="decision-trees.html"><a href="decision-trees.html#disadvantages"><i class="fa fa-check"></i><b>5.5.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="other-simple-interpretable-models.html"><a href="other-simple-interpretable-models.html"><i class="fa fa-check"></i><b>5.6</b> Other simple, interpretable models</a><ul>
<li class="chapter" data-level="5.6.1" data-path="other-simple-interpretable-models.html"><a href="other-simple-interpretable-models.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>5.6.1</b> Naive bayes classifier</a></li>
<li class="chapter" data-level="5.6.2" data-path="other-simple-interpretable-models.html"><a href="other-simple-interpretable-models.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>5.6.2</b> k-nearest neighbours</a></li>
<li class="chapter" data-level="5.6.3" data-path="other-simple-interpretable-models.html"><a href="other-simple-interpretable-models.html#rulefit"><i class="fa fa-check"></i><b>5.6.3</b> RuleFit</a></li>
<li class="chapter" data-level="5.6.4" data-path="other-simple-interpretable-models.html"><a href="other-simple-interpretable-models.html#and-so-many-more"><i class="fa fa-check"></i><b>5.6.4</b> And so many more …</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>6</b> Model-agnostic tools for interpretability</a><ul>
<li class="chapter" data-level="6.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>6.1</b> Partial dependence plot</a></li>
<li class="chapter" data-level="6.2" data-path="individual-conditional-expectation-ice-plot.html"><a href="individual-conditional-expectation-ice-plot.html"><i class="fa fa-check"></i><b>6.2</b> Individual Conditional Expectation (ICE) plot</a></li>
<li class="chapter" data-level="6.3" data-path="permutation-feature-importance.html"><a href="permutation-feature-importance.html"><i class="fa fa-check"></i><b>6.3</b> Permutation feature importance</a><ul>
<li class="chapter" data-level="6.3.1" data-path="permutation-feature-importance.html"><a href="permutation-feature-importance.html#model-dependent-feature-importance"><i class="fa fa-check"></i><b>6.3.1</b> Model dependent feature importance</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="local-surrogate-models-lime.html"><a href="local-surrogate-models-lime.html"><i class="fa fa-check"></i><b>6.4</b> Local surrogate models (LIME)</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable machine learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="limo" class="section level2">
<h2><span class="header-section-number">5.2</span> Linear models</h2>
<p>Linear models have been used since a long time by statisticians, computer scientists and other people with quantitative problems. Linear models learn linear (and therefore monotonic) relationships between the features and the target. The linearity of the learned relationship makes the interpretation easy.</p>
<p>Linear models can be used to model the dependency of a regression target <span class="math inline">\(y\)</span> on <span class="math inline">\(p\)</span> features <span class="math inline">\(x\)</span>. The learned relationships are linear and, for a singular instance <span class="math inline">\(i\)</span>, can be written as:</p>
<p><span class="math display">\[y_{i} = \beta_{0} + \beta_{1} \cdot x_{i1} + \ldots + \beta_{p} x_{ip} + \epsilon_{i}\]</span></p>
<p>The i-th observation’s outcome is a weighted sum of it’s <span class="math inline">\(p\)</span> features. The <span class="math inline">\(\beta_{j}\)</span> represent the learned feature weights or coefficients. The <span class="math inline">\(\epsilon_{i}\)</span> is the error we are still making, the difference between the predicted and actual outcome.</p>
<p>Different methods can be used to estimate the optimal weight vector <span class="math inline">\(\mathbf{\hat{\beta}}\)</span>. The ordinary least squares method is commonly used to find the weights that minimise the squared difference between the actual and the estimated outcome: <span class="math display">\[\mathbf{\hat{\beta}} = \arg\!\min_{\beta_0, \ldots, \beta_p} \sum_{i=1}^n \left(y_i - \left(\beta_0 + \sum_{j=1}^p \beta_j x_{ij}\right)\right)\]</span></p>
<p>We won’t go into detail about how the optimal weights can be found, but if you are interested you can read Chapter 3.2 of the book “Elements of Statistical Learning” <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie2009">2009</a>)</span> or one of the other zillions of sources about linear regression models.</p>
<p>The biggest advantage of linear regression models is the linearity: It makes the estimation procedure straightforward and most importantly these linear equations have an easy to understand interpretation on a modular level (i.e. the weights). That is one of the main reasons why the linear model and all similar models are so widespread in academic fields like medicine, sociology, psychology and many more quantitative research fields. In this areas it is important to not only predict e.g. the clinical outcome of a patient, but also to quantify the influence of the medication while at the same time accounting for things like sex, age and other features in an interpretable manner.</p>
<p>Linear regression models also come with some assumptions that make them easy to use and interpret but which are often not satisfied in reality. Assumed is: Linearity, normality, homoscedasticity, independence, fixed features and absence of multicollinearity.</p>
<ul>
<li><strong>Linearity</strong>: Linear regression models force the estimated response to be a linear combination of the features, which is both the greatest strength and biggest limitation. Linearity leads to interpretable models: linear effects are simple to quantify and describe (see also next chapter) and are additive, so it is easy to separate the effects. If you suspect interactions of features or a non-linear association of a feature with the target value, then you can add interaction terms and use techniques like regression splines to estimate non-linear effects.</li>
<li><strong>Normality</strong>: The target outcome given the features are assumed to follow a normal distribution. If this assumption is violated, then the estimated confidence intervals of the feature weights are not valid. Any interpretation of the features p-values is not valid.</li>
<li><strong>Homoscedasticity</strong> (constant variance): The variance of the error terms <span class="math inline">\(\epsilon_{i}\)</span> is assumed to be constant over the whole feature space. Let’s say you want to predict the value of a house given the living area in square meters. You estimate a linear model, which assumes that no matter how big the flat, the error terms around the predicted response have the same variance. This assumption is often violated in reality. In the house example it is plausible that the variance of error terms around the predicted price is higher for bigger houses, since also the prices are higher and there is more room for prices to vary.</li>
<li><strong>Independence</strong>: Each observation is assumed to be independent from the next one. If you have repeated measurements, like multiple records per patient, the data points are not independent from each other and there are special linear model classes to deal with these cases, like mixed effect models or GEEs.</li>
<li><strong>Fixed features</strong>: The input features are seen as ‘fixed’, carrying no errors or variation, which, of course, is very unrealistic and only makes sense in controlled experimental settings. But not assuming fixed features would mean that you have to fit very complex measurement error models that account for the measurement errors of your input features. And usually you don’t want to do that.</li>
<li><strong>Absence of multicollinearity</strong>: Basically you don’t want features to be highly correlated, because this messes up the estimation of the weights. In a situation where two features are highly correlated (something like correlation &gt; 0.9) it will become problematic to estimate the weights, since the feature effects are additive and it becomes indeterminable to which of the correlated features to attribute the effects.</li>
</ul>
<div id="interpretation" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Interpretation</h3>
<p>The interpretation of the coefficients:</p>
<ul>
<li>Continuous regression variable: For an increase of one point of the variable <span class="math inline">\(x_{j}\)</span> the estimated outcome changes by <span class="math inline">\(\beta_{j}\)</span></li>
<li>Binary categorical variables: One of the variables is the reference level (in some languages the one that was coded in 0). A change of the variable <span class="math inline">\(x_{i}\)</span> the reference level to the other category changes the estimated outcome by <span class="math inline">\(\beta_{i}\)</span></li>
<li>categorical variables with many levels: One solution to deal with many variables is to one-hot-encode them, meaning each level gets it’s own column. From a categorical variable with L levels, you only need L-1 columns, otherwise it is over parameterised. The interpretation for each level is then according to the binary variables. Some language like R allow to</li>
<li>Intercept <span class="math inline">\(\beta_{0}\)</span>: The interpretation is: Given all continuous variables are zero and the categorical variables are on the reference level, the estimated outcome of <span class="math inline">\(y_{i}\)</span> is <span class="math inline">\(\beta_{0}\)</span>. The interpretation of <span class="math inline">\(\beta_{0}\)</span> is usually not relevant.</li>
</ul>
<p>Another important measurement for interpreting linear models is the <span class="math inline">\(R^2\)</span> measurement. <span class="math inline">\(R^2\)</span> tells you how much of the total variance of your target variable is explained by the model. The higher <span class="math inline">\(R^2\)</span> the better your model explains the data. The formula to calculate <span class="math inline">\(R^2\)</span> is: <span class="math inline">\(R^2 = 1 - SSE/SST\)</span>, where SSE is the squared sum of the error terms (<span class="math inline">\(SSE = \sum_{i=1}^n (y_i - \hat{y}_i)^2\)</span>) and SST is the squared sum of the data variance (<span class="math inline">\(SST = \sum_{i=1}^n (y_i - \bar{y})^2\)</span>). <span class="math inline">\(R^2\)</span> ranges between 0 for models that explain nothing and 1 for models that explain all of the datas variance.</p>
<p>There is a catch, because <span class="math inline">\(R^2\)</span> increases with the number of features in the model, even if they carry no information about the target value at all. So it is better to use the adjusted R-squared <span class="math inline">\(\bar{R}^2\)</span>, which accounts for number of features used in the model. It’s calculation is <span class="math inline">\(\bar{R}^2 = R^2 - (1-R^2)\frac{p}{n - p - 1}\)</span>, where p is the number of features and n the number of observations.</p>
<p>It isn’t helpful to do interpretation on a model with very low <span class="math inline">\(R^2\)</span> or <span class="math inline">\(\bar{R}^2\)</span>, because basically the model is not explaining much of the variance, so any interpretation of the weights are not meaningful.</p>
</div>
<div id="interpretation-example" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Interpretation example</h3>
<p>We now use the linear model to predict the bike rentals on a day, given weather and calendrical information.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">Estimate</th>
<th align="right">Std. Error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td align="right">2579.832417</td>
<td align="right">251.5665243</td>
</tr>
<tr class="even">
<td>seasonSUMMER</td>
<td align="right">864.839727</td>
<td align="right">131.0342448</td>
</tr>
<tr class="odd">
<td>seasonFALL</td>
<td align="right">54.305585</td>
<td align="right">174.9524606</td>
</tr>
<tr class="even">
<td>seasonWINTER</td>
<td align="right">319.626870</td>
<td align="right">119.3930717</td>
</tr>
<tr class="odd">
<td>holidayHOLIDAY</td>
<td align="right">-639.783043</td>
<td align="right">217.2195639</td>
</tr>
<tr class="even">
<td>workingdayWORKING DAY</td>
<td align="right">67.239296</td>
<td align="right">78.1596156</td>
</tr>
<tr class="odd">
<td>weathersitMISTY</td>
<td align="right">-394.473877</td>
<td align="right">94.2803595</td>
</tr>
<tr class="even">
<td>weathersitRAIN/SNOW/STORM</td>
<td align="right">-1863.482641</td>
<td align="right">225.0341002</td>
</tr>
<tr class="odd">
<td>temp</td>
<td align="right">108.961908</td>
<td align="right">7.6228315</td>
</tr>
<tr class="even">
<td>hum</td>
<td align="right">-18.001893</td>
<td align="right">3.3250623</td>
</tr>
<tr class="odd">
<td>windspeed</td>
<td align="right">-45.306120</td>
<td align="right">7.2812447</td>
</tr>
<tr class="even">
<td>days_since_2011</td>
<td align="right">4.977309</td>
<td align="right">0.1870971</td>
</tr>
</tbody>
</table>
<p>Interpretation of a numerical variable (‘Temperature’): An increase of the temperature of 1 degree Celsius increases the number of bikes by 108.96 given all other features stay the same.</p>
<p>Interpretation of a categorical variable (‘weathersituation’)): The number of bikes is -1863.48 lower when it is rainy, snowing or stormy, compared to good weather, given that all features stay the same. Also if the weather was only misty, the number of bike rentals was -394.47 lower, compared to good weather, given all features stay the same.</p>
<p>As you can see in the interpretation examples, the interpretations are always coming with the clause that ‘all other features stay the same’. That’s because of the nature of linear models: All features are input linearly into the function with no interactions (unless explicitly specified). The good side is, that is isolates the interpretation. If you think of the features as turn-switches that you can turn up or down, it is nice to see what happens when you would just turn the switch for one feature.</p>
</div>
<div id="interpretation-templates" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Interpretation templates</h3>
<p><strong>Interpretation of a numerical feature</strong>:</p>
<p>An increase of <span class="math inline">\(x_{k}\)</span> by one unit increases the expectation for <span class="math inline">\(y\)</span> by <span class="math inline">\(\beta_x{k}\)</span> units if all other features X stay the same.</p>
<p><strong>Interpretation of a categorical feature</strong>:</p>
<p>The category coded with 1 of <span class="math inline">\(x_{k}\)</span> increases the expectation for <span class="math inline">\(y\)</span> by <span class="math inline">\(\beta_{k}\)</span> compared to the reference category (coded with 0).</p>
</div>
<div id="visual-parameter-interpretation" class="section level3">
<h3><span class="header-section-number">5.2.4</span> Visual parameter interpretation</h3>
<div id="weight-plot" class="section level4">
<h4><span class="header-section-number">5.2.4.1</span> Weight plot</h4>
<p>The information of the coefficient table can also be put into a visualization, which makes the weights and the uncertainty about them can be made understandable on one glance. The weight is displayed as a point and the 95% confidence interval around the point with a line. The 95% confidence interval means that if the linear model was repeated 100 times on <img src="xai-book_files/figure-html/linear_weights-1.svg" width="672" /> TODO: Add interpetation</p>
</div>
<div id="effect-plot" class="section level4">
<h4><span class="header-section-number">5.2.4.2</span> Effect plot</h4>
<p>The weights of the linear model only have meaning, when combined with the actual features. The weights depend on the scale of the features and will be different if you have a features measuring some height and you switch from inches to centemeters. The weight will change, but the actual relationships in your data will not. Also it is important to know the distribution of your feature in the data, because if you have a very low variance, it means that almost all instances will get a similar contribution from this feature. The effect plot can help to understand how much the combination of a weight and a feature contributes to the predictions in your data. Start with the computation of the effects, which is the weight per feature times the feature of an instance: <span class="math inline">\(eff_{i,k} = w_{k} \cdot x_{i,k}\)</span>. The resulting effects are visualized with boxplots: The box contains the effect range for half of your data (25% to 75% effect quantiles). The line in the box is the median effect, so 50% of the instances have a lower and the other half a higher effect on the prediction than the median value. The whiskers are <span class="math inline">\(+/i 1.58 IQR / \sqrt{n}\)</span>, with IQR being the inter quartile range ($q_{0.75} - q_{0.25}). The points are outlier to the whiskers.</p>
<p><img src="xai-book_files/figure-html/linear_effects-1.svg" width="672" /> The largest contributions are from temperature and the days variable, which capture the trend that the bike rental became more popular over time. The temperature has a high contribution distribution. The day trend variable has goes from zero to large positive contribution, because the first day in the dataset (1.1.2011) get’s a very low contribution, and the estimated weight with this feature is positive (4.98), so the effect gets higher with every day and is highest for the latest day in the dataset (31.12.2012). Note that for effects from a feature with a negative effect, the instances with a positive effect (or the least negatives) are the ones that have a negative feature value (negative times negative is positive), so days with a high positive effect of windspeed on the bike rental count have the lowest windspeeds.</p>
</div>
</div>
<div id="explaining-single-predictions" class="section level3">
<h3><span class="header-section-number">5.2.5</span> Explaining single predictions</h3>
<p>Why did a certain instance get the prediction it got from the linear model? This can again be answered by bringing together the weights and features and computing the effect. Now the effect will tell you how much each feature contributed towards the sum of the prediction. This is only meaningful if you compare the instance specific effects with the mean effects. <img src="xai-book_files/figure-html/linear_effects_single-1.svg" width="672" /></p>
<p>Let’s have a look at the effect realization for the rental bike count of one observation (= one day). Some features contribute unusually much to the predicted bike count: Temperature (5.20089) and the trend variable “days_since_2011”, because this instance is from late 2011 (value = 698).</p>
</div>
<div id="coding-categorical-variables" class="section level3">
<h3><span class="header-section-number">5.2.6</span> Coding categorical variables:</h3>
<p>There are several ways to represent a categorical variable, which has an influence on the interpretation: <a href="http://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/" class="uri">http://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/</a> and <a href="http://heidiseibold.github.io/page7/" class="uri">http://heidiseibold.github.io/page7/</a></p>
<p>Described above is the treatment coding, which is usually sufficient. Using different codings boils down to creating different matrices from your one column with the categorical feature. I present three different codings, but there are many more. The example has six instances and one categorical feature with 3 levels. The first two instances are in category A, instances three and four are in category B and the last two instances are in category C.</p>
<ul>
<li><strong>Treatment coding</strong> compares each level to the reference level. The intercept is the mean of the reference group. The first column is the intercept, which is always 1. Column two is an indicator whether instance <span class="math inline">\(i\)</span> is in category B, columns three is an indicator for category C. There is no need for a column for category A, because than the system would be over specified. Knowing that an instance is neither in category B or C is enough. <span class="math display">\[
\begin{pmatrix}
1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 \\
1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
\end{pmatrix}
\]</span></li>
<li><strong>Effect coding</strong> compares each level to the overall mean of <span class="math inline">\(y\)</span>. The first column is again the intercept. The weight <span class="math inline">\(\beta_{0}\)</span> which is associated to the intercept represents the overall mean and <span class="math inline">\(\beta_{1}\)</span>, the weight for column two is the difference between the overall mean and category B. The overall effect of category B is <span class="math inline">\(\beta_{0} + \beta_{1}\)</span>. Interpretation for category C is equivalent. For the reference category A, <span class="math inline">\(-(\beta_{1} + \beta_{2})\)</span> is the difference of the category C to the overall mean and <span class="math inline">\(\beta_{0} -(\beta_{1} + \beta_{2})\)</span> the overall effect of category C. <span class="math display">\[
\begin{pmatrix}
1 &amp; -1 &amp; -1 \\
1 &amp; -1 &amp; -1 \\
1 &amp; 1 &amp; 0 \\
1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
\end{pmatrix}
\]</span></li>
<li><strong>Dummy coding</strong> compares each level to the level mean of <span class="math inline">\(y\)</span>. If all level are have the same frequency the resulting coefficients will be the same as in effect coding. Note that the intercept was dropped here. <span class="math display">\[
\begin{pmatrix}
1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 \\
\end{pmatrix}
\]</span></li>
</ul>
</div>
<div id="the-disadvantages-of-linear-models" class="section level3">
<h3><span class="header-section-number">5.2.7</span> The disadvantages of linear models</h3>
<p>They can only represent linear relationships as the name suggests. Each non-linearity or interaction has to be hand-crafted and explicitly given to the model as an input feature. Because of possible high correlation between features, it is possible that a feature that is positively correlated with the outcome might get a negative weight in a linear model, because in the high dimensional space it is negatively correlated. An example: You have a model to predict the rent price and have features like number of rooms and size of the flat. Of course flat size and room number are highly correlated, the bigger a flat the more rooms it has. If you now take both variables into a linear model it might happen, that the flat size is the better predictor and get’s a large positive weight. The room number might end up getting a negative weight, because given that a flat has the same size, increasing the number of rooms could make it less valuable.</p>
</div>
<div id="towards-complexer-relationships-within-linear-model-class" class="section level3">
<h3><span class="header-section-number">5.2.8</span> Towards complexer relationships within linear model class</h3>
<ul>
<li>Adding interactions</li>
<li>Adding non-linear terms like polynomials</li>
<li>Stratifying data by variable and fitting linear models on subsets</li>
</ul>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Hastie2009">
<p>Hastie, T, R Tibshirani, and J Friedman. 2009. <em>The elements of statistical learning</em>. <a href="http://link.springer.com/content/pdf/10.1007/978-0-387-84858-7.pdf" class="uri">http://link.springer.com/content/pdf/10.1007/978-0-387-84858-7.pdf</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="terminology.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sparse-linear-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/xai-book/edit/master/04-interpretable-models.Rmd",
"text": "Edit"
},
"download": ["xai-book.pdf"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

<!-- </html> -->
