<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Interpretable machine learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Interpretable machine learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable." />
  <meta name="github-repo" content="christophM/xai-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Interpretable machine learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable." />
  

<meta name="author" content="Christoph Molnar">


<meta name="date" content="2017-11-09">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="permutation-feature-importance.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="who-should-read-this-book.html"><a href="who-should-read-this-book.html"><i class="fa fa-check"></i><b>1.1</b> Who should read this book</a></li>
<li class="chapter" data-level="1.2" data-path="outline.html"><a href="outline.html"><i class="fa fa-check"></i><b>1.2</b> Outline</a></li>
<li class="chapter" data-level="1.3" data-path="what-is-machine-learning-and-why-is-it-important.html"><a href="what-is-machine-learning-and-why-is-it-important.html"><i class="fa fa-check"></i><b>1.3</b> What is machine learning and why is it important?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> Interpretability</a><ul>
<li class="chapter" data-level="2.1" data-path="when-is-interpretability-important.html"><a href="when-is-interpretability-important.html"><i class="fa fa-check"></i><b>2.1</b> When is interpretability important?</a></li>
<li class="chapter" data-level="2.2" data-path="the-bigger-picture.html"><a href="the-bigger-picture.html"><i class="fa fa-check"></i><b>2.2</b> The bigger picture</a></li>
<li class="chapter" data-level="2.3" data-path="scope-of-intepretability.html"><a href="scope-of-intepretability.html"><i class="fa fa-check"></i><b>2.3</b> Scope of intepretability</a><ul>
<li class="chapter" data-level="2.3.1" data-path="scope-of-intepretability.html"><a href="scope-of-intepretability.html#algorithm-transparency"><i class="fa fa-check"></i><b>2.3.1</b> Algorithm transparency</a></li>
<li class="chapter" data-level="2.3.2" data-path="scope-of-intepretability.html"><a href="scope-of-intepretability.html#global-holistic-model-explainability"><i class="fa fa-check"></i><b>2.3.2</b> Global, holistic model explainability</a></li>
<li class="chapter" data-level="2.3.3" data-path="scope-of-intepretability.html"><a href="scope-of-intepretability.html#global-model-explainability-on-a-modular-level"><i class="fa fa-check"></i><b>2.3.3</b> Global model explainability on a modular level</a></li>
<li class="chapter" data-level="2.3.4" data-path="scope-of-intepretability.html"><a href="scope-of-intepretability.html#explain-the-decision-for-a-single-instance"><i class="fa fa-check"></i><b>2.3.4</b> Explain the decision for a single instance</a></li>
<li class="chapter" data-level="2.3.5" data-path="scope-of-intepretability.html"><a href="scope-of-intepretability.html#explain-the-decisions-for-a-group-of-instances"><i class="fa fa-check"></i><b>2.3.5</b> Explain the decisions for a group of instances</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="evaluating-interpretability.html"><a href="evaluating-interpretability.html"><i class="fa fa-check"></i><b>2.4</b> Evaluating interpretability</a><ul>
<li class="chapter" data-level="2.4.1" data-path="evaluating-interpretability.html"><a href="evaluating-interpretability.html#approaches-for-evaluation-of-the-explanation-quality"><i class="fa fa-check"></i><b>2.4.1</b> Approaches for evaluation of the explanation quality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="datasets.html"><a href="datasets.html"><i class="fa fa-check"></i><b>3</b> Datasets</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> Bike sharing counts (regression)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> Youtube spam comments (text classification)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical-data.html"><a href="cervical-data.html"><i class="fa fa-check"></i><b>3.3</b> Risk factors for cervical cancer (classification)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="definitions.html"><a href="definitions.html"><i class="fa fa-check"></i><b>4</b> Definitions</a></li>
<li class="chapter" data-level="5" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>5</b> Interpretable models</a><ul>
<li class="chapter" data-level="5.1" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>5.1</b> Terminology</a></li>
<li class="chapter" data-level="5.2" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>5.2</b> Overview</a></li>
<li class="chapter" data-level="5.3" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>5.3</b> Linear models</a><ul>
<li class="chapter" data-level="5.3.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>5.3.1</b> Interpretation</a></li>
<li class="chapter" data-level="5.3.2" data-path="limo.html"><a href="limo.html#interpretation-example"><i class="fa fa-check"></i><b>5.3.2</b> Interpretation example</a></li>
<li class="chapter" data-level="5.3.3" data-path="limo.html"><a href="limo.html#interpretation-templates"><i class="fa fa-check"></i><b>5.3.3</b> Interpretation templates</a></li>
<li class="chapter" data-level="5.3.4" data-path="limo.html"><a href="limo.html#visual-parameter-interpretation"><i class="fa fa-check"></i><b>5.3.4</b> Visual parameter interpretation</a></li>
<li class="chapter" data-level="5.3.5" data-path="limo.html"><a href="limo.html#explaining-single-predictions"><i class="fa fa-check"></i><b>5.3.5</b> Explaining single predictions</a></li>
<li class="chapter" data-level="5.3.6" data-path="limo.html"><a href="limo.html#coding-categorical-variables"><i class="fa fa-check"></i><b>5.3.6</b> Coding categorical variables:</a></li>
<li class="chapter" data-level="5.3.7" data-path="limo.html"><a href="limo.html#the-disadvantages-of-linear-models"><i class="fa fa-check"></i><b>5.3.7</b> The disadvantages of linear models</a></li>
<li class="chapter" data-level="5.3.8" data-path="limo.html"><a href="limo.html#towards-complexer-relationships-within-linear-model-class"><i class="fa fa-check"></i><b>5.3.8</b> Towards complexer relationships within linear model class</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="sparse-linear-models.html"><a href="sparse-linear-models.html"><i class="fa fa-check"></i><b>5.4</b> Sparse linear models</a></li>
<li class="chapter" data-level="5.5" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html"><i class="fa fa-check"></i><b>5.5</b> Logistic regression: a linear model for classification</a><ul>
<li class="chapter" data-level="5.5.1" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html#whats-wrong-with-linear-regression-for-classification"><i class="fa fa-check"></i><b>5.5.1</b> What’s wrong with linear regression for classification?</a></li>
<li class="chapter" data-level="5.5.2" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html#logistic-regression"><i class="fa fa-check"></i><b>5.5.2</b> Logistic regression</a></li>
<li class="chapter" data-level="5.5.3" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html#interpretation-1"><i class="fa fa-check"></i><b>5.5.3</b> Interpretation</a></li>
<li class="chapter" data-level="5.5.4" data-path="logistic-regression-a-linear-model-for-classification.html"><a href="logistic-regression-a-linear-model-for-classification.html#example"><i class="fa fa-check"></i><b>5.5.4</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>5.6</b> Decision trees</a><ul>
<li class="chapter" data-level="5.6.1" data-path="decision-trees.html"><a href="decision-trees.html#interpretation-2"><i class="fa fa-check"></i><b>5.6.1</b> Interpretation</a></li>
<li class="chapter" data-level="5.6.2" data-path="decision-trees.html"><a href="decision-trees.html#interpretation-example-1"><i class="fa fa-check"></i><b>5.6.2</b> Interpretation example</a></li>
<li class="chapter" data-level="5.6.3" data-path="decision-trees.html"><a href="decision-trees.html#advantages"><i class="fa fa-check"></i><b>5.6.3</b> Advantages</a></li>
<li class="chapter" data-level="5.6.4" data-path="decision-trees.html"><a href="decision-trees.html#disadvantages"><i class="fa fa-check"></i><b>5.6.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="other-simple-interpretable-models.html"><a href="other-simple-interpretable-models.html"><i class="fa fa-check"></i><b>5.7</b> Other simple, interpretable models</a><ul>
<li class="chapter" data-level="5.7.1" data-path="other-simple-interpretable-models.html"><a href="other-simple-interpretable-models.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>5.7.1</b> Naive bayes classifier</a></li>
<li class="chapter" data-level="5.7.2" data-path="other-simple-interpretable-models.html"><a href="other-simple-interpretable-models.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>5.7.2</b> k-nearest neighbours</a></li>
<li class="chapter" data-level="5.7.3" data-path="other-simple-interpretable-models.html"><a href="other-simple-interpretable-models.html#rulefit"><i class="fa fa-check"></i><b>5.7.3</b> RuleFit</a></li>
<li class="chapter" data-level="5.7.4" data-path="other-simple-interpretable-models.html"><a href="other-simple-interpretable-models.html#and-so-many-more"><i class="fa fa-check"></i><b>5.7.4</b> And so many more …</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="model-agnostic-tools-for-interpretability.html"><a href="model-agnostic-tools-for-interpretability.html"><i class="fa fa-check"></i><b>6</b> Model-agnostic tools for interpretability</a><ul>
<li class="chapter" data-level="6.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>6.1</b> Partial dependence plot</a></li>
<li class="chapter" data-level="6.2" data-path="individual-conditional-expectation-ice-plot.html"><a href="individual-conditional-expectation-ice-plot.html"><i class="fa fa-check"></i><b>6.2</b> Individual Conditional Expectation (ICE) plot</a></li>
<li class="chapter" data-level="6.3" data-path="permutation-feature-importance.html"><a href="permutation-feature-importance.html"><i class="fa fa-check"></i><b>6.3</b> Permutation feature importance</a><ul>
<li class="chapter" data-level="6.3.1" data-path="permutation-feature-importance.html"><a href="permutation-feature-importance.html#model-dependent-feature-importance"><i class="fa fa-check"></i><b>6.3.1</b> Model dependent feature importance</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="local-surrogate-models-lime.html"><a href="local-surrogate-models-lime.html"><i class="fa fa-check"></i><b>6.4</b> Local surrogate models (LIME)</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable machine learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="local-surrogate-models-lime" class="section level2">
<h2><span class="header-section-number">6.4</span> Local surrogate models (LIME)</h2>
<p>Local interpretable model-agnostic explanations (LIME) is a method for fitting local, interpretable models that can explain single predictions or classifications of any black-box machine learning model. LIME explanations are local surrogate models. Instead of trying to fit a global surrogate model, LIME focuses on a prediction done by a black-box algorithm and explains it’s outcome.</p>
<p>The idea is quite simple, really. First of all, forget about the training data and imagine you only have the black box model where you can input data points and get the models outcome. You can probe the box as often as you want. Your goal is to understand why the machine learning model gave the outcome it produced. LIME tests out what happens to the model’s predictions when you put some variations of your data point of interest into the machine learning model. This basically generates a new dataset consisting of the perturbed samples and the associated model’s outcome. On this dataset LIME then trains a simple model weighted by the proximity of the sampled instances to the instance of interest. The simple mode can basically be any from Section [#simple], for example LASSO or a short tree. The learned model should be a good approximation of the machine learning model locally, but it does not have to be so globally. This kind of accuracy is also called local fidelity.</p>
<p>The recipe:</p>
<ul>
<li>Choose your instance of interest for which you want to have an explanation of it’s black box outcome</li>
<li>Make some variations of the instances and check what the black box predicts in the neighbourhood of the instance of interest.</li>
<li>Fit a local, interpretable model on the dataset with the variations</li>
<li>Explain prediction by interpreting the local simple model.</li>
</ul>
<p>In the current implementation, only LASSO can be chosen as a simple model. Upfront you have to choose K, the number of features that you want to have in your simple model. The lower the K, the simpler the model is to understand, higher K potentially creates models with higher fidelity. There are different methods for how to fit models with exactly K features. The most natural with LASSO is the lasso path. Starting from a model with a very high regularisation parameter <span class="math inline">\(\lambda\)</span> yields a model with only the intercept. By refitting the LASSO models with slowly decreasing <span class="math inline">\(\lambda\)</span> one after each other the features are getting weight estimates different from zero. When K features are in the model, you reached the desired number of features. Other strategies are forward or backward selection of features. This means you either start with the full model (=containing all features) or with a model with only the intercept and then testing which feature would create the biggest improvement when added or removed, until a model with K features are reached. Other simple models like decision trees are currently not implemented.</p>
<p>As always, the devil is in the details. In a high-dimensional space, defining a neighbourhood is not trivial. Distance measures are quite arbitrary and distances in different dimensions (aka features) might not be comparable at all. How big should the neighbourhood be you look into? If it is too small, then there might be no difference in the predictions of the machine learning model at all. The other question is: How do you get the variations of the data? This differs depending on the type of data, which can be either text, an image or tabular data. For text and image the solution is turning off and on single words or superpixels. In the case of tabular data LIME creates new samples by pertubing each feature individually, by drawing from a normal distribution with mean and standard deviation from the feature.</p>
<div id="lime-for-tabular-data" class="section level4">
<h4><span class="header-section-number">6.4.0.1</span> LIME for tabular data</h4>
<p>Tabular data means any data that comes in tables, where each row represents an instance and each column a feature. Sampling is not done around the point, but from the training data’s mass center. Has it’s problems. But it increases the likelihood that the outcome for some of the sampled points predictions differ from the data point of interest and that LIME can learn at least some explanation.</p>
<p>Figure <a href="local-surrogate-models-lime.html#fig:lime-fitting">6.10</a> explains how the sampling and local model fitting works.</p>
<div class="figure"><span id="fig:lime-fitting"></span>
<img src="xai-book_files/figure-html/lime-fitting-1.svg" alt="How LIME sampling works: A) The trainig data has two classes. The most data points have class 0, and the ones with class 1 are grouped in an upside-down V-shape. The plot displays the decision boundaries learned by a machine learning model. In this case it was a Random Forest, but it does not matter, because LIME is model-agnostic and we only care about the decision boundaries. B) The yellow is the instance of interest, for which an explanation is desired. The black dots are data sampled from a normal distribution around the means of the features in the training sample. This has only to be done once and can be reused for other explanations. C) Introducing locality by giving points near the instance of interest a higher weights. D) The colors and signs of the grid display the classifications of the locally learned model form the weighted samples. The white line marks the decision boundary (P(class) = 0.5) at which the classification changes." width="864" />
<p class="caption">
FIGURE 6.10: How LIME sampling works: A) The trainig data has two classes. The most data points have class 0, and the ones with class 1 are grouped in an upside-down V-shape. The plot displays the decision boundaries learned by a machine learning model. In this case it was a Random Forest, but it does not matter, because LIME is model-agnostic and we only care about the decision boundaries. B) The yellow is the instance of interest, for which an explanation is desired. The black dots are data sampled from a normal distribution around the means of the features in the training sample. This has only to be done once and can be reused for other explanations. C) Introducing locality by giving points near the instance of interest a higher weights. D) The colors and signs of the grid display the classifications of the locally learned model form the weighted samples. The white line marks the decision boundary (P(class) = 0.5) at which the classification changes.
</p>
</div>
</div>
<div id="example-3" class="section level4">
<h4><span class="header-section-number">6.4.0.2</span> Example</h4>
<p>Let’s look at a concrete example. We go back to the bike rental and turn the prediction problem into a classification: After accounting for the trend that the bike rental get’s more popular over time we want to know on a given day if the number of rented bikes will be above or below the trend line. You can also interpret ‘above’ as being above the mean bike counts, but adjusted for the trend.</p>
First we train a Random Forest on the classification task. Given seasonal and wheather information, on which day will the number of bike rentals be above the trend-free average? The Random Forest has 100 trees.
<div class="figure"><span id="fig:lime-tabular-example-explain-plot-1"></span>
<img src="xai-book_files/figure-html/lime-tabular-example-explain-plot-1-1.svg" alt="Explanations for two instances. The first instance got a probability of 0.13 for a match by the Random Forest. The correlation of interests between the participant and the partner is low which reduces the probability of a match for this participant. Also the importance for race of the partner is higher than 6, which lowers the probability of a match.  In the second case, the probability for a match is quite high with 0.46. The reasons are the high correlation (&gt;0.43) and that the partner is younger then 25." width="672" />
<p class="caption">
FIGURE 6.11: Explanations for two instances. The first instance got a probability of 0.13 for a match by the Random Forest. The correlation of interests between the participant and the partner is low which reduces the probability of a match for this participant. Also the importance for race of the partner is higher than 6, which lowers the probability of a match. In the second case, the probability for a match is quite high with 0.46. The reasons are the high correlation (&gt;0.43) and that the partner is younger then 25.
</p>
</div>
<p>The continuous features are categorised into bins by quantiles for the explanation models. The explanations are set to contain 3 features. Figure @ref{fig:lime-tabular-example-explain-plot-1} shows the results of the sparse local linear model that was fitted for two instances with different predicted classes. It becomes clear from the figure, that it is easier to interpret categorical features than continuous features. Figure @ref{fig:lime-tabular-example-explain-plot-2} shows a variant where the continuous features are turned into categorial features by putting them into bins along the quantiles.</p>
<div class="figure"><span id="fig:lime-tabular-example-explain-plot-2"></span>
<img src="xai-book_files/figure-html/lime-tabular-example-explain-plot-2-1.svg" alt="Explanations for two instances. This time continuous features were turned into categorial features by binning them." width="672" />
<p class="caption">
FIGURE 6.12: Explanations for two instances. This time continuous features were turned into categorial features by binning them.
</p>
</div>
</div>
<div id="lime-for-images" class="section level4">
<h4><span class="header-section-number">6.4.0.3</span> LIME for images</h4>
<p>For images the sampling procedure works differently. Instead of sampling single pixels, LIME create variations of the image by turning off superpixel.</p>
</div>
<div id="lime-for-text" class="section level4">
<h4><span class="header-section-number">6.4.0.4</span> LIME for text</h4>
<p>LIME for text works a bit differently than for tabular data. Variation of the point to be explained are created differently: Starting from the original text, new texts are created by randomly removing words from it.</p>
</div>
<div id="example-4" class="section level4">
<h4><span class="header-section-number">6.4.0.5</span> Example</h4>
<p>In this example we classify spam vs. ham of YouTube comments. The dataset is described in [#TubeSpam].</p>
<p>The black box model is a decision tree on the document word matrix. Each comment is one document (= one row) and each column is a the number of occurences of a specific word. A decision tree was trained on this data. As discussed in Section [#simple], decision trees are easy to understand, but in this case the tree is very deep. Also in the place of this tree there could have been a recurrent neural network or a support vector machine that was trained on the embeddings from word2vec. The machine learning model was trained on 80% of the approximately 2000 comments. From the remaining comments two were selected for showing the explanations.</p>
<p>Let’s look at tow comments of this dataset and the corresponding classes:</p>
<div id="htmlwidget-94285ba5c6903b17154e" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-94285ba5c6903b17154e">{"x":{"filter":"none","data":[["2","192"],["Hey guys check out my new channel and our first vid THIS IS US THE  MONKEYS!!! I'm the monkey in the white shirt,please leave a like comment  and please subscribe!!!!","The Guy in the yellow suit kinda looks like Jae-suk "],[1,0]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>CONTENT<\/th>\n      <th>CLASS<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":2},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false},"selection":{"mode":"multiple","selected":null,"target":"row"}},"evals":[],"jsHooks":[]}</script>
<p>In the next step we create some variations of the datasets, which are used in a local model. For example some variations of one of the comments. <div id="htmlwidget-aa0035eeecbb5583be9e" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-aa0035eeecbb5583be9e">{"x":{"filter":"none","data":[["2","3","4","5","6"],[1,0,1,1,1],[0,1,1,1,1],[0,1,0,1,1],[0,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[0,0,0,1,1],[1,0,1,0,1],[1,0,1,0,1],[1,1,1,1,1],[0.0775,0.0775,0.0775,0.0775,0.0775],[0.6,0.6,0.8,0.8,1]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>The<\/th>\n      <th>Guy<\/th>\n      <th>in<\/th>\n      <th>the<\/th>\n      <th>yellow<\/th>\n      <th>suit<\/th>\n      <th>kinda<\/th>\n      <th>looks<\/th>\n      <th>like<\/th>\n      <th>Jae-suk<\/th>\n      <th>prob<\/th>\n      <th>weight<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[1,2,3,4,5,6,7,8,9,10,11,12]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false},"selection":{"mode":"multiple","selected":null,"target":"row"}},"evals":[],"jsHooks":[]}</script> Each column corresponds to one word in the sentence. Each row is a variation. 1 indicates that the word is part of this variation. 0 indicates that the word was removed. The corresponding sentence for the first variation is “<code>Guy in the yellow suit Jae-suk</code>”.</p>
<p><img src="xai-book_files/figure-html/lime-text-explanations-1.svg" width="672" /> #### Problems with LIME</p>
<ul>
<li>LIME does not work if the classification is very unbalanced (one class is very common) and the black box only predicts one class</li>
<li>Defining the neighbourhood is tricky.</li>
</ul>


<div id="refs" class="references">
<div>
<p>Alberto, Túlio C, Johannes V Lochter, and Tiago A Almeida. 2015. “Tubespam: Comment Spam Filtering on Youtube.” In <em>Machine Learning and Applications (Icmla), 2015 Ieee 14th International Conference on</em>, 138–43. IEEE.</p>
</div>
<div>
<p>Breiman, Leo. 2001. “Random Forests.” <em>Machine Learning</em> 45 (1). Springer: 5–32.</p>
</div>
<div>
<p>“Definition of Algorithm.” 2017. <a href="https://www.merriam-webster.com/dictionary/algorithm" class="uri">https://www.merriam-webster.com/dictionary/algorithm</a>.</p>
</div>
<div>
<p>Doshi-Velez, Finale, and Been Kim. 2017. “Towards A Rigorous Science of Interpretable Machine Learning,” no. Ml: 1–13. <a href="http://arxiv.org/abs/1702.08608" class="uri">http://arxiv.org/abs/1702.08608</a>.</p>
</div>
<div>
<p>Fanaee-T, Hadi, and Joao Gama. 2013. “Event Labeling Combining Ensemble Detectors and Background Knowledge.” <em>Progress in Artificial Intelligence</em>. Springer Berlin Heidelberg, 1–15. doi:<a href="https://doi.org/10.1007/s13748-013-0040-3">10.1007/s13748-013-0040-3</a>.</p>
</div>
<div>
<p>Fernandes, Kelwin, Jaime S Cardoso, and Jessica Fernandes. 2017. “Transfer Learning with Partial Observability Applied to Cervical Cancer Screening.” In <em>Iberian Conference on Pattern Recognition and Image Analysis</em>, 243–50. Springer.</p>
</div>
<div>
<p>Friedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” <em>Annals of Statistics</em>. JSTOR, 1189–1232.</p>
</div>
<div>
<p>Friedman, Jerome H, and Bogdan E Popescu. 2008. “Predictive Learning via Rule Ensembles.” <em>The Annals of Applied Statistics</em>. JSTOR, 916–54.</p>
</div>
<div>
<p>Goldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. “Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation.” <em>Journal of Computational and Graphical Statistics</em> 24 (1). Taylor &amp; Francis: 44–65.</p>
</div>
<div>
<p>Hastie, T, R Tibshirani, and J Friedman. 2009. <em>The elements of statistical learning</em>. <a href="http://link.springer.com/content/pdf/10.1007/978-0-387-84858-7.pdf" class="uri">http://link.springer.com/content/pdf/10.1007/978-0-387-84858-7.pdf</a>.</p>
</div>
<div>
<p>Lipton, Zachary C. 2016. “The Mythos of Model Interpretability.” <em>ICML Workshop on Human Interpretability in Machine Learning</em>, no. Whi.</p>
</div>
<div>
<p>Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. “Model-Agnostic Interpretability of Machine Learning.” <em>ICML Workshop on Human Interpretability in Machine Learning</em>, no. Whi.</p>
</div>
<div>
<p>Strobl, Carolin, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin, and Achim Zeileis. 2008. “Conditional variable importance for random forests.” <em>BMC Bioinformatics</em> 9 (January): 307. doi:<a href="https://doi.org/10.1186/1471-2105-9-307">10.1186/1471-2105-9-307</a>.</p>
</div>
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="permutation-feature-importance.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/xai-book/edit/master/05.5-agnostic-lime.Rmd",
"text": "Edit"
},
"download": ["xai-book.pdf"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

<!-- </html> -->
