
### Interpretation
The interpretation of the logistic regression weights differs from the linear regression case, because in logistic regression the outcome is a probability between 0 and 1, and the weights don't affect the probability linearly, but are squeezed through the logistic function.
That's why we need to reformulate the equation for the interpretation, so that there is only the linear term left on the right side of the formula.

{$$}log\left(\frac{P(y_{i}=1)}{1-P(y_{i}=1)}\right)=log\left(\frac{P(y_{i}=1)}{P(y_{i}=0)}\right)=\beta_{0}+\beta_{1}x_{i,1}+\ldots+\beta_{p}x_{i,p}{/$$}

{$$}\frac{P(y_{i}=1)}{1-P(y_{i}=1)}{/$$} is also called odds (probability of event divided by probability of no event) and {$$}log\left(\frac{P(y_{i}=1)}{1-P(y_{i}=1)}\right){/$$} is called log odds.
So with a logistic regression model we have a linear model for the log odds.
Great!
Doesn't sound helpful!
Well, with a bit of shuffling again, you can find out how the prediction changes, when one of the features {$$}x_j{/$$} is changed by 1 point.
For this we can first apply the {$$}exp(){/$$} function on both sides of the equation:

{$$}\frac{P(y_{i}=1)}{(1-P(y_{i}=1))}=odds_i=exp\left(\beta_{0}+\beta_{1}x_{i,1}+\ldots+\beta_{p}x_{i,p}\right){/$$}

Then we compare what happens when we increase one of the {$$}x_{i,j}{/$$} by 1.
But instead of looking at the difference, we look at the ratio of the two predictions:

$$\frac{odds_{i,x_j+1}}{odds_i}=\frac{exp\left(\beta_{0}+\beta_{1}x_{i,1}+\ldots+\beta_{j}(x_{i,j} + 1)+\ldots+\beta_{p}x_{i,p}\right)}{exp\left(\beta_{0}+\beta_{1}x_{i,1}+\ldots+\beta_{j}x_{i,j}+\ldots+\beta_{p}x_{i,p}\right)}$$

Using the rule that {$$}\frac{exp(a)}{exp(b)}=exp(a-b){/$$} gives us:

{$$}\begin{aligned}\frac{odds_{i,x_j+1}}{odds_i}&=&exp\left(\beta_{0}+\beta_{1}x_{i,1}\ldots\beta_{j}(x_{i,j}+1)+\ldots+\beta_{i,p}x_{i,p}\right)\\&&-exp\left(\beta_{0}+\beta_{1}x_{i,1}+\ldots+\beta_{j}x_{i,j}+\ldots+\beta_{p}x_{i,p}\right)\end{aligned}{/$$}



