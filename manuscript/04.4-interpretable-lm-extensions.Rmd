```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

## Linear model extensions: GLM, GAM and more {#extend-lm}


I recommend reading the [chapter on linear regression models](#linear) first.

Recall the formula of a linear model:

$$y_{i}=\beta_{0}+\beta_{1}x_{i1}+\ldots+\beta_{p}x_{ip}+\epsilon_{i}$$

The i-th instance's outcome $y_i$ is assumed to be a weighted sum of its $p$ features $x_j$ with an individual error $\epsilon_i$ that follows a Gaussian distribution.

This formula is too restrictive for many real world prediction problems.
The only allowed relationship between the prediction and the features is linear.
But what if the real relationship is multiplicative or the influence of the feature follows a complex curve?
The errors are assumed to have a Gaussian distribution.
As a consequence, we also assume a Gausisan distribution of the target Y, after accounting for the features X. 
This assumption is often violated, for example when the outcome can only have positive values, like the duration of an event.
What if we want to model a classification task, or if the output we are interested in is a count of something, like number of children in a family?

The linear regression model has been extensively studied and extended.
The chapter will cover some of those extensions that leverage the linear model for more complex tasks.

- What to do when  the target y is not normal -> GLM  
- What to do when the relation between the features and y is not linear -> feature transformations, GAMs (splines)
- What to do when there are interactions between features -> manual integration of interaction effects, automatic detection  


Other scenarios, not covered here:
- What to do when observations are not i.i.d? Mixed models or GEEs.
- What to do with heterogeneity in the errors? 


### Different types of targets

Normal linear regression assumes that the target has a Gaussian distribution, given the input features.
Logistic regression assumes that the target has a Bernoulli distribution, given the input features.
But what if we want to predict the time between two events? 
This time is always positive and a Gaussian distribution might be just wrong.
What if we want to predict some count feature, like the number of children a person will have in the lifetime?
That's certainly not a Gaussian distribution and also not Bernoulli.
So we need a more flexible approach to handle different types of outcomes, but would love to keep the interpretability of linear models and all the tools connected to it, like having weights that can be interpreted, having confidence intervals, being able to make predictions and so on.
The solution to generalizing linear models to arbitrary outcome distributions is called GLM: generalized linear model.

At the core, the generalized linear model has still a linear composition of the features, but it is linked to a function g of the distribution mean, which can be chosen flexibly depending on the type of outcome. 
The link function links the linear predicted to the mean of the distribution we are assuming.

$$\beta_0+\beta_1{}x_1+\ldots{}\beta_p{}x_p=g(E(y))$$
The GLM consists of three components. 
Besides the link function and the linear predictor, we also need a probability distribution from the exponential family.
In the exponential family are all distribution function that kind of follow the same pattern.
They can all be written in the same format, which involves and exponent, the mean and variance of the distribution and some other parameters.
I won't go into details here, because that's a very big universe of it's own I don't want to open up.
Wikipedia has a nice [list of distributions from the exponential family](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions). 
Any distribution from this list can be chosen as the distribution.
The choice depends on the nature of the outcome you have.
Your outcome is some kind of count (e.g. number of children)?
Then the Poisson distribution might be a good choice.
Your outcome is always positive (e.g. time between two events)?
Then the Exponential distribution might be a good choice.

Each distribution comes with parameters that describe the distribution.
For example, the Gaussian distribution is described by the $\mu$ which tells us where the expected value of the distribution is and the variance $\sigma$ which tells us how flat or peaked the distribution is.

GLMs are easier to understand when you don't think of the linear regression model as a model of $y$, but of the $\mu$ of the Gaussian distribution, conditional on the features.
When we formulate a linear model, we use the link function to link the linear equation - the weighted sum of the features - to the mean of the Gaussian distribution.
The link function for the Gaussian distribution, the standard linear model, is the identity function.
This concept generalizes under the name GLM to any distribution.
For example, when we switch out the Gaussian distribution with the Poisson distribution, which is used for counts, the natural link function is the logarithm with the natural base.

$$x\beta=ln(\lambda)$$
Where $\lambda$ is the parameter describing the intensity of the poisson distribution, a larger value means that we expect a higher outcome.

You have also come across the logit link function already in the logistic regression formula:

$$X\beta=ln\left(\frac{\mu}{1-\mu}\right)$$
The mean $\mu$ of the binomial distribution, which is used in logistic regression is the probability that y is 1.
So you might also see:

$$x\beta=ln\left(\frac{P(Y=1)}{1-P(Y=1)}\right)$$
And if we solve this equation to have P(Y=1) on one side, we get the classic logistic regression formula:

$$P(y_{i}=1)=\frac{1}{1+exp(-x_i\beta)}$$

You are quite flexible with the link function you choose.
But each distribution also comes with a canonical link function, which can be derived mathematically from the distribution, when it is reformulated as the exponential family.


If you are reading the book from start to end, you have already came across a GLM: logistic regression. 

*Examples*

How many coffees did you drink today?
Features:
- Stress level between 1 and 10
- How well did you sleep last night on a scala from 1 to 10?
- Is it weekend?

(Writing this as I am sipping my second coffee at 11AM).

```{r poisson-data}
# simulate data where the normal linear model fails.
n = 200
df = data.frame(stress  = runif(n = n, min = 1, max = 10), 
  sleep = runif(n = n, min = 1, max = 10), 
  weekend = sample(c("YES", "NO"), size = n, replace = TRUE))
lambda = exp(1* df$stress/10 - 2 * (df$sleep - 5)/10  - 1 * (df$weekend == "YES"))
df$y = rpois(lambda = lambda, n = n)


kable(table(df$y))

```


```{r failing-linear-model}
mod.gaus = glm(y ~ ., data = df, x = TRUE)
pred.gauss = data.frame(pred = predict(mod.gaus), actual = df$y)
ggplot(pred.gauss)  + geom_histogram(aes(x = pred))
```

Predicting negative values. well. That's awkward.


Better idea: Use poisson distribution with exp link function.

```{r linear-model-positive}
mod.pois = glm(y ~ ., data = df, x = TRUE, family = poisson(link = "log"))
pred.pois = data.frame(pred = predict(mod.pois, type = "response"), actual = df$y)
ggplot(pred.pois)  + geom_histogram(aes(x = pred))
```


Interpretation of parameters


The interpretation depends on the chosen distribution and how we link the weighted sum of features to the expected outcome.
In this case, the assumed distribution of the outcome given the features is a poisson distribution.
The link function between the weighted sum and the number of expected coffees is:

$$\beta_0+\beta_{stress}x_{stress}+\beta_{sleep}x_{sleep}+\beta_{like}x_{like}=ln(E(coffees|stress,sleep,like))$$

A Poisson distribution has one parameter: $\lambda$, the intensity at which 'events' happen (here the number of consumed coffees)
A high value of $\lambda$ means that we expect a high count.
The expected number of counts is $\lambda$.
Ok, how do we connect that to the interpretation of the $\beta$'s?

Here it comes:
By increasing an x by one, the expected outcome increases multiplicatively by $exp(\beta)$.
The exp comes from inversing the link function:

$$exp(\beta_0+\beta_{stress}x_{stress}+\beta_{sleep}x_{sleep}+\beta_{like}x_{like})=E(coffees|stress,sleep,like)$$
Let's have a look at the weights and the exp(weights) and their variance:
```{r poisson-model-params}
cc = data.frame(summary(mod.pois)$coefficients)
cc = cc[,c("Estimate", "Std..Error")]
colnames(cc) = c("beta", 'var.beta')
cc$exp.beta = exp(cc[, 'beta'])
cc = cc[c("beta", "exp.beta")]
cc = cbind(cc, exp(confint(mod.pois)))
cc$ci = sprintf("%.2f [%.2f, %.2f]", cc$exp.beta, cc$`2.5 %`, cc$`97.5 %`)
kable(cc[c("beta", "ci")], col.names = c("weight", "exp(weight) [2.5%, 97.5%]"), digits = 2)
```

Let's interpret the parameters in the made-up coffee prediction example.
Increasing the stress by one point increases the expected number of coffees by a factor of `r round(cc["stress", "exp.beta"], 2)`. 
Increasing the sleep by one point decreases the expected number of coffees by a factor of `r round(cc["sleep", "exp.beta"], 2)`.
Compared to a weekday, on a weekend the predicted number of cofees is lower by a factor of `r round(cc["weekendYES", "exp.beta"], 2)`.

In the next part, we will talk about the solution when the features interact with each other.

### Adding interactions

What if the features interact with each other?
Say for predicting the number of bikes that will be rented, there might be an interaction betweeen the temperature and if it's a working day or not.
Maybe when people have to work, the temperature does not affect the number of rented bikes, because only the tough people ride, because they have to get to work.
On the free days, many people decide more based on the weather. 
So we might suspect an interaction between the two.
By default, the linear model doesn't model interactions explicitly.
It might actually suck pretty hard when interactions are involved, because the effects are not independent from each other any more.

What can you do about it?
You can simply add a term to the features that represents the interaction between features.
In the working day x temperature example, we would add a new features, that is zero whenever it's a weekend, otherwise the temperature.
The working day is then the reference category.

A toy example.
Let's say our features look like this:
```{r data-frame}
x = data.frame(workingday = c("Yes", "No", "No", "Yes"), temp = c(25, 12, 30, 5))
knitr::kable(x)
```
What the model receives, when we want both the linear effects and the additional interaction effect:
```{r data-frame-lm}
mod = lm(1:4 ~ workingday * temp, data = x)
knitr::kable(data.frame(model.matrix(mod)))
```

The first term is the intercept term which usually gets added automatically in the most programs for linear regression models.
The second column is the workingday feature, which is now only the indicator for the feature value Yes, since No is assumed as the reference category.
Where the workingdayY column is 1, the original category was "Y".
The column temp is the same as before.
New is the column "workingdayYes" which captures the interaction between the features workingday and temperature.
This new feature is zero when the workingday feature has the reference category "No", otherwise it takes on the value of the temp feature.
With this type of coding, the linear model can learn a different linear effect of the temperature for both types of days.
This is the interaction effect between the two features.
For two categorical features, the process is the same:
We create additional features which represents combinations of categories. 
So this artificial data: 

```{r data-frame-lm-cat}
x = data.frame(workingday = c("Yes", "No", "No", "Yes"), wday = c("Mon", "Sun", "Sat", "Mon"))
knitr::kable(x)
```

Becomes this: 

```{r data-frame-lm-cat2}
mod = lm(1:4 ~ workingday * wday, data = x)
knitr::kable(data.frame(model.matrix(mod)))
```
For any category of both features (excluding the reference categories), we create a new feature column that is 1 if both features have a certain category, otherwise 0.
For two numerical features, the interaction column is even easier to construct: 
We simply multiply both numerical features.
There are lots of ways and types of interactions that can be added (nested, 2way, 3way, ...). 
There are approaches to detect and add interaction terms automatically. 
One is presented in this book called [RuleFit](#rulefit), which first mines interaction terms and then estimates a linear regression model with this new data.

### Example


```{r example-lm-interaction}
data(bike)
X = bike[bike.features.of.interest]
y = bike[,'cnt']
dat = cbind(X, y)

mod = lm(y ~ . + temp * workingday, data = dat, x = TRUE)
lm_summary = summary(mod)$coefficients

lm_summary_print = lm_summary
rownames(lm_summary_print) = pretty_rownames(rownames(lm_summary_print))
confint(mod)

kable(cbind(lm_summary_print[,c('Estimate', 'Std. Error')], confint(mod)), digits = 1, col.names = c('Weight estimate', 'Std. Error', "2.5%","97.5%"))
```

The interaction term drastically changes how we interpret the terms of the involved features.
Does the temperature have a negative effect on a working day?
The answer is no, even though the weights suggest it in the first moment.
We can't interpret the "workingdayWORKING DAY:temp" interaction term directly, since the interpretation would be
All feature values being equal, increasing the additional temperature effect for the working day, decreases the predicted number of bike rents. 
But this comes only on top of the effect of the temperature feature, which we would have to increase at the same time.
Let's say we have a working day, and want to know what would happen if the temperature today would be 1 degree warmer. 
Then we have to add both the temp and "workindayWORKING DAY:temp" weights to get how much the estimate increases.

It's easier to understand visually. 
By introducing an interaction term between a categorical and a numerical feature, we now have two slopes for the temperature instead of one.
```{r interaction-plot, fig.cap = "The effect (including interaction) of temperature and working day on the predicted number of bike rental for a linear model. Effectively we get two slopes for the temperature, by the factor working day or not. Without the interaction it would be a single slope for the temperature, only shifted by the different means of non-working vs. working day."}
jtools::interact_plot(mod, pred = "temp", modx = "workingday")
```


Problem: The interpretation of the weights is always given that all else stays equal. 
This is problematic with interaction term, since they consist of two other features that are in the model.
It is not meaningful to interpret an increase of the interaction feature, but leave the main feature as is.


*Examples*


TODO: Example: Interaction between features in bike sharing.
e.g. Interaction between temperature and the season.


### Non-linear effects

What if we suspect or know that one of the features has a non-linear effect on the prediction.
For example, the temperature effect might be linear for lower temperatures up to a certain degree, but then level off.
The linear regression model would not capture that effect on it's own.

We have multiple solutions for allowing non-linear effects:
- Simple transformation of the feature
- Categorize the feature
- Use splines, which is a new model class called Generalized Additive Models (GAMs).

Before I explain each of them, will start with an example right away.
We take the bike rental dataset and try to predict the number of rented bikes using only the temperature feature.
The following figure shows the estimated slope without doing anything and with either transforming the feautre (the square root), cutting it into intervals or using splines (also called GAM then).

```{r non-linear-effects, fig.cap = "Predicting the number of rented bikes given the temperature. Using a standard linear model (top left) doesn't capture the flattening of the temperature effect as suggested by the point cloud and overestimates the number of bikes at low temperatures. One solution is to transform the feature (top right), to categorize it (bottom left), which is usually a bad decision or to use Generalized Additive Models, which can automatically fit a smooth curve for temperature (bottom right), which is most often the best solution."}
mod.simpel = lm(cnt ~ temp, data = bike)
bike.plt = bike
bike.plt$pred.lm = predict(mod.simpel)

bike.plt$sqrt.temp = log(bike$temp + 10)
mod.simpel = lm(cnt ~ sqrt.temp, data = bike.plt)
bike.plt$pred.sqrt = predict(mod.simpel)

bike.plt$cat.temp = cut(bike$temp, breaks = seq(from = min(bike$temp), to = max(bike$temp), length.out = 10), include.lowest = TRUE)
mod.simpel = lm(cnt ~ cat.temp, data = bike.plt)
bike.plt$pred.cat = predict(mod.simpel)

library(mgcv)
mod.gam = gam(cnt ~ s(temp), data = bike)
bike.plt$pred.gam = predict(mod.gam)


bike.plt = data.table::melt(bike.plt[c("pred.lm", "pred.sqrt", "pred.cat", "pred.gam")])
bike.plt$temp = rep(bike$temp, times = 4)
bike.plt$cnt = rep(bike$cnt, times = 4)

model.type = c(pred.lm = "Linear model", 
  pred.sqrt = "Linear model with sqrt(temp)", 
  pred.cat = "Linear model with categorized temp", 
  pred.gam = "GAM")

ggplot(bike.plt) + 
  geom_point(aes(x = temp, y = cnt), size = 1 , alpha = 0.3)  + 
  geom_line(aes(x = temp, y = value), size = 1.2, color = "blue") + 
  facet_wrap("variable", labeller = labeller(variable = model.type)) + 
  scale_x_continuous("Temperature (temp)") + 
  scale_y_continuous("(Predicted) Number of rented bikes")
```


You can be very creative when transforming your feature:
Very common is taking the logarithm of the feature. 
Maybe the logarithm of the temperature has a linear effect on the outcome?
Meaning that each 10x increase of the temperature has the same linear effect on the number of bikes, 
so going from 1 degree Celsius to 10 degree celsius has the same effect as going from 0.1 to 1 (sounds wrong).

Another solution: We discretize the feature, turn it into many categorical features.
So the temperature would have values [-10, -5), [-5, 0), ... you get it.
Then the linear model would estimate a step function, because each level get's it's own estimate.
The problem with this approach is that you need a lot more data, are more likely to overfit and have no clue what meaningful way to discretize it.
Also, you loose information, like going from an anlogue picture to a pixel image (which is definitely not 4K). 
I would only use discretiziation if you have a very strong case for it:
For example to make the model comparable to another study or so.

The more sophisticated solution: Use splines.
Splines are actually similar to discretization, because you replace the feature by many new columns with which you allow to estimate a non-linear effect on the outcome.

When you use splines, then most people will call their linear model generalized additive models (GAMs), which are - well - GLMs with non-linear effects.


```{r splines, fig.cap = "Instead of a single temperature feature, we use 10 spline functions. Each temperature value gets mapped to (here) 9 spline values. If an instance has a temperature of 30 C, then the value for the first spline feature is -1, for the second 0.5 ... and for the 9th it's 2. The we feed the the GAM with those features."}
mm = model.matrix(mod.gam)
mm2 = data.table::melt(mm)
mm2 = mm2[mm2$Var2 != "(Intercept)",]

ggplot(mm2) + geom_line(aes(x = rep(bike$temp, times = 9), y = value)) + facet_wrap("Var2") + 
  scale_x_continuous("Temperature") + 
  scale_y_continuous("Value of spline feature")
```

But don't we have the same problem as with categorizing the feature:
We get too many parameters and might overfit the data.
GAMs introduce some penalty term for the weights, so that they can't change to much away from zero. 
This effectively reduces the flexibility of the splines approach (how much they can overfit). 
How flexible the spline curve is usually tuned by some smoothness parameter, which is usually tuned using cross-validation.


TODO:  Explain how to interpret splines



### Advantages

- Has lots of theory behind it for calculating things like confidence intervals and testing hypothesis
- Very rich literature and experience with these models
- Accepted in many fields as the standard for testing hypothesis
- Can be made sparse by using regularization, which favors estimation of weights with 0.
- Really good interfaces (for example in R) to describe how the relationship between input and output should be handled. 
Can be written as formula.

TODO: continues advantages

### Disadvantages

- Each step makes the interpretation more complex. 
Different link function make the interpretation more complicated;
Interactions make the interpretation more complicated;
Non-linear feature effects are either not very intuitive (like log transform) or cannot be summarized with single number any longer.
- When features are transformed, coefficients are typically not interpretable at all.
- The world of linear regression models and their extensions can be quite overwhelming
- There are attempts to automate these steps, but mainly it's manual.
- For interactions: the estimated coefficient (main effect) of each feature only caries meaning of an unconditional marginal effect when the effect of the interaction is zero, which is neve


TODO: Continues disadvantages


### Software and Further Readings

- Any major programming language for stats has all those
- TODO: List languages
- TODO: List good book reference
- TODO: Mention Lasso
- TODO: Mention GEE and mixed models
