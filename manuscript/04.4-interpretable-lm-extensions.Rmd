```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

## Linear model 2.0: GLMs, GAMs and more {#extend-lm}

Linear regression models handle the relationship between features and outcome with a weighted sum.
The bad news is (well, not really news), that they are very restrictive in terms of how the output may look like, not handling interactions between features (by default), only allowing linear relationships.
The good news is that the statistics community has come up with a vast array of modifications that make the linear regression model a powerful swiss knife.

There are just so many ways of extending linear models.
And it's so confusing, because different disciplines use linear models and have (re-)invented methods but with different namings.
This chapter will definitely not be your definite guide to extending linear models. 
It serves rather as an overview to give you some intuition about some important extensions like GLMs and GAMs.
After reading this chapter, you should have a solid overview over how to extend linear models, which should serve as a starting point if you want to apply those.

If you don't feel familiar enough with the linear regression model, I recommend reading the [chapter on linear regression models](#linear) first.

Let's recall the formula of a linear regression model:

$$y_{i}=\beta_{0}+\beta_{1}x_{i1}+\ldots+\beta_{p}x_{ip}+\epsilon_{i}$$

The linear regression model assumes that the i-th instance's outcome $y_i$ can be expressed by a weighted sum of its $p$ features $x_{ij}$ with an individual error $\epsilon_i$ which follows a Gaussian distribution.
By forcing the data into this corset-type of formula, we get a lot of interpretability: 
The features don't act in arbitrary ways like in many machine learning methods and the relationship is linear, meaning an increase of a feature by one unit, can be directly translated into an increase/decrease of the predicted outcome.
Knowing a single number per feature, the weight or coefficient, is enough.


But this formula is too restrictive for many real world prediction problems.
In this chapter, we will talk about three problems with the classic linear regression model and how to solve them.
There are much more problems with possibly violated assumptions, but we will focus on those three.

TODO: INSERT FIGURE WITH 3 PLOTS FOR THE PROBLEMS

*Problem*: The target outcome y is not normal distributed given the features. 
*Example*: Let's say I want to predict how many minutes I will ride my bike on a given day, depending on type of day, weather and so on.  
When I use a linear model, it can happen that it predicts negative values, because it assumes a Gaussian distribution which does not stop at 0 minutes.
The same problem arises when we want to predict probabilities.  
*Solution*: Generalized Linear Models (GLMs)

*Problem*: The relation between the features and y is not linear.
*Example*: The world is full of non-linearities are more often than not violated.
For example the effect of the temperature on my desire to go biking might be linear between 0 and 25 degrees Celsius, meaning that an increase from 0 to 1 degree causes the same increase in biking desire than  from 20 to 21. 
But at a certain point, it levels of and might even have a negative effect - I don't like to bike when it is too hot.
*Solutions*: Generalized Additive Models (GAMs); manually transforming features interactions.

*Problem*: The features interact.  
*Example*: The world is full of interactions.  
On average, light rain has a slight negative effect on my biking desire.
But in Summer during rush hour, I welcome rain, because than all those fair-weather riders stay at home and I have the cycling routes for myself!
So there is an interaction between the time and the weather, which can't be explained in an additive model.
*Solution*: Manually adding interactions.


There are many other scenarios in which you will reach the boundaries of the linear regression model, but I don't have the time nor the motivation to cover it all here.
This would quickly become a book within a book.
But I understand that it's very useful to have it in one place, so here are some other common problems or "boundaries" of the linear model and a search term that can be your starting place to solve that problem.
If you scroll to the end of the chapter, there is a list of problem statement + the name of a solution that you can copy-paste into search.



Now, let's start

### Different types of targets: GLMs

The linear regression model assumes that the outcome given the input features follows a Gaussian distribution.
This assumption excludes a lot of cases:
The outcome could also be a category (cancer vs. healthy), or a count (number of children), the time until an event happens (time until a machine breaks down) or a very skewed variable with many high values (household incomes).
As we have seen in the [logistic regression chapter](#logistic), the binary categorization case can be handled when we modify the linear regression model with the logistic function, but at its core is still the weighted sum of the inputs.
This concept of keeping the linear weighted sum at the core, but potentially transforming the sum and modeling the mean of a distribution different from the Gaussian (the Bernoulli distribution in the logistic regression case) is called Generalized Linear Models (GLMs).
GLMs is a flexible class of models, which partially keeps the interpretability of the linear model, like interpretable weights, confidence intervals and so on.

Mathematically the GLM has the linear composition of the features at the core which is linked to the mean of the assumed distribution through a so called link function g, which can be chosen flexibly depending on the type of outcome. 

$$\beta_0+\beta_1{}x_1+\ldots{}\beta_p{}x_p=g(E_Y(y|X))$$

The GLM consists of three components.
Besides the link function g and the linear predictor ($\beta{}X$), we also need a probability distribution from the exponential family, which sets $E_Y$.
Note that I also replaced the y from the linear model with $E_Y(y|X)$. 
For the Gaussian model it's the same, but for example for logistic regression, y is either 0 or 1, and the expected y is the probability for the event.
The exponential family is a set of distributions that all follow the same pattern, meaning they can be formulated in a general form, which involves an exponent, the mean and variance of the distribution and some other parameters.
I won't go into details here, because that's a very big universe of it's own I don't want to open up.
Wikipedia has a nice [list of distributions from the exponential family](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions). 
Any distribution from this list can be chosen as the distribution.
Not all distributions that exist are in the exponential family, but enough for the average statistician/data scientist/machine learner.
The choice of distribution depends on the nature of the outcome you have and is a bit of an art.
Your outcome is some kind of count (e.g. number of children)?
Then the Poisson distribution might be a good choice.
Your outcome is always positive (e.g. time between two events)?
Then the exponential distribution might be a good choice.

Each distribution comes with parameters that describe it.
The Gaussian distribution is parameterized by the mean $\mu$ and the variance $\sigma$.
The mean describes the value that we expect on average and the variance how much the values vary around the mean.
In the linear model the link function to links the weighted sum of the features to the mean of the Gaussian distribution.
The link function for the Gaussian distribution in the classic linear model is simply the identity function.
This concept generalizes under the name GLM to any distribution and arbitrary link functions.
For example if y is a count of something, like the number of coffees someone drinks on a given day, we would use a GLM with a Poisson distribution and the natural logarithm as the link function:

$$x\beta=ln(E_Y(y|x))$$

The logistic regression model is also a GLM, assuming a Bernoulli distribution and the logistic function as link.

$$x\beta=ln\left(\frac{E_Y(y|x)}{1-E_Y(y|x)}\right)=ln\left(\frac{P(y=1|x)}{1-P(y=1|x)}\right)$$

The mean $\mu$ of the binomial distribution, which is used in logistic regression is the probability that y is 1.
So you might also see:

$$x\beta=ln\left(\frac{P(Y=1)}{1-P(Y=1)}\right)$$
And if we solve this equation to have P(Y=1) on one side, we get the classic logistic regression formula:

$$P(y_{i}=1)=\frac{1}{1+exp(-x_i\beta)}$$

You can choose the link function independently from the distribution, but  each distribution also comes with a canonical link function.
The canonical link can be derived mathematically from the distribution.
How to choose a link function:
There is no perfect recipe. 
You take into account your knowledge about the distribution of your target, but also theoretical considerations and how well the model fits your actual data.
For some distributions, like the exponential distribution, the canonical link (the negative inverse) can link the weighted sum to a off-domain value, in this case to negative numbers.
A solution can be to use a noncanonical link.
You can also simply check whether one link function or a another leads to better performance on test data and choose the better one.

*Examples*

I completely made up an artificial dataset about coffee drinking behaviour to highlight the need for GLMs.
Suppose you collected data about yourself each day about your coffee drinking behaviour.
Along with number of cups, you note your current stress level between 1 and 10, how well you slept the night before on a scale from 1 to 10 and if you have to work on that day.
The goal is to predict the number of coffes given the features stress, sleep and work.
I simulated 200 days.
Stress and sleep were drawn uniformly between 1 and 10, work YES/NO was drawn with a 50/50 chance (what a life!). 
The number was then drawn from a Poisson distribution, where the intensity $\lambda$ (which is also the expected value of the distribution) was modeled as a function of the features sleep, stress and work.
The true data generating process is a Poisson distribution. 
You can guess where my story will be going ("Hey let's model this data with a linear model ... Oh it doesn't work ... Let's try a GLM with Poisson distribution ... SURPRISE! Now it works!").
I hope I didn't spoil the story to much for you.

Let's have a look at the target variable, the number of coffees on a given day:

```{r poisson-data}
# simulate data where the normal linear model fails.
n = 200
df = data.frame(stress  = runif(n = n, min = 1, max = 10), 
  sleep = runif(n = n, min = 1, max = 10), 
  work = sample(c("YES", "NO"), size = n, replace = TRUE))
lambda = exp(1* df$stress/10 - 2 * (df$sleep - 5)/10  - 1 * (df$work == "NO"))
df$y = rpois(lambda = lambda, n = n)

tab = data.frame(table(df$y))


knitr::kable(tab, col.names = c("#coffees", "#days"))

```

On `r tab[1,2]` days, you have no coffee at all and at the most extreme day you have `r tab[nrow(tab),1]`.
Let's naively use a linear model to model this dataset, using the features sleep level, stress level and work Yes/No to predict the number of coffees.
What can go wrong when we falsely assume a Gaussan distribution?
A wrong assumption can invalidate the estimates, especially the confidence intervals of the weights and all the interpretation we do about them. 
Another, more obvious problem is that the predictions doesn't match the "allowed" domain of the true outcome.

```{r failing-linear-model, fig.cap = "Predicted number of coffees dependent on stress, sleep and working day. The linear model also predicts negative values."}
mod.gaus = glm(y ~ ., data = df, x = TRUE)
pred.gauss = data.frame(pred = predict(mod.gaus), actual = df$y)
ggplot(pred.gauss) + 
  geom_histogram(aes(x = pred)) + 
  scale_x_continuous("Predicted number of coffees") + 
  scale_y_continuous("Frequency")
```

Negative amount of coffee?
That's awkward and a failure of the linear model in respecting the domain of the count outcome.
This problem can be solved using Generalized Linear Models (GLMs).
There are two things we can change:
The link function and the distribution assumption.
We could keep the Gaussian assumption, but don't use the identity function as link, but something that always maps the weighted sum to a positve value, like the log-link (the inverse is the exp() function, which always yields positive values).
The better idea here is to choose a distribution that matches the data generating process and an appropriate link function. 
Since the outcome is a count, the Poisson distribution is a natural choice, along with the log link function (in our case, the data was even generated using the Poisson distribution, so this GLM is the perfect choice).
Fitting a Poisson model yields the following predicted values:

```{r linear-model-positive, fig.cap = "Predicted number of coffees dependent on stress, sleep and working day. The GLM with Poisson assumption and log link is an appropriate model for this dataset."}
mod.pois = glm(y ~ ., data = df, x = TRUE, family = poisson(link = "log"))
pred.pois = data.frame(pred = predict(mod.pois, type = "response"), actual = df$y)
ggplot(pred.pois)  + 
  geom_histogram(aes(x = pred))+ 
  scale_x_continuous("Predicted number of coffees") + 
  scale_y_continuous("Frequency")
```

*Interpretation of GLM weights*

The assumed distribution and the link function dictate the interpretation of the features.
In the coffee example I assumed a Poisson distribution and used a log link.
This results in the following relationship between the features and the expected outcome:

$$\beta_0+\beta_{stress}x_{stress}+\beta_{sleep}x_{sleep}+\beta_{work}x_{work}=ln(E(coffees|stress,sleep,work))$$

For the interpretation of the parameters we have to invert the link function, so that we can interpret the effect of the features on the expected outcome, not on the logarithm of the expected outcome. 

$$exp(\beta_0+\beta_{stress}x_{stress}+\beta_{sleep}x_{sleep}+\beta_{work}x_{work})=E(coffees|stress,sleep,work)$$
Last ingredient for showing the interpretation of the weights: The actual weights for the toy example.
The following table shows the weights and the exponent of the weights together with the 95% confidence interval:
```{r poisson-model-params}
cc = data.frame(summary(mod.pois)$coefficients)
cc = cc[,c("Estimate", "Std..Error")]
colnames(cc) = c("beta", 'var.beta')
cc$exp.beta = exp(cc[, 'beta'])
cc = cc[c("beta", "exp.beta")]
cc = cbind(cc, exp(confint(mod.pois)))
cc$ci = sprintf("%.2f [%.2f, %.2f]", cc$exp.beta, cc$`2.5 %`, cc$`97.5 %`)
kable(cc[c("beta", "ci")], col.names = c("weight", "exp(weight) [2.5%, 97.5%]"), digits = 2)
```

Let's interpret the weights from the made-up coffee prediction example.
Increasing the stress by one point multiplicatively increases the expected number of coffees by a factor of `r round(cc["stress", "exp.beta"], 2)`. 
Increasing the sleep by one point multiplicatively decreases the expected number of coffees by a factor of `r round(cc["sleep", "exp.beta"], 2)`.
The predicted number of coffees is lower by a factor of `r round(cc["weekendYES", "exp.beta"], 2)` on a non-working day compared to a working day.

In this section you have learned a bit about Generalized Linear Models which are useful when the target does not follow a Gaussian distribution. 
Next we will look into the case when features interact with each other, so the relationship is not covered with the default additive model.

### Adding interactions

The classic linear model assumes that feature effects can just be added independently from each other, but in real life, the features often have some kind of interaction effect on the target.
Say for predicting the [number of bikes](#bike-data) that will be rented, there might be an interaction between the temperature and whether it's a working day or not.
Maybe when people have to work, the temperature doesn't affect the number of rented bikes much, because the people will ride the bike to work no matter what.
On free days many people bike for pleasure, but only if it warm enough.
In this case we might suspect an interaction between temperature and working day on the rented bikes.
The linear regression model doesn't consider interactions by itself.

How can we make the linear model include interactions?
You can add a column to the feature matrix that represents the interaction between the features and fit the model as usual.
In the working day and temperature example, we would add a new feature that is zero whenever it's a weekend, otherwise the value of the temperature feature.
This assumes that working day is the reference category.
Let's say our data looks like this:

```{r data-frame}
x = data.frame(workingday = c("Yes", "No", "No", "Yes"), temp = c(25, 12, 30, 5))
knitr::kable(x)
```

The following table shows what the model receives when we don't specify interactions.


```{r data-frame-lm-no-interaction}
mod = lm(1:4 ~ ., data = x)
knitr::kable(data.frame(model.matrix(mod)))
```

The first column is the intercept term.
The categorical feature is coded (2nd column), with 0 for the reference category and 1 for the other.
The third column contains the temperature.

When we want the linear model to include the interaction between temperature and the working day feature, we have to give it the following matrix with the additional column for the interaction:

```{r data-frame-lm}
mod = lm(1:4 ~ workingday * temp, data = x)
knitr::kable(data.frame(model.matrix(mod)))
```

The new column "workingdayYes" column captures the interaction between the features workingday and temperature.
This new feature is zero when the workingday feature has the reference category "No", otherwise it takes on the value of the temp feature.
With this type of coding, the linear model can learn a different linear effect of the temperature for both types of days.
This is the interaction effect between the two features.
Without interaction, the combined effect of a categorical and a numerical feature can be described by a line which is shifted vertically for the different categories.
When we include interactions, we allow the effect of the numerical features (the slope) have a different value in each category.

For two categorical features, the process is the same.
We create additional features which represents combinations of categories. 
So this artificial data: 

```{r data-frame-lm-cat}
x = data.frame(workingday = c("Yes", "No", "No", "Yes"), weekday = c("Mon", "Sun", "Sat", "Mon"))
knitr::kable(x)
```

Becomes this: 

```{r data-frame-lm-cat2}
mod = lm(1:4 ~ workingday * weekday, data = x)
knitr::kable(data.frame(model.matrix(mod)))
```

For any category of both features (excluding the reference categories), we create a new feature column that is 1 if both features have a certain category, otherwise 0.

For two numerical features, the interaction column is even easier to construct: 
We simply multiply both numerical features.
There are lots of ways and types of interactions that can be added (nested, 2way, 3way, ...). 
There are approaches to detect and add interaction terms automatically. 
One is presented in this book called [RuleFit](#rulefit), which first mines interaction terms and then estimates a linear regression model with this new data.

### Example

Let's return to the bike rental prediction task that we already modeled with a linear model in the [linear model chapter](#linear).
This time, we additionally include an interaction between the temperature and working day.
This results in the following estimated weights and confidence intervals.

```{r example-lm-interaction}
data(bike)
X = bike[bike.features.of.interest]
y = bike[,'cnt']
dat = cbind(X, y)

mod = lm(y ~ . + temp * workingday, data = dat, x = TRUE)
lm_summary = summary(mod)$coefficients

lm_summary_print = lm_summary
rownames(lm_summary_print) = pretty_rownames(rownames(lm_summary_print))
confint(mod)

kable(cbind(lm_summary_print[,c('Estimate', 'Std. Error')], confint(mod)), digits = 1, col.names = c('Weight', 'Std. Error', "2.5%","97.5%"))
```

The additional interaction effect is negative (`r round(lm_summary_print['workingdayWORKING DAY:temp','Estimate'], 1)`) and significantly different from zero, as indicated by the 95% confidence interval boundaries.
The interaction term changes how we interpret the terms of the involved features.
Does the temperature have a negative effect on a working day?
The answer is no, even though the weights suggest it to the untrained eye.
We can't interpret the "workingdayWORKING DAY:temp" interaction term directly, since the interpretation would be:
"All feature values being equal, increasing the additional temperature effect for the working day, decreases the predicted number of bike rents."
But this comes only on top of the main effect of the temperature, which we would have to increase at the same time.
Let's say we have a working day, and want to know what would happen if the temperature today would be 1 degree warmer. 
Then we have to add both the temp and "workindayWORKING DAY:temp" weights to get how much the estimate increases.

It's easier to understand the interaction visually. 
By introducing an interaction term between a categorical and a numerical feature, we now have two slopes for the temperature instead of one.
The temperature slope for the reference category ('NO WORKING DAY') can be read directly from the table (`r round(lm_summary_print['temp','Estimate'], 1)`).
The temperature slope for the category 'WORKING DAY' is the sum of both temperature weights (`r round(lm_summary_print['temp','Estimate'], 1)` + `r round(lm_summary_print['workingdayWORKING DAY:temp','Estimate'], 1)` = `r round(lm_summary_print['temp','Estimate'], 1) + round(lm_summary_print['workingdayWORKING DAY:temp','Estimate'], 1)`).

```{r interaction-plot, fig.cap = "The effect (including interaction) of temperature and working day on the predicted number of bike rental for a linear model. Effectively we get two slopes for the temperature one for each category of the  working day feature. Without the interaction it would be a single slope for the temperature, only shifted by the different means of no working vs. working day."}
jtools::interact_plot(mod, pred = "temp", modx = "workingday")
```

### Non-linear effects: GAMs and more.

What if we suspect or know that one of the features has a non-linear effect on the prediction.
For example, the temperature effect might be linear for lower temperatures up to a certain degree, but then level off.
The linear regression model would not capture that effect on it's own.

We have multiple solutions for allowing non-linear effects:
- Simple transformation of the feature
- Categorize the feature
- Use splines, which is a new model class called Generalized Additive Models (GAMs).

Before I explain each of them, will start with an example right away.
We take the bike rental dataset and try to predict the number of rented bikes using only the temperature feature.
The following figure shows the estimated slope without doing anything and with either transforming the feautre (the square root), cutting it into intervals or using splines (also called GAM then).

```{r non-linear-effects, fig.cap = "Predicting the number of rented bikes given the temperature. Using a linear model (top left) doesn't capture the flattening of the temperature effect as suggested by the point cloud and overestimates the number of bikes at low temperatures. One solution is to transform the feature (top right), to categorize it (bottom left), which is usually a bad decision or to use Generalized Additive Models, which can automatically fit a smooth curve for temperature (bottom right), which is most often the best solution."}
mod.simpel = lm(cnt ~ temp, data = bike)
bike.plt = bike
bike.plt$pred.lm = predict(mod.simpel)

bike.plt$sqrt.temp = log(bike$temp + 10)
mod.simpel = lm(cnt ~ sqrt.temp, data = bike.plt)
bike.plt$pred.sqrt = predict(mod.simpel)

bike.plt$cat.temp = cut(bike$temp, breaks = seq(from = min(bike$temp), to = max(bike$temp), length.out = 10), include.lowest = TRUE)
mod.simpel = lm(cnt ~ cat.temp, data = bike.plt)
bike.plt$pred.cat = predict(mod.simpel)

library(mgcv)
mod.gam = gam(cnt ~ s(temp), data = bike)
bike.plt$pred.gam = predict(mod.gam)


bike.plt = data.table::melt(bike.plt[c("pred.lm", "pred.sqrt", "pred.cat", "pred.gam")])
bike.plt$temp = rep(bike$temp, times = 4)
bike.plt$cnt = rep(bike$cnt, times = 4)

model.type = c(pred.lm = "Linear model", 
  pred.sqrt = "Linear model with sqrt(temp)", 
  pred.cat = "Linear model with categorized temp", 
  pred.gam = "GAM")

ggplot(bike.plt) + 
  geom_point(aes(x = temp, y = cnt), size = 1 , alpha = 0.3)  + 
  geom_line(aes(x = temp, y = value), size = 1.2, color = "blue") + 
  facet_wrap("variable", labeller = labeller(variable = model.type)) + 
  scale_x_continuous("Temperature (temp)") + 
  scale_y_continuous("(Predicted) Number of rented bikes")
```


You can be very creative when transforming your feature:
Very common is taking the logarithm of the feature. 
Maybe the logarithm of the temperature has a linear effect on the outcome?
Meaning that each 10x increase of the temperature has the same linear effect on the number of bikes, 
so going from 1 degree Celsius to 10 degree celsius has the same effect as going from 0.1 to 1 (sounds wrong).

Another solution: We discretize the feature, turn it into many categorical features.
So the temperature would have values [-10, -5), [-5, 0), ... you get it.
Then the linear model would estimate a step function, because each level get's it's own estimate.
The problem with this approach is that you need a lot more data, are more likely to overfit and have no clue what meaningful way to discretize it.
Also, you loose information, like going from an anlogue picture to a pixel image (which is definitely not 4K). 
I would only use discretiziation if you have a very strong case for it:
For example to make the model comparable to another study or so.

The more sophisticated solution: Use splines.
Splines are actually similar to discretization, because you replace the feature by many new columns with which you allow to estimate a non-linear effect on the outcome.

When you use splines, then most people will call their linear model generalized additive models (GAMs), which are - well - GLMs with non-linear effects.


```{r splines, fig.cap = "Instead of a single temperature feature, we use 10 spline functions. Each temperature value gets mapped to (here) 9 spline values. If an instance has a temperature of 30 C, then the value for the first spline feature is -1, for the second 0.5 ... and for the 9th it's 2. The we feed the the GAM with those features."}
mm = model.matrix(mod.gam)
mm2 = data.table::melt(mm)
mm2 = mm2[mm2$Var2 != "(Intercept)",]

ggplot(mm2) + geom_line(aes(x = rep(bike$temp, times = 9), y = value)) + facet_wrap("Var2") + 
  scale_x_continuous("Temperature") + 
  scale_y_continuous("Value of spline feature")
```

But don't we have the same problem as with categorizing the feature:
We get too many parameters and might overfit the data.
GAMs introduce some penalty term for the weights, so that they can't change to much away from zero. 
This effectively reduces the flexibility of the splines approach (how much they can overfit). 
How flexible the spline curve is usually tuned by some smoothness parameter, which is usually tuned using cross-validation.


The interpretation of splines is more visual, but still in line with other features.
To interpret the effect, you can first look at the curve, which will show you if an effect is monotone, and what shape it has.
To understand how much it contributes to a prediction at a given value, you look at the x-axis for that given value, for example 25 degrees C temperature and read the y-axis value, which will be added to the weighted sum of features.


### Advantages

- Has lots of theory behind it for calculating things like confidence intervals and testing hypothesis.
- Very rich literature and experience with these models
- Accepted in many fields as the standard for testing hypothesis.
- Understanding linear models helps you understanding statistical hypothesis testing.
Replaces many tests, like analysis of variance stuff, t-tests and so on.
- Can be made sparse by using regularization, which favors estimation of weights with 0.
- Really good interfaces (for example in R) to describe how the relationship between input and output should be handled. 
Can be written as formula.
- GAM makes very smooth non-linear effects, which usually also have derivatives, so you can more automatically quantify them. 
Other models, like tree-based methods (random forest) have often ugly effects
- What's the source of machine learning opaqueness compared to a linear model? I would say it's that the feature are treated in a non-linear fashion (meaning you need more than a single weight to describe the effect) and also you have lots of interactions in the model.
If we assume that the linear regression model is the easiest interpretable model, but not so flexible and therefore often underfitting reality, than the tools in this chapter provide a good way to transition towards more flexibility, while keeping some of the interpretability.
- Lots of theory to deal with repeated measurements.
- Frameworks for survival analysis
- Heterocedasticity can be solved by using more robut estimators
- 

### Disadvantages

- Each step makes the interpretation more complex. 
Different link function make the interpretation more complicated;
Interactions make the interpretation more complicated;
Non-linear feature effects are either not very intuitive (like log transform) or cannot be summarized with single number any longer.
- When features are transformed, coefficients are typically not interpretable at all.
- The world of linear regression models and their extensions can be quite overwhelming
- There are attempts to automate these steps, but mainly it's manual.
- For interactions: the estimated coefficient (main effect) of each feature only caries meaning of an unconditional marginal effect when the effect of the interaction is zero, which is neve
- Relies on assumptions for lots of the interpretaiton stuff
- 


### Software

Any major programming language for stats has all those. The R language, SPSS, SAS, Python, ...

### More extensions

A list of problem statement + the name of a solution that you can copy-paste into search.

- My data is not iid (independent and identically distributed).  
For example you might have repeated measurements from the same patient.  
Search for: *mixed models* or *generalized estimation equations*.
- My model has heteroscedastic errors.  
For example when predicting the value of a house, the errors are usually higher for expensive houses. 
Search for: *robust regression*.  
- I have outliers that severely influence my model.  
Search for: *robust regression*.
- I want to predict time to event.  
Time to event data usually comes with censored measurements, meaning for some instances we didn't have enough time to observe the event.
For example when we a company wants to predict the failure of machines, but has only data for two years.
Some machines will still be intact after two years, but might break later.  
Search for: *parametric survival models*, *cox regression*, *survival analysis*.
- My outcome to predict is a category.   
If you have only two categories use a [logistic regression model](#logistic), which will model the probability for the categories.  
If more categories, search for: *multinomial regression*.
Both regression models are GLMs.
- My outcome is ordered categories.  
Search for: *proportional odds model*.
- My outcome is a count (like number of children in a family).
Search for: *poisson regression*.  
This model is a GLM as well.
Additionally, if you have the problem that the count value of 0 is very frequent:  
Search for: *zero-inflated poisson regression*, *hurdle model*.
- I am not sure which features to include to do proper inference.  
For example I want to know the effect of a drug on some outcome.
The drug directly affects also some blood value, but this blood value also affects the outcome. 
Should I include the blood value into the regression model?
(Answer is no btw., because it will hide the effect of the drug.)  
Search for: *causal inference*, *mediation analysis*.
- I have missing data.  
Search for:  *multiple imputation*.
- I want to integrate prior knowledge into my models.  
Search for: *bayesian inference*.
- I am feeling a bit down lately.  
Search for: *"Amazon Alexa Gone Wild!!! Full version from beginning to end"*
