```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

## Linear model 2.0: GLMs, GAMs and more {#extend-lm}

Linear regression models handle the relationship between features and outcome with a weighted sum.
The bad news is (well, not really news), that they are very restrictive in terms of how the output may look like, not handling interactions between features (by default), only allowing linear relationships.
The good news is that the statistics community has come up with a vast array of modifications that make the linear regression model a powerful swiss knife.


If you don't feel familiar enough with the linear regression model, I recommend reading the [chapter on linear regression models](#linear) first.

Let's recall the formula of a linear regression model:

$$y_{i}=\beta_{0}+\beta_{1}x_{i1}+\ldots+\beta_{p}x_{ip}+\epsilon_{i}$$

The linear regression model assumes that the i-th instance's outcome $y_i$ can be expressed by a weighted sum of its $p$ features $x_{ij}$ with an individual error $\epsilon_i$ which follows a Gaussian distribution.
By forcing the data into this corset-type of formula, we get a lot of interpretability: 
The features don't act in arbitrary ways like in many machine learning methods and the relationship is linear, meaning an increase of a feature by one unit, can be directly translated into an increase/decrease of the predicted outcome.
Knowing a single number per feature, the weight or coefficient, is enough.


But this formula is too restrictive for many real world prediction problems.
In this chapter, we will talk about three problems with the classic linear regression model and how to solve them.
There are much more problems with possibly violated assumptions, but we will focus on those three.

*Problem*: The target outcome y is not normal distributed given the features. 
*Example*: Let's say I want to predict how many minutes I will ride my bike on a given day, depending on type of day, weather and so on.  
When I use a linear model, it can happen that it predicts negative values, because it assumes a Gaussian distribution which does not stop at 0 minutes.
The same problem arises when we want to predict probabilities.  
*Solution*: Generalized Linear Models (GLMs)

*Problem*: The relation between the features and y is not linear.
*Example*: The world is full of non-linearities are more often than not violated.
For example the effect of the temperature on my desire to go biking might be linear between 0 and 25 degrees Celsius, meaning that an increase from 0 to 1 degree causes the same increase in biking desire than  from 20 to 21. 
But at a certain point, it levels of and might even have a negative effect - I don't like to bike when it is too hot.
*Solutions*: Generalized Additive Models (GAMs); manually transforming features interactions.

*Problem*: The features interact.  
*Example*: The world is full of interactions.  
On average, light rain has a slight negative effect on my biking desire.
But in Summer during rush hour, I welcome rain, because than all those fair-weather riders stay at home and I have the cycling routes for myself!
So there is an interaction between the time and the weather, which can't be explained in an additive model.
*Solution*: Manually adding interactions.


There are many other scenarios in which you will reach the boundaries of the linear regression model, but I don't have the time nor the motivation to cover it all here.
This would quickly become a book within a book.
But I understand that it's very useful to have it in one place, so here are some other common problems or "boundaries" of the linear model and a search term that can be your starting place to solve that problem.
If you scroll to the end of the chapter, there is a list of problem statement + the name of a solution that you can copy-paste into search.



Now, let's start

### Different types of targets: GLMs

The linear regression model assumes that the outcome given the input features follows a Gaussian distribution.
This assumption excludes a lot of cases:
The outcome could also be a category (cancer vs. healthy), or a count (number of children), the time until an event happens (time until a machine breaks down) or a very skewed variable with many high values (household incomes).
As we have seen in the [logistic regression chapter](#logistic), the binary categorization case can be handled when we modify the linear regression model with the logistic function, but at its core is still the weighted sum of the inputs.
This concept of keeping the linear weighted sum at the core, but potentially transforming the sum and modeling the mean of a distribution different from the Gaussian (the Bernoulli distribution in the logistic regression case) is called Generalized Linear Models (GLMs).
GLMs is a flexible class of models, which partially keeps the interpretability of the linear model, like interpretable weights, confidence intervals and so on.

Mathematically the GLM has the linear composition of the features at the core which is linked to the mean of the assumed distribution through a so called link function g, which can be chosen flexibly depending on the type of outcome. 

$$\beta_0+\beta_1{}x_1+\ldots{}\beta_p{}x_p=g(E_Y(y|X))$$

The GLM consists of three components.
Besides the link function g and the linear predictor ($\beta{}X$), we also need a probability distribution from the exponential family, which sets $E_Y$.
The exponential family is a set of distributions that all follow the same pattern, meaning they can be formulated in a general form, which involves an exponent, the mean and variance of the distribution and some other parameters.
I won't go into details here, because that's a very big universe of it's own I don't want to open up.
Wikipedia has a nice [list of distributions from the exponential family](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions). 
Any distribution from this list can be chosen as the distribution.
Not all distributions that exist are in the exponential family, but enough for the average statistician/data scientist/machine learner.
The choice of distribution depends on the nature of the outcome you have and is a bit of an art.
Your outcome is some kind of count (e.g. number of children)?
Then the Poisson distribution might be a good choice.
Your outcome is always positive (e.g. time between two events)?
Then the Exponential distribution might be a good choice.

TODO: CONTINUE HERE

Each distribution comes with parameters that describe it.
The Gaussian distribution is parameterized by the mean $\mu$ and the variance $\sigma$.
The mean describes the value that we expect on average and the variance how much the values vary around the mean.
In the linear model the link function to links the weighted sum of the features to the mean of the Gaussian distribution.
The link function for the Gaussian distribution in the classic linear model is simply the identity function.
This concept generalizes under the name GLM to any distribution and arbitrary link functions.
For example if y is a count of something, like the number of coffees someone drinks on a given day, we would use a GLM with a Poisson distribution and the natural logarithm as the link function:

$$x\beta=ln(E_Y(y|x))$$

The logistic regression model is also a GLM, assuming a Bernoulli distribution and the logistic function as link.

$$x\beta=ln\left(\frac{E_Y(y|x)}{1-E_Y(y|x)}\right)=ln\left(\frac{P(y=1|x)}{1-P(y=1|x)}\right)$$
The mean $\mu$ of the binomial distribution, which is used in logistic regression is the probability that y is 1.
So you might also see:

$$x\beta=ln\left(\frac{P(Y=1)}{1-P(Y=1)}\right)$$
And if we solve this equation to have P(Y=1) on one side, we get the classic logistic regression formula:

$$P(y_{i}=1)=\frac{1}{1+exp(-x_i\beta)}$$

You can choose the link function independently from the distribution, but  each distribution also comes with a canonical link function.
The canonical link can be derived mathematically from the distribution.
How to choose a link function:
There is no perfect recipe. 
You take into account your knowledge about the distribution of your target, but also theoretical considerations and how well the model fits your actual data.
For some distributions, like the exponential distribution, the canonical link (the negative inverse) can link the weighted sum to a off-domain value, in this case to negative numbers.
A solution can be to use a noncanonical link.
You can also simply check whether one link function or a another leads to better performance on test data and choose the better one.

TODO: CONTINUE HERE

*Examples*

How many coffees did you drink today?
Features:
- Stress level between 1 and 10
- How well did you sleep last night on a scala from 1 to 10?
- Is it weekend?

(Writing this as I am sipping my second coffee at 11AM).

```{r poisson-data}
# simulate data where the normal linear model fails.
n = 200
df = data.frame(stress  = runif(n = n, min = 1, max = 10), 
  sleep = runif(n = n, min = 1, max = 10), 
  weekend = sample(c("YES", "NO"), size = n, replace = TRUE))
lambda = exp(1* df$stress/10 - 2 * (df$sleep - 5)/10  - 1 * (df$weekend == "YES"))
df$y = rpois(lambda = lambda, n = n)


kable(table(df$y))

```


```{r failing-linear-model}
mod.gaus = glm(y ~ ., data = df, x = TRUE)
pred.gauss = data.frame(pred = predict(mod.gaus), actual = df$y)
ggplot(pred.gauss)  + geom_histogram(aes(x = pred))
```

Predicting negative values. well. That's awkward.


Better idea: Use poisson distribution with exp link function.

```{r linear-model-positive}
mod.pois = glm(y ~ ., data = df, x = TRUE, family = poisson(link = "log"))
pred.pois = data.frame(pred = predict(mod.pois, type = "response"), actual = df$y)
ggplot(pred.pois)  + geom_histogram(aes(x = pred))
```


Interpretation of parameters


The interpretation depends on the chosen distribution and how we link the weighted sum of features to the expected outcome.
In this case, the assumed distribution of the outcome given the features is a poisson distribution.
The link function between the weighted sum and the number of expected coffees is:

$$\beta_0+\beta_{stress}x_{stress}+\beta_{sleep}x_{sleep}+\beta_{like}x_{like}=ln(E(coffees|stress,sleep,like))$$

A Poisson distribution has one parameter: $\lambda$, the intensity at which 'events' happen (here the number of consumed coffees)
A high value of $\lambda$ means that we expect a high count.
The expected number of counts is $\lambda$.
Ok, how do we connect that to the interpretation of the $\beta$'s?

Here it comes:
By increasing an x by one, the expected outcome increases multiplicatively by $exp(\beta)$.
The exp comes from inversing the link function:

$$exp(\beta_0+\beta_{stress}x_{stress}+\beta_{sleep}x_{sleep}+\beta_{like}x_{like})=E(coffees|stress,sleep,like)$$
Let's have a look at the weights and the exp(weights) and their variance:
```{r poisson-model-params}
cc = data.frame(summary(mod.pois)$coefficients)
cc = cc[,c("Estimate", "Std..Error")]
colnames(cc) = c("beta", 'var.beta')
cc$exp.beta = exp(cc[, 'beta'])
cc = cc[c("beta", "exp.beta")]
cc = cbind(cc, exp(confint(mod.pois)))
cc$ci = sprintf("%.2f [%.2f, %.2f]", cc$exp.beta, cc$`2.5 %`, cc$`97.5 %`)
kable(cc[c("beta", "ci")], col.names = c("weight", "exp(weight) [2.5%, 97.5%]"), digits = 2)
```

Let's interpret the parameters in the made-up coffee prediction example.
Increasing the stress by one point increases the expected number of coffees by a factor of `r round(cc["stress", "exp.beta"], 2)`. 
Increasing the sleep by one point decreases the expected number of coffees by a factor of `r round(cc["sleep", "exp.beta"], 2)`.
Compared to a weekday, on a weekend the predicted number of cofees is lower by a factor of `r round(cc["weekendYES", "exp.beta"], 2)`.

In the next part, we will talk about the solution when the features interact with each other.

### Adding interactions

What if the features interact with each other?
Say for predicting the number of bikes that will be rented, there might be an interaction betweeen the temperature and if it's a working day or not.
Maybe when people have to work, the temperature does not affect the number of rented bikes, because only the tough people ride, because they have to get to work.
On the free days, many people decide more based on the weather. 
So we might suspect an interaction between the two.
By default, the linear model doesn't model interactions explicitly.
It might actually suck pretty hard when interactions are involved, because the effects are not independent from each other any more.

What can you do about it?
You can simply add a term to the features that represents the interaction between features.
In the working day x temperature example, we would add a new features, that is zero whenever it's a weekend, otherwise the temperature.
The working day is then the reference category.

A toy example.
Let's say our features look like this:
```{r data-frame}
x = data.frame(workingday = c("Yes", "No", "No", "Yes"), temp = c(25, 12, 30, 5))
knitr::kable(x)
```
What the model receives, when we want both the linear effects and the additional interaction effect:
```{r data-frame-lm}
mod = lm(1:4 ~ workingday * temp, data = x)
knitr::kable(data.frame(model.matrix(mod)))
```

The first term is the intercept term which usually gets added automatically in the most programs for linear regression models.
The second column is the workingday feature, which is now only the indicator for the feature value Yes, since No is assumed as the reference category.
Where the workingdayY column is 1, the original category was "Y".
The column temp is the same as before.
New is the column "workingdayYes" which captures the interaction between the features workingday and temperature.
This new feature is zero when the workingday feature has the reference category "No", otherwise it takes on the value of the temp feature.
With this type of coding, the linear model can learn a different linear effect of the temperature for both types of days.
This is the interaction effect between the two features.
For two categorical features, the process is the same:
We create additional features which represents combinations of categories. 
So this artificial data: 

```{r data-frame-lm-cat}
x = data.frame(workingday = c("Yes", "No", "No", "Yes"), wday = c("Mon", "Sun", "Sat", "Mon"))
knitr::kable(x)
```

Becomes this: 

```{r data-frame-lm-cat2}
mod = lm(1:4 ~ workingday * wday, data = x)
knitr::kable(data.frame(model.matrix(mod)))
```
For any category of both features (excluding the reference categories), we create a new feature column that is 1 if both features have a certain category, otherwise 0.
For two numerical features, the interaction column is even easier to construct: 
We simply multiply both numerical features.
There are lots of ways and types of interactions that can be added (nested, 2way, 3way, ...). 
There are approaches to detect and add interaction terms automatically. 
One is presented in this book called [RuleFit](#rulefit), which first mines interaction terms and then estimates a linear regression model with this new data.

### Example


```{r example-lm-interaction}
data(bike)
X = bike[bike.features.of.interest]
y = bike[,'cnt']
dat = cbind(X, y)

mod = lm(y ~ . + temp * workingday, data = dat, x = TRUE)
lm_summary = summary(mod)$coefficients

lm_summary_print = lm_summary
rownames(lm_summary_print) = pretty_rownames(rownames(lm_summary_print))
confint(mod)

kable(cbind(lm_summary_print[,c('Estimate', 'Std. Error')], confint(mod)), digits = 1, col.names = c('Weight estimate', 'Std. Error', "2.5%","97.5%"))
```

The interaction term drastically changes how we interpret the terms of the involved features.
Does the temperature have a negative effect on a working day?
The answer is no, even though the weights suggest it in the first moment.
We can't interpret the "workingdayWORKING DAY:temp" interaction term directly, since the interpretation would be
All feature values being equal, increasing the additional temperature effect for the working day, decreases the predicted number of bike rents. 
But this comes only on top of the effect of the temperature feature, which we would have to increase at the same time.
Let's say we have a working day, and want to know what would happen if the temperature today would be 1 degree warmer. 
Then we have to add both the temp and "workindayWORKING DAY:temp" weights to get how much the estimate increases.

It's easier to understand visually. 
By introducing an interaction term between a categorical and a numerical feature, we now have two slopes for the temperature instead of one.
```{r interaction-plot, fig.cap = "The effect (including interaction) of temperature and working day on the predicted number of bike rental for a linear model. Effectively we get two slopes for the temperature, by the factor working day or not. Without the interaction it would be a single slope for the temperature, only shifted by the different means of non-working vs. working day."}
jtools::interact_plot(mod, pred = "temp", modx = "workingday")
```


Problem: The interpretation of the weights is always given that all else stays equal. 
This is problematic with interaction term, since they consist of two other features that are in the model.
It is not meaningful to interpret an increase of the interaction feature, but leave the main feature as is.



### Non-linear effects: GAMs and more.

What if we suspect or know that one of the features has a non-linear effect on the prediction.
For example, the temperature effect might be linear for lower temperatures up to a certain degree, but then level off.
The linear regression model would not capture that effect on it's own.

We have multiple solutions for allowing non-linear effects:
- Simple transformation of the feature
- Categorize the feature
- Use splines, which is a new model class called Generalized Additive Models (GAMs).

Before I explain each of them, will start with an example right away.
We take the bike rental dataset and try to predict the number of rented bikes using only the temperature feature.
The following figure shows the estimated slope without doing anything and with either transforming the feautre (the square root), cutting it into intervals or using splines (also called GAM then).

```{r non-linear-effects, fig.cap = "Predicting the number of rented bikes given the temperature. Using a linear model (top left) doesn't capture the flattening of the temperature effect as suggested by the point cloud and overestimates the number of bikes at low temperatures. One solution is to transform the feature (top right), to categorize it (bottom left), which is usually a bad decision or to use Generalized Additive Models, which can automatically fit a smooth curve for temperature (bottom right), which is most often the best solution."}
mod.simpel = lm(cnt ~ temp, data = bike)
bike.plt = bike
bike.plt$pred.lm = predict(mod.simpel)

bike.plt$sqrt.temp = log(bike$temp + 10)
mod.simpel = lm(cnt ~ sqrt.temp, data = bike.plt)
bike.plt$pred.sqrt = predict(mod.simpel)

bike.plt$cat.temp = cut(bike$temp, breaks = seq(from = min(bike$temp), to = max(bike$temp), length.out = 10), include.lowest = TRUE)
mod.simpel = lm(cnt ~ cat.temp, data = bike.plt)
bike.plt$pred.cat = predict(mod.simpel)

library(mgcv)
mod.gam = gam(cnt ~ s(temp), data = bike)
bike.plt$pred.gam = predict(mod.gam)


bike.plt = data.table::melt(bike.plt[c("pred.lm", "pred.sqrt", "pred.cat", "pred.gam")])
bike.plt$temp = rep(bike$temp, times = 4)
bike.plt$cnt = rep(bike$cnt, times = 4)

model.type = c(pred.lm = "Linear model", 
  pred.sqrt = "Linear model with sqrt(temp)", 
  pred.cat = "Linear model with categorized temp", 
  pred.gam = "GAM")

ggplot(bike.plt) + 
  geom_point(aes(x = temp, y = cnt), size = 1 , alpha = 0.3)  + 
  geom_line(aes(x = temp, y = value), size = 1.2, color = "blue") + 
  facet_wrap("variable", labeller = labeller(variable = model.type)) + 
  scale_x_continuous("Temperature (temp)") + 
  scale_y_continuous("(Predicted) Number of rented bikes")
```


You can be very creative when transforming your feature:
Very common is taking the logarithm of the feature. 
Maybe the logarithm of the temperature has a linear effect on the outcome?
Meaning that each 10x increase of the temperature has the same linear effect on the number of bikes, 
so going from 1 degree Celsius to 10 degree celsius has the same effect as going from 0.1 to 1 (sounds wrong).

Another solution: We discretize the feature, turn it into many categorical features.
So the temperature would have values [-10, -5), [-5, 0), ... you get it.
Then the linear model would estimate a step function, because each level get's it's own estimate.
The problem with this approach is that you need a lot more data, are more likely to overfit and have no clue what meaningful way to discretize it.
Also, you loose information, like going from an anlogue picture to a pixel image (which is definitely not 4K). 
I would only use discretiziation if you have a very strong case for it:
For example to make the model comparable to another study or so.

The more sophisticated solution: Use splines.
Splines are actually similar to discretization, because you replace the feature by many new columns with which you allow to estimate a non-linear effect on the outcome.

When you use splines, then most people will call their linear model generalized additive models (GAMs), which are - well - GLMs with non-linear effects.


```{r splines, fig.cap = "Instead of a single temperature feature, we use 10 spline functions. Each temperature value gets mapped to (here) 9 spline values. If an instance has a temperature of 30 C, then the value for the first spline feature is -1, for the second 0.5 ... and for the 9th it's 2. The we feed the the GAM with those features."}
mm = model.matrix(mod.gam)
mm2 = data.table::melt(mm)
mm2 = mm2[mm2$Var2 != "(Intercept)",]

ggplot(mm2) + geom_line(aes(x = rep(bike$temp, times = 9), y = value)) + facet_wrap("Var2") + 
  scale_x_continuous("Temperature") + 
  scale_y_continuous("Value of spline feature")
```

But don't we have the same problem as with categorizing the feature:
We get too many parameters and might overfit the data.
GAMs introduce some penalty term for the weights, so that they can't change to much away from zero. 
This effectively reduces the flexibility of the splines approach (how much they can overfit). 
How flexible the spline curve is usually tuned by some smoothness parameter, which is usually tuned using cross-validation.


The interpretation of splines is more visual, but still in line with other features.
To interpret the effect, you can first look at the curve, which will show you if an effect is monotone, and what shape it has.
To understand how much it contributes to a prediction at a given value, you look at the x-axis for that given value, for example 25 degrees C temperature and read the y-axis value, which will be added to the weighted sum of features.


### Advantages

- Has lots of theory behind it for calculating things like confidence intervals and testing hypothesis.
- Very rich literature and experience with these models
- Accepted in many fields as the standard for testing hypothesis.
- Understanding linear models helps you understanding statistical hypothesis testing.
Replaces many tests, like analysis of variance stuff, t-tests and so on.
- Can be made sparse by using regularization, which favors estimation of weights with 0.
- Really good interfaces (for example in R) to describe how the relationship between input and output should be handled. 
Can be written as formula.
- GAM makes very smooth non-linear effects, which usually also have derivatives, so you can more automatically quantify them. 
Other models, like tree-based methods (random forest) have often ugly effects
- What's the source of machine learning opaqueness compared to a linear model? I would say it's that the feature are treated in a non-linear fashion (meaning you need more than a single weight to describe the effect) and also you have lots of interactions in the model.
If we assume that the linear regression model is the easiest interpretable model, but not so flexible and therefore often underfitting reality, than the tools in this chapter provide a good way to transition towards more flexibility, while keeping some of the interpretability.
- Lots of theory to deal with repeated measurements.
- Frameworks for survival analysis
- Heterocedasticity can be solved by using more robut estimators
- 

### Disadvantages

- Each step makes the interpretation more complex. 
Different link function make the interpretation more complicated;
Interactions make the interpretation more complicated;
Non-linear feature effects are either not very intuitive (like log transform) or cannot be summarized with single number any longer.
- When features are transformed, coefficients are typically not interpretable at all.
- The world of linear regression models and their extensions can be quite overwhelming
- There are attempts to automate these steps, but mainly it's manual.
- For interactions: the estimated coefficient (main effect) of each feature only caries meaning of an unconditional marginal effect when the effect of the interaction is zero, which is neve
- Relies on assumptions for lots of the interpretaiton stuff
- 


### Software

Any major programming language for stats has all those. The R language, SPSS, SAS, Python, ...

### More extensions

A list of problem statement + the name of a solution that you can copy-paste into search.

- My data is not iid (independent and identically distributed).
For example you might have repeated measurements from the same patient.
Search for: *mixed models* or *generalized estimation equations*.
- My model has heteroscedastic errors.
For example when predicting the value of a house, the errors are usually higher for expensive houses. 
Search for: *robust regression*.
- I have outliers that severely influence my model.
Search for: *robust regression*.
- I want to predict time to event.
Time to event data usually comes with censored measurements, meaning for some instances we didn't have enough time to observe the event.
For example when we a company wants to predict the failure of machines, but has only data for two years.
Some machines will still be intact after two years, but might break later. 
Search for: *parametric survival models*, *cox regression*, *survival analysis*.
- My outcome to predict is a category. 
If you have only two categories use a [logistic regression model](#logistic), which will model the probability for the categories.
If more categories, search for: *multinomial regression*.
Both regression models are GLMs.
- My outcome is ordered categories. 
Search for: *proportional odds model*.
- My outcome is a count (like number of children in a family).
Search for: *poisson regression*.
This model is a GLM as well.
Additionally, if you have the problem that the count value of 0 is very frequent:
Search for: *zero-inflated poisson regression*, *hurdle model*.
- I am not sure which features to include to do proper inference. 
For example I want to know the effect of a drug on some outcome.
The drug directly affects also some blood value, but this blood value also affects the outcome. 
Should I include the blood value into the regression model?
(Answer is no btw., because it will hide the effect of the drug.)
Search for: *causal inference*, *mediation analysis*.
- I have missing data. 
Search for:  *multiple imputation*.
- I want to integrate prior knowledge into my models.
Search for: *bayesian inference*.
- I am feeling a bit down lately.
Search for: *"Amazon Alexa Gone Wild!!! Full version from beginning to end"*
