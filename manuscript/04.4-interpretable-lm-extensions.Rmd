## Linear model extensions: GLM, GAM and more {#extend-lm}

The chapter will cover:

What to do when  the target y is not normal -> GLM
What to do when the relation between the features and y is not linear -> feature transformations, GAMs (splines)
What to do when there are interactions between features -> manual integration of interaction effects, automatic detection
What to do when the data is not i.i.d. -> mixed models, GEEs (not going to deep into this topic)



### Different types of targets

Normal linear regression assumes that the target has a Gaussian distribution, given the input features.
Logistic regression assumes that the target has a Bernoulli distribution, given the input features.
But what if we want to predict the time between two events? 
This time is always positive and a Gaussian distribution might be just wrong.
What if we want to predict some count feature, like the number of children a person will have in the lifetime?
That's certainly not a Gaussian distribution and also not Bernoulli.
So we need a more flexible approach to handle different types of outcomes, but would love to keep the interpretability of linear models and all the tools connected to it, like having weights that can be interpreted, having confidence intervals, being able to make predictions and so on.
The solution to generalizing linear models to arbitrary outcome distributions is called GLM: generalized linear model.

At the core, the generalized linear model has still a linear composition of the features, but it is wrapped into a link function h, which can be chosen flexibly depending on the type of outcome. 
The link function links the linear predicted to our expected outcome:

$$\beta_0+\beta_1{}x_1+\ldots{}\beta_p{}x_p=g(E(y))$$
The GLM consists of three components. 
Besides the link function and the linear predictor, we also need a probability distribution from the exponential family.
In the exponential family are all distribution function that kind of follow the same pattern.
They can all be written in the same format, which involves and exponent, the mean and variance of the distribution and some other parameters.
I won't go into details here, because that's a very big universe of it's own I don't want to open up.
Wikipedia has a nice [list of distributions from the exponential family](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions). 
Any distribution from this list can be chosen as the distribution.
The choice depends on the nature of the outcome you have.
Your outcome is some kind of count (e.g. number of children)?
Then the Poisson distribution might be a good choice.
Your outcome is always positive (e.g. time between two events)?
Then the Exponential distribution might be a good choice.

You also have to choose the link function and have some kind of flexibility there.
But each distribution also comes with a canonical link function.
When using the canonical link, then:

$$\theta_{i}=\beta_i{}x_i$$

If you are reading the book from start to end, you have already came across a GLM: logistic regression. 
The link (?) function is the logistic function.


*Examples*

Poisson regression.

### Adding interactions

What if the features interact with each other?
We can simply add interactions.

*Examples*

### Non-linear effects

Simple solution: Transform your features

Advanced and more flexible: GAMs.

*Examples*



### Advantages

- Has lots of theory behind it for calculating things like confidence intervals and testing hypothesis
- Very rich literature and experience with these models
- Accepted in many fields as the standard for testing hypothesis

### Disadvantages

- Each step makes the interpretation more complex. 
Different link function make the interpretation more complicated;
Interactions make the interpretation more complicated;
Non-linear feature effects are either not very intuitive (like log transform) or cannot be summarized with single number any longer.
- The world of linear regression models and their extensions can be quite overwhelming
- 








