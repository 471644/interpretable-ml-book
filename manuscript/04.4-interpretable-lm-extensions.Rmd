```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

## Linear model 2.0: GLMs, GAMs and more {#extend-lm}

[Linear regression models](#linear) restrict the outcome to be modeled by a weighted sum of the features.
Additionally it comes with a lot of other assumptions and restrictions.
The bad news is (well, not really news) that all those assumptions are often violated in reality: the output may have a non-Gaussian distribution given the features, there might be interactions between the features, the relationship between a feature and the outcome might be non-linear.
The good news is that the statistics community has come up with a vast array of modifications that turns the linear regression model from a simple blade into a resourceful Swiss knife.


This chapter is definitely not your definite guide to extending linear models. 
It rather serves as a high-level overview over extensions like Generalized Linear Models (GLMs) and Generative Additive Models (GAMs) and give you some intuition.
After reading you should have a solid overview over how to extend linear models.
If you want to learn more about the linear regression model first, I recommend reading the [chapter on linear regression models](#linear), if you haven't already.

Let's recall the formula of a linear regression model:

$$y_{i}=\beta_{0}+\beta_{1}x_{i1}+\ldots+\beta_{p}x_{ip}+\epsilon_{i}$$

The linear regression model assumes that the i-th instance's outcome $y_i$ can be expressed by a weighted sum of its p features $x_{ij}$ with an individual error $\epsilon_i$ which follows a Gaussian distribution.
By forcing the data into this corset of a formula, we get a lot of interpretability: 
The feature effects are additive, meaning no crazy interaction like in many machine learning methods, and the relationship is linear, meaning an increase of a feature by one unit can be directly translated into an increase/decrease of the predicted outcome.
The linear model allows us to compress the relationship between a feature and the expected outcome into a single number, the estimated weight or coefficient.

But this formula is too restrictive for many real world prediction problems.
In this chapter, we will see three possible problems of the classic linear regression model and how to solve them.
There are many more problems with possibly violated assumptions, but we will focus on those three that are visualized in the following figure:

```{r three-lm-problems, fig.cap = "Left: Three assumptions of the linear model. Gaussian distribution of the outcome given the features, additivity (= no interactions) and linear relationship. Right: Reality usually doesn't adhere to those assumptions. Outcomes might have non-Gaussian distributions, features might interact and the relationship might be non-linear."}
theme_blank = theme(axis.line=element_blank(),axis.text.x=element_blank(),
          axis.text.y=element_blank(),axis.ticks=element_blank(),
          axis.title.x=element_blank(),
          axis.title.y=element_blank(),legend.position="none",
          panel.background=element_blank(),panel.border=element_blank(),panel.grid.major=element_blank(),
          panel.grid.minor=element_blank(),plot.background=element_blank())

## For the GLM
n = 10000
df = data.frame(x = c(rnorm(n), rexp(n, rate = 0.5)), dist = rep(c("Gaussian", "Definitely not Gaussian"), each = n))
df$dist  = relevel(factor(df$dist), "Gaussian")
p.glm = ggplot(df) + geom_density(aes(x = x)) + facet_grid(. ~ dist, scales = "free") + theme_blank

# For the interaction
df = data.frame(x1 = seq(from = -3, to = 3, length.out = n), x2 = sample(c(1,2), size = n, replace = TRUE))
df$y = 3 + 5 * df$x1 + (2  - 8 * df$x1 ) * (df$x2 == 2)
df$interaction = "Interaction"
df2 = df
df2$y = 3  + 5 * df$x1 + 0.5 * (- 8 * df$x1 ) + 2 * (df$x2 == 2)
df2$interaction = "No Interaction"

df = rbind(df, df2)
df$interaction  = relevel(factor(df$interaction), "No Interaction")
df$x2 = factor(df$x2)
p.interaction = ggplot(df) + geom_line(aes(x = x1, y = y, group = x2, lty = x2)) + facet_grid(. ~ interaction) + theme_blank


# For the gam
df = data.frame(x  = seq(from = 0, to = 10, length.out = 200))
df$y = 5 + 2 * df$x
df$type = "Linear"
df2 = df
df2$y = 3 + 2 * df$x + 3 * sin(df$x)
df2$type = "Non-linear"
df = rbind(df, df2)

p.gam = ggplot(df) + geom_line(aes(x = x, y = y)) + facet_grid(. ~ type) + theme_blank

gridExtra::grid.arrange(p.glm, p.interaction, p.gam)

```

For all those problems, there is a solution.

**Problem**: The target outcome y given the features doesn't follow a Gaussian distributed . 
**Example**: Let's say I want to predict how many minutes I will ride my bike on a given day, depending on type of day, weather and so on.  
When I use a linear model, it might predict negative minutes, because it assumes a Gaussian distribution which does not stop at 0 minutes.
Wwhen you want to predict probabilities with a linear model, you might also get probabilities that are negative or larger than 1.
**Solution**: [Generalized Linear Models (GLMs)](#glm)

**Problem**: The features interact.  
**Example**: On average, light rain has a slight negative effect on my biking desire.
But in summer during rush hour, I welcome rain, because then all those fair-weather riders stay at home and I have the cycling routes for myself!
This is an interaction between time and weather, which can't be explained in a purely additive model.
**Solution**: [Manually adding interactions](#lm-interact)

**Problem**: The relation between the features and y is not linear.
**Example**: The effect of the temperature on my desire to go cycling might be linear between 0 and 25 degrees Celsius, meaning that an increase from 0 to 1 degree causes the same increase in biking desire than  from 20 to 21. 
But at a certain point, it levels off and might even decrease the my motivation to cycle - I don't like to bike when it's too hot.
**Solutions**: [Generalized Additive Models (GAMs); manually transforming features interactions.](#gam)

The presented extensions (GLM, GAM, ...) are only more or less superficially presented here. 
Many other extensions of the linear model are omitted.
If I attempted to cover it all here, the chapter would quickly turn into a book within a book on a topic which is covered in many books already.
But since you are here already, I made a little problem - solution overview for linear model extensions, which you will find at the [end of the chapter](#more-lm-extension).
The name of the solution is meant to be a starting point for a search.

### Non-Gaussian Outcomes - GLMs

The linear regression model assumes that the outcome given the input features follows a Gaussian distribution.
This assumption excludes a lot of cases:
The outcome can also be a categories (cancer vs. healthy), a count (number of children), the time until an event happens (time until a machine breaks down) or a very skewed variable with many high values (household incomes).
The binary classification case can be handled when we modify the linear regression model with the logistic function, while the weighted sum of the inputs remains at the core as described in the [logistic regression chapter](#logistic).
The linear regression model can be extended to model all those kinds of outcomes.
This new class of models are called **Generalized Linear Models**, or short **GLMs**.
Throughout the chapter I will use the name GLM for both the general framework and particular models that are from that framework.
The core concept of any GLM is: 
Keep the  weighted sum of features, but allow non-Gaussian outcome distributions connect the expected mean of that distribution and the weighted sum through a possibly non-linear function.
The logistic regression model, for example, assumes a Bernoulli distribution for the outcome and uses the logistic function to connect the expected mean and the weighted sum.

Mathematically, the GLM links the weighted sum of the features to the mean of the assumed distribution through the link function g, which can be chosen flexibly depending on the type of outcome. 
In the following formula, y from the linear model is replaced with $E_Y(y|X)$. 
The observed y's in the data and the expected outcome can be different things.
In logistic regression we observe either class 0 or class 1 and the expected outcome is the probability for class 1.

$$g(E_Y(y_i|x_i))=\beta_0+\beta_1{}x_{i1}+\ldots{}\beta_p{}x_{ip}$$

GLMs consist of three components:
The link function g, the weighted sum (sometimes called linear predictor) ($\beta{}X$), and a probability distribution from the exponential family, which defines $E_Y$.

The exponential family is a set of distributions that can be written with the same (parameterized) formula involving an exponent, the mean and variance of the distribution and some other parameters.
I won't go into the mathematical details because that's a very big universe of it's own that I don't want to open up.
Wikipedia has a neat [list of distributions from the exponential family](https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions). 
Any distribution from this list can be chosen for your GLM.
Based on the nature of the outcome to be predicted, a suitable distribution has to be chosen.
The outcome a count of something (e.g. number of children living in a household)?
Then the Poisson distribution might be a good choice.
The outcome is always positive (e.g. time between two events)?
Then the exponential distribution might be a good choice.

Let's look at the classic linear model as a GLM.
The link function for the Gaussian distribution in the classic linear model is simply the identity function.
The Gaussian distribution is parameterized by the mean $\mu$ and the variance $\sigma^2$.
The mean describes the value that we expect on average and the variance how much the values vary around that mean.
In the linear model the link function links the weighted sum of the features to the mean of the Gaussian distribution.

This concept generalizes under the GLM framework to any distribution and arbitrary link functions.
If y is a count of something, like the number of coffees someone drinks on a given day, we could model it with a GLM with a Poisson distribution and the natural logarithm as the link function:

$$ln(E_Y(y|x_i))=x\beta$$

The logistic regression model is also a GLM, assuming a Bernoulli distribution and the logistic function as link.
The mean $\mu$ of the binomial distribution, which is used in logistic regression is the probability that y is 1.

$$x_i\beta=ln\left(\frac{E_Y(y|x)}{1-E_Y(y|x)}\right)=ln\left(\frac{P(y=1|x_i)}{1-P(y=1|x_i)}\right)$$

And if we solve this equation to have P(Y=1) on one side, we get the classic logistic regression formula:

$$P(y_{i}=1)=\frac{1}{1+exp(-x_i\beta)}$$

Each distribution comes with a canonical link function, which can be derived mathematically from the distribution.
The GLM framework allos to choose the link function independently from the distribution.
How do you choose the right link function?
There is no perfect recipe. 
You take into account your knowledge about the distribution of your target, but also theoretical considerations and how well the model fits your actual data.
For some distributions the canonical link function can lead to values that are invalide for that distribution.
In the case of the exponential distribution, the link function is the negative inverse, which can lead to negative predictions, which are outside the domain of the exponential distribution.
Snce you can choose any link function, the simple solution is to choose another function that respects the domain of the distribution.

*Examples*

I simulated a dataset about coffee drinking behaviour to highlight the need for GLMs.
Suppose you collected data about your daily coffee drinking behaviour.
Along with number of cups, you note down your current stress level on a scale from 1 to 10, how well you slept the night before on a scale from 1 to 10 and if you work on that day.
The goal is to predict the number of coffees given the features stress, sleep and work.
I simulated data for 200 days.
Stress and sleep were drawn uniformly between 1 and 10, work YES/NO was drawn with a 50/50 chance (what a life!). 
The number of coffees was then drawn from a Poisson distribution, where the intensity $\lambda$ (which is also the expected value of the Poisson distribution) was modeled as a function of the features sleep, stress and work.
You can guess where this story will be going:
*"Hey let's model this data with a linear model ... Oh it doesn't work ... Let's try a GLM with Poisson distribution ... SURPRISE! Now it works!".*
I hope I didn't spoil the story too much for you.

Let's have a look at the distribution target variable, the number of coffees on a given day:

```{r poisson-data}
# simulate data where the normal linear model fails.
n = 200
df = data.frame(stress  = runif(n = n, min = 1, max = 10), 
  sleep = runif(n = n, min = 1, max = 10), 
  work = sample(c("YES", "NO"), size = n, replace = TRUE))
lambda = exp(1* df$stress/10 - 2 * (df$sleep - 5)/10  - 1 * (df$work == "NO"))
df$y = rpois(lambda = lambda, n = n)

tab = data.frame(table(df$y))

ggplot(tab) + geom_col(aes(x = Var1, y = Freq)) +
  scale_x_discrete("Number of coffees on a given day") + 
  scale_y_continuous("Number of days")

```

On `r tab[1,2]` of the `r n` days, you have no coffee at all and at the most extreme day you have `r tab[nrow(tab),1]`.
Let's naively use a linear model to predict the number of coffees using sleep level, stress level and work Yes/No as features.
What can go wrong when we falsely assume a Gaussan distribution?
A wrong assumption can invalidate the estimates, especially the confidence intervals of the weights. 
A more obvious problem is that the predictions don't match the "allowed" domain of the true outcome, as the following figure shows.

```{r failing-linear-model, fig.cap = "Predicted number of coffees dependent on stress, sleep and working day. The linear model also predicts negative values."}
mod.gaus = glm(y ~ ., data = df, x = TRUE)
pred.gauss = data.frame(pred = predict(mod.gaus), actual = df$y)
ggplot(pred.gauss) + 
  geom_histogram(aes(x = pred)) + 
  scale_x_continuous("Predicted number of coffees") + 
  scale_y_continuous("Frequency")
```

The linear model is not useful, because it predicts negative amount of coffee.
This problem can be solved with Generalized Linear Models (GLMs).
We can change the link function and the assumed distribution. 
One option is to keep the Gaussian distribution and use a link function that always results in positive predictions, like the log-link (the inverse is the exp-function) instead of the identity function.
Even better: 
We choose a distribution that matches the data generating process and an appropriate link function. 
Since the outcome is a count, the Poisson distribution is a natural choice, along with the log link function. 
In this case, the data was even generated using the Poisson distribution, so the GLM is the perfect choice.
The fitted Poisson GLM yields the following distribution of predicted values:

```{r linear-model-positive, fig.cap = "Predicted number of coffees dependent on stress, sleep and working day. The GLM with Poisson assumption and log link is an appropriate model for this dataset."}
mod.pois = glm(y ~ ., data = df, x = TRUE, family = poisson(link = "log"))
pred.pois = data.frame(pred = predict(mod.pois, type = "response"), actual = df$y)
ggplot(pred.pois)  + 
  geom_histogram(aes(x = pred))+ 
  scale_x_continuous("Predicted number of coffees") + 
  scale_y_continuous("Frequency")
```

Looks much better now.

**Interpretation of GLM weights**

The assumed distribution together with the link function dictate how tho intepret the estimated feature weights.
In the coffee count example I used a GLM with Poisson distribution and log link, which implies the following relationship between the features and the expected outcome.

$$ln(E(\text{coffees}|\text{stress},\text{sleep},\text{work}))=\beta_0+\beta_{\text{stress}}x_{\text{stress}}+\beta_{\text{sleep}}x_{\text{sleep}}+\beta_{\text{work}}x_{\text{work}}$$

For the interpretation of the weights we invert the link function, so that we can interpret the effect of the features on the expected outcome and not on the logarithm of the expected outcome. 

$$E(\text{coffees}|\text{stress},\text{sleep},\text{work})=exp(\beta_0+\beta_{\text{stress}}x_{\text{stress}}+\beta_{\text{sleep}}x_{\text{sleep}}+\beta_{\text{work}}x_{\text{work}})$$

Since all the weights are in the exponential function, the effect interpretation is not additive, but multiplicative (because exp(a + b) is exp(a) times exp(b)).
The last ingredient for the interpretation are actual weights of the toy example.
The following table lists the estimated weights and exp(weights) together with the 95% confidence interval:

```{r poisson-model-params}
cc = data.frame(summary(mod.pois)$coefficients)
cc = cc[,c("Estimate", "Std..Error")]
colnames(cc) = c("beta", 'var.beta')
cc$exp.beta = exp(cc[, 'beta'])
cc = cc[c("beta", "exp.beta")]
cc = cbind(cc, exp(confint(mod.pois)))
cc$ci = sprintf("%.2f [%.2f, %.2f]", cc$exp.beta, cc$`2.5 %`, cc$`97.5 %`)
kable(cc[c("beta", "ci")], col.names = c("weight", "exp(weight) [2.5%, 97.5%]"), digits = 2)
```

Increasing the stress by one point multiplicatively increases the expected number of coffees by a factor of `r round(cc["stress", "exp.beta"], 2)`. 
Increasing the sleep by one point multiplicatively decreases the expected number of coffees by a factor of `r round(cc["sleep", "exp.beta"], 2)`.
The predicted number of coffees is lower by a factor of `r round(cc["weekendYES", "exp.beta"], 2)` on a non-working day compared to a working day.
In summary the more stress, the less sleep and on work days the most coffee is consumed.

In this section you learned a bit about Generalized Linear Models which are useful when the target does not follow a Gaussian distribution. 
Next, we take a look into how to incorporate interactions between two features in the linear regression model.

### Interactions {#lm-interact}

The linear model assumes that the effect of one feature is the same, no matter what values the other features have (= no interactions).
But often there are interactions in the data.
For predicting the [number of bikes](#bike-data) that will be rented, there might be an interaction between the temperature and whether it's a working day or not.
Maybe when people have to work, the temperature doesn't affect the number of rented bikes much, because the people will ride the rented bike to work no matter what.
On free days many people bike for pleasure, but only if it's warm enough.
We might expect an interaction between temperature and working day on the rented bikes.

How can we make the linear model include interactions?
The solution is in some ways quite elegant, because it doesn't require a modification of the linear model, but only additional columns in the data.
Before fitting the linear model, you add a column to the feature matrix that represents the interaction between the features and fit the model as usual.
In the working day and temperature example, we would add a new feature that is zero whenever it's a weekend, otherwise the value of the temperature feature.
This assumes that working day is the reference category.
Let's say our data looks like this:

```{r data-frame}
x = data.frame(workingday = c("Yes", "No", "No", "Yes"), temp = c(25, 12, 30, 5))
knitr::kable(x)
```

The data matrix that is used by the linear model looks a bit different.
The following table shows how the data looks like when we don't specify interactions to be modeled.
Usually this transformation is automatically done by any statistical software.

```{r data-frame-lm-no-interaction}
mod = lm(1:4 ~ ., data = x)
knitr::kable(data.frame(model.matrix(mod)))
```

The first column is the intercept term.
The second column encodes the categorical feature, with 0 for the reference category and 1 for the other.
The third column contains the temperature.

When we want the linear model to include the interaction between temperature and the working day feature, we have to give it the following matrix with the additional column for the interaction:

```{r data-frame-lm}
mod = lm(1:4 ~ workingday * temp, data = x)
knitr::kable(data.frame(model.matrix(mod)))
```

The new column "workingdayYes" column captures the interaction between the features workingday and temperature.
This new feature is zero for an instance when the workingday feature is at the reference category "No", otherwise it takes on the value of the instances temp feature.
With this type of coding, the linear model can learn a different linear effect of the temperature for both types of days.
This is the interaction effect between the two features.
Without an interaction term, the combined effect of a categorical and a numerical feature can be described by a line which is shifted vertically for the different categories.
When we include the interaction, we allow the effect of the numerical features (the slope) to have a different value in each category.

Including the interaction of two categorical features works similarly.
We create additional features which represents combinations of categories. 
So this artificial data: 

```{r data-frame-lm-cat}
x = data.frame(workingday = c("Yes", "No", "No", "Yes"), weekday = c("Mon", "Sun", "Sat", "Mon"))
knitr::kable(x)
```

Becomes this: 

```{r data-frame-lm-cat2}
mod = lm(1:4 ~ workingday * weekday, data = x)
knitr::kable(data.frame(model.matrix(mod)))
```

The first column is again for the estimation of the intercept.
The second column is the encoded first categorical feature.
Columns three and four are for the second categorical feature, which needs two columns because you need two weights for capturing the effect for three categories, with one of the categories being the reference or default category.
The rest of the columns capture the interactions.
For any category of both features (excluding the reference categories), we create a new feature column that is 1 if both features have a certain category, otherwise 0.

For two numerical features, the interaction column is even easier to construct: 
We simply multiply both numerical features.

There are approaches to detect and add interaction terms automatically. 
One is presented in this book called [RuleFit](#rulefit), which first mines interaction terms and then estimates a linear regression model including interactions.

**Example**

Let's return to the [bike rental prediction task](#bike-data) which we already modeled with a linear model in the [linear model chapter](#linear).
This time, we additionally include an interaction between the temperature and the working day feature.
This results in the following estimated weights and confidence intervals.

```{r example-lm-interaction}
data(bike)
X = bike[bike.features.of.interest]
y = bike[,'cnt']
dat = cbind(X, y)

mod = lm(y ~ . + temp * workingday, data = dat, x = TRUE)
lm_summary = summary(mod)$coefficients

lm_summary_print = lm_summary
rownames(lm_summary_print) = pretty_rownames(rownames(lm_summary_print))
confint(mod)

kable(cbind(lm_summary_print[,c('Estimate', 'Std. Error')], confint(mod)), digits = 1, col.names = c('Weight', 'Std. Error', "2.5%","97.5%"))
```

The additional interaction effect is negative (`r round(lm_summary_print['workingdayWORKING DAY:temp','Estimate'], 1)`) and significantly different from zero, as indicated by the 95% confidence intervall, which does not include the zero.
The interaction term changes the interpretation of the terms of the involved features.
Does the temperature have a negative effect on a working day?
The answer is no, even though the weights suggest it to the untrained eye.
We can't interpret the "workingdayWORKING DAY:temp" interaction term directly, since the interpretation would be:
"All other feature values being equal, increasing the interaction effect of temperature for working day, decreases the predicted number of bikes."
But this comes only on top of the main effect of the temperature.
Let's say we have a working day, and want to know what would happen if the temperature today would be 1 degree warmer. 
Then we have to add both the temp and "workindayWORKING DAY:temp" weights to get how much the estimate increases.

It's easier to understand the interaction visually. 
By introducing an interaction term between a categorical and a numerical feature, we now have two slopes for the temperature instead of one.
The temperature slope for days on which people don't have to work ('NO WORKING DAY') can be read directly from the table (`r round(lm_summary_print['temp','Estimate'], 1)`).
The temperature slope for days on which people have to work ('WORKING DAY') is the sum of both temperature weights (`r round(lm_summary_print['temp','Estimate'], 1)` + `r round(lm_summary_print['workingdayWORKING DAY:temp','Estimate'], 1)` = `r round(lm_summary_print['temp','Estimate'], 1) + round(lm_summary_print['workingdayWORKING DAY:temp','Estimate'], 1)`).
The intercept of the 'NO WORKING DAY'-line at temperature = 0 is determined by the intercept term of the linear model (`r round(lm_summary_print['(Intercept)','Estimate'], 1)`).
The intercept of the 'WORKING DAY'-line ate temperature = 0 is determined by the intercept term + the effect of working day (`r round(lm_summary_print['(Intercept)','Estimate'], 1)` + `r round(lm_summary_print['workingdayWORKING DAY','Estimate'], 1)` = `r round(lm_summary_print['(Intercept)','Estimate'], 1) + round(lm_summary_print['workingdayWORKING DAY','Estimate'], 1)`).

```{r interaction-plot, fig.cap = "The effect (including interaction) of temperature and working day on the predicted number of bikes for a linear model. Effectively we get two slopes for the temperature one for each category of the working day feature. Without the interaction it would be a single slope for the temperature, only shifted by the different means of no working vs. working day."}
jtools::interact_plot(mod, pred = "temp", modx = "workingday")
```

### Non-linear effects - GAMs {#gam}


*The world is not linear.*
Linearity means no matter which value an instance has in a particular feature, increasing the value by one unit always has the same effect on the predicted outcome.
Is it reasonable to assume that increasing the temperature by one degree has the same effect at 10 degrees C as at 40 C for the effect on the number of rented bikes? 
Intuitively you would expect that increasing the temperature from 10 to 11 degrees Celsius has a positive effect and from 40 to 41 a negative effect, which is also the case as you will see in many examples in the book.
Increasing the temperature has a linear, positive effect on the number of rented bikes, but at some point it levels off and even has a negative effect at high temperatures.
The linear model doesn't care about that. 
If you feed it the data, it will dutifully find the best plane (by minimizing the Euclidean distance).

You can model non-linear relationships with one of the following techniques:
- Simple transformation of the feature (e.g. logarithm)
- Categorization of the feature
- Generalized Additive Models (GAMs) which use regression splines

Before I go into details of each method, we will start with an example that illustrates all three right away.
I took the [bike rental dataset](#bike-data) and fit a linear model with only the temperature feature to predict the number of rented bikes.
The following figure shows the estimated slope with: 1) the default linear model, 2) a linear model with a transformed temperature feature (logarithm), 3) a linear model with temperature treated as categorical feature or 4) using regression splines (GAM).

```{r non-linear-effects, fig.cap = "Predicting the number of rented bikes given only the temperature. Using a linear model (top left) doesn't capture the flattening of the temperature effect at high temperatures as suggested by the data and overestimates the number of bikes at low temperatures. One solution is to transform the feature with e.g. the logarithm (top right), to categorize it (bottom left) which is usually a bad decision or to use Generalized Additive Models, which can automatically fit a smooth curve for temperature (bottom right). GAMs are often the best solution."}
mod.simpel = lm(cnt ~ temp, data = bike)
bike.plt = bike
bike.plt$pred.lm = predict(mod.simpel)

bike.plt$log.temp = log(bike$temp + 10)
mod.simpel = lm(cnt ~ log.temp, data = bike.plt)
bike.plt$pred.sqrt = predict(mod.simpel)

bike.plt$cat.temp = cut(bike$temp, breaks = seq(from = min(bike$temp), to = max(bike$temp), length.out = 10), include.lowest = TRUE)
mod.simpel = lm(cnt ~ cat.temp, data = bike.plt)
bike.plt$pred.cat = predict(mod.simpel)

library(mgcv)
mod.gam = gam(cnt ~ s(temp), data = bike)
bike.plt$pred.gam = predict(mod.gam)


bike.plt = data.table::melt(bike.plt[c("pred.lm", "pred.sqrt", "pred.cat", "pred.gam")])
bike.plt$temp = rep(bike$temp, times = 4)
bike.plt$cnt = rep(bike$cnt, times = 4)

model.type = c(pred.lm = "Linear model", 
  pred.sqrt = "Linear model with log(temp + 10)", 
  pred.cat = "Linear model with categorized temp", 
  pred.gam = "GAM")

ggplot(bike.plt) + 
  geom_point(aes(x = temp, y = cnt), size = 1 , alpha = 0.3)  + 
  geom_line(aes(x = temp, y = value), size = 1.2, color = "blue") + 
  facet_wrap("variable", labeller = labeller(variable = model.type)) + 
  scale_x_continuous("Temperature (temp)") + 
  scale_y_continuous("(Predicted) Number of rented bikes")
```


**Feature transformation**

You can be very creative when you transform the feature:
Very common is taking the logarithm of the feature. 
Using the logarithm suggests that each 10x increase of the temperature has the same linear effect on the number of bikes, so going from 1 degree Celsius to 10 degree Celsius has the same effect as going from 0.1 to 1 (sounds wrong).
Other options are the square root, the square function, the exponential function and so on.
Using a feature transformation means that you replace the column of that feature in the data with a function of the feature, like the logarithm and fit the linear model as usual.
Some statistical programs allow specifying transformations also in the call of the linear model.
The interpretation of the feature changes with the transformation. 
When you use a log transformation, the interpretation becomes:
"Increasing the logarithm of the feature by one increases the prediction by the corresponding weight."

**Feature categorization**

Another, perhaps naive solution to get a non-linear effect is to discretize the feature; turn it into a categorical feature.
For example, you could cut the temperature feature into 20 into 12 intervals with the categories [-10, -5), [-5, 0), ... and so on, you get it.
When you use the categorized temperature instead of the continuous, the linear model would estimate a step function, because each level gets its own estimate.
The problem with this approach is that it needs more data, it's more likely to overfit and it's additionally unclear how to meaningfully discretize the feature (equidistant intervals? quantiles? how many intervals?).
I would only use discretiziation if you have a very strong case for it.
For example to make the model comparable to another study.

CONTINUE HERE

**Generalized Additive Models (GAMs)**

Why not 'simply' allow the (generalized) linear model to learn non-linear relationships?
That's the motivation behind GAMs. 
The relationship becomes this:

$$\beta_0+f_1(x_1)+f_2(x_2)+\ldots+f_m(x_m)=g(E_Y(y|x))$$

The formula is the same as the GLM formula, except that the linear $\beta_j{}x_J$ became a more flexible function $f_j(x_j)$.
The core of a GAM is still a sum, because the effects are all added up, and you can mix flexible function for some of the features and estimate weights for other features.

Now the big question is, how do we have to modify the generalized linear regression model to enable it to learn non-linear functions of some features.
The answer is called splines.
Splines are functions that can be added up to approximate arbitrary functions.
A bit like Lego. 
There is a confusing amount of different ways to define these spline functions.
If you are interested in learning more about those different ways to define splines, good luck and have fun. 
I will not go into details here, but build up some intuition.
What helped me personally the most was the realization that you can simply visualize the splines and the realization what actually happens in the data matrix: 
For the temperature feature, we remove the temperature, and replace it with e.g. 10 columns, where each represents a spline function and estimate weights for those instead of the original feature.
It's a little more complex than that, because GAMs introduce some penalty term for the weights, so that they can't change to much away from zero. 
This effectively reduces the flexibility of the splines approach (how much they can overfit). 
How flexible the spline curve is usually tuned by some smoothness parameter, which is usually tuned using cross-validation.
But ignoring the penalty stuff, spline function can also be seen as fancy feature engineering.

In the example where we predict the number of bikes using only temperature with a GAM, the model feature matrix becomes this:

```{r splines-df}
kable(head(model.matrix(mod.gam)), digits = 2)
```

Each row is still an instance from the data (a day) and each column is the value of the spline function at each particular temperature value of the instance. 
The following graphic shows how those functions look like:

```{r splines, fig.cap = "Instead of a single temperature feature, we use 10 spline functions. Each temperature value gets mapped to (here) 9 spline values. If an instance has a temperature of 30 C, then the value for the first spline feature is -1, for the second 0.5 ... and for the 9th it's 2. The we feed the the GAM with those features."}
mm = model.matrix(mod.gam)
mm2 = data.table::melt(mm)
mm2 = mm2[mm2$Var2 != "(Intercept)",]

ggplot(mm2) + geom_line(aes(x = rep(bike$temp, times = 9), y = value)) + facet_wrap("Var2") + 
  scale_x_continuous("Temperature") + 
  scale_y_continuous("Value of spline feature")
```

The GAM assigns weights to each temperature spline feature:

```{r splines-weights}
kable(coef(mod.gam), digits = 2, col.names = "weight")
```

And the actual curve looks like this:

```{r splines-curve, fig.cap = "Fitted smooth feature effect of the temperature for predicting the number of rented bikes (temperature used as the only feature)"}
plot(mod.gam)
```


The interpretation of splines requires to visually explore the fitted curve, but is otherwise similar to the weight interpretation.
The effect of a feature fitted with splines can be read directly from the curve.
Splines are usually centered around the mean prediction, so the value of the curve is the difference to the mean prediction.
For example, at 0 degrees C, the predicted number of bikes is on 3000 lower than average.

### Advantages

- Linear models with all their extension is a bit of a universe of its own. 
For whatever special needs you have, **you will probably find some extension that solves your problem**.
There is a really rich literature about the linear model and its extensions.
- Most of the methods have been in use for some decades, for example GAMs are almost 30 years old. 
The linear models and extensions have been established in many research field as the status quo for modeling data. 
Because of that, in the research community, but also in industry, there is **a lot of experience** with linear models and the methods are **accepted as status quo** in many communities.
- All of the linear models and extensions come from statistics including the whole battery of statistical tools:
Confidence intervals for the parameters, significance tests, prediction intervals and so on. 
Additionally to making predictions, you can use the models to **do inference**, draw conclusions about the data - given all the assumptions are not violated.
This maturity and widespread is accompanied by really good interfaces (for example in R) to fit GLMs, GAMs and more special linear models.
- The source of opaqueness of complex non-parametetic models is: 1) a lack of sparseness meaning many features are used 2), features are treated in a non-linear fashion (meaning you need more than a single weight to describe the effect) 3) and the methods capture interaction between the features.
Assuming that linear regression is highly interpretable but often underfits reality, then the tools in this chapter provide a good way to **smoothly transition towards more flexible models**, while keeping some of the interpretability.


### Disadvantages

- I listed in advantages that linear models are living in their own universe.
Actually it's multiple universes, because lots of communities have their own naming for methods that actually do the same.
Research is happening in parallel in different communities. 
Also the sheer number of ways how you can extend the simple linear model is overwhelming.
There are just so many ways of extending the linear regression model.
And it's so confusing, because researchers and practitioners from different disciplines use linear models and use different names and notation.
- Most of the modifications of the linear model make the model less interpretable.
Link function that are not the simple identity function make the interpretation more complicated;
Interactions make the interpretation of the weights more complicated;
Non-linear feature effects are either not very intuitive (like log transform) or cannot be summarized with single number any longer, like with spline functions in GAMs.
- For interactions: the estimated coefficient (main effect) of each feature only caries meaning of an unconditional marginal effect when the effect of the interaction is zero, which is neve
- GLMs, GAMs and so on rely on assumptions regarding the data generating process.
If those are violated, then the interpretation of the weights is not valid any longer.
- The performance of tree based ensembles like the random forest or gradient tree boosting is usually better than even the most sophisticated linear models and can be trained more automatically.
This is partly my own experience and partly observations from the winning models on plattforms like kaggle.com.


### More extensions {#more-lm-extension}

As promised, there are a lot more ways to expand the linear model.
Here you find a list of problem statements and the name of a solution for that problem, which you can copy-paste into your favorite search engine.

- My data is not independent and identically distributed (iid).  
For example repeated measurements from the same patient.  
Search for: *mixed models* or *generalized estimation equations*.
- My model has heteroscedastic errors.  
For example when predicting the value of a house, the errors are usually higher for expensive houses, which violates the homoscedaticity assumption of the linear model.
Search for: *robust regression*.  
- I have outliers that severely influence my model.  
Search for: *robust regression*.
- I want to predict the time until some event happens.  
Time-to-event data usually comes with censored measurements, meaning for some instances we didn't have enough time to observe the event.
For example, a company wants to predict the failure of ice cream machines, but has only data for two years.
Some machines will still be intact after two years, but might break later.  
Search for: *parametric survival models*, *cox regression*, *survival analysis*.
- The outcome to predict is a category.   
If you have only two categories use a [logistic regression model](#logistic), which will model the probability for the categories.  
If you have more categories, search for: *multinomial regression*.
Both regression models are GLMs.
- My outcomes are ordered categories.  
Search for: *proportional odds model*.
- My outcome is a count (like number of children in a family).  
Search for: *poisson regression*.  
The Poisson model is a GLM as well.
Additionally, if you have the problem that the count value of 0 is very frequent:  
Search for: *zero-inflated poisson regression*, *hurdle model*.
- I am not sure which features to include in the model to do proper causal inference.  
For example I want to know the effect of a drug on the blood pressure.
The drug directly affects also some blood value and this blood value affects the outcome. 
Should I include the blood value into the regression model?
(Answer is no btw., because it will hide the effect of the drug.)  
Search for: *causal inference*, *mediation analysis*.
- I have missing data.  
Search for:  *multiple imputation*.
- I want to integrate prior knowledge into my models.  
Search for: *bayesian inference*.
- I am feeling a bit down lately.  
Search for: *"Amazon Alexa Gone Wild!!! Full version from beginning to end"*
