    devtools::load_all("../")

    ## Loading iml.book

    ## Loading required package: knitr

    ## Warning: package 'knitr' was built under R version 3.4.3

    ## Warning: replacing previous import 'BBmisc::isFALSE' by
    ## 'backports::isFALSE' when loading 'mlr'

    ## Warning in as.POSIXlt.POSIXct(Sys.time()): unknown timezone 'zone/tz/2018c.
    ## 1.0/zoneinfo/Europe/Berlin'

RuleFit
-------

The RuleFit algorithm \[@friedman2008predictive\] fits sparse linear
models which include automatically detected interaction effects in the
form of binary decision rules.

The standard linear model doesn't account for interactions between the
features. Wouldn't it be convenient to have a model that is as simple
and interpretable as linear models, but that also integrates feature
interactions? RuleFit fills this gap. RuleFit fits a sparse linear model
with the original features and also a set of new features which are
decision rules. These new features capture interactions between the
original features. RuleFit generates these features automatically from
decision trees. Each path through a tree can be turned into a decision
rule by combining the split decisions to a rule, as in Figure
@ref(fig:rulefit-split). The node predictions are thrown away and only
the splits are used in the decision rules.

    knitr::include_graphics("images/rulefit.jpg")

<img src="images/rulefit.jpg" alt="4 rules can be generated from a tree with 3 terminal nodes." width="80%" />
<p class="caption">
4 rules can be generated from a tree with 3 terminal nodes.
</p>

Where do the decision trees come from? These are trees that are trained
to predict the outcome of interest, so that the splits are meaningful
for the task at hand and not arbitrary. Any algorithm that creates a lot
of trees can be used for RuleFit, like a Random Forest for example. Each
tree is disassembled into decision rules, which are used as additional
features in a linear Lasso model.

The RuleFit paper uses the Boston housing data for illustration: The
goal is to predict the median house value in the Boston neighbourhood.
One of the rules (read: features) generated by RuleFit: "if (number of
rooms &gt;6.64) and (concentration of nitric oxide &lt;0.67) then 1 else
0"

RuleFit also comes with a feature importance measurement, which helps to
identify linear terms and rules that are important for the prediction.
The feature importance is calculated from the weights of the regression
model. The importance measure can be aggregated for the original
features (which appear once untransformed and possibly in many decision
rules).

RuleFit also introduces partial dependence plots to plot the average
change of the prediction by changing one feature. The partial dependence
plot is a model-agnostic method, which can be used with any model, and
it has its own part in the book, see Chapter @ref(pdp).

### Interpretation and Example

Since RuleFit estimates a linear model in the end, the interpretation is
equivalent to linear models described in Chapter @ref(limo). The only
difference is that the model has new features that are coming from
decision rules. Decision rules are binary features: A value of 1 means
that all conditions of the rule are met, otherwise the value is 0. For
linear terms in RuleFit, the interpretation is the same as in linear
regression models: If *x*<sub>*j*</sub> increases by one unit, the
predicted outcome changes by *Î²*<sub>*j*</sub>.

    library(pre)
    library(dplyr)

    ## 
    ## Attaching package: 'dplyr'

    ## The following objects are masked from 'package:stats':
    ## 
    ##     filter, lag

    ## The following objects are masked from 'package:base':
    ## 
    ##     intersect, setdiff, setequal, union

    data("bike")

    X = bike[bike.features.of.interest]

    y = bike[,'cnt']
    dat = cbind(X, y)
    mod = pre(y ~ ., data = dat, maxdepth = 2, ntrees = 100)
    coefs <- coef(mod)
    coefs$description[is.na(coefs$description)] = coefs$rule[is.na(coefs$description)]
    coefs = left_join(coef(mod), pre::importance(mod, plot=FALSE)$baseimp)

    ## Joining, by = c("rule", "coefficient", "description")

    coefs = coefs[!is.na(coefs$coefficient), ]
    coefs$imp = round(coefs$imp, 1)
    coefs$coefficient = round(coefs$coefficient, 1)
    coefs$sd = round(coefs$sd, 2)
    coefs$rule = NULL
    coefs = filter(coefs, !is.na(description))

In this example, we use RuleFit to predict the number of bike rentals on
a given day (see Chapter @ref(bike-data)). The rules generated for the
bike rental prediction task can be seen in Figure
@ref(fig:rulefit-example). The most important rule was:
"days\_since\_2011 &gt; 430 & temp &gt; 5.081651" and the associated
weight is 567.4. The interpretation is: If days\_since\_2011 &gt; 430 &
temp &gt; 5.081651, then the predicted number of bike rentals increases
by 567.4, given all other features values stay fixed. In total, 347 such
rules were created from the original 8 features. Quite a lot! But thanks
to Lasso, only 41 of the 347 got a weight different from zero.

    kable(coefs[1:20, c('description', 'coefficient', 'imp', 'sd')],
    col.names = c('Description', 'Weight', 'Importance', 'Std. Dev'),
    row.names=FALSE)

<table>
<thead>
<tr class="header">
<th align="left">Description</th>
<th align="right">Weight</th>
<th align="right">Importance</th>
<th align="right">Std. Dev</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">days_since_2011 &gt; 430 &amp; temp &gt; 5.081651</td>
<td align="right">567.4</td>
<td align="right">277.4</td>
<td align="right">0.49</td>
</tr>
<tr class="even">
<td align="left">temp &lt;= 14.09 &amp; days_since_2011 &lt;= 368</td>
<td align="right">-564.6</td>
<td align="right">244.3</td>
<td align="right">0.43</td>
</tr>
<tr class="odd">
<td align="left">temp &gt; 9.703349 &amp; weathersit %in% c(&quot;GOOD&quot;, &quot;MISTY&quot;)</td>
<td align="right">473.3</td>
<td align="right">224.7</td>
<td align="right">0.47</td>
</tr>
<tr class="even">
<td align="left">temp &lt;= 6.2175 &amp; workingday %in% c(&quot;NO WORKING DAY&quot;)</td>
<td align="right">-472.4</td>
<td align="right">117.1</td>
<td align="right">0.25</td>
</tr>
<tr class="odd">
<td align="left">days_since_2011 &gt; 111 &amp; weathersit %in% c(&quot;GOOD&quot;, &quot;MISTY&quot;)</td>
<td align="right">464.1</td>
<td align="right">177.0</td>
<td align="right">0.38</td>
</tr>
<tr class="even">
<td align="left">season %in% c(&quot;SUMMER&quot;, &quot;FALL&quot;, &quot;WINTER&quot;) &amp; weathersit %in% c(&quot;GOOD&quot;, &quot;MISTY&quot;)</td>
<td align="right">447.1</td>
<td align="right">198.8</td>
<td align="right">0.44</td>
</tr>
<tr class="odd">
<td align="left">days_since_2011 &gt; 428 &amp; temp &gt; 15.6175</td>
<td align="right">410.3</td>
<td align="right">178.5</td>
<td align="right">0.44</td>
</tr>
<tr class="even">
<td align="left">temp &lt;= 8.184356 &amp; temp &lt;= 3.945849</td>
<td align="right">-287.0</td>
<td align="right">83.4</td>
<td align="right">0.29</td>
</tr>
<tr class="odd">
<td align="left">days_since_2011 &gt; 440 &amp; temp &gt; 11.544151</td>
<td align="right">281.7</td>
<td align="right">129.5</td>
<td align="right">0.46</td>
</tr>
<tr class="even">
<td align="left">days_since_2011 &gt; 408 &amp; weathersit %in% c(&quot;GOOD&quot;)</td>
<td align="right">271.5</td>
<td align="right">122.9</td>
<td align="right">0.45</td>
</tr>
<tr class="odd">
<td align="left">days_since_2011 &gt; 108 &amp; weathersit %in% c(&quot;GOOD&quot;, &quot;MISTY&quot;)</td>
<td align="right">245.8</td>
<td align="right">92.9</td>
<td align="right">0.38</td>
</tr>
<tr class="even">
<td align="left">temp &gt; 6.958267 &amp; days_since_2011 &gt; 381</td>
<td align="right">241.0</td>
<td align="right">118.3</td>
<td align="right">0.49</td>
</tr>
<tr class="odd">
<td align="left">temp &gt; 12.719151 &amp; days_since_2011 &gt; 517</td>
<td align="right">231.3</td>
<td align="right">92.8</td>
<td align="right">0.40</td>
</tr>
<tr class="even">
<td align="left">temp &lt;= 16.0875</td>
<td align="right">-166.6</td>
<td align="right">83.3</td>
<td align="right">0.50</td>
</tr>
<tr class="odd">
<td align="left">temp &gt; 9.703349 &amp; weathersit %in% c(&quot;GOOD&quot;)</td>
<td align="right">155.2</td>
<td align="right">77.1</td>
<td align="right">0.50</td>
</tr>
<tr class="even">
<td align="left">days_since_2011 &gt; 104 &amp; weathersit %in% c(&quot;GOOD&quot;)</td>
<td align="right">149.1</td>
<td align="right">74.2</td>
<td align="right">0.50</td>
</tr>
<tr class="odd">
<td align="left">temp &lt;= 5.16</td>
<td align="right">-144.3</td>
<td align="right">49.6</td>
<td align="right">0.34</td>
</tr>
<tr class="even">
<td align="left">days_since_2011 &gt; 414 &amp; temp &lt;= 15.6175</td>
<td align="right">-133.9</td>
<td align="right">51.4</td>
<td align="right">0.38</td>
</tr>
<tr class="odd">
<td align="left">temp &lt;= 12.719151 &amp; days_since_2011 &lt;= 385</td>
<td align="right">-125.0</td>
<td align="right">54.1</td>
<td align="right">0.43</td>
</tr>
<tr class="even">
<td align="left">temp &gt; 8.306979 &amp; days_since_2011 &gt; 419</td>
<td align="right">120.3</td>
<td align="right">57.7</td>
<td align="right">0.48</td>
</tr>
<tr class="odd">
<td align="left">Computing the global feature importances reveals that temperature and the time tr</td>
<td align="right">end are t</td>
<td align="right">he most impor</td>
<td align="right">tant features, see Figure @ref(fig:rulefit-importance).</td>
</tr>
<tr class="even">
<td align="left">The feature importance measurement includes the importance of the raw feature ter</td>
<td align="right">m and all</td>
<td align="right">the decision</td>
<td align="right">rules the feature appears in.</td>
</tr>
</tbody>
</table>

    pre::importance(mod)

![Feature importance measures for a RuleFit model predicting bike
rentals. The most important features for the predictions were
temperature and the time
trend.](04.5-interpretable-rulefit_files/figure-markdown_strict/rulefit-importance-1.png)

### Guidelines

In this section we will talk about the advantages and disadvantages of
RuleFit and how to interpret it.

**Interpretation template**

The interpretation is analogue to linear models: The predicted outcome
changes by *Î²*<sub>*j*</sub> if feature *x*<sub>*j*</sub> changes by one
unit, given all other features stay the same. The weight interpretation
of a decision rule is a special case: If all conditions of a decision
rule *r*<sub>*k*</sub> apply, the predicted outcome changes by
*Î±*<sub>*k*</sub> (the learned weight for rule *r*<sub>*k*</sub> in the
linear model). And, respectively, for classification: If all conditions
of decision rule *r*<sub>*k*</sub> apply, the odds for event vs.
no-event changes by a factor of *Î±*<sub>*k*</sub>.

**The Good**:

-   RuleFit adds **feature interactions** automatically to linear
    models. Therefore it solves the problem of linear models that you
    have to add interaction terms manually and it helps a bit with the
    issue of modeling non-linear relationships.
-   RuleFit can handle both classification and regression tasks.
-   The created rules are easy to interpret, because they are binary
    decision rules. Either the rule applies to an instance or not. Good
    interpretability is only guaranteed as long as the number of
    conditions within a rule is not to big. A rule with 1 to 3
    conditions seems reasonable to me. This translates into a maximum
    depth of 3 for the trees in the tree ensemble.
-   Even if there are many rules in the model, they do not apply to each
    instance, so for one instance only a handful of rules are important
    (non-zero weights). This improves local interpretability.
-   RuleFit comes with a bunch of tools, namely an importance
    measurement, interaction effects calculation and partial dependence
    plots.

**The Bad**:

-   Sometimes RuleFit creates many rules which get a non-zero weight in
    the Lasso model. The interpretability degrades with higher number of
    features in the model. A promising solution is to force feature
    effects to be monotonic, meaning that an increase in a feature has
    to result in an increase of the predicted outcome.
-   An anecdotal drawback: The papers claim good performance of
    RuleFit - often close to the predictive performance of Random
    Forests! - yet in the few cases where I personally tried it, the
    performance was disappointing.
-   The end product of the RuleFit procedure is a linear model with
    additional fancy features (the decision rules). But since it is a
    linear model, the weight interpretation is still unintuitive (given
    all features are fixed, increasing feature *x*<sub>*j*</sub> by one
    unit, increases the predicted outcome by *Î²*<sub>*j*</sub>). It gets
    a bit more tricky if you have overlapping rules: For example one
    decision rule (feature) for the bike prediction could be: "temp &gt;
    15" and another rule could be "temp &gt; 10 & weather='GOOD'". When
    the weather is good and the temperature is above 10 degrees, the
    temperature is automatically also always bigger then 15, which means
    in the cases where the second rule applies, the first one also
    always applies. The interpretation of the estimated weight for the
    second rule is: 'Given all other features are fixed, the predicted
    number of bikes increases by *Î²*<sub>2</sub>'. BUT, now it becomes
    really clear that the 'all other feature fixed' is problematic,
    because if rule 2 applies, also rule 1 applies and the
    interpretation is nonsensical.

The RuleFit algorithm is implemented in R by @pre2017 and you can find a
[Python version on Github](https://github.com/christophM/rulefit).

### Theory

Let's dive deeper into the technicalities of the RuleFit algorithm.
RuleFit consists of two components: The first component produces "rules"
from decision trees and the second component fits a linear model with
the original features and the new rules as input (hence the name
"RuleFit"). It enables automatic integration of interactions between
features into a linear model, while having the interpretability of a
sparse linear model.

**Step 1: Rule generation**

How does a rule look like? The rules that the algorithm generates have a
simple form: For example: "if *x*2â&lt;â3 and *x*5â&lt;â7 then 1 else
0". The rules are constructed by disassembling decision trees: Each path
to a node in a tree can be turned into a decision rule. The trees used
for the rules are fitted to predict the target outcome. The splits and
resulting rules are optimised to predict the outcome you are interested
in. Figure @ref(fig:rulefit-split) illustrates the rule generation. You
simply chain the binary decisions that lead to a certain node with a
logical "AND", and voilÃ , you have a rule. It is desirable to generate a
lot of diverse and meaningful rules. Gradient boosting is used to fit an
ensemble of decision trees (by regressing or classifying *y* with your
original features *X*). Each resulting tree is turned into multiple
rules. Not only boosted trees, but any type of ensemble of trees can be
used to generate the trees for RuleFit:
$$f(x) = a\_0 + \\sum\_{m=1}^M a\_m f\_m(X)$$
 where *M* is the number of trees and *f*<sub>*m*</sub>(*x*) represents
the prediction function of the *m*-th tree. Bagged ensembles, Random
forests, AdaBoost and MART yield ensemble of trees and can be used for
RuleFit.

From all of the trees of the ensemble, we produce the rules. See again
Figure @ref(fig:rulefit-split). Each rule *r*<sub>*m*</sub> takes on the
form:
*r*<sub>*m*</sub>(*x*)=â<sub>*j*âââT<sub>*m*</sub></sub>*I*(*x*<sub>*j*</sub>âââ*s*<sub>*j**m*</sub>)
 where T<sub>*m*</sub> is the set of features used in *m*-th tree,
*I*(â) is the indicator function, which is 1 if the feature
*x*<sub>*j*</sub> is in the specified subset of values
*s*<sub>*j**m*</sub> for *x*<sub>*j*</sub> (as specified by the tree
splits) and otherwise 0. For numerical features, *s*<sub>*j**m*</sub> is
one to multiple intervals in the value range of the feature
*x*<sub>*j*</sub>, depending on the number of splits in that feature. In
case of a single split, the *s*<sub>*j**m*</sub> looks like one of the
two cases:
*x*<sub>*s*<sub>*j**m*</sub>,âlower</sub>â&lt;â*x*<sub>*j*</sub> or
*x*<sub>*j*</sub>â&lt;â*x*<sub>*s*<sub>*j**m*</sub>,â*u**p**p**e**r*</sub>.
Further splits in that feature create more complicated intervals. For
categorical features the subset *s*<sub>*j**m*</sub> contains some
specific categories of *x*<sub>*j*</sub>.

A made up example for the bike rental data set:
$$ \\begin{eqnarray} r\_{17}(x) &=& I(x\_{\\text{temp}} &lt; 15) \\cdot  \\\\
            & & I(x\_{\\text{weather}} \\in \\{\\text{good}, \\text{cloudy}\\}) \\cdot  \\\\
            & & I(10 \\leq x\_{\\text{windspeed}} &lt; 20)
            \\end{eqnarray}$$
 This rule will only be equal to 1 if all of the three conditions are
met, otherwise 0. RuleFit extracts all possible rules from a tree, not
only from the leaf nodes. So another rule that would be created is:
$$ \\begin{eqnarray} r\_{18}(x) &=& I(x\_{\\text{temp}} &lt; 15) \\cdot \\\\
            & &  I(x\_{\\text{weather}} \\in \\{\\text{good}, \\text{cloudy}\\})
            \\end{eqnarray}$$
 In total, $K = \\sum\_{m=1}^M 2(t\_m -1 )$ rules are created from the
ensemble of *M* trees, with *t*<sub>*m*</sub> terminal nodes each. A
trick that is introduced by the RuleFit authors is to fit trees with
random depth, so that a lot of diverse rules are generated with
different lengths. Note that we throw away the predicted value in each
node and only keep the conditions that lead us to the node and create a
rule from it. The weighting of the decision rules will happen in step 2
of fitting RuleFit.

Another way to see the first step is, that it generates a new set of
features *X*â² out of your original features *X*. Those features are
binary and can represent quite complex interactions of your original
*X*. The rules are chosen to maximise the prediction task at hand. The
rules are automatically generated from the covariates matrix X. You can
see the rules simply as new features based on your original features.

**Step 2: Sparse linear model**

You will get A LOT of rules from the first step. Since the first step is
only a feature transformation function on your original data set you are
still not done with fitting a model and also you want to reduce the
number of rules. Next to the rules, also all your 'raw' features from
your original dataset will be used in the Lasso linear model. Every rule
and original feature becomes a feature in Lasso and gets a weight
estimate. The original, raw features are added because trees suck at
representing simple linear relationships between y and x. Before we put
everything into Lasso to get a sparse linear model, we winsorise the
original features, so that they are more robust against outliers:
*l*<sub>*j*</sub><sup>\*</sup>(*x*<sub>*j*</sub>)=*m**i**n*(*Î´*<sub>*j*</sub><sup>+</sup>,â*m**a**x*(*Î´*<sub>*j*</sub><sup>â</sup>,â*x*<sub>*j*</sub>))
 where *Î´*<sub>*j*</sub><sup>â</sup> and *Î´*<sub>*j*</sub><sup>+</sup>
are the *Î´* quantiles of the data distribution of *x*<sub>*j*</sub>. A
choice of 0.05 for *Î´* means that every value of *x*<sub>*j*</sub> that
is in the 5% lowest or 5% highest values will be set to the values at 5%
or 95% respectively. As a rule of thumb, you can choose *Î´*â=â0.025. In
addition, the linear terms have to be normalised so that they have the
same prior influence as a typical decision rule:
*l*<sub>*j*</sub>(*x*<sub>*j*</sub>)=0.4âââ*l*<sub>*j*</sub><sup>\*</sup>(*x*<sub>*j*</sub>)/*s**t**d*(*l*<sub>*j*</sub><sup>\*</sup>(*x*<sub>*j*</sub>))
 The 0.4 is the average standard deviation of rules with a uniform
support distribution *s*<sub>*k*</sub>ââ¼â*U*(0,â1).

We combine both types of features to generate a new feature matrix and
estimate a sparse linear model with Lasso, with the following structure:
$$ \\hat{f}(x) = \\hat{\\beta}\_0 + \\sum\_{k=1}^K\\hat{\\alpha}\_k r\_k(x) + \\sum\_{j=1}^p\\hat{\\beta}\_j l\_j (x\_j)$$
 where $\\hat{\\alpha}$ are the estimated weights for the rule features
and $\\hat{\\beta}$ for the original features. Since RuleFit uses Lasso,
the loss function gets the additional constraint that forces some of the
weights to get a zero estimate:
$$(\\{\\hat{\\alpha}\\}\_1^K, \\{\\hat{\\beta}\\}\_0^p) = argmin\_{\\{\\hat{\\alpha}\\}\_1^K, \\{\\hat{\\beta}\\}\_0^p} \\sum\_{i=1}^n L(y\_i, f(x)) + \\lambda \\cdot (\\sum\_{k=1}^K |\\alpha\_k| + \\sum\_{j=1}^p |b\_j|)$$
 The outcome is a linear model that has linear effects for all of the
original features and for the rules. The interpretation is the same as
with linear models, the only difference is that some features are now
binary rules.

**Step 3 (optional): Feature importance** For the linear terms of the
original features, the feature importance is measured with the
standardised predictor:
$$I\_j = |\\hat{\\beta}\_j| \\cdot std(l\_j(x\_j)) $$
 where *Î²*<sub>*j*</sub> is the weight from the Lasso model and
*s**t**d*(*l*<sub>*j*</sub>(*x*<sub>*j*</sub>)) the standard deviation
of the linear term over the data.

For the decision rule terms, the importance is calculated with:
$$I\_k = |\\hat{\\alpha}\_k| \\cdot \\sqrt{s\_k(1 - s\_k)}$$
 where $\\hat{\\alpha}\_k$ is the associated Lasso weight of the
decision rule and *s*<sub>*k*</sub> is the support of the feature in the
data, which is the percentage of data points for which the decision rule
applies (where *r*<sub>*k*</sub>(*x*) = 0):
$s\_k = \\frac{1}{n}\\sum\_{i=1}^n r\_k(x\_i)$

A feature *x*<sub>*j*</sub> occurs as a linear term and possibly also
within many decision rules. How do we measure the total importance of
the feature *x*<sub>*j*</sub>? The importance *J*<sub>*j*</sub>(*x*) of
feature *x*<sub>*j*</sub> can be measured at each individual prediction:
*J*<sub>*j*</sub>(*x*)=*I*<sub>*l*</sub>(*x*)+â<sub>*x*<sub>*j*</sub>âââ*r*<sub>*k*</sub></sub>*I*<sub>*k*</sub>(*x*)/*m*<sub>*k*</sub>
 where *I*<sub>*l*</sub> is the importance of the linear term and
*I*<sub>*k*</sub> the importance of the decision rules in which
*x*<sub>*j*</sub> appears, and *m*<sub>*k*</sub> is the number of
features that constitute rule *r*<sub>*k*</sub>. Summing the feature
importance over all instances gives us the global feature importance:
$$J\_j(X) = \\sum\_{i=1}^n J\_j(x\_i)$$
 It is possible to choose a subset of instances and calculate the
feature importance for this group.
