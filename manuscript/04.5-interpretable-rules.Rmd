```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all("../")
set.seed(42)
```

# IF-THEN rules

IF-THEN rules (aka decision rules) are simple statements consisting of a series of conditions and a prediction:
IF the all of the conditions apply, THEN we make a certain prediction.
An example: 
IF it rains today (condition), THEN it will rain tomorrow (prediction). 

keywords: decision rules/sets/lists, association rules


## What are decision rules?

Decision rules are models that consists of if X then Y statements. 
Within the if statement there are sets of features and conditions on them (like X > 4) and Y is a statement about the outcome of interest.

Simple if-then statements can and are often written manually. 
In machine learning, these types of rules are learned automatically.
Can be derived from decision trees, but don't have to.

They have the following general structure:
RULE: IF CONDITIONS THEN TARGET


One decision rule from a model predicting rents could be:
If a flat is bigger than 100 square meters and has a garden the average rent is 2000 Euro per month.

Let's split that up:
- 'Flat is bigger than 100 square meters' is the first part of the IF statement
- 'Flat has a garden' is a second conditions in the IF.
- Both are connected with an 'AND', so both have to be true for the rule to apply
- Predicted outcome (THEN) is that the average rent is 2000 Euro per month.



Arguably decision rules are the most simple classifier in terms of understandability. They are very close to natural language and the way we think. Of course it relies on using sensible, understandable features and the number of conditions within the if statement.

Usually there is more then one rule to describe the associations of the features with the outcome. In our example we only have one rule covering big appartments with flats, but what about a smaller apartment?

Rules can be induced from trees. Each path through the tree is one rule. 
Rules made from trees have the nice attribute of covering the whole feature space and being mutually exclusive.

We can make following distinctions:
- descriptive rule discovery: concerned with finding single rules 
    - 
- predictive rule learning: 

There are many ways how to induce those kind of rules from a training dataset.
The induction algorithms differ in how they create the rules:
- Do they allow overlapping rules and how are conflicts handled?
- Do they cover the whole training set? Or can there be a situation where none of the rules apply?


Conflict resolution if many rules are triggered:
- size ordering: use rule with the toughest requierements
- class based: based on cost of missclass
-rule-based (decision list): by quality of rule


Mutually exclusive rules: Rules are independent of each other and every recored is covered at most by one rule. 

Exhaustive rules: For every possible combination of feature values there is a rule. exhaustive coverage.


Methods to learn:
- Directly from data
- Decision tree converted to rules
- Learn other model, then extract rules


You can use a rule-learning algorithm that learns a single rule. 
And use that within sequential covering, where training records covered by this rule are removed.


**What algorithms are out there?**

We can distinguish between: 

- supervised: 
    - RIPPER
    - OPUS
    - CPAR (Yin et. al 2003)
    - FRL (Wang et al. 2015)
    - BRL (Letham et al. 2015)
    - TLBR (Su et al. 2015)
    - IDS (Lakkaraju et al. 2016)
    - Rule Set (Wang et al. 2016)
    - 1Rule (Malioutov et al. 2017)
    
- unsupervised: association rules



https://github.com/Jasamba/Rule-Extraction-in-Python

## Theory: Learn1Rule and the Covering Algorithm

First let's start with a simple algorithm to learn a single rule in a classification setting:

- Start with empty rule
- Add conditions as long as they improve the information gain heuristic.
This heuristic evaluates  the improvement of a rule with respect to its predecessors:
  - this is an exhaustive search over all possible features and split points. For categorical features, test combination of levels as rule. 
For numerical features, try different cut-offs as condition for a rule.
  - add the condition that improves the rule the most according to a quality criterion.
- Stop when the rules does not cover negative samples
- prune rule using incremental reduced error pruning
- measure for pruning v = (p-n) / (p+n), where p is the number of positive examples covered by the rule in the validation set and n for negative samples. 
- delete any finals sequence of conditaions that maximizes v.

The quality criterion can be XXX.

The simplest way to learn decision rules: 
Take some algorithm that can learn a single rule on a dataset. 
Apply that algorithm to a dataset. 
Remove from that dataset all the examples where the rule applies. 
Repeat until all data points are covered or some other stopping criterium is reached.

This second part of repeating the single rule fitting is also called covering algorithm. 


Algorithm for rule set:

- use sequential covering algorithm
    - find best rle that covers current set of positive examples
    - remove all samples (positive and negative) that are covered by this rule
- for each new rule, compute the new description lenght of the set. stop adding new rules when the description lenght is d bits longer than the smalles description length obtained so far.


Optimize the ruleset:

- for each rule r in the rule set:
    - consider 2 alternative rules:
        - replacement rule: grow ne rule from scratch
        - revised rule: add conjuncts to extend the rule r
    - compare original with replacement and revised 
    - choose rule set that minimizes the minimum description length principle
- repeat rule generation and optimization for the remaining positive examples.

This whole procedure is prone to overfit the data. 
There are many variants of this to improve this simple idea. 
One is RIPPER, which was invented 1995 and is used a lot.
The RIPPER algorithm introduces some pruning of the rules and different stopping criteria. 
The examples will be shown with the RIPPER algorithm.



## Rules vs. tree

Rule set can be more clear, because decision tree has replicated subtrees often.

In multi-class siutations, the covering algorithm concentrates on one class at a time whereas decision tree learner takes into account all the classes

Decision tree can always be turned into a mutually exclusive, exhaustive set of decision rules.

Example: C4.5 rules. Extract rules from an unpruned decision tree that was induced by C4.5 algorithm (TODO: Cite both tree paper and rule paper)

## Theory: Association rules (unsupervised)

## Examples

## Advantages

- Ultra easy to interpret: Probably the easiest from all interpretable models.
- Can be as expressive as decision trees
- Straightforward to generate
- Fast to do the prediction
- 

## Disadvantages

- Research focused on classification and not regression


## Software and Alternatives

- RIPPER
- 1Rule
- BRL



