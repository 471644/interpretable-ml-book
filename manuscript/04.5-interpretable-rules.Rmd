```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all("../")
set.seed(42)
```

## IF-THEN decision rules

IF-THEN rules (also called decision rules) are simple statements consisting of a series of conditions and a prediction.
For example: 
IF it rains today (condition) if it's April (another condition), THEN it will rain tomorrow (prediction). 

keywords: decision rules/sets/lists, association rules


Decision rules are the most interpretable prediction models.
They are very close to natural language and the way we think. 
Assuming we use sensible, understandable features and keep the number of conditions within the if statement short.
In programming it's very natural to write IF-THEN rules.
What's new in machine learning is that these types of rules are learned by the machine.

They have the following general structure:
RULE: IF all conditions apply THEN make prediction.

One decision rule from a model predicting rents could be:
If a flat is bigger than 100 square meters and has a garden the value is high.

Let's split that up:
- 'Flat is bigger than 100 square meters' is the first condition in the IF-part.
- 'Flat has a garden' is the second condition in the IF-part.
- Both are connected with an 'AND', so both have to be true for the rule to apply.
- Predicted outcome (THEN) is that the value is high.

How good is a rule?
For this we can look at two measures: accuracy and coverage.

- **Accuracy of a rule**: 
The accuracy of a rule is a measurement for how good the rule is for the instances it covers. 
Different for regression and classification. 
In classification you can measure the missclassification rate of the rule. 
In regression the mean squared error of the rule.
- **Coverage of a rule**:
A rule applies to a certain region of the feature space. 
This is called the coverage of the rule. 
It can also expressed to how many instances in the dataset it applies, or if you know the distribution, then it can even be measured as percentage of the population it covers. 
For example a rule for house value that says: 
A house that is in a nice area costs 400k. 
And 10% percent of all houses are in nice areas. 
Then the rule has a coverage of 10%. 

This example is very nice and clean, because it contains only one rule.  
Usually you need more rules to cover the prediction problem at hand. 
To predict appartment you might not want to learn only one rule, but maybe 10, or 20.
Then things can become more complicated: 

1. Rules can be overlapping: 
What if I want to predict the value for a new flat and two or more rules apply, giving conflicting predictions?
1. No rule applies:
What if I want to predict the value for a new flat and none of the rules apply?

In case of multiple rules, we distinguish lists (ordered) and sets (unordered) of rules:  

- **Decision list**: 
An ordered list of conjunctive rules (order is important). 
They are basically one-sided decision trees. 
The first rule is the root node. 
One of the child nodes is already the terminal node with the prediction when the first rule applies. 
The second child is the second rule. 
This repeats for all the rules. 
Decision lists solve the problem of rule overlaps by introducing an order to the rules.
Easy to make a prediction if it is an ordered decision list, because the rules are tied in order. 
The first rule where the conditions apply to the instance that shall be classified is used. 
And usually there is some default rule when none of the other rules fires which predicts the majority class of the uncovered instances for example.
- **Decision set**: 
A bunch of rules without an order (in contrast to a decision list). 
In a set, the rules are either mutually exclusive, or there is some strategy to resolve conflicts.
- Mutually exclusive rules: Rules are independent of each other and every recored is covered at most by one rule. 
Conflict resolution if many rules are triggered is often done by some measurement of the quality of the rule (accuracy, coverage, cost-based missclassification, etc.)
To make a prediction, either a single rule applies and we use that prediction or, with multiple rules applying, we use the conflict resolution (e.g. majority vote) to make the prediction.
Interpretability suffers potentially when multiple rules apply.


Both decision rules and decision lists can suffer from the problem that no rule applies to an instance. 
This can be solved by introducing a default rule. 
The default rule is the rule the applies when no other rule applies.
This can be used in both decision lists and decision sets.
The default rule can be the most common outcome of the training data that is not covered by other rules.
If a set or list of rules covers the whole feature space, we call it exhaustive.


There are many ways how to induce those kind of rules from a training dataset.

Methods to learn:
- Directly from data. 
This bullet point covers the most algorithms.
- Decision tree converted to rules.
Each path through the tree is one rule.
Rules made from trees have the nice attribute of covering the whole feature space and being mutually exclusive.
- Learn other model, then extract rules
- Learn assocation rules, then filter for the ones that have the target on the right hand side.


There are lots of ways to build decision rule (lists/sets).
This chapter will show you three of them:

1. Building rules from a single feature.
1. Covering algorithm: Covering the dataset sequentially with rules.
1. Pre-mined rules combination: Pre-mining rules and combining them into a decision list.

Another way is ripping apart decision trees as used in the [RuleFit algorithm](#rulefit).
Let's start with the simplest: Using the single, best feature to learn rules.

### Learn One Rule (OneR)

The OneR algorithm is one of the simplest rule induction algorithm.
It finds the feature that produces the best rules and uses this single feature to create decision rules.
OneR can do classification but not regression.

Despite the name OneR, the algorithm produces more than one rule: 
It's actually one rule per unique feature value of the chosen best feature. 
The name 'OneR' (which stands for one rule) is actually misleading.
A better name might be OneFeatureClassifier or something.

The algorithm is simple and fast:

- Bin all continuous features.
- For each feature, do:
  - Create a cross table between feature levels and outcome classes. 
  - For each feature level:
    - Create a rule which predicts the most frequent class for each level of the feature (can simply be read from the cross table).
  - Calculate the total error of the rules for the feature.
- Choose the feature with the smallest total error.

OneR always covers all instances of the dataset, since it uses all levels of the chosen feature.
Missing values can be either treated as its own level or be imputed before.


Another way to see OneR: 
It's a decision tree with only one split.
This approach can be seen as fitting trees with only depth of only one, and splits are not necessarily binary as in CART.


Let's look at an example how the best feature is chosen.
The following table shows an artificial dataset about houses, containing information about location, size and if pets are allowed. 
We are interested in learning a simple model to predict the value of a house.

```{r OneR-freq-table1}
df = data.frame(
  location = c("good", "good", "good", "bad", "good", "good", "bad", "bad", "bad", "bad"),
  size = c("small", "big", "big", "medium", "medium", "small", "medium", "small", "medium", "small"), 
  pets = c("yes", "no", "no", "no", "only cats", "only cats", "yes", "yes", "yes", "no")
)
value = factor(c("high", "high", "high", "medium", "medium", "medium", "medium", "low", "low", "low"), levels = c("low", "medium", "high"))
```


From this table, OneR creates the cross tables between each feature and the outcome:
```{r OneR-freq-table2}
kable(table(df[,"location"], value))
kable(table(df[,"size"], value))
kable(table(df[,"pets"], value))
```

For each feature, we go through the table row by row: 
Each feature level is the IF-part of a rule and the class with the highest occurence for the instances with this feature level is the prediction, the THEN-part of the rule.
For example, a feature with 5 different levels produces 5 rules.
Then, for each feature we compute the total error rate of the rules that are produced, which is simply the sum of the errors each rule of that feature makes.
For example, the location has possible values, bad and good. 
The predicted price class for houses in bad locations would be `low` and with this we make 2 mistakes, because two houses are actually medium priced.
The predicted price class for houses in good locations would be `high` and with this we make 2 mistakes, because two houses are actually medium priced.
All in all the accuracy of using the location feature is 6/10.
The accuracy is 7/10 for the size feature and 6/10 for the pets feature. 
The size feature produces the rules with the lowest error (3/10) and will be used as the final model:

- IF size=small THEN PREDICT value=small
- IF size=medium THEN PREDICT value=medium
- IF size=big THEN PREDICT value=high


OneR favors features with many possible levels, because it can more easily overfit the target: 
Imagine a dataset with only noise and no signal, meaning that all features take on random values and have no predictive value for the target.
Some features have more levels than others. 
The features with more levels can now more easily overfit. 
The most extreme feature would be one that has a unique level for each instance from the data. 
Now the OneR rule can perfectly overfit on the data. 
Keep that in mind. 

An issue can be ties, i.e. when two features produce the same error rates.
Solutions for ties are either to simply take the first feature with the best performance or the one with the lowest p-value of a chi-squared test.

Let's try it out with some real data.
We use the [cervical cancer classification task](#cervical) to test out the OneR algorithm. 
The following rules are created:
```{r oner-cervical}
library("OneR")
data("cervical")
rule = OneR::OneR(Biopsy ~ ., data = cervical)

rule.to.table = function(rule){
  dt = data.frame(x = names(rule$rules), prediction = unlist(rule$rules))
  colnames(dt) = c(rule$feature, "prediction")
  dt
}

kable(rule.to.table(rule))
```
The age feature was chosen by OneR as the best predictive feature. 
Since cancer is rare and for each rule, the majority is still healthy, the prediction is still always 'Healthy'. 
Using the OneR algorithm is not useful in this case.
The cross table between age category and cancer/healthy looks like this:

```{r oner-cervical-confusion}
kable(table(bin(cervical$Age), cervical$Biopsy))
```


We can also use the OneR algorithm for regression task by binning the continuous outcome into a few levels.
This turns the regression problem into a classification problem.
That's what we do with the regression task of predicting [bike rentals](#bike):

```{r oner-bike}
data("bike")
bike$days_since_2011 = max(0, bike$days_since_2011)
rule = OneR::OneR(cnt ~ ., data = bike)

kable(rule.to.table(rule), row.names = FALSE)
```

The chosen feature is the year (which covers a time trend).
In 2012 there are more bike rentals than in 2011, probably because the rental service became more popular.

Now we move from a very simple algorithm to a more complex procedure (but still handable though).



### Sequential Covering

Sequential covering is a procedure that uses an algorithm that can learn single rules to seequentially produce a decision list that covers the whole feature space:

A very simple approach: 
Find a really good rule for the data, which covers part of it. 
Remove all datapoints covered by the rule, that is, any data point for which the conditions apply, no matter if classified correctly or not. 
Repeat rule learning and removal of covered points with the remaining points until no points are left or some other stop condition is met.
The result is a decision list.

The approach of covering a part of the data, removing it and repeating is also called separate-and-conquer.


For now, let's assume we already have an algorithm that can produce a single rule that covers part of the data. 
Then the covering algorithm for two classes (one positive, one negative) goes like this:

- Start with an empty list of rules ($rlist$).
- Learn a rule $r$.
- While the rule set has not reached a quality threshold or some positive examples are not covered yet:
  - Add rule $r$ to $rlist$.
  - Remove the data that is covered by rule $r$.
  - Learn another rule on the remaining data.
- Return the decision list.

In case of more than two target classes:
First the classes are ordered by increasing prevalence.
RIPPER starts with the least prevalent class, learns a rule for it, removes all instances covered, then moves on to the second least prevalent class and so on. 
The current class is always treated as the positive class and all classes with higher prevalence as the single negative class.
The last class is the default rule.
This is also called the one-versus-all strategy in classification.


This second part of repeating the single rule fitting is also called covering algorithm. 



```{r covering-algo, fig.cap = "The covering algorithm works by sequentially covering the feature space with single rules and removing the data points that are already covered by those rules. The approach is called separate-and-conquer."}
set.seed(42)
n = 100
dat = data.frame(x1 = rnorm(n), x2 = rnorm(n))
dat$class = rbinom(n = 100, size = 1, p = exp(dat$x1 + dat$x2) / (1 + exp(dat$x1 + dat$x2)))
dat$class = factor(dat$class)

min.x1 = min(dat$x1)
min.x2 = min(dat$x2)
p1 = ggplot(dat) + geom_point(aes(x = x1, y = x2, color = class))+ 
  scale_color_discrete(guide = "none")
p2 = ggplot(dat) + 
  geom_rect(xmin = -3, xmax = 0,   ymin = -2, ymax = -0.5, color = "black", fill = NA) + geom_point(aes(x = x1, y = x2, color = class)) + 
  scale_color_discrete(guide = "none")

dat.reduced = filter(dat, !(x1 <= 0 & x2 <= -0.5))

p3 = ggplot(dat.reduced) + 
  geom_rect(xmin = -3, xmax = 0,   ymin = -2, ymax = -0.5, color = "black", fill = NA)  + 
  geom_point(aes(x = x1, y = x2, color = class)) + 
  scale_x_continuous(limits = c(min.x1, NA)) + 
  scale_y_continuous(limits = c(min.x2, NA)) + 
  scale_color_discrete(guide = "none")

gridExtra::grid.arrange(p1, p2, p3, ncol = 2)

```
For example: 
We have a house value dataset and no rules learned yet. 
We learn the first rule, which turns out to be: 
'If the house is big and in a good neighbourhood, then the value of the house is high'. 
Then we remove all the big houses in a good neighbourhood.
With the remaining data we search the next rule: 
Maybe: 'If the house is in a good neighbourhood, then the value is medium' (Note that this rule is learned on data with big houses in good neighbourhoods removed, so only medium and small houses in good neighbourhoods remain).


But how do we generate a single rule? 
The OnR algorithm would not work, since it would already cover the whole feature space.
But ther are many other ways. 
One is learning a single rule with beam search:

- Build a decision tree (can be a CART or some other algorithm).
- Start at the root node and recursively choose the purest node (e.g. lowest missclassification rate)
- The majority class of the node we end up in is selected as the rule's prediction and the split conditions in the features leading to that node are the rule conditions.

The following figure visualizes the search:

```{r learn-one-rule, fig.cap = "Learning one rule by using a decision tree. A decision tree is grown to predict the target of interest. To extract a single, accurate rule, we go at each split to the node which locally produces the purest subset (i.e. with highest accuracy of target classes) and add the split value to the  rule condition. The prediction of the rule is the majority class of the node we land in. This examples shows how we learn a rule for predicting the value of a house. A decision tree is grown and we follow each time the node with the highest accuracy. In this case we end up with: 'If the location is the house is big and in a good location, we predict that the value is high (assuming high as the most common class in that node)'"}
knitr::include_graphics("images/learn-one-rule.png")
```

Learning a single rule is equivalent to a search problem, where the search space is the space of all possible rules. 
The goal of the search is to find the best rule (according to some criteria).
There are many different search strategies: 
hill-climbing, beam search, exhaustive search, best-first search, ordered search, stochastic search, top-down search, bottom-up search, ...


RIPPER (Repeated Incremental Pruning to Produce Error Reduction) by Cohen (1995)[^ripper] is a variant of the Sequential Covering algorithm.
RIPPER is a bit more sophisticated and uses as post-processing phase (rule pruning) for the optimization of the rule-set.

RIPPER can run in ordered or unordered mode, either producing a decision list or decision set.


Let's turn to the data examples.
On the classification task for [cervical cancer](#cervical), the RIPPER algorithm does not find any rule:
```{r jrip-cervical}
library("RWeka")
library(rJava)

extract.rules.jrip = function (rule) {
rules = scan(text=.jcall(rule$classifier, "S", "toString"), sep="\n", what="")
# removes text
rules = rules[-c(1, 2, length(rules))]
rules = gsub("\\([0-9]*\\.[0-9]\\/[0-9]*\\.[0-9]\\)", "", rules)
rules = as.matrix(rules)[-c(1:2, 6), ,drop=FALSE]
rules  = data.frame(rules)
kable(rules)
}

rule = JRip(Biopsy ~ ., data = cervical)
extract.rules.jrip(rule)

```


When we use RIPPER on the regression task for predicting [bike rentals](#bike) some rules are found. 
Since RIPPER only works for classification, the bike counts has to be turned into a categorial outcome. 
I achieved this by binning the bike counts into the quartiles.

The following table shows the resulting rules. 
The conditions are followed behind the arrow by the predicted interval. 
For example $(4548, 5956)$ is the interval that covers predicted bike rentals between $4548$ and $5956$.
```{r jrip-bike}
bike2 = bike
bike2$cnt = round(bike2$cnt)
bike2$cnt =  cut(bike$cnt, breaks = quantile(bike$cnt), dig.lab = 10, include.lowest = TRUE)

rule = JRip(cnt  ~ ., data = bike2)
extract.rules.jrip(rule)
```
The interpretation is straightforward:
If the conditions apply, then the predicted (interval) of bike rentals is the interval on the right hand side.
The last rule is the default rule when none of the other rules fires for an instance. 
For predicting a new instance, start at the top of the list and check if a rule fires. 
When all conditions match, then the right hand side of the rule is the prediction for this instance. 
Because of the default rule it is guaranteed that there will always be a prediction.


### Bayesian Rule Lists

In this section I will show you another approach to learn a decision list, which follows this rough recipe:

1. Pre-mine frequent patterns from the data (unsupervised), which can be used as conditions (IF-part) for IF-THEN rules.
1. Construct a decision list from a selection of the pre-mined rules.

One specific approach that uses this recipe is called (Scalable) Bayesian Rule Lists (Letham et. al, 2015)[^brl], which are decision lists that use methods from Bayesian statistics to learn decision lists from rules pre-mined with the FP-tree algorithm (Borgelt 2005)[^fp-tree]

But let's start slowly:

**Pre-mining of frequent patterns**

A frequent pattern is the frequent (co-)occurence of feature levels.
As a pre-processing step for the BRL algorithm, we use the features (at this step we don't need the target outcome) and extract freqently ocurring patterns from them.
A pattern can be a single feature value (e.g. $\text{size=medium}$) or a combination of feature values, like $\text{size=medium}\land\text{location=bad}$.

The frequency of a pattern is measured with its support in the dataset:

$$Support(x_j=A)=\frac{\sum_{i=1}^nI(x_{ji}=A)}{n}$$

where $x_j=A$ is the feature level, $n$ the number of data points in the dataset and $I$ the indicator function which is $1$ if the feature has level $A$ else $0$.
For example we if 20% of the houses have no balcony (and 80% one or more), then the support for the pattern $x_{balcony}=0$ would be 20%.
The support can also be measured for combinations of feature levels, for example the support for $\text{balcony=0}\land\text{pets=allowed}$.

There are many algorithms to find such frequent patterns, for example the Apriori algorithm or the FP-Growth tree. 
Which one you use does not matter much, only the speed at which the patterns are found is different, but the resulting patterns are always the same.

I'll give you a rough idea how the Apriori algorithm works for finding frequent patterns.
Actually the Apriori algorithm has two parts, where the first part finds frequent patterns and the second part creates association rules.
For this application, we are only interested in the frequent patterns generated in the first part. It is also possible to use the Apriori  

For the first step, the Apriori algorithm starts with all feature levels that have a support larger than the minimal support defined by the user. 
If the use says minimal support should be 10% and there are only 5% big houses, we would remove that feature value and only keep house=medium and house=big as patterns. 
This does not mean that the houses are removed, it just means that $\text{size=big}$ will not be considered for an association rule.
From the patterns with a single feature value, the Apriori algorithm tries to find iteratively combinations of higher order 2 to number of features.
Patterns are constructed by adding a feature value a logical AND, e.g. $\text{size=medium}\land\text{location=bad}$.
Generated patterns with a support below the minimum support are removed.
In the end we have all the frequent patterns.
Any subset of a frequent pattern is again frequent, that's the the apriori property. 
It intuitively makes sense: By removing a condition from a pattern, the reduced pattern can only cover more or the same number of data points (support), but never less. 
For example, if 20% of the houses are $\text{size=medium}\land\text{location=good}$, then the support of houses that are only $\text{size=medium}$ is 20% or larger.
The apriori property is used to reduce the number of patterns we have to check. 
Only for frequent patterns we have to check patterns of higher order.

Now we are done with pre-mining conditions for the Bayesian Rule List algorithm.
But before we go into the details of BRL, I want to show you another way. 
Other approaches suggest including the outcome of interest into the frequent pattern mining process and also executing the second part of the Apriori algorithm, which builds IF-THEN rules. 
Since the algorithm is unsupervised, the THEN-part also contains feature values we are not interested in. 
But we can filter for rules that only have the outcome of interest in the THEN-part.
These rules already form a decision set, but it would also be possible to rank the rules, prune them, delete some or re-combine them somehow. 

In the current approach however, we only need the frequent patterns and the learn the THEN-part and how to arrange the patterns into a decision list using Bayesian statistics.


**(Scalable) Bayesian Rule Lists**

We want to reduce the number of the pre-mined conditions, prioritize short conditions over long ones and create a decision list (=with order).
The Bayesian Rule List (BRL) approach solves this by introducing a likelihood function with some prior distributions for the length of conditions (preferring shorter ones) and the number of rules (also preferring less rules over more). 

BRL can be used in any multi-class classification setting and requires all input features to be categorized.
Continuous features can be used if cut into intervals and made into a categorical feature (either fixed intervals or by distribution, i.e. quantiles).
The BRL approach defines a probability distribution of lists (kind of meta, right?), which let's us attach a likelihood to a list and we try to find the most likely list according to the distribution which incorporates both the assumptions of shortness and how well a list fits the data.
Since it's not so trivial to sample lists from the distributions of lists directly, the BRL has the following recipe:
1) Generate a (somewhat random) initial decision list 2) Iteratively change the list by adding/switching/removing rules, while making sure that the lists are from the distribution of lists and 3) choose the decision list from the sampled lists which has the highest likelihood.

Let's go through the the algorithm in more detail:
The algorithm starts with pre-mining rules with the FB-Growth algorithm, but theoretically any rule-mining algorithm would work.
Why didn't I present you the FP-Growth algorithm, but instead the Apriori algorithm?
Well, because they will yield the same results. 
The Apriori is easier to convey and the FP-Growth algorithm runs faster.
FP-Growth uses FP-trees a data structure that represents the patterns found in a dataset.

BRL makes the a bunch of assumptions about the distribution of the target and the distribution of the parameters that define the distribution of the target.
Well, that's Bayesian statistic. 
If you are unfamiliar with Bayesian statistic, don't get too caught up in the following explanations. 
It's important to know that the Bayesian approach is a way to combine in a model pre-existing knowlegde or requirements (called priori distributions) while also fitting the data.
In the decision list case, the Bayesian approach is useful because through the prior assumptions, it nudges the decision list to be short with also short rules.
The distribution assumption of the target looks like this:

**if** $\text{conditions}_1$ **then** $y\sim\text{Multinomial}(\theta_1)$, $\theta_1\sim\text{Dirichlet}(\alpha+N_1)$

**else if** $\text{conditions}_2$ **then** $y\sim\text{Multinomial}(\theta_2)$, $\theta_2\sim\text{Dirichlet}(\alpha+N_2)$

...

**else if** $\text{conditions}_m$ **then** $y\sim\text{Multinomial}(\theta_m)$, $\theta_m\sim\text{Dirichlet}(\alpha+N_m)$

**else**  $y\sim\text{Multinomial}(\theta_0)$, $\theta_0\sim\text{Dirichlet}(\alpha+N_0)$


The goal is to sample decision lists $d$ from: 

$$p(d|x,y,A,\alpha,\lambda,\eta)\propto{}p(y|x,d,\alpha)\cdot{}p(d|A,\lambda,\eta)$$
where $d$ is a decision list, $x$ are the features, $y$ is the target, $A$ the set of pre-mined rules, $\lambda$, the prior expected decision list length, $\eta$, the prior expected number of conditions in a rule, $\alpha$, the prior pseudo-count for the positive and negative classes (best fixed at $(1,1)$).
The term on the left side of the equation says how likely a list is, given the observed data and the priori assumptions. 
This is proportional to the likelihood of the outcome of interest given the list and the data times the likelihood of the list given prior assumptions and the pre-mined rules.
This means a decision list has a high likelihood if it explains the outcome $y$ well and is also likely according to the prior assumptions.

Bayesian statistics is always a bit tricky, because we usually can't simply compute the correct answer, but we have to draw candidates and evaluate them and update our posteriori estimates (usually and also here with Monte Carlo Markov Chains)
Here it is even more tricky, because we have to draw from the distribution of decision lists. 
The authors suggest following recipe to sequentially draw an initial decision lists. 
This recipe also shows the prior assumptions we make about the list length and the rule lengths.

- Pre-mine rules.
- Sample the length $m$ for the list from a truncated Poisson distribution.
- For the default rule: Sample the parameter $\theta_0$ for the distribution of the target value (i.e. the rule that applies when nothing else applies).
- For decision list rule $j=1,\ldots,m$, do:
  - Sample the length of the rule $l_j$ (i.e. number of conditions) for rule $j$
  - Sample a rule of length $l_j$ from the pre-mined rules.
  - Sample the distribution parameter of the THEN-part given the sampled rule (i.e. for the distribution of the target outcome given the rule)
- For each observation in the dataset:
  - Find the rule from the decision list that fires first
  - Draw the predicted outcome from the probability distributiion suggested by the rule that applies.


Once we can sample from the distribution of the decision lists, we can find the decision list that maximizes the posteriori likelihood, meaning it explains the data well (acccuracy), but is also interpretable because it has few and short rules, which is due to the priori distributions.

Since we can't directly optimize this problem, we do Metropolis-Hastings sampling: 
Starting from a random draw, we randomly either 1) move a rule to a different position in the list or 2) add a rule to the current decision list from the pre-mined rules 3) Remove a rule from the decision list. 
Which of the rules is switched/added/deleted is also random.
At each of this step, the algorithm evaluates the posteriori probability of the decision list (mix of how accuract it is and how much it satisfies the priori distributions).
The Metropolis Hastings algorithm ensures that we sample more heavily the decision lists that are likely in terms of accuracy and prior.
This procedure gives us lots of samples from the distribution of decision lists (a list of decision lists). 
We end up with the posterior distribution of decision lists, but of course we would like to have a single decision list and not a distribution. 
The BRL algorithm selects the decision list of the samples with the highest posterior probability.

The examples are using a faster variant of BRL called Scalable Bayesian Rule Lists (SBRL) by Yang et. al (2016) [^sbrl].

Let's see the SBRL in action.
First we apply the SBRL algorithm to predict [risk for cervical cancer](#cervical).
I had to discretize all input features first to make the SBRL algorithm work. 
For that I binned the continuous features based on the frequency of the values (by quantiles): 
If a continuous features is binned into 3 intervals by frequency, then the first interval contains 33% of the instances with the lowest feature value, then the second interval with the ones higher, then the rest with the heighest values.

We get the following rules:

```{r sbrl-cervical}
library("sbrl")
library("arules")
data("cervical")

cervical2 = as.data.frame(lapply(cervical, function(x) {
  if(is.factor(x) || length(unique(x)) < 3) {
    as.factor(x)
  } else {
    discretize(x, breaks = 3, method = "interval")
    #discretize(x, breaks = max(length(unique(x))-1, 5))
  }
}))

get.sbrl.rules = function(x) {
    res = lapply(1:nrow(x$rs), function(i) {
        if (i == 1) 
            sprintf("If      %s (rule[%d]) then positive probability = %.8f\n", 
                x$rulenames[x$rs$V1[i]], x$rs$V1[i], x$rs$V2[i])
        else if (i == nrow(x$rs)) 
            sprintf("else  (default rule)  then positive probability = %.8f\n", 
                x$rs$V2[nrow(x$rs)])
        else sprintf("else if %s (rule[%d]) then positive probability = %.8f\n", 
            x$rulenames[x$rs$V1[i]], x$rs$V1[i], x$rs$V2[i])
    })
    data.frame(rules = unlist(res))
}


cervical2$label = cervical2$Biopsy
cervical2$Biopsy = NULL
rules = sbrl(cervical2, pos_sign = "Cancer", neg_sign = "Healthy", rule_maxlen = 2)
kable(get.sbrl.rules(rules))
```
Notice how we get meaningful rules, since the prediction on the THEN-part is not the class outcome, but the predicted probability for cancer.

The conditions were chosen from patterns that were pre-mined with the FP-Growth algorithm. 
The following table displays the pool of conditions from which the SBRL algorithm could choose from by building a decision list.
The maximum length allowed by me as a user was patterns with two conditions. 
Here is a sample of ten patterns:

```{r sbrl-cervical-premined}
set.seed(1)
kable(sample(rules$rulenames, size = 10), col.names = "pre-mined conditions")
```

Next we apply the SBRL algorithm to the [bike rental prediction task](#bike).
This only works by turning the regression problem of predicting bike counts into a binary classification task. 
I arbitrarily created a classification task by creating a label that says if the bike counter was larger than 4000 bikes.

```{r sbrl-bike}
library("sbrl")
library("arules")
data("bike")

bike2 = bike
bike2$label = bike2$cnt > 4000
bike2$cnt = NULL
bike2 = as.data.frame(lapply(bike2, function(x) {
  if(is.factor(x) || length(unique(x)) < 3) {
    as.factor(x)
  } else {
    discretize(x, breaks = 3, method = "interval")
    #discretize(x, breaks = max(length(unique(x))-1, 5))
  }
}))
rules = sbrl(bike2, pos_sign = TRUE, neg_sign = FALSE, rule_maxlen = 3)
kable(get.sbrl.rules(rules))
```



### Advantages

This section covers the advantages of IF-THEN rules in general.

- IF-THEN rules are **easy to interpret**.
They are probably the easiest from all interpretable models.
This statement applies only if the number of rules is short, the conditions of the rules are short (maximum 3 I would say) and if the rules are organized in a decision list or a non-overlapping decision set.
- Decision rules can be **as expressive as decision trees, while being more compact**. 
Decision tree often has replicated subtrees.
- There are many ways to create decision rules.
- The **prediction with IF-THEN rules is fast**, since only a few binary statements have to be checked to see which rules apply.
- Compared to logistic regression and linear models: 
No assumption about distributions of features or target.
- Decision rules are **robust** against monotone transformations of the input features, because the only thing that changes is the threshold in the condition.
They are also robust against outliers, since the only thing that matters is if a condition applies or not.
- IF-THEN rules usually generate very sparse models, meaning not many features are included.
They **select only the relevant features** for the model.
A linear model for example, by default, assigns a weight to every input feature.
Features that are irrelevant can simply be ignored by IF-THEN rules.
- Simple rules like from OneR **can be used as baseline** for more complex algorithms.

### Disadvantages 

This section covers the disadvantages of IF-THEN rules in general.


- The research and literature for IF-THEN rules is focused on classification and almost **completely neglecting regression**.
While it's always possible to categorize a continuous target and turn it into a regression problem, it's always a loss of information.
In general approaches a more attractive when they can be used for both regression and classification.
- Often the **features also have to be categorical**.
That means numerical features have to be binned, if you want to use them. 
There are many ways to cut a continuous feature into intervals, but this is not as trivial and comes with many questions, where there is usually not a correct answer:
Into how many intervals should the feature be split? 
What's the splitting criteria: Fixed interval lengths or quantiles or something else?
Dealing with binning of continuous features is a non-trivial issue that is often neglected and one of the option simply applied (like I did in my examples).
- Many of the older rule fitting algorithms are prone to overfitting. 
The algorithms presented here all have at least some safeguards to prevent overfitting: 
OneR is limited because it can only use a single feature (only problematic if the feature has too many levels or if there are lots of features, which equates to the multiple testing problem), RIPPER does pruning and Bayesian Rule Lists impose a prior distribution on the decision lists.
- Decision rules are **bad at describing linear relationships** between the features and the output.
That's an issue they share with decision trees. 
Decision trees and rules can only produce step-like prediction functions, where the prediction sharply changes when a feature changes and not linearly with a features.
This is related with the issue that the inputs have to be categorical (in decision trees they are implicitly made categorical by finding split points in them).







### Software and Alternatives

- There is an [R package `OneR`](https://cran.r-project.org/web/packages/OneR/). 
OneR is also implemented in the [Weka machine learning library]((https://www.eecs.yorku.ca/tdb/_doc.php/userg/sw/weka/doc/weka/classifiers/rules/package-summary.html)) and as such available in Java, R and Python.
- RIPPER is also implemeted in Weka. For the examples I used the R implementation of JRIP [RWeka package](https://cran.r-project.org/web/packages/RWeka/index.html). 
- SBRL is vailable as [R package](https://cran.r-project.org/web/packages/sbrl/index.html) (which I used in for the examples), in [Python](https://github.com/datascienceinc/Skater) or as [pure C implementation](https://github.com/Hongyuy/sbrlmod).

I won't even attempt to create an exhaustive list of alternatives for learning decision rule sets/lists, but rather point towards some more recent research and summarizing work. 

- I recommend the book 'Foundations of Rule Learning' from Fuernkranz et. al (2012)[^fuernkranz]. 
It's an extensive work on rule learning, for the ones who want to dive deeper into the topic.
It gives a holistic framework for thinking about rule learning and features many rule learning algorithms.
- I also recommend to checkout [Weka rule learners](https://www.eecs.yorku.ca/tdb/_doc.php/userg/sw/weka/doc/weka/classifiers/rules/package-summary.html), which implements RIPPER, M5Rules, OneR, PART and many more.
- Another approach for building decision lists, that have some guarantee to deliver an optimal decision list structure, is called [CORELS](https://github.com/nlarusstone/corels) Certifiable Optimal RulE ListS).



[^corels]: Angelino, Elaine, et al. "Learning Certifiably Optimal Rule Lists." Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2017.

[^fuernkranz]: Fuernkranz, J., Gamberger, D., & Lavrač, N. (2012). Foundations of Rule Learning. Foundations of Rule Learning. https://doi.org/10.1007/978-3-540-75197-7_2

[^oner]: R. C. Holte (1993). Very simple classification rules perform well on most commonly used datasets. Machine Learning, 11, 63–91.

[^ripper]: Cohen, W. (1995). Fast effective rule induction. Icml. Retrieved from http://www.inf.ufrgs.br/~alvares/CMP259DCBD/Ripper.pdf

[^c4.5]: Quinlan, J. R. (1987). Generating production rules from decision trees. Proceedings of the Tenth International Joint Conference on Artificial Intelligence, 30107, 304–307. Retrieved from http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.9054&amp;rep=rep1&amp;type=pdf

[^brl]: Letham, B., Rudin, C., McCormick, T. H., & Madigan, D. (2015). Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model. Annals of Applied Statistics, 9(3), 1350–1371. https://doi.org/10.1214/15-AOAS848

[^sbrl]: Yang, H., Rudin, C., & Seltzer, M. (2016). Scalable Bayesian Rule Lists, 31. Retrieved from http://arxiv.org/abs/1602.08610

[^fp-tree]: Borgelt, C. (2005). An implementation of the FP-growth algorithm. Proceedings of the 1st International Workshop on Open Source Data Mining Frequent Pattern Mining Implementations - OSDM ’05, 1–5. http://doi.org/10.1145/1133905.1133907
