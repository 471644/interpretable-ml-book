```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all("../")
set.seed(42)
```

# IF-THEN rules

IF-THEN rules (aka decision rules) are simple statements consisting of a series of conditions and a prediction:
IF the all of the conditions apply, THEN we make a certain prediction.
An example: 
IF it rains today (condition), THEN it will rain tomorrow (prediction). 

keywords: decision rules/sets/lists, association rules

There are lots of algorithms how to produces rules. 
This chapters purpose is to help you how to think about rules and understand them fundamentally, but not to go through all possibilities. 
For this I recommend XXX.

## What are decision rules?

Decision rules are models that consists of if X then Y statements. 
Within the if statement there are sets of features and conditions on them (like X > 4) and Y is a statement about the outcome of interest.

Simple if-then statements can and are often written manually. 
In machine learning, these types of rules are learned automatically.
Can be derived from decision trees, but don't have to.

They have the following general structure:
RULE: IF CONDITIONS THEN TARGET


One decision rule from a model predicting rents could be:
If a flat is bigger than 100 square meters and has a garden the average rent is 2000 Euro per month.

Let's split that up:
- 'Flat is bigger than 100 square meters' is the first part of the IF statement
- 'Flat has a garden' is a second conditions in the IF.
- Both are connected with an 'AND', so both have to be true for the rule to apply
- Predicted outcome (THEN) is that the average rent is 2000 Euro per month.



Arguably decision rules are the most simple classifier in terms of understandability. They are very close to natural language and the way we think. Of course it relies on using sensible, understandable features and the number of conditions within the if statement.

Usually there is more then one rule to describe the associations of the features with the outcome. In our example we only have one rule covering big appartments with flats, but what about a smaller apartment?

Rules can be induced from trees. Each path through the tree is one rule. 
Rules made from trees have the nice attribute of covering the whole feature space and being mutually exclusive.

We can make following distinctions:
- descriptive rule discovery: concerned with finding single rules 
- 
- predictive rule learning: 

## Interpretation

Very easy: IF THEN
depending if rules overlap or not some differences. 

There are many ways how to induce those kind of rules from a training dataset.
The induction algorithms differ in how they create the rules:
- Do they allow overlapping rules and how are conflicts handled?
- Do they cover the whole training set? Or can there be a situation where none of the rules apply?


Conflict resolution if many rules are triggered:
- size ordering: use rule with the toughest requierements
- class based: based on cost of missclass
-rule-based (decision list): by quality of rule


Mutually exclusive rules: Rules are independent of each other and every recored is covered at most by one rule. 

Exhaustive rules: For every possible combination of feature values there is a rule. exhaustive coverage.


A rule applies to a certain region of the feature space. 
This is called the coverage of the rule. 
It can also expressed to how many instances in the dataset it applies, or if you know the distribution, then it can even be measured as percentage of the population it covers. 
For example a rule for house value that says: 
A house that is in a nice area costs 400k. 
And 10% percent of all houses are in nice areas. 
Then the rule has a coverage of 10%. 

The accuracy of a rule is a measurement for how good the rule is for the instances it covers. 
Different for regression and classification. 
In classification you can measure the missclassification rate of the rule. 
In regression the mean squared error of the rule.

Methods to learn:
- Directly from data
- Decision tree converted to rules
- Learn other model, then extract rules

Making a prediction:
Easy if it is an ordere decision list, because the rules are tied in order. 
The first rule where the conditions apply to the instance that shall be classified is used. 
And usually there is some default rule when none of the other rules fires which predicts the majority class of the uncovered instances for example.
It becomes difficult when there is an unordered set of decision rules, because two problems can occur:

1. The rules might be overlapping  and giving conflicting predictions. 
1. It might happen that no rule applies at all to an instance. 

These problems can be solved by 1) introducing a voting mechanism and 2) introducing a default rule. 


You can use a rule-learning algorithm that learns a single rule. 
And use that within sequential covering, where training records covered by this rule are removed.



We can distinguish between: 

## Theory: OneR

Simplest imaginable, which also covers the whole space: 
Find the best (single) feature that explains the classification the best. 
Continuous features are binned to make the categorical features.
For all features, a cross table between feature and outcome classes are built. 
It's like a decision tree with only one split.


OneR Algorithm		
- For each feature
  - For each value of that feature:
  - Create a rule which predicts the most frequent class for each level of the feature.
  - Calculate the total error of the rule.
- Choose the feature with the smallest total error.

It's that easy.
And it is guaranteed that it always covers the whole feature space. 

http://www.saedsayad.com/oner.htm
https://cran.r-project.org/web/packages/OneR/
https://github.com/vonjd/OneR



TODO: Example with table for how to compute the accuracy. Something similar to: http://www.saedsayad.com/oner.htm

```{r OneR-freq-table1}
df = data.frame(
  location = c("good", "good", "good", "bad", "good", "good", "bad", "bad", "bad", "bad"),
  size = c("small", "big", "big", "medium", "medium", "small", "medium", "small", "medium", "small"), 
  pets = c("yes", "no", "no", "no", "only cats", "only cats", "yes", "yes", "yes", "no")
)
value = factor(c("high", "high", "high", "medium", "medium", "medium", "medium", "low", "low", "low"), levels = c("low", "medium", "high"))
```
From this table we create all the cross tables for each feature with the class to predict:
```{r OneR-freq-table2}
kable(table(df[,"location"], value))
kable(table(df[,"size"], value))
kable(table(df[,"pets"], value))
```

For a feature, we go through the table row by row: 
We use the class that has the highest occurence per feature value as the prediction and count how often it would be wrong.
For example, the location has possible values, bad and good. 
The predicted price class for houses in bad locations would be `low` and with this we make 2 mistakes, because two houses are actually medium priced.
The predicted price class for houses in good locations would be `high` and with this we make 2 mistakes, because two houses are actually medium priced.
All in all the accuracy of using the location feature is 6/10. 
For the other rules it is 7/10 for the size feature and 6/10 for the pets feature 

Strictly speaking, we produce more than one rule: 
It's actually one rule per unique feature value of the chosen best feature. 
The name 'OneR' (which stands for one rule) is actually misleading.
A better name might be OneFeatureClassifier or something.

OneR favors features with many possible levels, because it can more easily overfit the target: 
Imagine a dataset with only noise and no signal, meaning that all features take on random values and have no predictive value for the target.
Some features have more levels than others. 
The features with more levels can now more easily overfit. 
The most extreme feature would be one that has a unique level for each instance from the data. 
Now the OneR rule can perfectly overfit on the data. 
Keep that in mind. 
TODO: Explain strategy for ties.
TODO: List strategies for binning of continuous features. There was some blogpost about it.

We use the [cervical cancer classification task](#cervical) to test out the OneR algorithm. 
The following rule is created:
```{r oner-cervical}
library("OneR")
data("cervical")
rule = OneR::OneR(Biopsy ~ ., data = cervical)

rule.to.table = function(rule){
  dt = data.frame(x = names(rule$rules), prediction = unlist(rule$rules))
  colnames(dt) = c(rule$feature, "prediction")
  dt
}

kable(rule.to.table(rule))
```
The age feature is chosen as the best predictive rule feature. 
Since cancer is rare and the features just increase the risk a little, the prediction is still always 'Healthy'. 
Using the OneR algorithm is not useful in this case.

We can also use the OneR algorithm for regression task by binning the continuous outcome into a few levels.
This turns the regression problem into a classification problem.
That's what we do with the regression task of predicting [bike rentals](#bike):

```{r oner-bike}
data("bike")
bike$days_since_2011 = max(0, bike$days_since_2011)
rule = OneR::OneR(cnt ~ ., data = bike)

kable(rule.to.table(rule), row.names = FALSE)
```

The chosen feature is the year (which covers a time trend).
In 2012 there are more bike rentals than in 2011, probably because the rental service became more popular.

It should also be possible to use a continuous featue as outcome and predict the mean value per feature level instead of most frequent class. 
The best feature can be chosen by selecting the one with the lowest mse.

## Theory: Sequential Covering

A simple algorithm to learn a single rule + a procedure to sequentially learn rules with the simple algorithm sequentially to eventually cover the whole feature space:

A very simple approach: 
Find a really good rule on the dataset. 
Remove all datapoints covered by the rule (no matter if classified correctly or not). 
Repeat with remaining points until no points are left or some other stop condition is met.
Results in disjunct rules.

While there are many different ways to induce decision rules, I want to present a rather simple procedure which sequentially searches for rules that cover parts of the feature space, using the separate-and-conquer approach.

This approach can be seen as fitting trees with only depth of only one, and splits are not necessarily binary as in CART.

The covering algorithm for two classes (one positive, one negative):

- Start with an empty list of rules ($rlist$)
- Learn a rule $r$ using learn-one-rule with all the data
- while the rule set has not reached a quality threshold or some positive examples are not covered yet:
  - Add rule $r$ to $rlist$
  - Remove the data that is covered by rule $r$
  - use learn-one-rule on the reamining data
- Return set of rules (sometimes called cover)

In case of more than two target classes, we repeate the covering algorithm for each class, where the current class is treated as the positive class and all the others as the negative class.
This is also called the one-versus-all strategy in classification.
This also leads (possibly) to overlapping rules. 
A solution: Assign weights to each rule and apply the rules with higher weights.


For example: 
We have a house value dataset and no rules learned yet. 
We learn the first rule, which turns out to be: 
'If the house is big and in a good neighbourhood, then the value of the house is high'. 
Then we remove all the big houses in a good neighbourhood.
With the remaining data we search the next rule: 
Maybe: 'If the house is in a good neighbourhood, then the value is medium' (Note that this rule is learned on data with big houses in good neighbourhoods removed, so only medium and small houses in good neighbourhoods remain).




This second part of repeating the single rule fitting is also called covering algorithm. 



```{r covering-algo, fig.cap = "The covering algorithm works by sequentially covering the feature space with single rules and removing the data points that are already covered by those rules. The approach is called separate-and-conquer."}
set.seed(42)
n = 100
dat = data.frame(x1 = rnorm(n), x2 = rnorm(n))
dat$class = rbinom(n = 100, size = 1, p = exp(dat$x1 + dat$x2) / (1 + exp(dat$x1 + dat$x2)))
dat$class = factor(dat$class)

min.x1 = min(dat$x1)
min.x2 = min(dat$x2)
p1 = ggplot(dat) + geom_point(aes(x = x1, y = x2, color = class))+ 
  scale_color_discrete(guide = "none")
p2 = ggplot(dat) + 
  geom_rect(xmin = -3, xmax = 0,   ymin = -2, ymax = -0.5, color = "black", fill = NA) + geom_point(aes(x = x1, y = x2, color = class)) + 
  scale_color_discrete(guide = "none")

dat.reduced = filter(dat, !(x1 <= 0 & x2 <= -0.5))

p3 = ggplot(dat.reduced) + 
  geom_rect(xmin = -3, xmax = 0,   ymin = -2, ymax = -0.5, color = "black", fill = NA)  + 
  geom_point(aes(x = x1, y = x2, color = class)) + 
  scale_x_continuous(limits = c(min.x1, NA)) + 
  scale_y_continuous(limits = c(min.x2, NA)) + 
  scale_color_discrete(guide = "none")

gridExtra::grid.arrange(p1, p2, p3, ncol = 2)

```


But how do we get a single rule?
There are many ways, one is using a greedy depth search in a decision tree.
a simple algorithm to learn a single rule in a classification setting:

Learning a single rule with beam search:
- Build a decision tree (can be CART or some other algorithm) on the input data. 
- Start at the root node and always choose the node that is the most pure (e.g. lowest missclassification rate)
- The majority class of the node we end up in is selected as the rule's prediction, the split conditions in the features leading to that node are the rule conditions.

To avoid local optima, a beam search can be performed which searches the search more exhaustively.

Learning a single rule is equivalent to a search problem.
The search space is the space of all possible rules. 
And the search is for the best rule (according to some criteria).
There are many different search strategies: 
hill-climbing, beam search, exhaustive search, best-first search, ordered search, stochastic search, top-down search, bottom-up search, ...
Here is not the place to go into details. 

```{r learn-one-rule, fig.cap = "Learning one rule by using a decision tree. A decision tree is grown to predict the target of interest. To extract a single, accurate rule, we go at each split to the node which locally produces the purest subset (i.e. with highest accuracy of target classes) and add the split value to the  rule condition. The prediction of the rule is the majority class of the node we land in. This examples shows how we learn a rule for predicting the value of a house. A decision tree is grown and we follow each time the node with the highest accuracy. In this case we end up with: 'If the location is the house is big and in a good location, we predict that the value is high (assuming high as the most common class in that node)'"}
knitr::include_graphics("images/learn-one-rule.png")
```


Difference to OneR algorithm: 
We want to learn a very accurate rule, that is allowed to cover only part of the feature space. 
Therefor we allow it to have many conjunction (AND), which the OneR does not include.




Let's look at one implementation of the Sequential covering algorithm: RIPPER (Repeated Incremental Pruning to Produce Error Reduction; Cohen, 1995). 
RIPPER is a bit more sophisticated and uses as post-processing phase (rule pruning) for the optimization of the rule-set.
RIPPER learns unordered rule sets. 
For rule conflict resolution it employs weighting of rules. 
The rules are weighted by the Laplace-value of a rule, which is used for making the final prediction.
The laplace estimate is a corrected estimate of the accuracy of a rule: 

$$ Laplace(rule) = \frac{pos + 1}{(pos + 1) + (neg + 1)}$$

where pos are the positive instances covered by the rule. 
The usual estimate 

$\frac{pos}{neg + pos}$

has the problem that when the rule covers only one instance, which is positive it has 100% accuracy, and a rule that covers 100 positive instances (and no negative) has the same but is obviously a better rule. 
The Laplace estimate puts a prior on the accuracy which affects especially the smaller rules.  

On the classification task for [cervical cancer](#cervical), the RIPPER algorithm does not find any rule:
```{r jrip-cervical}
library("RWeka")
library(rJava)

rule = JRip(Biopsy ~ ., data = cervical)
rules = as.matrix(scan(text=.jcall(rule$classifier, "S", "toString"), sep="\n", what="") )[-c(1:2, 6), ,drop=FALSE]
rules  = data.frame(rules)
kable(rules)
```


When we use RIPPER on the regression task for predicting [bike rentals](#bike) some rules are found. 
Since RIPPER only works for classification, the bike counts has to be turned into a categorial outcome. 
I achieved this by binning the bike counts into the quartiles.

The following table shows the resulting rules. 
The conditions are followed behind the arrow by the predicted interval. 
For example $(4548, 5956)$ is the interval that covers predicted bike rentals between $4548$ and $5956$.
```{r jrip-bike}
bike2 = bike
bike2$cnt = round(bike2$cnt)
bike2$cnt =  cut(bike$cnt, breaks = quantile(bike$cnt), dig.lab = 10, include.lowest = TRUE)

rule = JRip(cnt  ~ ., data = bike2)
rules = scan(text=.jcall(rule$classifier, "S", "toString"), sep="\n", what="")
# removes text
rules = rules[-c(1, 2, length(rules))]
rules = gsub("\\([0-9]*\\.[0-9]\\/[0-9]*\\.[0-9]\\)", "", rules)
rules = as.matrix(rules)[-c(1:2, 6), ,drop=FALSE]
rules  = data.frame(rules)
kable(rules)
```
The interpretation is straightforward:
If the conditions happen, then the predicted (interval) of bike rentals is the right hand side.
The last rule is the default rule when none of the other rules fires for an instance. 
For predicting a new instance, start at the top of the list and check if a rule fires. 
When all conditions match, then the right hand side of the rule is the prediction for this instance. 
Because of the default rule it is guaranteed that there will always be a prediction.


There are also many other ways to create single rules. 
Also there are many othe ways to create rule sets. 
One is to grow a tree and turn it into rules. 
This always works. 
There are more sophisticated versions 
For example the C4.5 RULES algorithm  (Quinlan, 1987)[^c4.5] adds some pruning and ranking of the rules

## Theory and Example: From assocation rules
From association rules (unsupervised method), by only allowing rules that have the class on the right hand side.

TODO: Shortly! explain the assocation rules algo. 
TODO: Explain step from assocation rules to predictive rules.
TODO: R example

## Theory and Example: SBRL
```{r sbrl}
library("sbrl")
data("cervical")
cervical2 = cervical
cervical2$label = cervical2$Biopsy
cervical2$Biopsy = NULL
sbrl(cervical2)

```
TODOL: Explain very shortly, not many formulas or anything.
TODO: Discretize input features or remove method

An approach inspired powered by Bayesian statistics:
SBRL: Scalable bayesian rules list
Consists of two steps: 
1. Mine IF-THEN rules from the data
1. Combine the rules into a model by maximizing the posterior probability of the rule.
It's pretty fast. 
The mathematical details are pretty tough, so skipping it here. 
TODO: Link paper and R package.
TODO: Maybe as alternative to covering? or alternative to learn1rule+covering?

TODO: R example

## Advantages

- Ultra easy to interpret: Probably the easiest from all interpretable models.
- Can be as expressive as decision trees
- Straightforward to generate
- Fast to do the prediction
- Rule set can be more clearer than decision trees, because decision tree has replicated subtrees often.
- Compared to logistic regression and linear models: 
No assumption about distributions of features or target.
- For trees: No need to transform the features.
- Feature selection: Has integrated sparsity of results
- Simple rules like from OneR can be used as baseline for more complex algorithms

## Disadvantages

- Research focused on classification and not regression.
- Prone to overfitting (at least the older decision rule algorithm like XXX)
- In multi-class siutations, the covering algorithm concentrates on one class at a time whereas decision tree learner takes into account all the classes.
- Rules created from decision trees may be harder to interpret. 
- Are bad at describing linear relationships between the features and the output.

## Software and Alternatives

(Notice how they all have fancy abbreviations in caps lock.)
- RIPPER
- OPUS
- CPAR (Yin et. al 2003)
- FRL (Wang et al. 2015)
- BRL (Letham et al. 2015) 
- SBRL: Only classification and only categorial features. Uses Bayesian statistics for learning the rules and a clever bit-wise implementation. Builds a one-sided tree or decision list.
- CORELS https://github.com/nlarusstone/corels
- TLBR (Su et al. 2015)
- IDS (Lakkaraju et al. 2016)
- Rule Set (Wang et al. 2016)
- 1Rule (Malioutov et al. 2017)
- https://github.com/Jasamba/Rule-Extraction-in-Python
- OneR


Further readings: 
Foundations of Rule Learning. Extensive book on rule learning.



[^oner]: R. C. Holte (1993). Very simple classification rules perform well on most commonly used datasets. Machine Learning, 11, 63–91.

[^ripper]: Cohen, W. (1995). Fast effective rule induction. Icml. Retrieved from http://www.inf.ufrgs.br/~alvares/CMP259DCBD/Ripper.pdf

[^c4.5]: Quinlan, J. R. (1987). Generating production rules from decision trees. Proceedings of the Tenth International Joint Conference on Artificial Intelligence, 30107, 304–307. Retrieved from http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.9054&amp;rep=rep1&amp;type=pdf


