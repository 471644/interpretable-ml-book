```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all("../")
set.seed(42)
```

# IF-THEN rules

IF-THEN rules (aka decision rules) are simple statements consisting of a series of conditions and a prediction:
IF the all of the conditions apply, THEN we make a certain prediction.
An example: 
IF it rains today (condition), THEN it will rain tomorrow (prediction). 

keywords: decision rules/sets/lists, association rules

There are lots of algorithms how to produces rules. 
This chapters purpose is to help you how to think about rules and understand them fundamentally, but not to go through all possibilities. 
For this I recommend XXX.

## What are decision rules?

Decision rules are models that consists of if X then Y statements. 
Within the if statement there are sets of features and conditions on them (like X > 4) and Y is a statement about the outcome of interest.

Simple if-then statements can and are often written manually. 
In machine learning, these types of rules are learned automatically.
Can be derived from decision trees, but don't have to.

They have the following general structure:
RULE: IF CONDITIONS THEN TARGET


One decision rule from a model predicting rents could be:
If a flat is bigger than 100 square meters and has a garden the average rent is 2000 Euro per month.

Let's split that up:
- 'Flat is bigger than 100 square meters' is the first part of the IF statement
- 'Flat has a garden' is a second conditions in the IF.
- Both are connected with an 'AND', so both have to be true for the rule to apply
- Predicted outcome (THEN) is that the average rent is 2000 Euro per month.



Arguably decision rules are the most simple classifier in terms of understandability. They are very close to natural language and the way we think. Of course it relies on using sensible, understandable features and the number of conditions within the if statement.

Usually there is more then one rule to describe the associations of the features with the outcome. In our example we only have one rule covering big appartments with flats, but what about a smaller apartment?

Rules can be induced from trees. Each path through the tree is one rule. 
Rules made from trees have the nice attribute of covering the whole feature space and being mutually exclusive.

We can make following distinctions:
- descriptive rule discovery: concerned with finding single rules 
- 
- predictive rule learning: 

There are many ways how to induce those kind of rules from a training dataset.
The induction algorithms differ in how they create the rules:
- Do they allow overlapping rules and how are conflicts handled?
- Do they cover the whole training set? Or can there be a situation where none of the rules apply?


Conflict resolution if many rules are triggered:
- size ordering: use rule with the toughest requierements
- class based: based on cost of missclass
-rule-based (decision list): by quality of rule


Mutually exclusive rules: Rules are independent of each other and every recored is covered at most by one rule. 

Exhaustive rules: For every possible combination of feature values there is a rule. exhaustive coverage.


Methods to learn:
- Directly from data
- Decision tree converted to rules
- Learn other model, then extract rules


You can use a rule-learning algorithm that learns a single rule. 
And use that within sequential covering, where training records covered by this rule are removed.



We can distinguish between: 



## Theory: Learn1Rule and the Covering Algorithm

While there are many different ways to induce decision rules, I want to present a rather simple procedure which sequentially searches for rules that cover parts of the feature space, using the separate-and-conquer approach.


A very simple approach: 
Find a really good rule on the dataset. 
Remove all datapoints covered by the rule (no matter if classified correctly or not). 
Repeat with remaining points until no points are left or some other stop condition is met.
Results in disjunct rules.


### Learn 1 Rule
First let's start with a simple algorithm to learn a single rule in a classification setting:

- Start with empty rule
- Add conditions as long as they improve the information gain heuristic.
This heuristic evaluates  the improvement of a rule with respect to its predecessors:
- this is an exhaustive search over all possible features and split points. For categorical features, test combination of levels as rule. 
For numerical features, try different cut-offs as condition for a rule.
- add the condition that improves the rule the most according to a quality criterion.
- Stop when the rules does not cover negative samples
- prune rule using incremental reduced error pruning
- measure for pruning v = (p-n) / (p+n), where p is the number of positive examples covered by the rule in the validation set and n for negative samples. 
- delete any finals sequence of conditaions that maximizes v.

The quality criterion can be XXX.

The simplest way to learn decision rules: 
Take some algorithm that can learn a single rule on a dataset. 
Apply that algorithm to a dataset. 
Remove from that dataset all the examples where the rule applies. 
Repeat until all data points are covered or some other stopping criterium is reached.

TODO: Add artificial example with rent or value of a house


TODO: Concepts of coverage and accuracy: 
A rule applies to a certain region of the feature space. 
This is called the coverage of the rule. 
It can also expressed to how many instances in the dataset it applies, or if you know the distribution, then it can even be measured as percentage of the population it covers. 
For example a rule for house value that says: 
A house that is in a nice area costs 400k. 
And 10% percent of all houses are in nice areas. 
Then the rule has a coverage of 10%. 

The accuracy of a rule is a measurement for how good the rule is for the instances it covers. 
Different for regression and classification. 
In classification you can measure the missclassification rate of the rule. 
In regression the mean squared error of the rule.

This second part of repeating the single rule fitting is also called covering algorithm. 

## Covering algorithm

Algorithm for rule set:

- use sequential covering algorithm
- find best rle that covers current set of positive examples
- remove all samples (positive and negative) that are covered by this rule
- for each new rule, compute the new description lenght of the set. stop adding new rules when the description lenght is d bits longer than the smalles description length obtained so far.

```{r covering-algo, fig.cap = "The covering algorithm works by sequentially covering the feature space with single rules and removing the data points that are already covered by those rules. The approach is called separate-and-conquer."}
set.seed(42)
n = 100
dat = data.frame(x1 = rnorm(n), x2 = rnorm(n))
dat$class = rbinom(n = 100, size = 1, p = exp(dat$x1 + dat$x2) / (1 + exp(dat$x1 + dat$x2)))
dat$class = factor(dat$class)

min.x1 = min(dat$x1)
min.x2 = min(dat$x2)
ggplot(dat) + geom_point(aes(x = x1, y = x2, color = class))
ggplot(dat) + 
  geom_rect(xmin = -3, xmax = 0,   ymin = -2, ymax = -0.5, color = "black", fill = NA) + geom_point(aes(x = x1, y = x2, color = class)) 

dat.reduced = filter(dat, !(x1 <= 0 & x2 <= -0.5))

ggplot(dat.reduced) + 
  geom_rect(xmin = -3, xmax = 0,   ymin = -2, ymax = -0.5, color = "black", fill = NA)  + 
  geom_point(aes(x = x1, y = x2, color = class)) + 
  scale_x_continuous(limits = c(min.x1, NA)) + 
  scale_y_continuous(limits = c(min.x2, NA)) 

```


Optimize the ruleset:

- for each rule r in the rule set:
- consider 2 alternative rules:
- replacement rule: grow ne rule from scratch
- revised rule: add conjuncts to extend the rule r
- compare original with replacement and revised 
- choose rule set that minimizes the minimum description length principle
- repeat rule generation and optimization for the remaining positive examples.



TODO: Recreate 2D graphic where x is feature 1, y is feature 2, shape of points is the class y. Two steps: In first step draws rectangle (rule) and in the second step removes all points from rectangle.

This whole procedure is prone to overfit the data. 
There are many variants of this to improve this simple idea. 
One is RIPPER, which was invented 1995 and is used a lot.
The RIPPER algorithm introduces some pruning of the rules and different stopping criteria. 
The examples will be shown with the RIPPER algorithm.


### Other approaches

Tree based for example.
Example: C4.5 rules. Extract rules from an unpruned decision tree that was induced by C4.5 algorithm (TODO: Cite both tree paper and rule paper)
From association rules (unsupervised method), by only allowing rules that have the class on the right hand side.

An approach inspired powered by Bayesian statistics:
SBRL: Scalable bayesian rules list
Consists of two steps: 

1. Mine IF-THEN rules from the data
1. Combine the rules into a model by maximizing the posterior probability of the rule.
It's pretty fast. 
The mathematical details are pretty tough, so skipping it here. 
TODO: Link paper and R package.
TODO: Maybe as alternative to covering? or alternative to learn1rule+covering?

## Examples


TODO: Classification example with RIPPER



## Advantages

- Ultra easy to interpret: Probably the easiest from all interpretable models.
- Can be as expressive as decision trees
- Straightforward to generate
- Fast to do the prediction
- Rule set can be more clearer than decision trees, because decision tree has replicated subtrees often.
- Compared to logistic regression and linear models: 
No assumption about distributions of features or target.
- For trees: No need to transform the features.
- Feature selection: Has integrated sparsity of results

## Disadvantages

- Research focused on classification and not regression.
- Prone to overfitting (at least the older decision rule algorithm like XXX)
- In multi-class siutations, the covering algorithm concentrates on one class at a time whereas decision tree learner takes into account all the classes

## Software and Alternatives

- RIPPER
- OPUS
- CPAR (Yin et. al 2003)
- FRL (Wang et al. 2015)
- BRL (Letham et al. 2015)
- TLBR (Su et al. 2015)
- IDS (Lakkaraju et al. 2016)
- Rule Set (Wang et al. 2016)
- 1Rule (Malioutov et al. 2017)
- https://github.com/Jasamba/Rule-Extraction-in-Python



