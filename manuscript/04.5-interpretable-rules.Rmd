```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all("../")
set.seed(42)
```

## Decision rules (IF-THEN)

A decision rule is a simple IF-THEN statement consisting of a condition (sometimes called antecedents) and a prediction.
For example: 
IF it rains today if it's April (condition), THEN it will rain tomorrow (prediction). 
A single decision rule or a combination of multiple rules can be used to make predictions.

keywords: decision rules, decision sets, decision lists, association rules, IF-THEN rules

Decision rules follow a general structure:
IF the condition applies THEN make prediction.
Decision rules are arguably the most interpretable prediction models.
Their IF-THEN structure semantically resembles natural language and the way we think, assuming the condition is constructed from intelligble features and the length of the condition (number of $feature=value$ pairs connected with an AND) and assuming there are not too many rules.
In programming it's very natural to write IF-THEN rules.
New with machine learning is that these types of rules are learned by an algorithm.

Imagine using an algorithm to learn decision rules fro predicting the value of a house ($low$, $medium$ or $high$)
One decision rule learned by that model could be:
If a flat is bigger than 100 square meters and has a garden the value is high or more formally:
IF $size>100\land{}garden=1$ THEN $value=high$.
Sometimes decision rules are also written like this:
$(size>100)\land(garden=1)\Rightarrow{}(value=high)$.


Let's split that up:

- $size>100$ is the first condition in the IF-part.
- $garden=1$ is the second condition in the IF-part.
- Both are connected with an 'AND', so both have to be true for the rule to apply.
- The predicted outcome (THEN) is that $value=high$.


A decision rule uses at least one $feature=value$ statement in the conditions, without any upper limit how many more can be added with an 'AND'.
An exception is the default rule, which has no explicit IF-part applies when no other rule applies, but more on that later.

How do we judge if a given decision rule is useful?
The usefulness of a decision rule is usually summarized with two numbers: accuracy and support.


**Support (or coverage) of a rule**:
The percentage of the number of instances for which the condition of a rule applies is called the support of a rule.
For example, let's take the rule $(size=big)\land(location=good)\Rightarrow(value=high)$ for predicting house values.
Let's say for 100 of 1000 houses the rule applies, because they are big and in a good location, then the support of the rule is 10%.
The prediction (IF-part) is not important for computing the support.

**Accuracy (or confidence) of a rule**: 
The accuracy of a rule is a measurement for how accurate the rule is in predicting the correct class for the instances for which the condition of the rule applies. 
For example: 
Let's say from the 100 houses, where the rule with condition $size=big$ applies, 85 have $value=high$, 14 $value=medium$ and 1 $value=low$, then the accuracy of the rules is 85%.


There is usually a tradeoff between accuracy and support:
By adding more features in the condition, we can achieve higher accuracy, but lose support.
For creating a good classifier, you need more than one rule.
To predict the value of a house you might need to learn not only one rule, but maybe 10 or 20.
Then things can become more complicated: 

1. Rules can overlap: 
What if I want to predict the value for a new flat and two or more rules apply, implying conflicting predictions?
1. No rule applies:
What if I want to predict the value for a new flat and none of the rules apply?

There are two major strategies for dealing with multiple rules:
decision lists (ordered) and decision sets (unordered). 
Both strategies imply different solutions for solving the problem of overlapping rules. 

A **decision list** is an ordered list of rules. 
When the first rule applies to an instance, we use the prediction of the first rule. 
If not, we go to the next rule and check if it applies and so on.
Decision lists are one-sided decision trees. 
The first rule is the root node. 
Decision lists solve the problem of overlapping rules by introducing an order.

A **decision set** is a bunch of rules without an order (in contrast to a decision list). 
In a set, the rules are either mutually exclusive, or there is some strategy to resolve conflicts, like majority voting, potentially weighted by the individual rule accuracies or other quality measures.
To make a prediction, either a single rule applies and we use the attached prediction or when multiple rules apply, we use the conflict resolution (e.g. majority vote) to make the prediction.
Interpretability suffers potentially when multiple rules apply.


Both decision lists and sets can suffer from the problem that no rule applies to an instance. 
This can be resolved by introducing a default rule. 
The default rule is the rule that applies when no other rule applies.
This can be used in both decision lists and decision sets.
The prediction of the default rule is often the most frequent class of the data points which are not covered by other rules.
If a set or list of rules covers the whole feature space, we call it exhaustive. 
By adding a default rule, a set or list is automatically exhaustive.


There are many ways to learn rules from data and the book does (by far not) not cover all of them.
This chapter will show you three of them.
The algorithms are chosen to cover a broad range of general ideas for learning rules, so all three are very different approaches.

1. **OneR**: Learning rules from a single feature.
OneR is great for its simplicity, interpretability and use as a benchmark.
1. **Sequential covering**: Covering the data sequentially with rules.
Sequential covering is a general procedure used by many rule learning algorithms and as such important as basic knowledge.
1. **Bayesian Rule Lists**: Combining pre-mined frequent feature values into a decision list using Bayesian statistics. 
This is a more sophisticated approach. 
It's interesting because it uses pre-mined patterns, a common approach used by many rule learning algorithms.
Additionally, in contrast to the other examples, it's using a statistical approach.

Let's start with the simplest: Using the single, best feature to learn rules.

### Learn One Rule (OneR)

The OneR algorithm is one of the simplest rule induction algorithm.
It chooses a single feature which produces the best decision rules.

Despite the name OneR, the algorithm produces more than one rule: 
It's actually one rule per unique feature value of the chosen best feature. 
A better name would be OneFeatureRules.

The algorithm is simple and fast:

1. Bin all continuous features.
1. For each feature, do:
    -Create a cross table between the feature levels and the outcome classes. 
    - For each feature level: Create a rule which predicts the most frequent class for each level of the feature (can simply be read from the cross table).
    - Calculate the total error of the rules for that feature.
1. Choose the feature with the smallest total error.

OneR always covers all instances of the dataset, since it uses all levels of the chosen feature.
Missing values can be either treated as an additional level or be imputed before.


OneR can be seen as a decision tree with only one split.
The split is not necessarily binary as in CART, but depends on the number of unique feature values.


Let's look at an example how the best feature is chosen.
The following table shows an artificial dataset about houses, containing information about location, size and if pets are allowed. 
We are interested in learning a simple model to predict the value of a house.

```{r OneR-freq-table1}
df = data.frame(
  location = c("good", "good", "good", "bad", "good", "good", "bad", "bad", "bad", "bad"),
  size = c("small", "big", "big", "medium", "medium", "small", "medium", "small", "medium", "small"), 
  pets = c("yes", "no", "no", "no", "only cats", "only cats", "yes", "yes", "yes", "no")
)
value = factor(c("high", "high", "high", "medium", "medium", "medium", "medium", "low", "low", "low"), levels = c("low", "medium", "high"))
value.f = factor(paste("value=", value, sep = ""), levels = c("value=low", "value=medium", "value=high"))
kable(df)
```


OneR creates the cross tables between each feature and the outcome:

```{r OneR-freq-table2}
kable(table(paste0("location=", df[,"location"]), value.f))
kable(table(paste0("size=", df[,"size"]), value.f))
kable(table(paste0("pets=", df[,"pets"]), value.f))
```

For each feature, we go through the table row by row: 
Each feature level is the IF-part of a rule;
the most frequent class for the instances with this feature level is the prediction, the THEN-part of the rule.
For example, the size feature with the levels $small$, $medium$ and $big$ procuces three rules.
For each feature we compute the total error rate of the rules that are produced, which is the sum of the errors.
For example, the location feature has possible values, $bad$ and $good$. 
The most frequent value for houses in bad locations is $low$ and by using $low$ as the prediction, we make two mistakes, because two houses have a $medium$ value.
The predicted price class for houses in good locations is $high$ and again we make two mistakes, because two houses have a $medium$ value.
The error we make by of using the location feature is 4/10, for the size feature it's 3/10  and 4/10 for the pets feature. 
The size feature produces the rules with the lowest error and will be used for the final OneR model:

- IF $size=small$ THEN  $value=small$
- IF $size=medium$ THEN  $value=medium$
- IF $size=big$ THEN  $value=high$


OneR favors features with many possible levels, because those features can more easily overfit the target.
Imagine a dataset with only noise and no signal, meaning that all features take on random values and have no predictive value for the target.
Some features have more levels than others. 
The features with more levels can now more easily overfit. 
A feature that has a unique level for each instance from the data would perfectly predict the whole training dataset.
A solutiion would be to split the data into training and validation sets, learn the rules on the training data and evaluate the total error for choosing the feature on the validation set.

Another issue are ties, i.e. when two features produce the same total error.
OneR solves ties by either taking the first feature with the lowest error or the one with the lowest p-value of a chi-squared test.

Let's try out OneR with real data.
We use the [cervical cancer classification task](#cervical) to test out the OneR algorithm. 
All continuous input features were binned into five intervals by frequency.
The following rules are created:
```{r oner-cervical}
library("OneR")
data("cervical")
rule = OneR::OneR(Biopsy ~ ., data = cervical)

rule.to.table = function(rule){
  dt = data.frame(x = names(rule$rules), prediction = unlist(rule$rules))
  colnames(dt) = c(rule$feature, "prediction")
  dt
}

kable(rule.to.table(rule), row.names = FALSE)
```

The age feature was chosen by OneR as the best predictive feature. 
Since $Cancer$ is rare and for each rule, the majority is still $Healthy$, the prediction is still always $Healthy$. 
Using the OneR algorithm is not useful in this case.
The cross table between the 'Age' intervals and $Cancer$/$Healthy$ looks like this:

```{r oner-cervical-confusion}
tt = table(paste0("Age=", bin(cervical$Age)), cervical$Biopsy)
kable(tt, col.names = paste0("# ", colnames(tt)))
```

The output would be more informative if OneR would return the percentage of instances with the majority class as a prediction instead of only the label of the majority class.


OneR can't do regression. 
But we can turn a regression task into a classification task by cutting the continuous outcome into intervals.
We apply this trick to predict the number of [bike rentals](#bike) with OneR, by cutting the bike count into it's four quartiles.
The following table shows the chosen feature:

```{r oner-bike}
data("bike")
bike2 = bike
bike2$days_since_2011 = max(0, bike2$days_since_2011)
bike2$cnt =  cut(bike2$cnt, breaks = quantile(bike$cnt), dig.lab = 10, include.lowest = TRUE)
rule = OneR::OneR(cnt ~ ., data = bike2)

kable(rule.to.table(rule), row.names = FALSE)
```

The chosen feature is the month.
The month feature has (surprise!) 12 feature levels, which is more than levels than most other features.
So there is some danger of overfitting. 
On the more optimistic side: the month feature can handle the seasonal trend (e.g. low rental numbers in winter) and seems reasonable.

Now we move from the simple OneR algorithm to a more complex procedure involving rules with more complex conditions from multiple features: Sequential Covering.

### Sequential Covering

Sequential covering is a general procedure that repeatedly applies an algorithm that learns a single rule to produce a decision list (or set) that covers the whole feature space rule by rule.
Many rule learning algorithms are variants of the sequntial covering algorithm.
This chapter introduces the main recipe and uses a variant, the RIPPER algorithm, for the examples.

The idea is simple:
First, find a good rule for the data, which applies to a part of the data.
Remove all data points which are covered by the rule, that is, any data point for which the conditions apply, no matter if the points are classified correctly or not. 
Repeat the rule learning and removal of covered points with the remaining points until no points are left or some other stop condition is met.
The result is a decision list. 
This approach of covering a part of the data, removing it and repeating is called 'separate-and-conquer'.


For now, let's assume we already have an algorithm that can produce a single rule that covers part of the data. 
The sequential covering algorithm for two classes (one positive, one negative) works like this:

- Start with an empty list of rules ($rlist$).
- Learn a rule $r$.
- While the list of rules is below some quality threshold or while some positive examples are not covered yet:
    - Add rule $r$ to $rlist$.
    - Remove all data points that are covered by rule $r$.
    - Learn another rule on the remaining data.
- Return the decision list.

For multi-class settings, the approach has to be modified.
First the classes are ordered by increasing prevalence.
The sequential covering algorithm starts with the least prevalent class, learns a rule for it, removes all instances covered, then moves on to the second least prevalent class and so on. 
The current class is always treated as the positive class and all classes with higher prevalence as the negative class.
The last class is the default rule.
This is also called the one-versus-all strategy in classification.


```{r covering-algo, fig.cap = "The covering algorithm works by sequentially covering the feature space with single rules and removing the data points that are already covered by those rules. The approach is called separate-and-conquer. For visualization purposes, the features x1 and x2 are continuous, but most rule learning algorithms require categorical features."}
set.seed(42)
n = 100
dat = data.frame(x1 = rnorm(n), x2 = rnorm(n))
dat$class = rbinom(n = 100, size = 1, p = exp(dat$x1 + dat$x2) / (1 + exp(dat$x1 + dat$x2)))
dat$class = factor(dat$class)

min.x1 = min(dat$x1)
min.x2 = min(dat$x2)
p1 = ggplot(dat) + geom_point(aes(x = x1, y = x2, color = class))+ 
  scale_color_discrete(guide = "none") + 
  ggtitle("Step 0: Data")
p2 = ggplot(dat) + 
  geom_rect(xmin = -3, xmax = 0,   ymin = -2, ymax = -0.5, color = "black", fill = NA) + geom_point(aes(x = x1, y = x2, color = class)) + 
  scale_color_discrete(guide = "none")+ 
  ggtitle("Step 1: Find rule")

dat.reduced = filter(dat, !(x1 <= 0 & x2 <= -0.5))

p3 = ggplot(dat.reduced) + 
  geom_rect(xmin = -3, xmax = 0,   ymin = -2, ymax = -0.5, color = "black", fill = NA)  + 
  geom_point(aes(x = x1, y = x2, color = class)) + 
  scale_x_continuous(limits = c(min.x1, NA)) + 
  scale_y_continuous(limits = c(min.x2, NA)) + 
  scale_color_discrete(guide = "none")+ 
  ggtitle("Step 2: Remove covered instances")


p4 = p3 + 
  geom_rect(xmin = 0.8, xmax = 2.5, ymin = -1.5, ymax = 1.5, color = "black", fill = NA)  + 
  ggtitle("Step 3: Find next rule")

gridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2)

```
For example: 
We have a task and dataset for predicting the values of houses from size, locatiion and if pets are allowed.
We learn the first rule, which turns out to be: 
'If $size=big$ and $location=good$, then $value=high$'.
Then we remove all the big houses in a good neighbourhood from the dataset.
With the remaining data we learn the next rule: 
Maybe: 'If $location=good$, then $value=medium$' (Note that this rule is learned on data with big houses in good neighbourhoods removed, so only medium and small houses in good neighbourhoods remain).


How do we learn a single rule? 
The OnR algorithm would be useless here, since it would always cover the whole feature space.
But there are many other ways. 
One way is learning a single rule from a decision tree with beam search:

- Build a decision tree (withCART or some other tree learning algorithm).
- Start at the root node and recursively choose the purest node (e.g. with the lowest missclassification rate).
- The majority class of the node we end up in is used as the rule's prediction; the path leading to that node is used as the rule condition.

The following figure visualizes the beam search in a tree:

```{r learn-one-rule, fig.cap = "Learning a rule by search a path through a decision tree. A decision tree is grown to predict the target of interest. To extract a single, accurate rule, start at the root node and greedily and iteratively follow the path which locally produces the purest subset (i.e. with highest accuracy of target classes) and add all the the split value to the rule condition. The prediction of the rule is the majority class of the node we land in. This example shows how we learn a rule for predicting the value of a house. We end up with: 'If $location=good$ and $size=big$, then $value=high$' (assuming high as the most common class in that node)/"}
knitr::include_graphics("images/learn-one-rule.png")
```

Learning a single rule is equivalent to a search problem, where the search space is the space of all possible rules. 
The goal of the search is to find the best rule (according to some criteria).
There are many different search strategies: 
hill-climbing, beam search, exhaustive search, best-first search, ordered search, stochastic search, top-down search, bottom-up search, ...


RIPPER (Repeated Incremental Pruning to Produce Error Reduction) by Cohen (1995)[^ripper] is a variant of the Sequential Covering algorithm.
RIPPER is a bit more sophisticated and uses as post-processing phase (rule pruning) for the optimization of the list (or set) of rules.
RIPPER can run in ordered or unordered mode, either producing a decision list or decision set.


We will use RIPPER to predict our data examples.
On the classification task for [cervical cancer](#cervical), the RIPPER algorithm does not find any rule:
```{r jrip-cervical}
library("RWeka")
library(rJava)

extract.rules.jrip = function (rule) {
rules = scan(text=.jcall(rule$classifier, "S", "toString"), sep="\n", what="")
# removes text
rules = rules[-c(1, 2, length(rules))]
rules = gsub("\\([0-9]*\\.[0-9]\\/[0-9]*\\.[0-9]\\)", "", rules)
rules = as.matrix(rules)[-c(1:2, 6), ,drop=FALSE]
rules  = data.frame(rules)
if (nrow(rules) == 0) {
  return(NULL)
} else {
  kable(rules)
}
}

rule = JRip(Biopsy ~ ., data = cervical)
extract.rules.jrip(rule)
```


When we use RIPPER on the regression task for predicting [bike rentals](#bike) some rules are found. 
Since RIPPER only works for classification, the bike counts have to be turned into a categorial outcome. 
I achieved this by binning the bike counts into the quartiles.
For example $(4548, 5956)$ is the interval that covers predicted bike rentals between $4548$ and $5956$.
The following table shows the learned rules. 

```{r jrip-bike}
bike2 = bike
bike2$cnt = round(bike2$cnt)
bike2$cnt =  cut(bike$cnt, breaks = quantile(bike$cnt), dig.lab = 10, include.lowest = TRUE)
bike2$temp = round(bike2$temp)
bike2$windspeed = round(bike2$windspeed)
bike2$hum = round(bike2$hum)

rule = JRip(cnt  ~ ., data = bike2)
extract.rules.jrip(rule)
```
The interpretation is straightforward:
If the conditions apply, then we predict as number of bike rentals the interval on the right hand side.
The last rule is the default rule which applies when none of the other rules fires applies for an instance. 
For predicting a new instance, start at the top of the list and check if a rule fires. 
When all conditions match, then the right hand side of the rule is the prediction for this instance. 
Because of the default rule it is guaranteed that there will always be a prediction.


### Bayesian Rule Lists

In this section I will show you another approach to learn a decision list, which follows this rough recipe:

1. Pre-mine frequent patterns from the data (unsupervised), which can be used as conditions (IF-part) for the decision rules.
1. Learn a decision list from a selection of the pre-mined rules.

One specific approach that uses this recipe is called Bayesian Rule Lists (Letham et. al, 2015)[^brl] or short BRL.
BRL uses Bayesian statistics to learn decision lists from frequent patterns which ar pre-mined with the FP-tree algorithm (Borgelt 2005)[^fp-tree]

But let's start slowly:

**Pre-mining of frequent patterns**

A frequent pattern is the frequent (co-)occurence of feature levels.
As a pre-processing step for the BRL algorithm, we use the features (at this step we don't need the target outcome) and extract freqently ocurring patterns from them.
A pattern can be a single feature value (e.g. $\text{size=medium}$) or a combination of feature values, like $\text{size=medium}\land\text{location=bad}$.

The frequency of a pattern is measured with its support in the dataset:

$$Support(x_j=A)=\frac{1}n{}\sum_{i=1}^nI(x_{ji}=A)$$

where $x_j=A$ is the feature level, $n$ the number of data points in the dataset and $I$ the indicator function which returns $1$ if the feature $x_j$ of instance $i$ has level $A$ else $0$.
In a dataset of house values, if 20% of the houses have no balcony (and 80% have one or more), then the support for the pattern $balcony=0$ would be 20%.
The support can also be measured for combinations of feature levels, for example the support for $\text{balcony=0}\land\text{pets=allowed}$.

There are many algorithms to find such frequent patterns, for example Apriori or the FP-Growth. 
Which one you use does not matter much, only the speed at which the patterns are found is different, but the resulting patterns are always the same.

I'll give you a rough idea how the Apriori algorithm works for finding frequent patterns.
Actually the Apriori algorithm consists of two parts, where the first part finds frequent patterns and the second part creates association rules.
For this application, we are only interested in the frequent patterns generated in the first part.

For the first step, the Apriori algorithm starts with all feature levels that have a support larger than the minimal support as defined by the user. 
If the user says minimal support should be 10% and only 5% of the houses have $size=big$, we would remove that feature value and only keep $size=medium$ and $size=big$ as patterns. 
This does not mean that the houses are removed from the data, it just means that $\text{size=big}$ will not be considered for finding frequent patterns.
Starting with frequent patterns with a single feature value, the Apriori algorithm tries to iteratively find combinations of feature values of higher order.
Patterns are constructed by combining $feature=value$ statements with a logical AND, e.g. $\text{size=medium}\land\text{location=bad}$.
Generated patterns with a support below the minimum support are removed.
In the end we have all the frequent patterns.
Any subset of a frequent pattern is again frequent, which is called the apriori property. 
It intuitively makes sense: By removing a condition from a pattern, the reduced pattern can only cover more or the same number of data points (support), but never less. 
For example, if 20% of the houses are $\text{size=medium}\land\text{location=good}$, then the support of houses that are only $\text{size=medium}$ is 20% or larger.
The apriori property is used to reduce the number of patterns we have to check. 
Only for frequent patterns we have to check patterns of higher order.

Now we are done with pre-mining conditions for the Bayesian Rule List algorithm.
But before we go into the details of BRL, I want to show you another way. 
Other rule learning approaches suggest including the outcome of interest into the frequent pattern mining process and also executing the second part of the Apriori algorithm, which builds IF-THEN rules. 
Since the algorithm is unsupervised, the THEN-part also contains feature values we are not interested in. 
But we can filter for rules that only have the outcome of interest in the THEN-part.
These rules already form a decision set, but it would also be possible to rank the rules, prune them, delete some or re-combine them somehow. 

In the BRL approach however, we only need the frequent patterns and learn the THEN-part and how to arrange the patterns into a decision list using Bayesian statistics.


**Learning Bayesian Rule Lists**

The goal of the BRL algorithm is to learn an accurate decision list using a selection of the pre-mined conditions, while prioritizing lists with few rules and short conditions.
BRL tackles this goal by defining a distribution of decision lists with prior distributions for the length of conditions (preferring shorter ones) and the number of rules (also preferring less rules over more). 

The posteriro probability distribution of lists (kind of meta, right?), enables to tell how likely  a decision list is given assumptions of shortness and how well the list fits the data.
Our goal is to find the list that maximizes this posterior probability.
Since it's not possible to find the exact best list from the distributions of lists directly, BRL suggests the following recipe:
1) Generate an initial decision list, randomly drawn from the priori distribution.
2) Iteratively change the list by adding/switching/removing rules, while making sure that the lists are from the distribution of lists and 
3) choose the decision list from the sampled lists which has the highest likelihood according to the posteriori distribution.

Let's go through the algorithm in more detail:
The algorithm starts with pre-mining feature value patterns with the FB-Growth algorithm.
BRL makes a bunch of assumptions about the distribution of the target and the distribution of the parameters that define the distribution of the target.
(Well, that's Bayesian statistic.)
If you are unfamiliar with Bayesian statistics, don't get too caught up in the following explanations. 
It's important to know that the Bayesian approach is a way to combine pre-existing knowlegde or requirements (called priori distributions) while also fitting the data.
In the decision list case, the Bayesian approach is useful, because through the prior assumptions, it nudges the decision list to be short with also short rules.

The goal is to sample decision lists $d$ from the posteriori distribution: 

$$\underbrace{p(d|x,y,A,\alpha,\lambda,\eta)}_{posteriori}\propto\underbrace{p(y|x,d,\alpha)}_{liklihood}\cdot\underbrace{p(d|A,\lambda,\eta)}_{priori}$$
where $d$ is a decision list, $x$ are the features, $y$ is the target, $A$ the set of pre-mined conditions, $\lambda$ the prior expected decision list length, $\eta$ the prior expected number of conditions in a rule, $\alpha$, the prior pseudo-count for the positive and negative classes (best fixed at $(1,1)$).


$$p(d|x,y,A,\alpha,\lambda,\eta)$$

quantifies how likely a decision list is, given the observed data and the priori assumptions. 
This is proportional to the likelihood of the outcome of interest given the list and the data times the likelihood of the list given prior assumptions and the pre-mined conditions.

$$p(y|x,d,\alpha)$$
is the likelihood of the obseved $y$, given the decision list and the data. 
BRL assumes that $y$ is generated by a Dirichlet-Multinomial distribution.
The better the decision list $d$ explains the data, the higher the likelihood.

$$p(d|A,\lambda,\eta)$$
is the prior distribution of the decision lists. 
It multiplicatively incorporates a truncated Poisson distribution (parameter $\lambda$) for the number of rules in the list, and a truncated Poisson distriubtion (parameter $\eta$) for the number of feature values in the conditions of the rules.

A decision list has a high posterior probability if it explains the outcome $y$ well and is also likely according to the prior assumptions.


Estimations in Bayesian statistics are always a bit tricky, because we usually can't directly compute the correct answer, but we have to draw candidates, evaluate them and update our posteriori estimates (using Markov chain Monte Carlo).
Here it is even more tricky, because we have to draw from the distribution of decision lists. 
The authors suggest to first draw an initial decision list and to then to change it iteratively to generate samples of decision lists from the posterior distribution of the lists (a Markov chain of decision lists).
The results are potentially dependent on the initial decision list, so it is advisable to repeat this procedure to get a broad sample of lists (default in the software implementation is 10 times).
The following recipe tells us how to draw an initial decision list:

- Pre-mine patterns with FP-Growth.
- Sample the length $m$ for the list from a truncated Poisson distribution.
- For the default rule: Sample the Dirichlet-Multinomial distribution parameter $\theta_0$ n of the target value (i.e. the rule that applies when nothing else applies).
- For decision list rule $j=1,\ldots,m$, do:
    - Sample the length of the rule $l_j$ (i.e. number of conditions) for rule $j$
    - Sample a condition of length $l_j$ from the pre-mined conditions.
    - Sample the Dirichlet-Multinomial distribution parameter for the THEN-part (i.e. for the distribution of the target outcome given the rule)
- For each observation in the dataset:
    - Find the rule from the decision list that applies first (top to bottom).
    - Draw the predicted outcome from the probability distribution (Binomial) suggested by the rule that applies.

The next step is to generate many new lists starting from this initials sample in order to get lots of samples from the posterior distribution of decision lists.
From these samples we can pick the decision list with the highest posterior probability.

Since we can't directly optimize this problem, we do Metropolis-Hastings sampling: 
Starting from a random draw, we randomly either 1) move a rule to a different position in the list or 2) add a rule to the current decision list from the pre-mined rules 3) Remove a rule from the decision list. 
Which of the rules is switched/added/deleted is also random.
At each of this step, the algorithm evaluates the posteriori probability of the decision list (mix of how accuract it is and how much it satisfies the priori distributions).
The Metropolis Hastings algorithm ensures that we sample more heavily the decision lists that are likely in terms of accuracy and prior.
This procedure gives us lots of samples from the distribution of decision lists (a list of decision lists). 
We end up with the posterior distribution of decision lists, but of course we would like to have a single decision list and not a distribution. 
The BRL algorithm selects the decision list of the samples with the highest posterior probability.

The examples are using a faster variant of BRL called Scalable Bayesian Rule Lists (SBRL) by Yang et. al (2016) [^sbrl].

Let's see the SBRL in action.
First we apply the SBRL algorithm to predict [risk for cervical cancer](#cervical).
I had to discretize all input features first to make the SBRL algorithm work. 
For that I binned the continuous features based on the frequency of the values (by quantiles): 
If a continuous features is binned into 3 intervals by frequency, then the first interval contains 33% of the instances with the lowest feature value, then the second interval with the ones higher, then the rest with the heighest values.

We get the following rules:

```{r sbrl-cervical}
library("sbrl")
library("arules")
data("cervical")

cervical2 = as.data.frame(lapply(cervical, function(x) {
  if(is.factor(x) || length(unique(x)) < 3) {
    as.factor(x)
  } else {
    discretize(x, breaks = 3, method = "interval")
    #discretize(x, breaks = max(length(unique(x))-1, 5))
  }
}))

get.sbrl.rules = function(x) {
    res = lapply(1:nrow(x$rs), function(i) {
        if (i == 1) 
            sprintf("If      %s (rule[%d]) then positive probability = %.8f\n", 
                x$rulenames[x$rs$V1[i]], x$rs$V1[i], x$rs$V2[i])
        else if (i == nrow(x$rs)) 
            sprintf("else  (default rule)  then positive probability = %.8f\n", 
                x$rs$V2[nrow(x$rs)])
        else sprintf("else if %s (rule[%d]) then positive probability = %.8f\n", 
            x$rulenames[x$rs$V1[i]], x$rs$V1[i], x$rs$V2[i])
    })
    data.frame(rules = unlist(res))
}


cervical2$label = cervical2$Biopsy
cervical2$Biopsy = NULL
rules = sbrl(cervical2, pos_sign = "Cancer", neg_sign = "Healthy", rule_maxlen = 2)
kable(get.sbrl.rules(rules))
```
Notice how we get meaningful rules, since the prediction on the THEN-part is not the class outcome, but the predicted probability for cancer.

The conditions were chosen from patterns that were pre-mined with the FP-Growth algorithm. 
The following table displays the pool of conditions from which the SBRL algorithm could choose from by building a decision list.
The maximum length allowed by me as a user was patterns with two conditions. 
Here is a sample of ten patterns:

```{r sbrl-cervical-premined}
set.seed(1)
kable(sample(rules$rulenames, size = 10), col.names = "pre-mined conditions")
```

Next we apply the SBRL algorithm to the [bike rental prediction task](#bike).
This only works by turning the regression problem of predicting bike counts into a binary classification task. 
I arbitrarily created a classification task by creating a label that says if the bike counter was larger than 4000 bikes.

```{r sbrl-bike}
library("sbrl")
library("arules")
data("bike")

bike2 = bike
bike2$label = bike2$cnt > 4000
bike2$cnt = NULL
bike2 = as.data.frame(lapply(bike2, function(x) {
  if(is.factor(x) || length(unique(x)) < 3) {
    as.factor(x)
  } else {
    discretize(x, breaks = 3, method = "interval")
    #discretize(x, breaks = max(length(unique(x))-1, 5))
  }
}))
rules = sbrl(bike2, pos_sign = TRUE, neg_sign = FALSE, rule_maxlen = 3)
kable(get.sbrl.rules(rules))
```



### Advantages

This section covers the advantages of IF-THEN rules in general.

- IF-THEN rules are **easy to interpret**.
They are probably the easiest from all interpretable models.
This statement applies only if the number of rules is short, the conditions of the rules are short (maximum 3 I would say) and if the rules are organized in a decision list or a non-overlapping decision set.
- Decision rules can be **as expressive as decision trees, while being more compact**. 
Decision tree often has replicated subtrees.
- There are many ways to create decision rules.
- The **prediction with IF-THEN rules is fast**, since only a few binary statements have to be checked to see which rules apply.
- Compared to logistic regression and linear models: 
No assumption about distributions of features or target.
- Decision rules are **robust** against monotone transformations of the input features, because the only thing that changes is the threshold in the condition.
They are also robust against outliers, since the only thing that matters is if a condition applies or not.
- IF-THEN rules usually generate very sparse models, meaning not many features are included.
They **select only the relevant features** for the model.
A linear model for example, by default, assigns a weight to every input feature.
Features that are irrelevant can simply be ignored by IF-THEN rules.
- Simple rules like from OneR **can be used as baseline** for more complex algorithms.

### Disadvantages 

This section covers the disadvantages of IF-THEN rules in general.


- The research and literature for IF-THEN rules is focused on classification and almost **completely neglecting regression**.
While it's always possible to categorize a continuous target and turn it into a regression problem, it's always a loss of information.
In general approaches a more attractive when they can be used for both regression and classification.
- Often the **features also have to be categorical**.
That means numerical features have to be binned, if you want to use them. 
There are many ways to cut a continuous feature into intervals, but this is not as trivial and comes with many questions, where there is usually not a correct answer:
Into how many intervals should the feature be split? 
What's the splitting criteria: Fixed interval lengths or quantiles or something else?
Dealing with binning of continuous features is a non-trivial issue that is often neglected and one of the option simply applied (like I did in my examples).
- Many of the older rule fitting algorithms are prone to overfitting. 
The algorithms presented here all have at least some safeguards to prevent overfitting: 
OneR is limited because it can only use a single feature (only problematic if the feature has too many levels or if there are lots of features, which equates to the multiple testing problem), RIPPER does pruning and Bayesian Rule Lists impose a prior distribution on the decision lists.
- Decision rules are **bad at describing linear relationships** between the features and the output.
That's an issue they share with decision trees. 
Decision trees and rules can only produce step-like prediction functions, where the prediction sharply changes when a feature changes and not linearly with a features.
This is related with the issue that the inputs have to be categorical (in decision trees they are implicitly made categorical by finding split points in them).







### Software and Alternatives

ADD Another way is ripping apart decision trees as used in the [RuleFit algorithm](#rulefit).


- There is an [R package `OneR`](https://cran.r-project.org/web/packages/OneR/). 
OneR is also implemented in the [Weka machine learning library]((https://www.eecs.yorku.ca/tdb/_doc.php/userg/sw/weka/doc/weka/classifiers/rules/package-summary.html)) and as such available in Java, R and Python.
- RIPPER is also implemeted in Weka. For the examples I used the R implementation of JRIP [RWeka package](https://cran.r-project.org/web/packages/RWeka/index.html). 
- SBRL is vailable as [R package](https://cran.r-project.org/web/packages/sbrl/index.html) (which I used in for the examples), in [Python](https://github.com/datascienceinc/Skater) or as [pure C implementation](https://github.com/Hongyuy/sbrlmod).

I won't even attempt to create an exhaustive list of alternatives for learning decision rule sets/lists, but rather point towards some more recent research and summarizing work. 

- I recommend the book 'Foundations of Rule Learning' from Fuernkranz et. al (2012)[^fuernkranz]. 
It's an extensive work on rule learning, for the ones who want to dive deeper into the topic.
It gives a holistic framework for thinking about rule learning and features many rule learning algorithms.
- I also recommend to checkout [Weka rule learners](https://www.eecs.yorku.ca/tdb/_doc.php/userg/sw/weka/doc/weka/classifiers/rules/package-summary.html), which implements RIPPER, M5Rules, OneR, PART and many more.
- Another approach for building decision lists, that have some guarantee to deliver an optimal decision list structure, is called [CORELS](https://github.com/nlarusstone/corels) Certifiable Optimal RulE ListS).



[^corels]: Angelino, Elaine, et al. "Learning Certifiably Optimal Rule Lists." Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2017.

[^fuernkranz]: Fuernkranz, J., Gamberger, D., & Lavrač, N. (2012). Foundations of Rule Learning. Foundations of Rule Learning. https://doi.org/10.1007/978-3-540-75197-7_2

[^oner]: R. C. Holte (1993). Very simple classification rules perform well on most commonly used datasets. Machine Learning, 11, 63–91.

[^ripper]: Cohen, W. (1995). Fast effective rule induction. Icml. Retrieved from http://www.inf.ufrgs.br/~alvares/CMP259DCBD/Ripper.pdf

[^c4.5]: Quinlan, J. R. (1987). Generating production rules from decision trees. Proceedings of the Tenth International Joint Conference on Artificial Intelligence, 30107, 304–307. Retrieved from http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.9054&amp;rep=rep1&amp;type=pdf

[^brl]: Letham, B., Rudin, C., McCormick, T. H., & Madigan, D. (2015). Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model. Annals of Applied Statistics, 9(3), 1350–1371. https://doi.org/10.1214/15-AOAS848

[^sbrl]: Yang, H., Rudin, C., & Seltzer, M. (2016). Scalable Bayesian Rule Lists, 31. Retrieved from http://arxiv.org/abs/1602.08610

[^fp-tree]: Borgelt, C. (2005). An implementation of the FP-growth algorithm. Proceedings of the 1st International Workshop on Open Source Data Mining Frequent Pattern Mining Implementations - OSDM ’05, 1–5. http://doi.org/10.1145/1133905.1133907
