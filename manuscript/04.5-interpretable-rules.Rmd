```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all("../")
set.seed(42)
```

# IF-THEN rules

IF-THEN rules (also called decision rules) are simple statements consisting of a series of conditions and a prediction:
An example: 
IF it rains today (condition) if it's April (another condition), THEN it will rain tomorrow (prediction). 

keywords: decision rules/sets/lists, association rules


Decision rules are the most interpretable prediction models.
They are very close to natural language and the way we think. 
Assuming we use sensible, understandable features and keep the number of conditions within the if statement short.
In programming it's very natural to write IF-THEN rules.
What's new in machine learning is that these types of rules are learned by the machine.

They have the following general structure:
RULE: IF all conditions apply THEN make prediction.

One decision rule from a model predicting rents could be:
If a flat is bigger than 100 square meters and has a garden the value is high.

Let's split that up:
- 'Flat is bigger than 100 square meters' is the first condition in the IF-part.
- 'Flat has a garden' is the second condition in the IF-part.
- Both are connected with an 'AND', so both have to be true for the rule to apply.
- Predicted outcome (THEN) is that the value is high.

How good is a rule?
For this we can look at two measures: accuracy and coverage.

- **Accuracy of a rule**: 
The accuracy of a rule is a measurement for how good the rule is for the instances it covers. 
Different for regression and classification. 
In classification you can measure the missclassification rate of the rule. 
In regression the mean squared error of the rule.
- **Coverage of a rule**:
A rule applies to a certain region of the feature space. 
This is called the coverage of the rule. 
It can also expressed to how many instances in the dataset it applies, or if you know the distribution, then it can even be measured as percentage of the population it covers. 
For example a rule for house value that says: 
A house that is in a nice area costs 400k. 
And 10% percent of all houses are in nice areas. 
Then the rule has a coverage of 10%. 

This example is very nice and clean, because it contains only one rule.  
Usually you need more rules to cover the prediction problem at hand. 
To predict appartment you might not want to learn only one rule, but maybe 10, or 20.
Then things can become more complicated: 

1. Rules can be overlapping: 
What if I want to predict the value for a new flat and two or more rules apply, giving conflicting predictions?
1. No rule applies:
What if I want to predict the value for a new flat and none of the rules apply?

In case of multiple rules, we distinguish lists (ordered) and sets (unordered) of rules:  

- **Decision list**: 
An ordered list of conjunctive rules (order is important). 
They are basically one-sided decision trees. 
The first rule is the root node. 
One of the child nodes is already the terminal node with the prediction when the first rule applies. 
The second child is the second rule. 
This repeats for all the rules. 
Decision lists solve the problem of rule overlaps by introducing an order to the rules.
Easy to make a prediction if it is an ordered decision list, because the rules are tied in order. 
The first rule where the conditions apply to the instance that shall be classified is used. 
And usually there is some default rule when none of the other rules fires which predicts the majority class of the uncovered instances for example.
- **Decision set**: 
A bunch of rules without an order (in contrast to a decision list). 
In a set, the rules are either mutually exclusive, or there is some strategy to resolve conflicts.
- Mutually exclusive rules: Rules are independent of each other and every recored is covered at most by one rule. 
Conflict resolution if many rules are triggered is often done by some measurement of the quality of the rule (accuracy, coverage, cost-based missclassification, etc.)
To make a prediction, either a single rule applies and we use that prediction or, with multiple rules applying, we use the conflict resolution (e.g. majority vote) to make the prediction.
Interpretability suffers potentially when multiple rules apply.


Both decision rules and decision lists can suffer from the problem that no rule applies to an instance. 
This can be solved by introducing a default rule. 
The default rule is the rule the applies when no other rule applies.
This can be used in both decision lists and decision sets.
The default rule can be the most common outcome of the training data that is not covered by other rules.
If a set or list of rules covers the whole feature space, we call it exhaustive.


There are many ways how to induce those kind of rules from a training dataset.

Methods to learn:
- Directly from data. 
This bullet point covers the most algorithms.
- Decision tree converted to rules.
Each path through the tree is one rule.
Rules made from trees have the nice attribute of covering the whole feature space and being mutually exclusive.
- Learn other model, then extract rules
- Learn assocation rules, then filter for the ones that have the target on the right hand side.

Let's start with a VERY simple strategy: Using the single, best feature to learn rules.

## Theory: OneR

The OneR algorithm ist the most simplest imaginable rule induction algorithm.
Find the best (single) feature that explains the classification the best. 
Continuous features are binned to make the categorical features.
For all features, a cross table between feature and outcome classes are built. 
It's a decision tree with only one split.
It covers the whole feature space.


OneR Algorithm		
- For each feature
- For each value of that feature:
- Create a rule which predicts the most frequent class for each level of the feature.
- Calculate the total error of the rule.
- Choose the feature with the smallest total error.

It's that easy.
And it is guaranteed that it always covers the whole feature space. 

http://www.saedsayad.com/oner.htm
https://cran.r-project.org/web/packages/OneR/
https://github.com/vonjd/OneR



TODO: Example with table for how to compute the accuracy. Something similar to: http://www.saedsayad.com/oner.htm

```{r OneR-freq-table1}
df = data.frame(
  location = c("good", "good", "good", "bad", "good", "good", "bad", "bad", "bad", "bad"),
  size = c("small", "big", "big", "medium", "medium", "small", "medium", "small", "medium", "small"), 
  pets = c("yes", "no", "no", "no", "only cats", "only cats", "yes", "yes", "yes", "no")
)
value = factor(c("high", "high", "high", "medium", "medium", "medium", "medium", "low", "low", "low"), levels = c("low", "medium", "high"))
```
From this table we create all the cross tables for each feature with the class to predict:
```{r OneR-freq-table2}
kable(table(df[,"location"], value))
kable(table(df[,"size"], value))
kable(table(df[,"pets"], value))
```

For a feature, we go through the table row by row: 
We use the class that has the highest occurence per feature value as the prediction and count how often it would be wrong.
For example, the location has possible values, bad and good. 
The predicted price class for houses in bad locations would be `low` and with this we make 2 mistakes, because two houses are actually medium priced.
The predicted price class for houses in good locations would be `high` and with this we make 2 mistakes, because two houses are actually medium priced.
All in all the accuracy of using the location feature is 6/10. 
For the other rules it is 7/10 for the size feature and 6/10 for the pets feature 

Strictly speaking, we produce more than one rule: 
It's actually one rule per unique feature value of the chosen best feature. 
The name 'OneR' (which stands for one rule) is actually misleading.
A better name might be OneFeatureClassifier or something.

OneR favors features with many possible levels, because it can more easily overfit the target: 
Imagine a dataset with only noise and no signal, meaning that all features take on random values and have no predictive value for the target.
Some features have more levels than others. 
The features with more levels can now more easily overfit. 
The most extreme feature would be one that has a unique level for each instance from the data. 
Now the OneR rule can perfectly overfit on the data. 
Keep that in mind. 
TODO: Explain strategy for ties.
TODO: List strategies for binning of continuous features. There was some blogpost about it.

We use the [cervical cancer classification task](#cervical) to test out the OneR algorithm. 
The following rule is created:
```{r oner-cervical}
library("OneR")
data("cervical")
rule = OneR::OneR(Biopsy ~ ., data = cervical)

rule.to.table = function(rule){
  dt = data.frame(x = names(rule$rules), prediction = unlist(rule$rules))
  colnames(dt) = c(rule$feature, "prediction")
  dt
}

kable(rule.to.table(rule))
```
The age feature is chosen as the best predictive rule feature. 
Since cancer is rare and the features just increase the risk a little, the prediction is still always 'Healthy'. 
Using the OneR algorithm is not useful in this case.

We can also use the OneR algorithm for regression task by binning the continuous outcome into a few levels.
This turns the regression problem into a classification problem.
That's what we do with the regression task of predicting [bike rentals](#bike):

```{r oner-bike}
data("bike")
bike$days_since_2011 = max(0, bike$days_since_2011)
rule = OneR::OneR(cnt ~ ., data = bike)

kable(rule.to.table(rule), row.names = FALSE)
```

The chosen feature is the year (which covers a time trend).
In 2012 there are more bike rentals than in 2011, probably because the rental service became more popular.

It should also be possible to use a continuous featue as outcome and predict the mean value per feature level instead of most frequent class. 
The best feature can be chosen by selecting the one with the lowest mse.

## Theory: Sequential Covering

A simple algorithm to learn a single rule + a procedure to sequentially learn rules with the simple algorithm sequentially to eventually cover the whole feature space:

A very simple approach: 
Find a really good rule on the dataset. 
Remove all datapoints covered by the rule (no matter if classified correctly or not). 
Repeat with remaining points until no points are left or some other stop condition is met.
Results in disjunct rules.

While there are many different ways to induce decision rules, I want to present a rather simple procedure which sequentially searches for rules that cover parts of the feature space, using the separate-and-conquer approach.

This approach can be seen as fitting trees with only depth of only one, and splits are not necessarily binary as in CART.

The covering algorithm for two classes (one positive, one negative):

- Start with an empty list of rules ($rlist$)
- Learn a rule $r$ using learn-one-rule with all the data
- while the rule set has not reached a quality threshold or some positive examples are not covered yet:
- Add rule $r$ to $rlist$
- Remove the data that is covered by rule $r$
- use learn-one-rule on the reamining data
- Return set of rules (sometimes called cover)

In case of more than two target classes, we repeate the covering algorithm for each class, where the current class is treated as the positive class and all the others as the negative class.
This is also called the one-versus-all strategy in classification.
This also leads (possibly) to overlapping rules. 
A solution: Assign weights to each rule and apply the rules with higher weights.


For example: 
We have a house value dataset and no rules learned yet. 
We learn the first rule, which turns out to be: 
'If the house is big and in a good neighbourhood, then the value of the house is high'. 
Then we remove all the big houses in a good neighbourhood.
With the remaining data we search the next rule: 
Maybe: 'If the house is in a good neighbourhood, then the value is medium' (Note that this rule is learned on data with big houses in good neighbourhoods removed, so only medium and small houses in good neighbourhoods remain).




This second part of repeating the single rule fitting is also called covering algorithm. 



```{r covering-algo, fig.cap = "The covering algorithm works by sequentially covering the feature space with single rules and removing the data points that are already covered by those rules. The approach is called separate-and-conquer."}
set.seed(42)
n = 100
dat = data.frame(x1 = rnorm(n), x2 = rnorm(n))
dat$class = rbinom(n = 100, size = 1, p = exp(dat$x1 + dat$x2) / (1 + exp(dat$x1 + dat$x2)))
dat$class = factor(dat$class)

min.x1 = min(dat$x1)
min.x2 = min(dat$x2)
p1 = ggplot(dat) + geom_point(aes(x = x1, y = x2, color = class))+ 
  scale_color_discrete(guide = "none")
p2 = ggplot(dat) + 
  geom_rect(xmin = -3, xmax = 0,   ymin = -2, ymax = -0.5, color = "black", fill = NA) + geom_point(aes(x = x1, y = x2, color = class)) + 
  scale_color_discrete(guide = "none")

dat.reduced = filter(dat, !(x1 <= 0 & x2 <= -0.5))

p3 = ggplot(dat.reduced) + 
  geom_rect(xmin = -3, xmax = 0,   ymin = -2, ymax = -0.5, color = "black", fill = NA)  + 
  geom_point(aes(x = x1, y = x2, color = class)) + 
  scale_x_continuous(limits = c(min.x1, NA)) + 
  scale_y_continuous(limits = c(min.x2, NA)) + 
  scale_color_discrete(guide = "none")

gridExtra::grid.arrange(p1, p2, p3, ncol = 2)

```


But how do we get a single rule?
There are many ways, one is using a greedy depth search in a decision tree.
a simple algorithm to learn a single rule in a classification setting:

Learning a single rule with beam search:
- Build a decision tree (can be CART or some other algorithm) on the input data. 
- Start at the root node and always choose the node that is the most pure (e.g. lowest missclassification rate)
- The majority class of the node we end up in is selected as the rule's prediction, the split conditions in the features leading to that node are the rule conditions.

To avoid local optima, a beam search can be performed which searches the search more exhaustively.

Learning a single rule is equivalent to a search problem.
The search space is the space of all possible rules. 
And the search is for the best rule (according to some criteria).
There are many different search strategies: 
hill-climbing, beam search, exhaustive search, best-first search, ordered search, stochastic search, top-down search, bottom-up search, ...
Here is not the place to go into details. 

```{r learn-one-rule, fig.cap = "Learning one rule by using a decision tree. A decision tree is grown to predict the target of interest. To extract a single, accurate rule, we go at each split to the node which locally produces the purest subset (i.e. with highest accuracy of target classes) and add the split value to the  rule condition. The prediction of the rule is the majority class of the node we land in. This examples shows how we learn a rule for predicting the value of a house. A decision tree is grown and we follow each time the node with the highest accuracy. In this case we end up with: 'If the location is the house is big and in a good location, we predict that the value is high (assuming high as the most common class in that node)'"}
knitr::include_graphics("images/learn-one-rule.png")
```


Difference to OneR algorithm: 
We want to learn a very accurate rule, that is allowed to cover only part of the feature space. 
Therefor we allow it to have many conjunction (AND), which the OneR does not include.


Let's look at one implementation of the Sequential covering algorithm: RIPPER (Repeated Incremental Pruning to Produce Error Reduction; Cohen, 1995). 
RIPPER is a bit more sophisticated and uses as post-processing phase (rule pruning) for the optimization of the rule-set.
RIPPER learns unordered rule sets. 
For rule conflict resolution it employs weighting of rules. 
The rules are weighted by the Laplace-value of a rule, which is used for making the final prediction.
The laplace estimate is a corrected estimate of the accuracy of a rule: 

$$ Laplace(rule) = \frac{pos + 1}{(pos + 1) + (neg + 1)}$$

where pos are the positive instances covered by the rule. 
The usual estimate 

$\frac{pos}{neg + pos}$

has the problem that when the rule covers only one instance, which is positive it has 100% accuracy, and a rule that covers 100 positive instances (and no negative) has the same but is obviously a better rule. 
The Laplace estimate puts a prior on the accuracy which affects especially the smaller rules.  

On the classification task for [cervical cancer](#cervical), the RIPPER algorithm does not find any rule:
```{r jrip-cervical}
library("RWeka")
library(rJava)

rule = JRip(Biopsy ~ ., data = cervical)
rules = as.matrix(scan(text=.jcall(rule$classifier, "S", "toString"), sep="\n", what="") )[-c(1:2, 6), ,drop=FALSE]
rules  = data.frame(rules)
kable(rules)
```


When we use RIPPER on the regression task for predicting [bike rentals](#bike) some rules are found. 
Since RIPPER only works for classification, the bike counts has to be turned into a categorial outcome. 
I achieved this by binning the bike counts into the quartiles.

The following table shows the resulting rules. 
The conditions are followed behind the arrow by the predicted interval. 
For example $(4548, 5956)$ is the interval that covers predicted bike rentals between $4548$ and $5956$.
```{r jrip-bike}
bike2 = bike
bike2$cnt = round(bike2$cnt)
bike2$cnt =  cut(bike$cnt, breaks = quantile(bike$cnt), dig.lab = 10, include.lowest = TRUE)

rule = JRip(cnt  ~ ., data = bike2)
rules = scan(text=.jcall(rule$classifier, "S", "toString"), sep="\n", what="")
# removes text
rules = rules[-c(1, 2, length(rules))]
rules = gsub("\\([0-9]*\\.[0-9]\\/[0-9]*\\.[0-9]\\)", "", rules)
rules = as.matrix(rules)[-c(1:2, 6), ,drop=FALSE]
rules  = data.frame(rules)
kable(rules)
```
The interpretation is straightforward:
If the conditions happen, then the predicted (interval) of bike rentals is the right hand side.
The last rule is the default rule when none of the other rules fires for an instance. 
For predicting a new instance, start at the top of the list and check if a rule fires. 
When all conditions match, then the right hand side of the rule is the prediction for this instance. 
Because of the default rule it is guaranteed that there will always be a prediction.


There are also many other ways to create single rules. 
Also there are many othe ways to create rule sets. 
One is to grow a tree and turn it into rules. 
This always works. 
There are more sophisticated versions 
For example the C4.5 RULES algorithm  (Quinlan, 1987)[^c4.5] adds some pruning and ranking of the rules

## Decision rules from decision trees

See RuleFit chapter.


## Building decision rules from pre-mined rules

In this section I will show you another approch to learn decision rules:

- Use some algorithm to pre-mine lots of rules
- Use another algorithm to build to aggregate those rules to a decision list (or a decision set)

Scalable bayesian rule lists is as the name suggest an example for a decision list.
The other examples where decision sets.
Specifically we will learn about Bayesian Rule Lists (Letham et. al, 2015)[^brl], which are decision lists that use methods from Bayesian statistics to learn decision lists from rules pre-mined with the FP-tree algorithm (Borgelt 2005)[^fp-tree] 
But let's start slowly:

### Pre-mining

Before the theory, think about a recommendation system. 
Like on a shopping website "People that bought this bycicle also bought this helmet" or for movies "People how watched 'Princess Mononoke' also watched 'Spirited Away'". 
These are association rules and are a kind of IF-THEN rule ("IF someone watched 'Princess Mononoke' THEN also 'Spirited Away'").
Association rule mining or frequent pattern mining extracts association rules from a dataset. 
These algorithms are categorized under unsupervised machine learning algorithm, meaning it does not need any label.
That also means that we get all kinds of rules in the IF-THEN format, where the things that are in the THEN-part are not necessarily from the target we are interested in. 
In the appartment example, there might also be an association rule that says: IF the value=high and location=medium THEN the size=big.
We can still use those rules by keeping only the ones that have the target outcome on the right hand side (the THEN-part).

A popular and effective method in assocation rule mining is the FP-Growth Algorithm. 
It's a data structure that represents the patterns found in a dataset.
It assumes that each feature is categorical.
The user has to define the minimum support of a feature value at which it is considered 'frequent'.
If 50% of the houses had a medium size, then 50% would be the support of this feature value, also sometimes called 'item' in the context of pattern mining.
Infrequent items are discarded. 
At which threshold the occurence of a feature value is 'infrequent' is up to the user (1%? 10%? 0.0032%?). 
Let's say we discard feature values with less then 10% frequency. 
If a feature in the dataset would be number of balconies with following distribution: 50% no balcony, 45% one balcony, 5% no balcony, then we would discard the no balcony feature value. 
This doesn't mean we delete the instances, just that we disregard this feature value for IF-THEN rules.

The FP-Growth algorithm then grows a FP-tree, which can be used to extract frequent patterns in the dataset (IF-THEN rules). 
These have to be filtered to contain only those where the target outcome is included (which, because the pattern mining is unsupervised is treated like the other features).
I want go into details about FP-Growth, so if you want to learn about FP-Growth, I recommend [Data Mining Algorithms in R](https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_FP-Growth_Algorithm).

Alternatives to the FP-Growth algorithm is for example the Apriori algorithm, which is slower but easier to understand.

The outcome of this procedure is a lot of rules, which might be overlapping. 
We could now simply build a decision set (with majority vote for example), but this is not necessarily interpretable, since the rules might be heavily overlapping and there could be hundreds or thousands of rules. 

So we need something to cut the number of rules, prefer shorter rules over the long ones and also have a decision list (=with order) and not a decision set.
One way to do this is by introducing a likelihood function with some prior distributions for the length of rules (preferring shorter ones) and the number of rules (also preferring less rules over more). 
Say hello to Bayesian Rule List algorithm!
It can be used in any multi-class classification task and requires all input features to be categorized.
Continuous features can be used if cut into intervals and made into a categorical feature (either fixed intervals or by distribution, i.e. quantiles).

The algorithm starts with premining rules, for example with the FB-Growth algorithm, but theoretically any rule-mining algorithm would work.

BRL makes the following distribution assumption of the rules:

**if** conditions1 **then** $y\sim\text{Multinomial}(\theta_1)$, $\theta_1\sim\text{Dirichlet}(\alpha+N_1)$

**else if** conditions2 **then** $y\sim\text{Multinomial}(\theta_2)$, $\theta_2\sim\text{Dirichlet}(\alpha+N_2)$

...

**else if** conditionsm **then** $y\sim\text{Multinomial}(\theta_m)$, $\theta_m\sim\text{Dirichlet}(\alpha+N_m)$

**else**  $y\sim\text{Multinomial}(\theta_0)$, $\theta_0\sim\text{Dirichlet}(\alpha+N_0)$


But the real meat is the assumptions about the distributions of the rules and the rule list:


TODO: a-priori distributions.

Bayesian statistics is always a bit tricky, because we usually can't simply compute the correct answer, but we have to draw candidates and evaluate them and update our posteriori estimates (usually and also here with Monte Carlo Markov Chains)
Here it is even more tricky, because we have to draw from the distribution of decision lists. 
The authors suggest following recipe to sequentially draw new decision lists:

The for sampling a decision list:

- Pre-mine rules.
- Sample the length $m$ for the list from a truncated Poisson distribution.
- For the default rule: Sample the parameter for the distribution of the target value  (i.e. the rule that applies when nothing else applies).
- For decision list rule $j=1,\ldots,m$, do:
  - Sample the length of the rule $l_j$ (i.e. number of conditions) for rule $j$
  - Sample a rule of length $l_j$ from the pre-mined rules.
  - Sample the distribution parameter of the THEN-part given the sampled rule (i.e. for the distribution of the target outcome given the rule)
- For each observation in the dataset:
  - Find the rule from the decision list that fires first
  - Draw the predicted outcome from the probability distributiion suggested by the rule that applies.

The estimation needs three parameters as input: 

- $\lambda$, the prior expected decision list length.
- $\eta$, the prior expected number of conditions in a rule.
- $\alpha$, the prior pseudo-count for the positive and negative classes. Best fixed at $(1,1)$.

The goal is to sample from: 

$$p(d|x,y,A,\alpha,\lambda,\eta)\propto{}p(y|x,d,\alpha)p(d|A,\lambda,\eta)$$


Once we can sample from the distribution of the decision lists, we can find the decision list that maximizes the posteriori likelihood, meaning it explains the data well (acccuracy), but is also interpretable because it has few and short rules, which is due to the priori distributions.

Since we can't directly optimize this problem, we do Metropolis-Hastings algorithm: 
Starting from a random draw, we randomly either 1) move a rule to a different position in the list or 2) add a rule to the current decision list from the pre-mined rules 3) Remove a rule from the decision list. 
Which of the rules is switched/added/deleted is also random.
At each of this step, the algorithm evaluates the posteriori probability of the decision list (mix of how accuract it is and how much it satisfies the priori distributions).
The Metropolis Hastings algorithm ensures that we sample more heavily the decision lists that are likely in terms of accuracy and prior.
This procedure gives us lots of samples from the distribution of decision lists. 
We end up with the posterior distribution of decision lists, but of course we would like to have a single decision list and not a distribution. 
The BRL algorithm selects the decision list of the samples with the highte posterior probability.

The mathematical details are pretty tough, so skipping it here. 



It's pretty fast. 
TODO: Link paper and R package.

For the examples I had to discretize all input features first to make the SBRL algorithm work. 
I used frequency based binnning for that (aka quantiles).

```{r sbrl-cervical, eval = FALSE}
library("sbrl")
library("arules")
data("cervical")

cervical2 = as.data.frame(lapply(cervical, function(x) {
  if(is.factor(x) || length(unique(x)) < 3) {
    as.factor(x)
  } else {
    discretize(x, breaks = 3, method = "interval")
    #discretize(x, breaks = max(length(unique(x))-1, 5))
  }
}))
cervical2$label = cervical2$Biopsy
cervical2$Biopsy = NULL
sbrl(cervical2[c("label", "Age")], pos_sign = "Cancer")
```


```{r sbrl-bike}
library("sbrl")
library("arules")
data("bike")

bike2 = bike
bike2$label = bike2$cnt > 5000
bike2 = as.data.frame(lapply(bike2, function(x) {
  if(is.factor(x) || length(unique(x)) < 3) {
    as.factor(x)
  } else {
    discretize(x, breaks = 3, method = "interval")
    #discretize(x, breaks = max(length(unique(x))-1, 5))
  }
}))
bike2$label = bike2$cnt
bike2$cnt = NULL
sbrl(bike2, pos_sign = TRUE)
```

## Advantages

- Ultra easy to interpret: Probably the easiest from all interpretable models.
- Can be as expressive as decision trees
- Straightforward to generate
- Fast to do the prediction
- Rule set can be more clearer than decision trees, because decision tree has replicated subtrees often.
- Compared to logistic regression and linear models: 
No assumption about distributions of features or target.
- For trees: No need to transform the features.
- Feature selection: Has integrated sparsity of results
- Simple rules like from OneR can be used as baseline for more complex algorithms

## Disadvantages

- Research focused on classification and not regression.
- Prone to overfitting (at least the older decision rule algorithm like XXX)
- In multi-class siutations, the covering algorithm concentrates on one class at a time whereas decision tree learner takes into account all the classes.
- Rules created from decision trees may be harder to interpret. 
- Are bad at describing linear relationships between the features and the output.

## Software and Alternatives

(Notice how they all have fancy abbreviations in caps lock.)
- RIPPER
- OPUS
- CPAR (Yin et. al 2003)
- FRL (Wang et al. 2015)
- BRL (Letham et al. 2015) 
- SBRL: Only classification and only categorial features. Uses Bayesian statistics for learning the rules and a clever bit-wise implementation. Builds a one-sided tree or decision list.
- CORELS https://github.com/nlarusstone/corels
- TLBR (Su et al. 2015)
- IDS (Lakkaraju et al. 2016)
- Rule Set (Wang et al. 2016)
- 1Rule (Malioutov et al. 2017)
- https://github.com/Jasamba/Rule-Extraction-in-Python
- OneR


Further readings: 
Foundations of Rule Learning. Extensive book on rule learning.



[^oner]: R. C. Holte (1993). Very simple classification rules perform well on most commonly used datasets. Machine Learning, 11, 63–91.

[^ripper]: Cohen, W. (1995). Fast effective rule induction. Icml. Retrieved from http://www.inf.ufrgs.br/~alvares/CMP259DCBD/Ripper.pdf

[^c4.5]: Quinlan, J. R. (1987). Generating production rules from decision trees. Proceedings of the Tenth International Joint Conference on Artificial Intelligence, 30107, 304–307. Retrieved from http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.9054&amp;rep=rep1&amp;type=pdf


[^brl]: Letham, B., Rudin, C., McCormick, T. H., & Madigan, D. (2015). Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model. Annals of Applied Statistics, 9(3), 1350–1371. https://doi.org/10.1214/15-AOAS848

[^sbrl]: Yang, H., Rudin, C., & Seltzer, M. (2016). Scalable Bayesian Rule Lists, 31. Retrieved from http://arxiv.org/abs/1602.08610

[^fp-tree]: Borgelt, C. (2005). An implementation of the FP-growth algorithm. Proceedings of the 1st International Workshop on Open Source Data Mining Frequent Pattern Mining Implementations - OSDM ’05, 1–5. http://doi.org/10.1145/1133905.1133907