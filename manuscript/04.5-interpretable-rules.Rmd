```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all("../")
set.seed(42)
```

# IF-THEN rules

IF-THEN rules (aka decision rules) are simple statements consisting of a series of conditions and a prediction:
IF the all of the conditions apply, THEN we make a certain prediction.
An example: 
IF it rains today (condition), THEN it will rain tomorrow (prediction). 

keywords: decision rules/sets/lists, association rules

There are lots of algorithms how to produces rules. 
This chapters purpose is to help you how to think about rules and understand them fundamentally, but not to go through all possibilities. 
For this I recommend XXX.

## What are decision rules?

Decision rules are models that consists of if X then Y statements. 
Within the if statement there are sets of features and conditions on them (like X > 4) and Y is a statement about the outcome of interest.

Simple if-then statements can and are often written manually. 
In machine learning, these types of rules are learned automatically.
Can be derived from decision trees, but don't have to.

They have the following general structure:
RULE: IF CONDITIONS THEN TARGET


One decision rule from a model predicting rents could be:
If a flat is bigger than 100 square meters and has a garden the average rent is 2000 Euro per month.

Let's split that up:
- 'Flat is bigger than 100 square meters' is the first part of the IF statement
- 'Flat has a garden' is a second conditions in the IF.
- Both are connected with an 'AND', so both have to be true for the rule to apply
- Predicted outcome (THEN) is that the average rent is 2000 Euro per month.



Arguably decision rules are the most simple classifier in terms of understandability. They are very close to natural language and the way we think. Of course it relies on using sensible, understandable features and the number of conditions within the if statement.

Usually there is more then one rule to describe the associations of the features with the outcome. In our example we only have one rule covering big appartments with flats, but what about a smaller apartment?

Rules can be induced from trees. Each path through the tree is one rule. 
Rules made from trees have the nice attribute of covering the whole feature space and being mutually exclusive.

We can make following distinctions:
- descriptive rule discovery: concerned with finding single rules 
- 
- predictive rule learning: 

There are many ways how to induce those kind of rules from a training dataset.
The induction algorithms differ in how they create the rules:
- Do they allow overlapping rules and how are conflicts handled?
- Do they cover the whole training set? Or can there be a situation where none of the rules apply?


Conflict resolution if many rules are triggered:
- size ordering: use rule with the toughest requierements
- class based: based on cost of missclass
-rule-based (decision list): by quality of rule


Mutually exclusive rules: Rules are independent of each other and every recored is covered at most by one rule. 

Exhaustive rules: For every possible combination of feature values there is a rule. exhaustive coverage.


Methods to learn:
- Directly from data
- Decision tree converted to rules
- Learn other model, then extract rules


You can use a rule-learning algorithm that learns a single rule. 
And use that within sequential covering, where training records covered by this rule are removed.



We can distinguish between: 

## Theory: OneR

Simplest imaginable, which also covers the whole space: 
Find the best (single) feature that explains the classification the best. 
Continuous features are binned to make the categorical features.
For all features, a cross table between feature and outcome classes are built. 


OneR Algorithm		
For each predictor,
     For each value of that predictor, make a rule as follows;
           Count how often each value of target (class) appears
           Find the most frequent class
           Make the rule assign that class to this value of the predictor
     Calculate the total error of the rules of each predictor
Choose the predictor with the smallest total error.

It's that easy.
And it is guaranteed that it always covers the whole feature space. 

http://www.saedsayad.com/oner.htm
https://cran.r-project.org/web/packages/OneR/
https://github.com/vonjd/OneR



TODO: Example with table for how to compute the accuracy. Something similar to: http://www.saedsayad.com/oner.htm

```{r OneR-freq-table1}
df = data.frame(
  location = c("good", "good", "good", "bad", "good", "good", "bad", "bad", "bad", "bad"),
  size = c("small", "big", "big", "medium", "medium", "small", "medium", "small", "medium", "small"), 
  pets = c("yes", "no", "no", "no", "only cats", "only cats", "yes", "yes", "yes", "no")
)
value = factor(c("high", "high", "high", "medium", "medium", "medium", "medium", "low", "low", "low"), levels = c("low", "medium", "high"))
```
From this table we create all the cross tables for each feature with the class to predict:
```{r OneR-freq-table2}
kable(table(df[,"location"], value))
kable(table(df[,"size"], value))
kable(table(df[,"pets"], value))
```

For a feature, we go through the table row by row: 
We use the class that has the highest occurence per feature value as the prediction and count how often it would be wrong.
For example, the location has possible values, bad and good. 
The predicted price class for houses in bad locations would be `low` and with this we make 2 mistakes, because two houses are actually medium priced.
The predicted price class for houses in good locations would be `high` and with this we make 2 mistakes, because two houses are actually medium priced.
All in all the accuracy of using the location feature is 6/10. 
For the other rules it is 7/10 for the size feature and 6/10 for the pets feature 

Strictly speaking, we produce more than one rule: 
It's actually one rule per unique feature value of the chosen best feature. 
The name 'OneR' (which stands for one rule) is actually misleading.
A better name might be OneFeatureClassifier or something.

TODO: Explain possibility of overfitting.
TODO: Explain strategy for ties.
TODO: List strategies for binning of continuous features. There was some blogpost about it.

TODO: R Example

## Theory: Covering Algorithm

The simplest thing you can do: A simple algorithm to learn a single rule + a procedure to sequentially learn rules with the simple algorithm sequentially to eventually cover the whole feature space:

A very simple approach: 
Find a really good rule on the dataset. 
Remove all datapoints covered by the rule (no matter if classified correctly or not). 
Repeat with remaining points until no points are left or some other stop condition is met.
Results in disjunct rules.

While there are many different ways to induce decision rules, I want to present a rather simple procedure which sequentially searches for rules that cover parts of the feature space, using the separate-and-conquer approach.

This approach can be seen as fitting trees with only depth of only one, and splits are not necessarily binary as in CART.

### Learning a single rule
First let's start with a simple algorithm to learn a single rule in a classification setting:

Difference to OneR algorithm: 
We want to learn a very accurate rule, that is allowed to cover only part of the feature space. 
Therefor we allow it to have many conjunction (AND), which the OneR does not include.

- Start with empty rule
- Add conditions as long as they improve the information gain heuristic.
This heuristic evaluates  the improvement of a rule with respect to its predecessors:
- this is an exhaustive search over all possible features and split points. For categorical features, test combination of levels as rule. 
For numerical features, try different cut-offs as condition for a rule.
- add the condition that improves the rule the most according to a quality criterion.
- Stop when the rules does not cover negative samples
- prune rule using incremental reduced error pruning
- measure for pruning v = (p-n) / (p+n), where p is the number of positive examples covered by the rule in the validation set and n for negative samples. 
- delete any finals sequence of conditaions that maximizes v.

The quality criterion can be XXX.

The simplest way to learn decision rules: 
Take some algorithm that can learn a single rule on a dataset. 
Apply that algorithm to a dataset. 
Remove from that dataset all the examples where the rule applies. 
Repeat until all data points are covered or some other stopping criterium is reached.

TODO: Add artificial example with rent or value of a house


TODO: Concepts of coverage and accuracy: 
A rule applies to a certain region of the feature space. 
This is called the coverage of the rule. 
It can also expressed to how many instances in the dataset it applies, or if you know the distribution, then it can even be measured as percentage of the population it covers. 
For example a rule for house value that says: 
A house that is in a nice area costs 400k. 
And 10% percent of all houses are in nice areas. 
Then the rule has a coverage of 10%. 

The accuracy of a rule is a measurement for how good the rule is for the instances it covers. 
Different for regression and classification. 
In classification you can measure the missclassification rate of the rule. 
In regression the mean squared error of the rule.

This second part of repeating the single rule fitting is also called covering algorithm. 

### Theory and Example: Covering algorithm

Algorithm for rule set:

- use sequential covering algorithm
- find best rle that covers current set of positive examples
- remove all samples (positive and negative) that are covered by this rule
- for each new rule, compute the new description lenght of the set. stop adding new rules when the description lenght is d bits longer than the smalles description length obtained so far.

```{r covering-algo, fig.cap = "The covering algorithm works by sequentially covering the feature space with single rules and removing the data points that are already covered by those rules. The approach is called separate-and-conquer."}
set.seed(42)
n = 100
dat = data.frame(x1 = rnorm(n), x2 = rnorm(n))
dat$class = rbinom(n = 100, size = 1, p = exp(dat$x1 + dat$x2) / (1 + exp(dat$x1 + dat$x2)))
dat$class = factor(dat$class)

min.x1 = min(dat$x1)
min.x2 = min(dat$x2)
ggplot(dat) + geom_point(aes(x = x1, y = x2, color = class))
ggplot(dat) + 
  geom_rect(xmin = -3, xmax = 0,   ymin = -2, ymax = -0.5, color = "black", fill = NA) + geom_point(aes(x = x1, y = x2, color = class)) 

dat.reduced = filter(dat, !(x1 <= 0 & x2 <= -0.5))

ggplot(dat.reduced) + 
  geom_rect(xmin = -3, xmax = 0,   ymin = -2, ymax = -0.5, color = "black", fill = NA)  + 
  geom_point(aes(x = x1, y = x2, color = class)) + 
  scale_x_continuous(limits = c(min.x1, NA)) + 
  scale_y_continuous(limits = c(min.x2, NA)) 

```


Optimize the ruleset:

- for each rule r in the rule set:
- consider 2 alternative rules:
- replacement rule: grow ne rule from scratch
- revised rule: add conjuncts to extend the rule r
- compare original with replacement and revised 
- choose rule set that minimizes the minimum description length principle
- repeat rule generation and optimization for the remaining positive examples.


This whole procedure is prone to overfit the data. 
There are many variants of this to improve this simple idea. 
One is RIPPER, which was invented 1995 and is used a lot.
The RIPPER algorithm introduces some pruning of the rules and different stopping criteria. 
The examples will be shown with the RIPPER algorithm.

TODO: R example

## Theory and Example: From trees

Tree based for example.
Example: C4.5 rules. Extract rules from an unpruned decision tree that was induced by C4.5 algorithm (TODO: Cite both tree paper and rule paper)

TODO: R example

## Theory and Example: From assocation rules
From association rules (unsupervised method), by only allowing rules that have the class on the right hand side.

TODO: Shortly! explain the assocation rules algo. 
TODO: Explain step from assocation rules to predictive rules.

TODO: R example

## Theory and Example: SBRL

TODOL: Explain very shortly, not many formulas or anything.

An approach inspired powered by Bayesian statistics:
SBRL: Scalable bayesian rules list
Consists of two steps: 
1. Mine IF-THEN rules from the data
1. Combine the rules into a model by maximizing the posterior probability of the rule.
It's pretty fast. 
The mathematical details are pretty tough, so skipping it here. 
TODO: Link paper and R package.
TODO: Maybe as alternative to covering? or alternative to learn1rule+covering?

TODO: R example

## Advantages

- Ultra easy to interpret: Probably the easiest from all interpretable models.
- Can be as expressive as decision trees
- Straightforward to generate
- Fast to do the prediction
- Rule set can be more clearer than decision trees, because decision tree has replicated subtrees often.
- Compared to logistic regression and linear models: 
No assumption about distributions of features or target.
- For trees: No need to transform the features.
- Feature selection: Has integrated sparsity of results
- Simple rules like from OneR can be used as baseline for more complex algorithms

## Disadvantages

- Research focused on classification and not regression.
- Prone to overfitting (at least the older decision rule algorithm like XXX)
- In multi-class siutations, the covering algorithm concentrates on one class at a time whereas decision tree learner takes into account all the classes

## Software and Alternatives

- RIPPER
- OPUS
- CPAR (Yin et. al 2003)
- FRL (Wang et al. 2015)
- BRL (Letham et al. 2015)
- TLBR (Su et al. 2015)
- IDS (Lakkaraju et al. 2016)
- Rule Set (Wang et al. 2016)
- 1Rule (Malioutov et al. 2017)
- https://github.com/Jasamba/Rule-Extraction-in-Python
- OneR



