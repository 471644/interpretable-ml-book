```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

## Partial Dependence Plot (PDP) and ALE Plot {#pdp}

The partial dependence plot (PDP) shows the marginal effect of a feature on the predicted outcome of a previously fit model (J. H. Friedman 2001[^Friedman2001]). 
The prediction function is fixed at a few values of the chosen features and averaged over the other features.
The Accumulated Local Effects (ALE) plot [^ALE] is an alternative to the PDP and fixes the problem that PDPs have when features are correlated.

*Keywords: partial dependence plots, PDP, marginal means, predictive margins, marginal effects, ALE plot*


A partial dependence plot can show if the relationship between the target and a feature is linear, monotonic or more complex.
Applied to a linear regression model, partial dependence plots will always show a linear relationship, for example.

The partial dependence function for regression is defined as:

$$\hat{f}_{x_S}(x_S)=E_{x_C}\left[\hat{f}(x_S,x_C)\right]=\int\hat{f}(x_S,x_C)d\mathbb{P}(x_C)$$

The term $x_S$ is the set of features for which the partial dependence function should be plotted and $x_C$ are the other features that were used in the machine learning model $\hat{f}$.
Usually, there are only one or two features in $x_S$.
Concatenated, $x_S$ and $x_C$ make up $x$.
Partial dependence works by marginalizing the machine learning model output $\hat{f}$ over the distribution of the features $x_C$, so that the remaining function shows the relationship between the $x_S$, in which we are interested, and the predicted outcome.
By marginalizing over the other features, we get a function that only depends on features $x_S$, interactions between $x_S$ and other features included.

The partial function $\hat{f}_{x_S}$ along $x_S$ is estimated by calculating averages in the training data, which is also known as Monte Carlo method:

$$\hat{f}_{x_S}(x_S)=\frac{1}{n}\sum_{i=1}^n\hat{f}(x_S,x_{Ci})$$

In this formula, $x_{iC}$ are actual feature values from the dataset for the features in which we are not interested and $n$ is the number of instances in the dataset.
One assumption made for the PDP is that the features in $x_C$ are uncorrelated with the features in $x_S$. 
If this assumption is violated, the averages, which are computed for the partial dependence plot, incorporate data points that are very unlikely or even impossible (see disadvantages).

For classification, where the machine model outputs probabilities, the partial dependence function displays the probability for a certain class given different values for features $x_S$.
A straightforward way to handle multi-class problems is to plot one line or one plot per class.


The partial dependence plot is a global method:
The method takes into account all instances and makes a statement about the global relationship of a feature with the predicted outcome.


BUUUUUTTTT there is a problem with partial dependence plots, and that's correlated features.
When two input features are strongly correlated, the partial dependence plot for one of those features will not make sense.
How come? 
When features are correlated and we compute the partial dependence, we use grid values in combination with the correlated feature that don't make sense. 
For example we want to predict the worth of a house using number of rooms and the size of the living area.
Then we analyse the partial dependence of the predicted price on the living area.
For the first grid values - let's say a living area of 30 square meters - we use all data points, even flats with 10 rooms, which don't yield a realistic data instance. 
But in PD plots we use those to compute the average effect. 
So what can we do?
The answer is to only use instance for which the grid values that are forced as feature values make realistic instances.
One solution is to use only the neighbours, meaning that for the 30 square meters, we only compute the partial dependence with houses that are between let's say 25 and 35 square meters (wait, are those real houses? maybe houses for ants). 
The solution also has a name: ALE plots, which is short for Accumulated Local Effects (ALE) Plots.

### Accumulated Local Effects (ALE) Plots

ALEplots are partial dependence plots that solve the problem the PDPs have with correlated features.
Both types solve the problem of computing the average marginal effects: 

$$E_{x_C}\left[\hat{f}(x_S,x_C)\right]$$

The difference is that PDPs assume that the feature are not strongly correlated for computing the average effects. 
The ALEplot also works for correlated features, by only computing the partial effects locally.
So how do ALEplots solve this problem?
First, the method divides a feature into intervals instead of grid points.
This is almost the same, but the difference is important. 
Because ALEplots now use all the data points in each interval to compute the partial effects.


```{r aleplot-motivation, fig.cap = "Two strongly correlated features x1 and x2. To compute the partial dependence at x1 = 0.75, the PDP averages the prediction over the whole range of x2 (left plot), even if unrealistic combinations of x1 and x2 are used. ALE plots average only over the conditional distribution at x1 = 0.75 (right plot). "}

set.seed(1)
n = 100
intercept = 0.75

x1 = runif(n)
x2 = x1 + rnorm(n, sd = 0.1)
df = data.frame(x1, x2)  

p = ggplot(df) + geom_point(aes(x = x1, y = x2)) +  
  theme(panel.grid.major.y = element_blank(), 
    panel.grid.minor.y = element_blank(), 
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank())

p.int = p + geom_vline(xintercept = intercept)
x1.dens = density(x1)
x1.dens.df = data.frame(dens = x1.dens$y, x = x1.dens$x)


p1 = p.int + geom_path(data = x1.dens.df, aes(x = intercept - dens/10, y = x)) + 
  ggtitle(sprintf("Marginal distribution p(x2)", intercept)) + 
  scale_y_continuous(limits = c(-0.2, 1.2))

x1.dens.ale = density(x1[(x1 > (intercept - 0.1)) & (x1 < (intercept + 0.1))])
x1.dens.ale.df = data.frame(dens = x1.dens.ale$y, x = x1.dens.ale$x)

p2 = p.int + geom_path(data = x1.dens.ale.df, aes(x = intercept - dens/10, y = x)) + 
  ggtitle(sprintf("Conditional distribution p(x2|x1=%.2f)", intercept)) + 
  scale_y_continuous(limits = c(-0.2, 1.2))

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

The following graphic show how the ALEplots are calculated. 

TODO: Create plot 
1. only data points
2. division into intervals
3. exemplary calculation for one interval
4. resulting ALE plot

```{r aleplot-computation, fig.cap = ""}

set.seed(12)
n = 25

x1 = runif(n)
x2 = x1 + rnorm(n, sd = 0.1)
df = data.frame(x1, x2)  

p = ggplot(df) + geom_point(aes(x = x1, y = x2)) +
  theme(panel.grid.major.y = element_blank(), 
    panel.grid.minor.y = element_blank(), 
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank()) 


grid.df = data.frame(x1 = seq(from = 0,  to = 1, length.out = 6)[1:6], x2 = NA)
label.df = grid.df[1:5,]
label.df$x1 = label.df$x1 + 0.1
label.df$x2 = 0.95
label.df$label = sprintf("N1(%i)", 1:5)

break.labels = c(expression(z[0~","~1]),  expression(z[1~","~1]), expression(z[2~","~1]), expression(z[3~","~1]),
  expression(z[4~","~1]), expression(z[5~","~1]))

diff.df = df[df$x1 <= 0.8 & df$x1 > 0.6, ]

p + geom_vline(data = grid.df, aes(xintercept = x1), linetype = 3) + 
  scale_x_continuous(breaks = seq(from = 0,  to = 1, length.out = 6), limits = c(0, 1), labels = break.labels) + 
  geom_label(data = label.df, aes(x = x1, y = x2, label = label)) + 
  geom_segment(data = diff.df, aes(x = 0.6, xend = 0.8, y = x2, yend  = x2), arrow = arrow(ends = "both", angle = 90, length = unit(0.07, "inches")))
```


First we have to estimate the uncentered effect: 

$$\hat{\tilde{f}}_{j,ALE}(x)=\sum_{k=1}^{k_j(x)}\frac{1}{n_j(k)}\sum_{i:x_{i,j}\in{}N_j(k)}\left[f(z_{k,j},x_{i\setminus{}j}) - f(z_{k-1,j},x_{i\setminus{}j})\right]$$

This effect is then centered, so that the mean effect is zero:

$$\hat{f}_{j,ALE}(x)=\hat{\tilde{f}}_{j,ALE}(x)-\frac{1}{n}\sum_{i=1}^{n}\hat{\tilde{f}}_{j,ALE}(x_{i,j})$$



```{r correlation-problem, fig.cap = "Two features and a predicted outcome. The model simply predicts for y (shaded background) the sum of the two features, but there is also some artifact where the model always predicts 2, when x1 is larger than 0.7 and x2 smaller than 0.3. Since the distribution doesn't touch this area, there shouldn't be a problem. Shoudl the distribution shift, then there would be one."}

set.seed(12)
n = 25

x1 = runif(n)
x2 = x1 + rnorm(n, sd = 0.1)
df = data.frame(x1, x2)  
df$y = x1 + x2

mod  = lm(y ~ ., data = df)

y.fun = function(X.model, newdata) {
  pred = predict(X.model, newdata)
  pred[newdata$x1 > 0.7 & newdata$x2 < 0.3] = 2
  pred
}

grid.dat = expand.grid(x1 = seq(from = 0, to = 1, length.out = 20), x2 = seq(from = 0, to = 1, length.out = 20))
grid.dat$predicted = y.fun(mod, grid.dat)

ggplot(df) + geom_tile(data = grid.dat, aes(x = x1, y = x2, fill = predicted)) + 
  geom_point(aes(x = x1, y = x2), size = 3) + 
  scale_fill_gradient(low = "white", high = "blue")
```

"This is a completely made up example", I here some people yelling.
"What kind of model would do such a thing?! Unrealistic!", other join. 
Yes, this is a toy example. 
BUT, remember that, when you train a model, the model only fits the existing data points. 
Any area outside of the distributions, depending on the type of model anything can happen, because the model will not be penalized for doing weird stuff abroad.
"Where there is no judge, there will be no judgement".
It's like dancing like nobodies watching.
Linear models for example have extreme behaviour outside the are of the data, because they are linear functions. 
Imagine a linear regression model for predicting the worth of a house which has a postive weight for the influence of the living area.
Going outside the data means going beyond certain levels of square meters (let's say 3000) will make your predictions grow extreme.
The phenomenon is called extrapolation and that's also what's happening when you have correlated features with partial dependence plots.
Extrapolation of values to values that don't even exist.
See what happens with the PD plots and how they compare to the ALE plots in our little example:


```{r correlation-pdp-ale-compute, plot = FALSE}
library(ALEPlot)
pred = Predictor$new(mod, data = df, predict.fun = y.fun)
pdp = Partial$new(pred, feature = "x1", ice = FALSE)
pdp1 = pdp$plot() + ggtitle("PDP")
pdp = Partial$new(pred, feature = "x2", ice = FALSE)
pdp2 = pdp$plot() + ggtitle("PDP")

ale1.df = data.frame(ALEPlot(df, mod, J = "x1", pred.fun = y.fun))
ale1 = ggplot(ale1.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("x1") + scale_y_continuous(expression(hat(y))) + 
  ggtitle("ALE plot")
ale2.df = data.frame(ALEPlot(df, mod, J = "x2", pred.fun = y.fun))
ale2 = ggplot(ale2.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("x2") + scale_y_continuous(expression(hat(y))) + 
  ggtitle("ALE plot")
```


```{r correlation-pdp-ale-plot, fig.cap = "Comparing the marginal feature effects computed with PDP and ALE. The PDP picks up on the artifact in the data, which is not part of the data distribution, which shows as a jump in the plot. The ALE plot correctly identifies that the machine learning model has a linear relationship between features and prediction, ignoring areas where no data points are."}
gridExtra::grid.arrange(pdp1, pdp2, ale1, ale2)
```


ALE plot can also be used with more features, but it only makes sense with two at maximum.
Two feature works as one feature, with the difference that we don't build intervals, but rectangles and use for the computation of the marginal effects the  data instances in each rectangle.


TODO: Add to examples the ALE plot version

TODO: Change AlE plot entry in alternatives

TODO: Add Advantages and Disadvantages of ALE plots  
- Disadvantage: how many numbers of intervals?
- Dis: different number of points for each estimate
- Adv: problem of correlated features solved
- Adv: faster than pdp because much less predictions involved.
- Adv: centered at zero, makes for nicer interpretation, but can also be done for pdpd

TODO: Add ALE plot to description

### Examples
In practice, the set of features $x_S$ usually only contains one feature or a maximum of two, because one feature produces 2D plots and two features produce 3D plots.
Everything beyond that is quite tricky.
Even 3D on a 2D paper or monitor is already challenging.

Let's return to the regression example, in which we predict [bike rentals](#bike-data).
We first fit a machine learning model on the dataset, for which we want to analyse the partial dependencies.
In this case, we fitted a RandomForest to predict the bike rentals and use the partial dependence plot to visualize the relationships the model learned.
The influence of the weather features on the predicted bike counts:

```{r pdp-bike, fig.cap = 'Partial dependence plots for the rental bike prediction model and different weather measurements (Temperature, Humidity, Windspeed). The biggest differences can be seen in the temperature: On average, the hotter the more bikes are rented, until 20C degrees, where it stays the same also for hotter temperatures and drops a bit again towards 30C degrees. The marks on the x-axis indicate the distribution of the feature in the data.'}
data(bike)
library("mlr")
library("iml")
library("ggplot2")

bike.task = makeRegrTask(data = bike, target = "cnt")
mod.bike = mlr::train(mlr::makeLearner(cl = 'regr.randomForest', id = 'bike-rf'), bike.task)

pred.bike = Predictor$new(mod.bike, data = bike)
pdp = Partial$new(pred.bike, "temp", ice = FALSE) 
p1 = pdp$plot() +  scale_x_continuous('Temperature', limits = c(0, NA)) + scale_y_continuous('Predicted number of bike rentals', limits = c(0, 5500))
pdp$set.feature("hum")
p2 = pdp$plot() +  scale_x_continuous('Humidity', limits = c(0, NA)) + scale_y_continuous('', limits = c(0, 5500))
pdp$set.feature("windspeed")
p3 = pdp$plot() + scale_x_continuous('Windspeed', limits = c(0, NA)) + scale_y_continuous('', limits = c(0, 5500))

gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

For warm (but not too hot) weather, the model predicts a high number of bike rentals on average.
The potential bikers are increasingly inhibited in engaging in cycling when humidity reaches above 60%.
Also, the more wind the less people like to bike, which makes sense.
Interestingly, the predicted bike counts don't drop between 25 and 35 km/h windspeed, but there is just not so much training data, so we can't be confident about the effect.
At least intuitively I would expect the bike rentals to drop with any increase in windspeed, especially when the windspeed is very high.



TODO: Intro text for ALE example and some interpretation

```{r ale-bike-prep, fig.keep = FALSE}
y.fun = function(X.model, newdata) {
 X.model$predict(newdata)[[1]]
}

limits = c(-1100, 700)

ale1.df = data.frame(ALEPlot(bike, pred.bike, J = "temp", pred.fun = y.fun))
ale1 = ggplot(ale1.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("temp") + scale_y_continuous(expression(hat(y)), limits = limits)
ale2.df = data.frame(ALEPlot(bike, pred.bike, J = "hum", pred.fun = y.fun))
ale2 = ggplot(ale2.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("hum") + scale_y_continuous(expression(hat(y)), limits = limits)

ale3.df = data.frame(ALEPlot(bike, pred.bike, J = "windspeed", pred.fun = y.fun))
ale3 = ggplot(ale3.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("windspeed") + scale_y_continuous(expression(hat(y)), limits = limits)

```

```{r ale-bike, fig.cap = "ALE  plots for the rental bike prediction model and different weather measurements (Temperature, Humidity, Windspeed). The overall tendencies are similar to partial dependence plots. One difference is that the effects are centered at 0. For very high temperature or humidity, the ALE and PD plot differ. The ALE plot detects much larger negative effects on the predicted bike rentals when it's very hot or very humid."}
gridExtra::grid.arrange(ale1, ale2, ale3, ncol = 3)
```



We also compute the partial dependence for [cervical cancer classification](#cervical).
Again, we fit a linear disciminant analysis (LDA) to predict whether a woman has cervical cancer given some risk factors. 
Given the model, we compute and visualize the partial dependence of the cancer probability on different features: 

```{r pdp-cervical, fig.cap = 'Partial dependence plot of cancer probability and the risk factors age and number of years with hormonal contraceptives. For the age feature, the partial dependence plot shows that on average the cancer probability is until 40 and increases after that. The sparseness of data points after age of 50 indicates that the model did not have many data points to learn from above that age. The number of years on hormonal contraceptives is associated with a higher cancer risk after 10 years. But again, there are not many data points in that region, which implies that we might not be able to rely on the machine learning model predictions for >10 years on contraceptives.', dev.args = list(pointsize = 5.5)}
data(cervical)
cervical.task = makeClassifTask(data = cervical, target = "Biopsy")
mod = mlr::train(mlr::makeLearner(cl = 'classif.lda', id = 'cervical-rf', predict.type = 'prob'), cervical.task)

pred.cervical = Predictor$new(mod, data = cervical, class = "Cancer")
pdp = Partial$new(pred.cervical, "Age", ice = FALSE) 

p1 = pdp$plot() + 
  scale_x_continuous(limits = c(0, NA)) + 
  scale_y_continuous('Predicted cancer probability', limits = c(0, 0.4))
pdp$set.feature("Hormonal.Contraceptives..years.")
p2 = pdp$plot() + 
  scale_x_continuous(limits = c(0, NA)) + 
  scale_y_continuous('', limits = c(0, 0.4))

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

```{r ale-cervical-prep, fig.keep = FALSE}
ale1.df = data.frame(ALEPlot(cervical, pred.cervical, J = "Age", pred.fun = y.fun, K = 100))
ale1 = ggplot(ale1.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("Age") + scale_y_continuous(expression(hat(y))) 

ale2.df = data.frame(ALEPlot(cervical, pred.cervical, J = "Hormonal.Contraceptives..years.", pred.fun = y.fun))
ale2 = ggplot(ale2.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("Hormonal.Contraceptives..years.") + scale_y_continuous(expression(hat(y))) 


```

```{r ale-cervical, fig.cap = ""}
gridExtra::grid.arrange(ale1, ale2, ncol = 2)
```


We can also visualizes the partial dependence of two features at once:

```{r pdp-cervical-2d, fig.cap = 'Partial dependence plot of cancer probability and the interaction of age and number of pregnancies. The plot shows the increase in cancer probability at 45, regardless of number of pregnancies. An interesting interaction happens at ages below 25: Young women who had 1 or 2 pregnancies have a lower predicted cancer risk, compared with women who had zero or above two pregnancies. The model predicts a - kind of - protective effect against cancer for 1 or 2 pregnancies. But be careful with drawing conclusions: This might just be a correlation and not causal! The cancer risk and number of pregnancies could be caused by another, unmeasured factor in which the young women differ.'}
pd = Partial$new(pred.cervical, c("Num.of.pregnancies", "Age"), ice = FALSE) 
pd$plot()
```


TODO: Create GGplot version of 2D ALEPlot

```{r ale-cervical-2d, fig.cap = ''}
ale.l = ALEPlot(cervical, pred.cervical, J = which(names(cervical) %in% c("Num.of.pregnancies", "Age")), pred.fun = y.fun)
x.v = data.frame(ale.l$f.values)
x.v$x1 = 1:nrow(x.v)
x.v.l = data.table::melt(x.v, id.var = 'x1', value.name = "x2")
x.vl. = x.v.l[x.v.l$variable != "V1",]
xx = data.table::melt(x.v.l)

ale.df = data.frame(ALEPlot(cervical, pred.cervical, J = which(names(cervical) %in% c("First.sexual.intercourse", "Age")), pred.fun = y.fun), K = 5)

```



### Advantages 
- The computation of partial dependence plots is **intuitive**: 
The partial dependence curve at a certain feature value represents the average prediction when we force all data points to take on that feature value. 
In my experience, laypersons usually grasp the idea of PDPs quickly.
- If the feature for which you computed the PDP is uncorrelated with the other model features, then the PDPs are perfectly representing how the feature influences the target on average.
In this uncorrelated case the **interpretation is clear**: 
The partial dependence plots shows how on average the prediction changes in your dataset, when the j-th feature is changed. 
It's complicated when features are correlated, see also disadvantages.
- Partial dependence plots are **simple to implement**.
- **Causal interpretation** : The calculation for the partial dependence plots has a causal interpretation: 
We intervene on $x_j$ and measure the changes in the predictions. 
By doing this, we analyse the causal relationship between the feature and the outcome.[^pdpCausal]
The relationship is causal for the model - because we explicitly model the outcome on the feature - but not necessarily for the real world!


### Disadvantages
- The **maximum number of features** you can look at jointly is - realistically - two and - if you are stubborn and pretend that 3D plots on a 2D medium are useful - three.
That's not the fault of PDPs, but of the 2-dimensional representation (paper or screen) and also our inability to imagine more than 3 dimensions.
- Some PD visualisations don't include the **feature distribution**. 
Omitting the distribution can be misleading, because you might over-interpret the line in regions, with almost no feature values.
This problem is easy to fix by showing a rug (indicators for data points on the x-axis) or a histogram.
- The **assumption of independence** poses the biggest issue. 
The feature(s), for which the partial dependence is computed, is/are assumed to be independently distributed from the other model features we average over.
For example: Assume you want to predict how fast a person walks, given the person's weight and height. 
For the partial dependence of one of the features, let's say height, we assume that the other features (weight) are not correlated with height, which is obviously a wrong assumption. 
For the computation of the PDP at some height (for example at height = 200cm) we average over the marginal distribution of weight, which might include a weight below 50kg, which is unrealistic for a 2 meter person. 
In other words: When the features are correlated, we put weight on areas of the feature distribution where the actual probability mass is very low (for example it is unlikely that someone is 2 meters tall but weighting below 50kg).
A solution to this problem are ALEPlots[^ALE], that only average over close data points. 
- **Heterogenous effects might be hidden** because the PDP only shows the average over the observations. 
Assume that for feature $x_j$ half your data points have a positive assocation with the outcome - the greater $x_j$ the greater $\hat{y}$ - and the other half has negative assocation - the smaller $x_j$ the greater $\hat{y}$.
The PDP curve might be a straight, horizontal line, because the effects of both dataset halves cancel each other out. 
You then conclude that the feature has no effect on the outcome. 
By plotting the [individiual conditional expectation curves](#ice) instead of the aggregated line, we can uncover heterogeneous effects.


[^ALE]: Apley, D. W. (n.d.). Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models, 1–36. Retrieved from https://arxiv.org/ftp/arxiv/papers/1612/1612.08468.pdf



[^Friedman2001]: Friedman, Jerome H. 2001. "Greedy Function Approximation: A Gradient Boosting Machine." Annals of Statistics. JSTOR, 1189–1232.

[^pdpCausal]: Zhao, Q., & Hastie, T. (2016). Causal interpretations of black-box models. Technical Report.