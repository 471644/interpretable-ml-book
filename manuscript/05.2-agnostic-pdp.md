    devtools::load_all("../")

    ## Loading iml.book

    ## Loading required package: knitr

    ## Warning: package 'knitr' was built under R version 3.4.3

    ## Warning: replacing previous import 'BBmisc::isFALSE' by
    ## 'backports::isFALSE' when loading 'mlr'

    ## Warning in as.POSIXlt.POSIXct(Sys.time()): unknown timezone 'zone/tz/2018c.
    ## 1.0/zoneinfo/Europe/Berlin'

Partial Dependence Plot (PDP)
-----------------------------

The partial dependence plot shows the marginal effect of a feature on
the predicted outcome \[@friedman2001greedy\]. Have a glance at Figure
@ref(fig:pdp-cervical) to get some intuition before diving into the
theory of partial dependencies. A partial dependence plot can show if
the relationship between the target and a feature is linear, monotonic
or something else. Applied to a linear regression model, partial
dependence plots will always show a linear relationship, for example.

The partial dependence function for regression is defined as:
$$\\hat{f}\_{x\_S}(x\_S) = E\_{x\_C}\\left\[\\hat{f}(x\_S, x\_C)\\right\] = \\int \\hat{f}(x\_S, x\_C) dP(x\_C)$$
 The term *x*<sub>*S*</sub> is the set of features for which the partial
dependence function should be plotted and *x*<sub>*C*</sub> are the
other features that were used in the machine learning model $\\hat{f}$.
Usually, there are only one or two features in *x*<sub>*S*</sub>.
Together, *x*<sub>*S*</sub> and *x*<sub>*C*</sub> make up *x*. Partial
dependence works by averaging the machine learning model output
$\\hat{f}$ over the distribution of the features *x*<sub>*C*</sub>, so
that the remaining function shows the relationship between the
*x*<sub>*S*</sub>, in which we are interested, and the predicted
outcome.

The partial function $\\hat{f}\_{x\_S}$ along *x*<sub>*S*</sub> is
estimated by calculating averages in the training data, which is also
known as Monte Carlo method:
$$ \\hat{f}\_{x\_S}(x\_S) = \\frac{1}{n} \\sum\_{i=1}^n \\hat{f}(x\_S, x\_{Ci}) $$
 In this formula *x*<sub>*i**C*</sub> are concrete values taken from the
data for the features we are not interested and *n* the number of
instances in the dataset. Note that $\\hat{f}\_{x\_S}$ only depends on
features *x*<sub>*S*</sub> as input. For classification, where the
machine model outputs probabilities, the partial dependence function
displays the probability for a certain class given different values for
features *x*<sub>*S*</sub>. A straightforward way to handle multi-class
problems is to plot one line per class.

The partial dependence plot is a global method: The method takes into
account all instances and makes a statement about the global
relationship of a feature with the predicted outcome.

### Examples

In practice, the set of features *x*<sub>*S*</sub> usually only contains
one feature or a maximum of two, because one feature produces 2D plots
and two features produce 3D plots. Everything beyond that is quite
tricky. Even 3D on a 2D paper or monitor is already challenging.

Let's turn to the regression example with the bike counts from Chapter
@ref(bike-data) and have a look at how the weather affects the predicted
bike rentals. We first fit a machine learning model on the dataset, for
which we want to analyse the partial dependencies. In this case, we
fitted a RandomForest to predict the bike rentals and make use of the
partial dependence method to understand what relationships the model
learned. Figure @ref(fig:pdp-bike) shows the influence of the weather
features on the predicted bike counts. For warm (but not too hot)
weather, the model predicts a high number of bike rentals on average.
The potential bikers are increasingly inhibited in engaging in cycling
when humidity reaches above 60%. Also, the more wind the less people
like to bike, which makes sense. Interestingly, the predicted bike
counts don't drop between 25 and 35 km/h windspeed, but maybe there is
just not enough training data. At least intuitively I would expect the
bike rentals to drop with any increase in windspeed, especially when the
windspeed is very high.

Figure @ref(fig:pdp-cervical) shows a partial dependence plot example
for cervical cancer classification (see Chapter @ref(cervical) for the
data). Again, we fit a RandomForest to predict whether a woman has
cervical cancer given some risk factors. Any other type of model would
have worked as well. The interpretation is analogue to the partial
dependence plots for regression.

Figure @ref(fig:pdp-cervical-2d) shows an example of a partial
dependence plot with two features.

    data(bike)
    library("mlr")

    ## Loading required package: ParamHelpers

    library("ggplot2")
    bike.task = makeRegrTask(data = bike, target = "cnt")
    mod.bike = mlr::train(mlr::makeLearner(cl = 'regr.randomForest', id = 'bike-rf'), bike.task)
    pd1 = mlr::generatePartialDependenceData(mod.bike, bike.task, c('temp', 'hum', 'windspeed'))
    mlr::plotPartialDependence(pd1) + my_theme()+ scale_x_continuous('', limits = c(0, NA)) + scale_y_continuous('Predicted number of bike rentals', limits = c(0, NA))

    ## Warning: Removed 2 rows containing missing values (geom_point).

![Partial dependence plot of rental bike count and different weather
measurements (Temperature, Humidity, Windspeed). The biggest differences
can be seen in different temperatures: With rising temperatures, on
average the bike rentals rise, until 20C degrees, where it stays the
same also for hotter temperatures and drops a bit again towards 30C
degrees.](05.2-agnostic-pdp_files/figure-markdown_strict/pdp-bike-1.png)

    data(cervical)
    cervical.task = makeClassifTask(data = cervical, target = "Biopsy")
    mod = mlr::train(mlr::makeLearner(cl = 'classif.randomForest', id = 'cervical-rf', predict.type = 'prob'), cervical.task)
    pd1 = mlr::generatePartialDependenceData(mod, cervical.task, c('Age', "Hormonal.Contraceptives..years."))

    mlr::plotPartialDependence(pd1) + my_theme()+ scale_x_continuous('', limits = c(0, NA))  + scale_color_discrete(guide='none') + scale_y_continuous('Predicted cancer probability')

    ## Warning in melt.data.table(data.table(obj$data), id.vars =
    ## colnames(obj$data)[!colnames(obj$data) %in% : 'measure.vars' [Age,
    ## Hormonal.Contraceptives..years.] are not all of the same type. By order
    ## of hierarchy, the molten data value column will be of type 'double'. All
    ## measure variables not of type 'double' will be coerced to. Check DETAILS
    ## in ?melt.data.table for more on coercion.

![Partial dependence plot of cancer probability and the risk factors age
and number of years with hormonal contraceptives. For the age feature,
the models partial dependence shows that on average, the cancer
probability is low before 45, spikes between age 45 and 55 and plateaus
after that. The number of years on hormonal contraceptives is associated
with a higher cancer risk especially after 15
years.](05.2-agnostic-pdp_files/figure-markdown_strict/pdp-cervical-1.png)

    pd1 = mlr::generatePartialDependenceData(mod, cervical.task, c("Number.of.sexual.partners", "Hormonal.Contraceptives..years."), interaction = TRUE)
    mlr::plotPartialDependence(pd1, geom='tile') + my_theme()+ scale_x_continuous('Number of sexual partners', limits = c(0, NA)) + scale_y_continuous('Years on hormonal contraceptives')  +
      scale_fill_continuous('Predicted\ncancer\nprobability') +
      theme(strip.background = element_blank(), strip.text = element_blank())

![Partial dependence plot of cancer probability and the interaction of
number of years on hormonal contraceptives and number of sexual
partners. Interestingly, there is some odd interaction between the two
features when the number of sexual partners is 1 and the years of on
hormonal contraceptives larger than 12. There are actually only two
women in that group, who both happen to have cancer. So my best guess is
that this was random and the model did overfit on those two women, but
only more data could answer this
question.](05.2-agnostic-pdp_files/figure-markdown_strict/pdp-cervical-2d-1.png)
