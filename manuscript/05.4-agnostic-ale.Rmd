```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

## Accumulated Local Effects (ALE) Plot {#ale}

Accumulated local effects [^ALE] describe how features influence the predicted outcome of a machine learning model on average.
ALE plots are a faster and unbiased alternative to the [partial dependence plots](#pdp) (PDPs).

*Keywords: ALE plots, partial dependence plots, marginal means, predictive margins, marginal effects*

I recommend reading the chapter on [partial dependence plots](#pdp) first, as the goal of both methods is the same:
Describe how a feature affect the prediction on average.
In the following section, I will convince you that partial dependence plots have a serious problem when the features are correlated.


### Motivation and Intuition

If features of a machine learning model are correlated, the partial dependence plot cannot be trusted.
Calculating a partial dependence plot for a feature that is strongly correlated with other features involves creating and averaging over data instances that are in reality unlikely.
This can greatly bias the estimated feature effect.
Imagine calculating partial dependence plots for a  machine learning model that predicts the value of a house depending on the number of rooms and the size of the living area.
We are interested in the effect of the living area on the predicted value.
As a reminder, the recipe for partial dependence plots is: 1) Select feature 2) Define grid 3) Per grid value: a) Replace feature with grid value b) Average predictions 4) Draw curve.
For the first grid value - say a living area of 30 square meters - we replace the living area for all instances by 30, even for houses with 10 rooms.
Sounds to me like a very unusual house.
The partial dependence plots includes these unrealistic houses in their calculation and pretend that everything is fine.
The following figure illustrates two correlated features and why the partial dependence plot method averages over unlikely data areas.

```{r aleplot-motivation1, fig.cap = "Two strongly correlated features x1 and x2. To compute the partial dependence at x1 = 0.75, the PDP replaces x1 with 0.75 for all instances, over the entire range of x2. This results in combinations of x1 and x2 that are unlikely due to their strong correlation. PDPs average over the marginal distribution of features, which assumes that features are uncorrelated."}

set.seed(1)
n = 100
intercept = 0.75

x1 = runif(n)
x2 = x1 + rnorm(n, sd = 0.1)
df = data.frame(x1, x2)  

p = ggplot(df) + geom_point(aes(x = x1, y = x2)) +  
  theme(panel.grid.major.y = element_blank(), 
    panel.grid.minor.y = element_blank(), 
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank())

p.int = p + geom_vline(xintercept = intercept)
x1.dens = density(x1)
x1.dens.df = data.frame(dens = x1.dens$y, x = x1.dens$x)


p1 = p.int + geom_path(data = x1.dens.df, aes(x = intercept - dens/10, y = x)) + 
  ggtitle(sprintf("Marginal distribution p(x2)", intercept)) + 
  scale_y_continuous(limits = c(-0.2, 1.2))
p1
```


TODO: CONTINUE HERE

What can we do to get a better estimate how a feature influences the prediction?

We could use the conditional distribution of features,  meaning at a grid value, we average over predictions of nearby data instances.
The solution for computing feature effects using the conditional distribution is called Marginal Plots, or M-Plots (confusing naming, since they average over the conditional, not the marginal distribution).
Wait, didn't I promise you to talk about ALE plots?
M Plots are a step into the right direction, but not quite the solution we are looking for.
Why are M-Plots not solving our problem?
If we average over predictions of all houses that have around 30 square meters, we estimate the joint effect of the living area and of the number of rooms, because of their correlation.
Assume the living area has no effect on the predicted value of a house, only the numbers of rooms has.
The M-Plot would still show that the size of the living area increases the predicted value, since we use the conditional distribution and the number of rooms grows with the living area.
The following plot shows for two correlated features how M-Plots average over the conditional distribution.

```{r aleplot-motivation2, fig.cap = "Two strongly correlated features x1 and x2. M-Plots average over the conditional distribution, here showing the conditional distribution of x2 at x1 = 0.75."}

set.seed(1)
n = 100
intercept = 0.75

x1 = runif(n)
x2 = x1 + rnorm(n, sd = 0.1)
df = data.frame(x1, x2)  

p = ggplot(df) + geom_point(aes(x = x1, y = x2)) +  
  theme(panel.grid.major.y = element_blank(), 
    panel.grid.minor.y = element_blank(), 
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank())

p.int = p + geom_vline(xintercept = intercept)
x1.dens = density(x1)
x1.dens.df = data.frame(dens = x1.dens$y, x = x1.dens$x)


p1 = p.int + geom_path(data = x1.dens.df, aes(x = intercept - dens/10, y = x)) + 
  ggtitle(sprintf("Marginal distribution p(x2)", intercept)) + 
  scale_y_continuous(limits = c(-0.2, 1.2))

x1.dens.ale = density(x1[(x1 > (intercept - 0.1)) & (x1 < (intercept + 0.1))])
x1.dens.ale.df = data.frame(dens = x1.dens.ale$y, x = x1.dens.ale$x)

p2 = p.int + geom_path(data = x1.dens.ale.df, aes(x = intercept - dens/20, y = x)) + 
  ggtitle(sprintf("Conditional distribution p(x2|x1=%.2f)", intercept)) + 
  scale_y_continuous(limits = c(-0.2, 1.2))
p2
```


M-Plots solve the problem of averaging over unlikely data instances, but the estimate mixes up the effects of two features.
ALE plots overcome this issue by computing - conditional on the distribution of the features -  local effects in the form of differences in the prediction instead of averages.
This means for the effect of the living are at 30 square meter, we first condition on houses with 30 m2 and instead of averaging their local predictions (M-Plots) we compute the difference in prediction if those houses were e.g. 31 m2 or if they were 29 m2 large.
This gives us the pure effect of the living area without mixing the effect up with the effects of any correlated features.
Using the differences blocks out the effect of the other features.
The following graphic provides intuition how ALE plots are calculated.


```{r aleplot-computation, fig.cap = "Computation of ALE plots for feature x1, which is correlated with x2. We first divide the feature into intervals (vertical lines).For the data instances (points) in a given interval we compute the difference in prediction when we replace the feature with the upper and lower limit of the interval (horizontal lines). Not shown in the plot: These difference are later accumulated and centered, which yields the ALE curve."}

set.seed(12)
n = 25

x1 = runif(n)
x2 = x1 + rnorm(n, sd = 0.1)
df = data.frame(x1, x2)  

p = ggplot(df) + geom_point(aes(x = x1, y = x2)) +
  theme(panel.grid.major.y = element_blank(), 
    panel.grid.minor.y = element_blank(), 
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank()) 


grid.df = data.frame(x1 = seq(from = 0,  to = 1, length.out = 6)[1:6], x2 = NA)
label.df = grid.df[1:5,]
label.df$x1 = label.df$x1 + 0.1
label.df$x2 = 0.95
label.df$label = sprintf("N1(%i)", 1:5)

break.labels = c(expression(z[0~","~1]),  expression(z[1~","~1]), expression(z[2~","~1]), expression(z[3~","~1]),
  expression(z[4~","~1]), expression(z[5~","~1]))

diff.df = df[df$x1 <= 0.8 & df$x1 > 0.6, ]

p + geom_vline(data = grid.df, aes(xintercept = x1), linetype = 3) + 
  scale_x_continuous(breaks = seq(from = 0,  to = 1, length.out = 6), limits = c(0, 1), labels = break.labels) + 
  geom_label(data = label.df, aes(x = x1, y = x2, label = label)) + 
  geom_segment(data = diff.df, aes(x = 0.6, xend = 0.8, y = x2, yend  = x2), arrow = arrow(ends = "both", angle = 90, length = unit(0.07, "inches")))
```


To summarize how each type of plot (PDP, M, ALE) computes the effect of a feature xj at a certain grid value v:  
Partial Dependence Plots: "Let me show you what the model predicts on average if every data instance had value v for feature j. 
I ignore whether the value v is meaningful for all data instances".  
M-Plots: "Let me show you what the model predicts on average for data instances that have values close to v for feature xj. 
The effects I show you could be due to feature xj, but also due to some correlated features.  
ALE plots: "Let me show you how the model predictions change when we slightly increase value v for data instances that have values close to v for feature xj".

### Theory

How do PD, M and ALE plots differ mathematically?

All three methods have in common that they reduce the complex prediction function f to a function that only depends on a single (or two) features.
All three methods reduce the function by averaging out the effects of the other features, but they differ in whether averages of predictions or averages over *differences in prediction* are computed and whether averaging is done over the marginal or conditional distribution.

Partial dependence plots average the predicted outcome f over the marginal distribution.

$$\begin{align}\hat{f}_{x_S,PDP}(x_S)&=E_{X_C}\left[\hat{f}(x_S,X_C)\right]\\&=\int_{x_C}\hat{f}(x_S,x_C)\mathbb{P}(x_C)d{}x_C\end{align}$$

That's the value of the prediction function f, at feature values $x_S$, averaged over all features in $x_C$.
Averaging out means computing the marginal expectation E over the feature xc, which is the integral over the prediction weighted by the probability distribution.
Sounds fancy, but to actually calculate the expected value over the marginal distribution, we simply take all our data, force them to have a certain grid value for our feature of interest, and average over the predictions for this manipulated dataset.
By taking the data we ensure that we average over the marginal distribution of the features.

M-plots average the predicted outcome over the conditional distribution.

$$\begin{align}\hat{f}_{x_S,M}(x_S)&=E_{X_C|X_S}\left[\hat{f}(X_S,X_C)|X_S=x_s\right]\\&=\int_{x_C}\hat{f}(x_S,x_C)\mathbb{P}(x_C|x_S)d{}x_C\end{align}$$
The only thing that changes compared to PDPs is that we take the average the predictionso conditional on each grid value of the feature of interest.
In practice this means that we have to define a neighbourhood and average over it, for example for the effect of 30 square meters on the predicted house value, we might average the predictions of all houses between 28 and 32 square meters.

ALE plots average the change in the predicted outcome and accumulate it over the grid (more on the computation later).

$$\begin{align}\hat{f}_{x_S,ALE}(x_S)=&\int_{z_{0,1}}^{x_S}E_{X_C|X_S}\left[\hat{f}^S(X_s,X_c)|X_S=z_S\right]dz_S-\text{constant}\\=&\int_{z_{0,1}}^{x_S}\int_{x_C}\hat{f}^S(z_s,x_c)\mathbb{P}(x_C|z_S)d{}x_C{}dz_S-\text{constant}\end{align}$$


The formula reveals three differences to M-Plots. 
First, we average over the change of prediction, not the prediction itself.
The change is formulated as the gradient, but later, for the actual computation, replaced by the difference in prediction over an interval.


$$\hat{f}^S(x_s,x_c)=\frac{\delta\hat{f}(x_S,x_C)}{\delta{}x_S}$$

The second difference is the additional integral over  z.
We accumulate the local gradients over the range of $x_S$, which gives us the effect of the feature on the prediction.
For the actual computation the z's will be replaced by a grid of intervals, over which we compute the changes in the prediction.
The third difference is that we subtract some constant from the results.
This step centers the ALE plot, so that the average effect over the dat is zero.
Let's dive a bit deeper into the computation of ALE plots.

An ALE plot for a single feature xj is the accumulated integral of the expected gradient of the prediction over xj.
Instead of directly computing averaging the predictions, the ALE method calculates the prediction differences conditional on xj and integrates the derivative over xj to estimate the effect.
Now, that sounds stupid. 
Derivation and integration normally cancel each other out, like first subtracting, then adding the same number. 
Why does it make sense here?
The derivative is computed at different grid values and averages the effects locally, avoiding the mistake that partial dependence plotsmake by averaging over the marginal distribution which is so problematic when the features are correlated.
It also avoids the mistakes M-Plots make that they mix up the effect estimates with the effect of correlated features on the predicted outcome.
ALE plots block the effect of the other features.
One problem remains for how I described ALE plots so far: 
Not all models come with a gradient, for example random forests have no gradient.
But as you will see, the actual computation works without gradients, but with intervals.

### Estimation

First I will describe how ALE plots are estimated for a single numerical feature and lager for two numerical features and for a single categorical feature.
For the estimation of the local effects, we divide the feature into many intervals and compute the differences in the prediction, as visualized in Figure XXX.
This procdure approximates the gradients and works also for models without gradients.

First we estimate the uncentered effect: 

$$\hat{\tilde{f}}_{j,ALE}(x)=\sum_{k=1}^{k_j(x)}\frac{1}{n_j(k)}\sum_{i:x_{i,j}\in{}N_j(k)}\left[f(z_{k,j},x_{i\setminus{}j})-f(z_{k-1,j},x_{i\setminus{}j})\right]$$
Let's break this formula down, starting from the right side. 
The name *Accumulated Local Effects* nicely reflects all individual components of this formula.
At it's heart, the ALE plot computes the differences in predictions, where we replace the feature we are interested in with grid values z.
The difference in prediction is the *Effect* (in ALE) the feature has for an individual instance in a certain interval.
The right sum is the sum over all instances within one interval, indicated as neighbourhood N(k).
We divide this sum by the number of instances that are in this interval, to get the mean difference of the predictions for that interval.
This average in the interval is covered by the term *Local* in the name ALE.
The left sum symbol says that we accumulate the mean prediction differences over all intervals.
The (uncentered) ALE of a feature value that lies in the third interval is the sum of the effects of the first, second and third interval.
The word *Accumulated* in ALE reflects this.

This effect is centered, so that the mean effect is zero:

$$\hat{f}_{j,ALE}(x)=\hat{\tilde{f}}_{j,ALE}(x)-\frac{1}{n}\sum_{i=1}^{n}\hat{\tilde{f}}_{j,ALE}(x_{i,j})$$

The value of the ALE can be interpreted as the effect of the feature at a certain feature value compared to the average prediction of the data. 
For example if them ALE curve at $x_j=3$ is -2, it means that when $x_j$ has value 2, then the prediction is lower by 2 compared to the average prediction.

The quantiles of the distribution of the feature are used as the grid that defines the intervals.
Using the quantiles assures that there are roughly the same amount of data points in each of the intervals. 
Quantiles have the drawback that the intervals can have very different lengths.
This can result in some weird ALE plots, when the feature of interest is very skewed, for example lots of low values and just a few very high values.
Then the intervals are first are very short and later much larger.


### Motivation, Part II

Let's see ALE plots in action. 
I constructed a scenario in which partial dependence plot fail.
The scenario consists of two features, one outcome to predict and a prediction model.
The features are strongly correlated.
The prediction model does something weird at a combination of the two features for which we never observed instances.
The following figure shows how this scenario looks like.


```{r correlation-problem, fig.cap = "Two features and the predicted outcome. The model simply predicts the sum of the two features (shaded background). When x1 is larger than 0.7 and x2 smaller than 0.3 the model always predicts 2. This area lies far away from the distribution of the data and it doesn't affect the performance of the model and also shouldn't affect its interpretation."}

set.seed(1)
n = 25

x1 = runif(n)
x2 = x1 + rnorm(n, sd = 0.1)
df = data.frame(x1, x2)  
df$y = x1 + x2

mod  = lm(y ~ ., data = df)

y.fun = function(X.model, newdata) {
  pred = predict(X.model, newdata)
  pred[newdata$x1 > 0.7 & newdata$x2 < 0.3] = 2
  pred
}

grid.dat = expand.grid(x1 = seq(from = 0, to = 1, length.out = 20), x2 = seq(from = 0, to = 1, length.out = 20))
grid.dat$predicted = y.fun(mod, grid.dat)

ggplot(df) + geom_tile(data = grid.dat, aes(x = x1, y = x2, fill = predicted)) + 
  geom_point(aes(x = x1, y = x2), size = 3) + 
  scale_fill_gradient(low = "white", high = "blue")
```

Is this even a realistic, relevant scenario?
When you train a model, the learning algorithm minimizes the loss for the existing training points.
Weird stuff can happen outside the distribution of the training data, because the model will not be penalized for doing weird stuff in those areas.
Leaving the data distribution is called extrapolation.
By the way, extrapolation is also used to fool machine learning models, see [adversarial examples](#adversarial).
Extrapolation leads to values that are very unlikely, given the distribution of our training data.
See how the partial dependence performs in our little example and how it compares to an ALE plot.


```{r correlation-pdp-ale-plot, fig.cap = "Comparing the feature effects computed with PDP (top row) and ALE (bottom row). The PDP picks up on the artifact in the data (steep jumps in the plots), which lies outside of the data distribution. The ALE plot correctly identifies that the machine learning model has a linear relationship between features and the prediction, ignoring areas without data."}
pred = Predictor$new(mod, data = df, predict.fun = y.fun)
pdp = FeatureEffect$new(pred, feature = "x1", method = "pdp")
pdp1 = pdp$plot() + ggtitle("PDP")
pdp = FeatureEffect$new(pred, feature = "x2", method = "pdp")
pdp2 = pdp$plot() + ggtitle("PDP")

ale1 = FeatureEffect$new(pred, feature = "x1", method = "ale")$plot()
ale2 = FeatureEffect$new(pred, feature = "x2", method = "ale")$plot()
gridExtra::grid.arrange(pdp1, pdp2, ale1, ale2)
```



But isn't it interesting to see that our model behaves oddly at x1 > 0.7 and x2 < 0.3?
Well, yes and no.
Given these are data instances might be physically impossible or at least extremely unlikely, it's usually irrelevant to look into thos instances.
But if you suspect that your test distribution might be somewhat different and some instances may actually lie in this area, then it would be interesting to include this are in the calculation of the feature effects. 
But it has to be a deliberate choice to include areas where we haven't observed any data yet and shouldn't be a side-effect of the method of choice like PDP.
If you suspect that the model will later be used with differently distributed data, I recommend to use ALE plots and to simulate the distribution of data you are expecting.



**ALE plots for the interaction of two features**

ALE plot can also who interaction of two features.
The principles of the computation are the same as for a single feature, but we work with rectangular cells instead of intervals, because we have to accumulate the effects in two dimensions.
Also, additionally to adjusting for the overall mean effect, we adjust for the main effects of both features.
This means that ALE for two features represents the second-order effects and doesn't include the main effects of the features.
In other words, ALE for two features only shows the additional interaction effect of the two features.
I'll spare you with the formula for 2D ALE plots, as it is long and uncomfortable to read.
If you are interested, I refer to the paper, formulas (13) - (16) for the computation.
Let's instead rel on some visual intuition how second-order ALE plots are computed.

```{r aleplot-computation-2d, fig.cap = "Computation of accumulated local effects for two features. We lay a grid over the two features. In each grid cell (one is exemplary highlighted), we compute the second-order differences for all instance inside the cell. For the second order difference of an instance in that cell we first replace values for x1 and x2 with the values from the cell corners. Let a, b, c and d represent the 'corner'-predictions of the manipulated instance, then the second order difference is (d - c) - (b - a). The mean 2nd-order difference in each cell is  accumulated over the grid and centered."}

p = ggplot(df) + geom_point(aes(x = x1, y = x2)) +
  theme(panel.grid.major.y = element_blank(), 
    panel.grid.minor.y = element_blank(), 
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank()) 


grid.df1 = data.frame(x1 = seq(from = min(df$x1),  to = max(df$x1), length.out = 6)[1:6], x2 = NA)
grid.df2 = data.frame(x2 = seq(from = min(df$x2),  to = max(df$x2), length.out = 6)[1:6], x1 = NA)

chosen.tile = expand.grid(x1 = grid.df1$x1[4:5], x2 = grid.df2$x2[4:5])
chosen.tile2 = data.frame(x = grid.df1$x1[4], xend = grid.df1$x1[5], y = grid.df2$x2[4], yend = grid.df2$x2[5])

points.df =  df[df$x1 < grid.df1$x1[5] & df$x1 > grid.df1$x1[4] & df$x2 < grid.df2$x2[5] & df$x2 > grid.df2$x2[4], ]
mv.arr = 0.02
p + geom_vline(data = grid.df1, aes(xintercept = x1), linetype = 3) + 
  geom_hline(data = grid.df2, aes(yintercept = x2), linetype = 3) + 
  geom_rect(aes(xmin = x, xmax = xend,  ymin = y, ymax = yend), data = chosen.tile2, alpha = 0, color = "black", size = 1.1) +
  geom_label(data = chosen.tile, aes(x = x1, y = x2), label = letters[1:4]) + 
  geom_point(aes(x = x1, y = x2), data = points.df, size = 3)

```

In the example in the figure before, many cells are empty because of the correlation.
This can be visualized with a greyed or blacked out box. 
Alternatively, you can replace the missing ALE estimate of an empty cell by the ALE estimate of the nearest non-empty cell.

Since the ALE estimates for two features only show the second-order effect of the features, the interpretation needs some extra attention.
The second-order effect is the additional interaction effect of the features, after we have accounted for the main effects of the features.
Let's say two features don't interact, but each has a linear effect on the predicted outcome. 
In the 1D ALE plot for each feature we will see a straight line as the estimated ALE curve.
But when we plot the 2D ALE estimates, they should be close to zero, because the second-order effect is only the additional effect of the interaction.
ALE plots and PD plots differ in this regard:
PDPs always show the total effects, ALE plots only the first- or second-order effect.
These are design choices by the one who implement the methods, and does not depend on the underlying math or anything.
You can subtract the lower order effect in a partial dependence plot to get the pure main or second-order effects or you could have an estimate of the total ALE plots by refraining from subtracting the lower-order effects.

The accumulated local effects could also be computed for arbitrary higher orders (combinations of more than two features), but as argued in the [PDP chapter](#pdp), only up to 2D makes sense, because higher interactions can't be visualized meaningfully.


**ALE for categorical features?**

Accumulated local effects need - by definition - the feature values to have an order, because it accumulates effects in some direction.
Categorical features don't have any natural order.
In order to compute an ALE plot for a categorical feature we have to somehow create or find some order.

The order of the categories influences the interpretation and calculation of the accumulated local effects.
One solution is to order the categories based on how similar the categories are, when looking at the other features.

The distance between two categories is the sum over the distances of each feature.
The feature-wise distance compares either the cumulative distribution in both categories, also called Kolmogorov-Smirnov distance (for numerical features) or compares the relative frequency tables (for categorical features).
Once we have the distances between all categories, we use mult-dimensional scaling to reduce the distance matrix to a one-dimensional distance measure.
This gives us a similarity-based ordering of the categories.


To make is a bit clearer, here is one example:
Let's say with have the two categorical features 'season' and 'weather' and one numerical feature 'temperature'.
For the first categorical feature we want to calculate the ALEs. 
The feature has the categories "Spring", "Summer", "Fall", "Winter".
We start to compute the distance between categories "Spring" and "Summer". 
The distance is the sum of distances over the features temperature and weather.
For temperature, we take all instances with "Spring", compute the empirical cumulative distribution function and do the same for instances with "Summer" and measure their distance with the Kolmogorov-Smirnov statistic.
For the weather feature, we compute for all "Spring" instances the probabilities for each weather type, do the same for the "Summer"-instances and sum up the absolute distance in the probability distribution.
If "Spring" and "Summer" have very different temperatures and weather, the overall distance will be large.
We repeat the procedure with the other season pairs and reduce the resulting matrix to a single dimension using multi-dimensional scaling.


### Examples

Let's predict [bike rentals](#bike-data) based on weather and day and check if the ALE plots really work as well as promised.
We fit a regression tree to predict the number of rentals on a given day and use ALE plots to analyse how the temperature on a given day, the relative humidity and the wind speed influence the predictions.
Let's see what the ALE plots tell use: 

```{r ale-bike-train}
data(bike)
library("mlr")
library("ggplot2")

set.seed(42)
bike.task = makeRegrTask(data = bike, target = "cnt")
mod.bike = mlr::train(mlr::makeLearner(cl = 'regr.ctree'), bike.task)$learner.model

pred.bike = Predictor$new(mod.bike, data = bike, y = "cnt")
```



```{r ale-bike, fig.cap = "ALE  plots for the rental bike prediction model and different weather measurements (temperature, humidity, wind speed). Temperature has a strong effect on the predicted number of bike rentals. The average prediction increases the hotter it gets, but drops again above 25 degrees Celsius. Humidity is negatively impacting the prediction: The higher the relative humidity, the lower the prediction, notably when above 60 percent humidity."}
limits = c(-1500, 800)

ale1 = FeatureEffect$new(pred.bike, "temp", method = "ale")$plot() +
  scale_x_continuous("Temperature") + scale_y_continuous(limits = limits)
ale2 = FeatureEffect$new(pred.bike, "hum", method = "ale")$plot() +
  scale_x_continuous("Humidity") + scale_y_continuous(limits = limits)
ale3 = FeatureEffect$new(pred.bike, "windspeed", method = "ale")$plot() +
  scale_x_continuous("Wind speed") + scale_y_continuous(limits = limits)

gridExtra::grid.arrange(ale1, ale2, ale3, ncol = 3)
```

Let's look at the correlation of temperature, humidity and wind speed with all other features.
Since the data also contains categorical features, we can't only use the standard Pearson correlation coefficients, since it works only when both features are numerical.
Instead, I fit a linear model where e.g. temperature is the outcome to predict and one of the features is the input feature.
Then I measure how much variance the linear model  explains and take the square root. 
If the other feature was numerical, then the outcome is equal to the absolute value of the standard Pearson correlation coefficient.
But this model-based approach (also called ANOVA, which stands for analysis of variance) also works when the second feature is categorical. 
The measure is between always 0 (no association) and 1 (temperature can be perfectly predicted from the other feature).
We compute the association of temperature, humidity and wind speed with all the other features to see how much in trouble we might be, when we use partial dependence plots. 
The higher the association (correlation), the more trouble for PD plots.
The following figure visualizes how much the weather features are correlated with other features.

```{r ale-bike-cor, fig.cap="The strength of correlation between temperature, humidity and wind speed with all the features, measured as the amount of variance explained, when we fit a linear model with e.g. temperature as outcome to predict and e.g. season as feature. For the temperature we observe - not surprisingly - a high correlation with the season and the month. Humidity is correlated with the weather situation."}

mycor = function(cnames, dat) {
  x.num = dat[cnames[1]][[1]]
  x.cat = dat[cnames[2]][[1]]
  av = anova(lm(x.num ~ x.cat))
  sqrt(av$`Sum Sq`[1] / sum(av$`Sum Sq`))
}

cnames = c("temp", "hum", "windspeed")
combs = expand.grid(y = cnames, x = setdiff(colnames(bike), "cnt"))
combs$cor = apply(combs, 1, mycor, dat = bike)
combs$lab = sprintf("%.2f", combs$cor)
forder = c(cnames, setdiff(unique(combs$x), cnames))
combs$x = factor(combs$x, levels = forder)
combs$y = factor(combs$y, levels = rev(cnames))
ggplot(combs, aes(x = x, y = y, fill = cor, label = lab)) + 
  geom_tile() + 
  geom_label(fill = "white", size = 3) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))  + 
  scale_x_discrete("") + 
  scale_y_discrete("") + 
  scale_fill_continuous("Variance\nexplained")
```

This correlation analysis reveals that we might run into problems using partial dependence plots especially for the temperature feature.
When we compute a partial dependence plot for temperature, we estimate the average prediction of the model for any temperature grid point (e.g. -5 degree Celsius) with all instances.
This means we also use instances with season 'Summer' and month 'August', fill in -5 degrees, get the model prediction and pretend everything is legit.
But this data instance is highly unlikely and might bias the estimation of the effect the temperature has.
Well, let's see how partial dependence plots estimates the effects.

```{r pdp-bike-compare, fig.cap = "Partial dependence plots for the features temperature, humidity and wind speed. Compared to the ALE plots, the PDPs show a smaller drop in bike rentals for high temperature or high humidity. The PDP uses all data instances for calculating the effect of high temperature, even for instances with season 'winter' for example. The ALE plots are more reliable."}
pdp = FeatureEffect$new(pred.bike, "temp", method = "pdp") 
p1 = pdp$plot() +  scale_x_continuous('Temperature') + scale_y_continuous('Predicted number of bike rentals', limits = c(3700, 5300))
pdp$set.feature("hum")
p2 = pdp$plot() +  scale_x_continuous('Humidity') + scale_y_continuous('', limits = c(3000, 5500))
pdp$set.feature("windspeed")
p3 = pdp$plot() + scale_x_continuous('Windspeed') + scale_y_continuous('', limits = c(3000, 5500))

gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

Next, let's see ALE plots in action for a categorical feature.
The month is a categorical feature for which we want to analyse the effect on the predicted number of bike rentals.
Arguably, the months already have some order (January to December), but let's try and see what happens if we first reorder the categories by similarity and then compute the effects. 
The months are ordered according to the similarity of tdays of each month based on the other features, like temperature or whether it's a holiday.

```{r ale-bike-cat-prep, include = FALSE}

```

```{r ale-bike-cat, fig.cap = "ALE plot for the categorical feature 'month'. The months are ordered by their similarity to each other, based on the distributions of the other features by month. We observe that January, March and April, but especially December and November have a lowering effect on the predicted bike rentals compared to the other months."}
alecat1 = FeatureEffect$new(pred.bike, "mnth", method = "ale")
alecat1$plot()
```

Since many of the features are related to weather, the order of the months strongly reflects how similar the weather is between the months.
All the colder months are on the left (February to April) and the warmer months on the right (October to August). 
Keep in mind that also other features were considered for the similarity, for example relative frequency of holidays has the same weight as the temperature for calculating the similarity between the months.

Next, we look at the second-order effect of humidity and temperature on the predicted number of bike rentals.
Remember that the second-order effect is the additional interaction effect of the two features and does not include the main effects. 
This means that, for example, you won't see the main effect that high humidity results in a lower number of predicted bike rentals on average in the ALE 2D plot.

```{r ale-bike-2d, fig.cap = 'Accumulated local effect plot for the second-order effect of humidity and temperature on the predicted number of bike rentals. Yellow color indicates above average and red color below average number of predicted bike rentals, when the main effects are already accounted for. The plot reveals an interaction between temperature and humidity: Hot and humid weather increases the number of bike rentals. Keep in mind that both main effects of humidity and temperature say that the number of bike rentals are decreased when very hot and humid. So for hot and humid weather, the joint effect of temperature and humidity is not the sum of the main effects, but less than the sum. For cold and humid weather, an additional negative effect on the number of predicted bike rentals is shown.'}
FeatureEffect$new(pred.bike, feature = c("hum", "temp"), method = "ale", grid.size = 40)$plot() +   
  scale_fill_gradient("ALE", low = "red", high = "yellow") + 
  scale_x_continuous("Relative Humidity") + 
  scale_y_continuous("Temperature in C")

```

To emphasize the difference between the pure second-order effect (the 2D ALE plot you just saw) and the total effect, let's look at the partial dependence plot.
The PDP shows the total effect, which adds up the mean prediction, the two main effects and the second-order effect (the interaction).

```{r pdp-bike-vs-ale-2D, fig.cap = "Partial dependence plot of the total effect of temperature and humidity on the predicted bike rentals. The plot combines the main effect of each of the features and their interaction effect, unlike the 2D-ALE plot which only shows the interaction."}
pdp = FeatureEffect$new(pred.bike, c("hum", "temp"), method = "pdp")
pdp$plot() + 
  scale_fill_gradient(low = "red", high = "yellow") + 
  scale_x_continuous("Humidity") + 
  scale_y_continuous("Temperature")
```

In case you are only interested in the interaction, you should look at the second-order effects, because the total effects mingle the main effects into the plot.
But if you want to know the combined effect of the features, you should be looking at the total effects (like the PDP shows).
For example, if you want to know the expected number of bike rentals at 30 degrees Celsius and 80 percent humidity, you can directly read it from the 2D PDP. 
When you want to know the same thing using ALE plots, then you have to look at three plots: The ALE plot for temperature, for humidity and for temperature + humidity and you also need to know the overall mean prediction.
In a scenario in which two features have no interaction, the 2D total effects plot might be misleading, because it will show some complex landscape, but it is simply the product of the to main effects.
The second-order effect would immediately reveal that there is no interaction.


Enough bikes for now, let's turn to a classification task.
We train a random forest to predict the probability for [cervical cancer](#cervical) for a woman dependent on some risk factors.
We visualize the accumulated local effects for two of the features:

```{r ale-cervical-1D, fig.cap = "ALE plots of cancer probability and the risk factors age and number of years with hormonal contraceptives. For the age feature, the ALE shows that on average the cancer probability is low until 40 and increases after that. The number of years on hormonal contraceptives is associated with a higher cancer risk after 3 years."}
data(cervical)
cervical.task = makeClassifTask(data = cervical, target = "Biopsy")
mod = mlr::train(mlr::makeLearner(cl = 'classif.randomForest', id = 'cervical-rf', predict.type = 'prob'), cervical.task)

pred.cervical = Predictor$new(mod, data = cervical, class = "Cancer")
ale1 = FeatureEffect$new(pred.cervical, "Age", method = "ale")$plot()
ale2 = FeatureEffect$new(pred.cervical, "Hormonal.Contraceptives..years.", method = "ale")$plot()
gridExtra::grid.arrange(ale1, ale2, ncol = 2)
```

Next we look at the interaction between number of pregnancies and age.


```{r ale-cervical-2d, fig.cap = 'Accumulated local effects plot for the second-order effect of number of pregnancies and the age on the predicted risk of cancer. We see some interaction. The model shows some odd behaviour at age 18-20 and more than 3 pregnancies. There are not many women in the data with those constellations of age and number of pregnancies (actual data are shown as points), so the model is not punished much during the training for making mistakes for those women.'}
FeatureEffect$new(pred.cervical, c("Age", "Num.of.pregnancies"))$plot(show.data = TRUE) + 
    scale_fill_gradient("ALE", low = "red", high = "yellow") + 
  scale_y_continuous("Number of pregnancies") 

```

The ALE plot shows similar interactions between age and number of pregnancies as the [partial dependence plots](#pdp) showed.
The age and number of pregnancies are naturally correlated, with a correlation of `r sprintf("%.2f", cor(cervical$Age, cervical$Num.of.pregnancies))`.
This means I would trust the ALE plot more than the partial dependence plot.


### Advantages

- **ALE plots are unbiased**, meaning they still work when  features are correlated.
Partial dependence plots fail in this scenario because they marginalize over unlikely or even physically impossible combinations of feature values.
- **ALE plots are faster to compute** than PDPs and they scale with O(n), since the highest possible number of intervals is the number of instances, with one interval per instance.
The PDP needs n times the number of grid points estimations. 
For 20 grid points, PDPs need 20x more predictions compared to the worst case ALE plot, where each interval has only one instance.
- The **interpretation of ALE plots is also clear**: Conditional on a given x value, the effect of the feature on the prediction can be read from the ALE plot.
- **ALE plots are centered at zero**.
This makes their interpretation nice, because the value at each point of the ALE curve is the difference to the mean prediction.
**The 2D ALE plot only shows the interaction**: When two features don't interact, the plot doesn't show anything.
- All in all, I would in most situations prefer ALE plots over PDPs, because usually features are almost always correlated to some degree.

### Disadvantages
- When the features are uncorrelated and computation time is not an issue, PDPs are preferable, because they are easier to understand and can be plotted together with ICE curves.
- **ALE plots can become a bit wobbly** (many small ups and downs) with a high number of intervals. 
In this case reducing the number of intervals makes the estimates more stable, but also smooths out and hides some of the true complexity of the prediction model.
- There is **no perfect solution for setting the number of intervals**. 
If the number is too small, than the ALE plots will not be very accurate. 
If the number is too high, the curve can become wobbly.
- Unlike PDPs, **ALE plots are not accompanied by ICE curves**.
For PDPs, ICE curves are great because they can reveal heterogeneity in the feature effect, meaning that the effect of a feature looks different for subsets of the data.
For ALE plots you could only check per interval if the effect differs between the instances, but each interval has different instances so it is not equivalent to ICE curves.
- **Each point of the ALE curve has a different variance, which is not in any way visualized.** 
The reason is that each estimation of a local effect in an interval uses a different number of data instances.
As a result all estimates have a different accuracy (but they are still the best possible estimates).
- **Second-order effect plots can be a bit annoying to interpret**, since you always have to hold in your mind the main effects.
It's tempting to read the heat maps as the total effect of the two features, but it is only the additional effect of the interaction.
The pure second-order effect is interesting for discovering and exploring interactions, but for the interpretation of how the effect looks like, I think it more useful when the main effects are integrated into the plot.
- The implementation of ALE plots is a lot more complex and less intuitive compared to partial dependence plots.
- Even though ALE plots are not biased in case of correlated features, the interpretation remains difficult when features are very strongly correlated.
Because when they have a very strong correlation, it only make sense to analyse the effect of changing both features together and not in isolation.


### Implementation and Alternatives

- Did I mention that [partial dependence plots](#pdp) together with [individual conditional expectation curves](#ice) are an alternative? =) 
Another alternative is the Marginal plot (M-Plot), but they are more interesting for historical reasons than for actual use.
- To my best knowledge, ALE plots are currently only implemented in R, once in the [ALEPlot R package](https://cran.r-project.org/web/packages/ALEPlot/index.html) by the inventor himself and once in the [iml package](https://cran.r-project.org/web/packages/iml/index.html)


[^ALE]: Apley, D. W. (n.d.). Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models, 1–36. Retrieved from https://arxiv.org/ftp/arxiv/papers/1612/1612.08468.pdf
