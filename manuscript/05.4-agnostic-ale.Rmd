```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

## Accumulated Local Effects (ALE) {#ale}

Accumulated Local Effects [^ALE] plots show how a single feature (or of two features) affect predicted outcome of a machine learning model on average.
ALE Plots are an alternative to the [partial dependence plots](#pdp) (PDPs) and the bias of PDPs when features are correlated.

*Keywords: partial dependence plots, PDP, PD plot, marginal means, predictive margins, marginal effects, ALE plot*

**The Problem: Correlated features**

When features of a machine learning model are correlated, the partial dependence plot cannot be trusted.
Calculating the partial dependence plots for correlated features involves creating and averaging over data instances that are unlikely in reality.
This can bias the resulting effect heavily.
For example, we compute the partial dependence plots for a  machine learning model that predicts the worth of a house depending on the number of rooms and the size of the living area.
We calculate the partial dependence plot for the living area.
To compute the first grid value - let's say a living area of 30 square meters - we replace the living area for all instances with 30, even for houses with 10 rooms.
Sounds like a very unlikely house to me, but in partial dependence plots with correlated features, these instances are created and used for computing the averages, biasing the interpretation.
For interpreting what the model on average predicts, the partial dependence plots includes these unrealistic houses into it's calculation and pretend that that behaviour is OK.
The following plot gives an impression of that problem.

```{r aleplot-motivation1, fig.cap = "Two strongly correlated features x1 and x2. To compute the partial dependence at x1 = 0.75, the PDP averages the prediction over the whole range of x2."}

set.seed(1)
n = 100
intercept = 0.75

x1 = runif(n)
x2 = x1 + rnorm(n, sd = 0.1)
df = data.frame(x1, x2)  

p = ggplot(df) + geom_point(aes(x = x1, y = x2)) +  
  theme(panel.grid.major.y = element_blank(), 
    panel.grid.minor.y = element_blank(), 
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank())

p.int = p + geom_vline(xintercept = intercept)
x1.dens = density(x1)
x1.dens.df = data.frame(dens = x1.dens$y, x = x1.dens$x)


p1 = p.int + geom_path(data = x1.dens.df, aes(x = intercept - dens/10, y = x)) + 
  ggtitle(sprintf("Marginal distribution p(x2)", intercept)) + 
  scale_y_continuous(limits = c(-0.2, 1.2))
p1
```
What can we do to get a better estimate how a feature influences the prediction?

The problem of partial dependence plots with correlated features stems from the fact that PDPs average over the marginal distribution of features, which kind of assumes that the features are independent.
A first step to improvement is to use the conditional distribution of features,  meaning for each grid value, we only look at the predictions of nearby data instances.
Given the living area is 30 square meters, what is the average prediction?
The solution for computing feature effects using the conditional distribution is called Marginal Plots, or M-Plots (confusing naming, I know).
Wait, what?
Didn't I promise to talk about ALE Plots?
Where are M Plots coming from?
M Plots are not the solution, but a step into the right direction with ALE Plots as the final solution.
Why are M-Plots not solving our problem?
If we average over all houses that are around 30 square meters, we don't get the pure effect of the living area, but also the effect of the number of rooms, because of their correlation.
Assume that the living area itself has no effect on the predicted worth of a house, only the numbers of rooms have.
The M-Plot would still show that the size of the living affects the prediction, since we use the conditional distribution and the number of rooms grows with the living area, so we mix up both effects.
The following plot shows how M-Plots work:

```{r aleplot-motivation2, fig.cap = "Two strongly correlated features x1 and x2. M-Plots average only over the conditional distribution, here shown conditional on x1 = 0.75. Also ALE-Plots are based on the conditional distribution, but accumulate the average (conditional) change in prediction."}

set.seed(1)
n = 100
intercept = 0.75

x1 = runif(n)
x2 = x1 + rnorm(n, sd = 0.1)
df = data.frame(x1, x2)  

p = ggplot(df) + geom_point(aes(x = x1, y = x2)) +  
  theme(panel.grid.major.y = element_blank(), 
    panel.grid.minor.y = element_blank(), 
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank())

p.int = p + geom_vline(xintercept = intercept)
x1.dens = density(x1)
x1.dens.df = data.frame(dens = x1.dens$y, x = x1.dens$x)


p1 = p.int + geom_path(data = x1.dens.df, aes(x = intercept - dens/10, y = x)) + 
  ggtitle(sprintf("Marginal distribution p(x2)", intercept)) + 
  scale_y_continuous(limits = c(-0.2, 1.2))

x1.dens.ale = density(x1[(x1 > (intercept - 0.1)) & (x1 < (intercept + 0.1))])
x1.dens.ale.df = data.frame(dens = x1.dens.ale$y, x = x1.dens.ale$x)

p2 = p.int + geom_path(data = x1.dens.ale.df, aes(x = intercept - dens/20, y = x)) + 
  ggtitle(sprintf("Conditional distribution p(x2|x1=%.2f)", intercept)) + 
  scale_y_continuous(limits = c(-0.2, 1.2))
p2
```


We solve the problem of averaging over unlikely data points, but our effect estimate is now mixing up the effects of two features.
This problem can be overcome by computing local effects in the form of differences, conditional on the distribution.
This mean for the effect of 30 square meter houses, we first condition on similar small houses and instead of averaging their predictions (M-Plots) we compute the difference in prediction when we would increase the living area for these houses slightly (the slope or gradient of the price dependent on the living area).
In this way, we get the pure effect of the living area, conditional that living area is small and without mixing the effect up with the effects of any correlated features.
The solution also has a name: ALE plots, which is short for [Accumulated Local Effects (ALE) Plots](#ale).
The following graphic show how the ALEplots are calculated. 


```{r aleplot-computation, fig.cap = "For the computation of ALE plots, we first divide the feature into intervals (vertical lines). In each interval, we select the data instances (points) and compute the difference in prediction when we replace the feature with the upper and lower limit of the interval (horizontal lines). Not shown in the plot: These difference are later cumulated and centered, which yields the ALE curve."}

set.seed(12)
n = 25

x1 = runif(n)
x2 = x1 + rnorm(n, sd = 0.1)
df = data.frame(x1, x2)  

p = ggplot(df) + geom_point(aes(x = x1, y = x2)) +
  theme(panel.grid.major.y = element_blank(), 
    panel.grid.minor.y = element_blank(), 
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank()) 


grid.df = data.frame(x1 = seq(from = 0,  to = 1, length.out = 6)[1:6], x2 = NA)
label.df = grid.df[1:5,]
label.df$x1 = label.df$x1 + 0.1
label.df$x2 = 0.95
label.df$label = sprintf("N1(%i)", 1:5)

break.labels = c(expression(z[0~","~1]),  expression(z[1~","~1]), expression(z[2~","~1]), expression(z[3~","~1]),
  expression(z[4~","~1]), expression(z[5~","~1]))

diff.df = df[df$x1 <= 0.8 & df$x1 > 0.6, ]

p + geom_vline(data = grid.df, aes(xintercept = x1), linetype = 3) + 
  scale_x_continuous(breaks = seq(from = 0,  to = 1, length.out = 6), limits = c(0, 1), labels = break.labels) + 
  geom_label(data = label.df, aes(x = x1, y = x2, label = label)) + 
  geom_segment(data = diff.df, aes(x = 0.6, xend = 0.8, y = x2, yend  = x2), arrow = arrow(ends = "both", angle = 90, length = unit(0.07, "inches")))
```


To summarizes how each type of plot handles the effect of a feature j at a certain grid value v:  
Partial Dependence Plots: "Let me show you what the model predicts on average if every data instance had value v for feature j. I don't care if value v is a meaningful value for the other data instances".  
M-Plots: "Let me show you what the model predicts on average for data instances that have values close to v for feature j. I can't promise you that the averages you see are from feature j or from some correlated feature".  
ALE Plots: "Let me show you how the model predictions change when we slightly increase value v for data instances that have values close to v for feature j".

### Theory

**How do PD, M and ALE Plots compare mathematically?**

All plots have in common that they reduce our complex prediction function f to one or two dimensions, depending on the feature(s) we choose.
They also have in common that they estimate the expectation of the model prediction function, but differ fo what they compute the expectation (f or derivative of f) and over what distribution (marginal or conditional).

Partial dependence plots average the predicted outcome over the marginal distribution.

$$\hat{f}_{x_S,PDP}(x_S)=E_{X_C}\left[\hat{f}(x_S,X_C)\right]=\int_{x_C}\hat{f}(x_S,x_C)\mathbb{P}(x_C)d{}x_C$$

That's the value of our prediction function f, at $x_S$, averaged over all features in $x_C$.
Because we are only interested in the effect of $x_S$ on the prediction, we need to average out the effects of the other features, which get the index C here. 
Averaging out means computing their marginal expectation E here, which is the integral over the prediction weighted by the probability distribution.
Sounds fancy pants, but when it gets down to computing this, we simply take all our data, get their prediction by forcing them to have a certain (grid) values for our feature of interest, and average over the predictions.
By taking the data we have, this already ensures that we average over the distribution of the features.

M plots average the predicted outcome over the conditional distribution:

$$\hat{f}_{x_S,M}(x_S)=E_{X_C|X_S}\left[\hat{f}(X_S,X_C)|X_S=x_s\right]=\int_{x_C}\hat{f}(x_S,x_C)\mathbb{P}(x_C|x_S)d{}x_C$$
The only thing that changes compared to PDPs is that we take the average of the data instance not over all the data, but conditional on a certain grid point at which we are with the feature of interest.
In practice this means that we have to define a neighbourhood and average over it, for example for the effect of 30 square meters, we average over all houses between 28 and 32 square meters.

ALE Plots average the change in the predicted outcome and accumulate it over the grid (more on the computation later).

$$\hat{f}_{x_S,ALE}(x_S)=\int_{z_{0,1}}^{x_S}E_{X_C|X_S}\left[\hat{f}^S(X_s,X_c)|X_S=z_S\right]dz_S-constant\\=\int_{z_{0,1}}^{x_S}\int_{x_C}\hat{f}^S(z_s,x_c)\mathbb{P}(x_C|z_S)d{}x_C{}dz_S-constant$$


There are three differences to M Plots. 
First, we do not average over the prediction, but we average over the change of prediction, represented by the gradient:


$$\hat{f}^S(x_s,x_c)=\frac{\delta\hat{f}(x_S,x_C)}{\delta{}x_S}$$

The second difference is the additional integral that integrates over some z.
That's because we accumulate these local gradient effects over the range of $x_S$, which gives us the effect of the feature on the prediction.
The z's indicate that we accumulate the effects over some intervals, where in each of them we compute this changes in prediction.
The third difference is that we subtract some constant from the results.
The constant is here to center the ALE, so that they are zero on average.
Let's dive a bit deeper into ALE Plots computation:

**How to compute the ALE Plots**

ALE plots are plots for estimates of the first order (1D) or second order (2D) effects of features on the prediction.
An ALE plot for feature $x_j$ is the accumulated integral of the expected gradient of the prediction over intervals of $x_j$.
Instead of directly computing the average of the function, the ALE method calculates the derivative in each interval and integrates again over the derivative to get the value of the function. 
Now, that sounds stupid. 
Derivation and integration cancel each other out, like first subtracting, then adding the same number. 
Why does this make sense?
Because the derivative is computed at different grid values and averages the effects locally, avoiding the mistake of partial dependence plots of averaging over the marginal distribution which is so problematic when the features are correlated.
It also avoids the mistakes of M Plots that we mix up our effect estimates with the effect that correlated features have on the predicted outcome.
One problem remains for how I described ALE Plots so far: 
Not all models come with a gradient, for example random forests have no gradient.
For the actual estimation of the local effects, we divide the feature into many intervals and compute the differences in the prediction.
This approximates the gradients, but is also computable for models without gradients.

First we have to estimate the uncentered effect, here for a single feature: 

$$\hat{\tilde{f}}_{j,ALE}(x)=\sum_{k=1}^{k_j(x)}\frac{1}{n_j(k)}\sum_{i:x_{i,j}\in{}N_j(k)}\left[f(z_{k,j},x_{i\setminus{}j})-f(z_{k-1,j},x_{i\setminus{}j})\right]$$
Let's break this formula down, starting from the right side. 
The name *Accumulated Local Effects* nicely reflects all individual components of this formula.
At it's heart, the ALE Plot computes the differences in predictions, where we replace the feature we are interested in with grid values z.
This difference in prediction is the *Effect* (in ALE) the feature has for an individual instance in a certain interval.
The inner sum is the sum over all instances within one interval, indicated as neighbourhood N(k).
We divide this sum by the number of instances that are in this interval, to get the mean difference in the prediction for that interval.
This average in the interval reflects the *Local* in the name ALE.
The left sum symbol means that we accumulate this mean prediction difference over all intervals.
The (uncentered) ALE at a feature value that lies in the third interval is the sum of the effects of the first, second and third interval.
The *Accumulated* in ALE comes from this outer sum.

This effect is then centered, so that the mean effect is zero:

$$\hat{f}_{j,ALE}(x)=\hat{\tilde{f}}_{j,ALE}(x)-\frac{1}{n}\sum_{i=1}^{n}\hat{\tilde{f}}_{j,ALE}(x_{i,j})$$

The value of the ALE can be interpreted as the effect of the feature at a certain feature value compared to the mean. 
For example if the curve at $x_j=3$ is -2, it means that when $x_j$ has value 2, then the prediction is lower by 2 compared to the average prediction.

One thing I haven't spoken about yet is how to choose the grid.
While many solutions are admissible, like random selection or uniform grid, the one that makes most sense are quantiles of the distribution of the feature.
Using the quantiles assures that there are roughly the same amount of data points in each of the intervals. 
Quantiles have the drawback that the intervals can have very different lengths.
This can result in some weird ALE plots, when the feature of interest is very skewed, for example lots of low values and just a few very high values.
Then the intervals in the beginning are very short and much larger for higher values.


**Demonstration that ALE Plots fix correlation problem**

Let's see ALE Plots now in action. 
I created a scenario in which the partial dependence plot fails, consisting of two features, one outcome to predict and a prediction model.
The features are strongly correlated and the prediction model does something weird at a combination of the two features for which we never observed instances.
The following figure shows how this scenario looks like.


```{r correlation-problem, fig.cap = "Two features and a predicted outcome. The model simply predicts the sum of the two features (shaded background), but there is also some artifact where the model always predicts 2, when x1 is larger than 0.7 and x2 smaller than 0.3. This artifact lies far away from the distribution of the data and are as such unrealistic values, so this artifact shouldn't affect our interpretation of the model."}

set.seed(12)
n = 25

x1 = runif(n)
x2 = x1 + rnorm(n, sd = 0.1)
df = data.frame(x1, x2)  
df$y = x1 + x2

mod  = lm(y ~ ., data = df)

y.fun = function(X.model, newdata) {
  pred = predict(X.model, newdata)
  pred[newdata$x1 > 0.7 & newdata$x2 < 0.3] = 2
  pred
}

grid.dat = expand.grid(x1 = seq(from = 0, to = 1, length.out = 20), x2 = seq(from = 0, to = 1, length.out = 20))
grid.dat$predicted = y.fun(mod, grid.dat)

ggplot(df) + geom_tile(data = grid.dat, aes(x = x1, y = x2, fill = predicted)) + 
  geom_point(aes(x = x1, y = x2), size = 3) + 
  scale_fill_gradient(low = "white", high = "blue")
```

"This is a completely made up example", I here some angry people yelling.  
"What kind of model would do such a thing?! Unrealistic!", other join.  
Yes, this is a toy example.  
BUT, remember that, when you train a model, the model only fits the existing data points. 
Anything can happen outside of the distribution of the training data, depending on the type of model, because the model will not be penalized for doing weird stuff abroad.
Since we optimize the model based on the training data, anything can happen outside of that distribution.
That's how we can generate [adversarial examples](#adversarial).
Linear models for example have extreme behaviour outside the distribution of the data, because they are linear functions. 
Imagine a linear regression model for predicting the worth of a house which has a positive weight for the influence of the living area.
Going outside the data means going beyond certain levels of square meters (let's say 3000) will make your predictions grow extreme.
The phenomenon is called extrapolation and that's also what's happening when you have correlated features with partial dependence plots.
Extrapolation leads to values that are very unlikely given the distribution of our training data.
See what happens with the PD plots and how they compare to the ALE plots in our little example:


```{r correlation-pdp-ale-compute, include = FALSE}
library(ALEPlot)
pred = Predictor$new(mod, data = df, predict.fun = y.fun)
pdp = Partial$new(pred, feature = "x1", ice = FALSE)
pdp1 = pdp$plot() + ggtitle("PDP")
pdp = Partial$new(pred, feature = "x2", ice = FALSE)
pdp2 = pdp$plot() + ggtitle("PDP")

ale1.df = data.frame(ALEPlot(df, mod, J = "x1", pred.fun = y.fun))
ale1 = ggplot(ale1.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("x1") + scale_y_continuous(expression(hat(y))) + 
  ggtitle("ALE plot")
ale2.df = data.frame(ALEPlot(df, mod, J = "x2", pred.fun = y.fun))
ale2 = ggplot(ale2.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("x2") + scale_y_continuous(expression(hat(y))) + 
  ggtitle("ALE plot")
```


```{r correlation-pdp-ale-plot, fig.cap = "Comparing the feature effects computed with PDP and ALE. The PDP picks up on the artifact in the data (steep jumps in the plots), which lies outside of the data distribution. The ALE plot correctly identifies that the machine learning model has a linear relationship between features and prediction, ignoring areas where no data instances lie"}
gridExtra::grid.arrange(pdp1, pdp2, ale1, ale2)
```


In the area of our training data, the model is simply a linear model, which the ALE Plots capture nicely.
The PDP plots, extrapolating into areas they shouldn't extrapolate, pick up the model artefact and give us a very weird impression of what the model does.
But is it not interesting to see that our model makes something weird?
Well, yes and no.
These data instances might be physically impossible or at least extremely unlikely, then I don't see how looking into these instances might be interesting. 
But if you suspect that you had this correlation between the training instances, but your test distribution might be somewhat different and some instances may actually lie in this area, then it would be interesting. 
But this has to be a deliberate choice to include areas where we haven't observed any data yet and not a side-effect of the method.
I recommend to still use ALE Plots, but generate the data you are expecting by simulating it for example, instead of relying on a flawed method.



**ALE Plots for two features**

ALE plot can also be used with more features, but, as with PDP, only makes sense with two at maximum and current software implementation allow only two at maximum.
The principles are the same as with a single feature, but we now work with rectangular cells instead of intervals, because we have to accumulate the effects in two dimensions.
Also, we don't only adjust for the overall mean prediction, but for the accumulated local effects of the single features.
This means that the ALE for two features represents the second-order effects and doesn't include the first-order effect.
This means that ALE for two features only shows the additional interaction effect of the two features, where the main effects of the features was subtracted.
Conceptually that's same thing we did for the single feature ALE, where we subtracted the overall mean, only extended now to the next dimension.
If we would compute the 3-feature ALE, then we would subtract all the 2-feature ALE.
I'll spare you with the formula for 2D ALE plots, as it is long and uncomfortable to read.
If you are interested, I refer to the paper, formulas (13) - (16) for the computation.
Let's go for some visual intuition how it is computed: 

```{r aleplot-computation-2d, fig.cap = "Illustration on how to calculate second-order Accumulated Local Effects for two features. We lay a grid over the two features we are interested in. In each cell of that grid (one is exemplary highlighted), we compute the second-order differences, which is the joint effect (indicated by an arrow from bottom left to top right of the cell) minus the two first order effects, as indicated by the vertical and horizontal arrows next to the cell and minus the overall weighted mean prediction of the cells. In each cell the effects are averaged for the points within that cell."}

p = ggplot(df) + geom_point(aes(x = x1, y = x2)) +
  theme(panel.grid.major.y = element_blank(), 
    panel.grid.minor.y = element_blank(), 
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank()) 


grid.df1 = data.frame(x1 = seq(from = min(df$x1),  to = max(df$x1), length.out = 6)[1:6], x2 = NA)
grid.df2 = data.frame(x2 = seq(from = min(df$x2),  to = max(df$x2), length.out = 6)[1:6], x1 = NA)

chosen.tile = data.frame(x = grid.df1$x1[4], xend = grid.df1$x1[5], y = grid.df2$x2[4], yend = grid.df2$x2[5])

mv.arr = 0.02
p + geom_vline(data = grid.df1, aes(xintercept = x1), linetype = 3) + 
  geom_hline(data = grid.df2, aes(yintercept = x2), linetype = 3) + 
  geom_rect(aes(xmin = x, xmax = xend,  ymin = y, ymax = yend), data = chosen.tile, alpha = 0, color = "black", size = 1.1) + 
  geom_segment(data = chosen.tile, aes(x = x, xend = xend, y = y, yend = yend), arrow = arrow(angle = 10)) + 
    geom_segment(data = chosen.tile, aes(x = x - mv.arr, xend = x - mv.arr, y = y, yend = yend), arrow = arrow(angle = 10)) + 
    geom_segment(data = chosen.tile, aes(x = x, xend = xend, y = y - mv.arr, yend = y - mv.arr), arrow = arrow(angle = 10))  
```

When spanning the grid, which is the Cartesian product of the quantiles of both features, some cells might remain empty, especially when the features are correlated. 
In the example in the figure before, many cells were empty because of the correlation.
In this case, the ALE can either be omitted and visualized with a greyed or blacked out box. 
Alternatively, you can replace the missing ALE of an empty cell by the ALE estimate of the nearest non-empty cell.
Is this problem distinct from the extrapolation problem that PDPs have?
Yes, for PDPs these empty areas are involved in estimating first-order univariate effects, which biases the estimates.
For ALE plots the empty cells only play a role in second-order effects and when a cell is empty it can also be indicated by plotting points where data lie into the plot to make it clear.

Since the ALE estimates for two features only show the second-order effect of the features, the interpretation needs some extra attention.
The second-order effect is the additional interaction effect of the features, after we have accounted for the main effects of the features and the mean effect.
Let's say two features don't interact, but each has a linear main-effect on the predicted outcome. 
In the 1D ALE plot for each feature we will see a straight line as the estimated ALE curve, with the slope equal to the estimated weight when the underlying model was a linear model.
But when we plot the 2D ALE estimates, they should be close to zero, because the second-order effect is only the additional effect of the interaction, which is zero in our little example.
ALE plots and PD plots differ in this regard:
PDPs always show the total effects, ALE plots only the first- or second-order effects.
These are design choices by the one who implement the methods, and does not depend on the underlying math or anything.
You can subtract the lower order effects in a partial dependence plots to get the pure first- or second-order effects or you could have an estimate of the total ALE plots by refraining from subtracting the lower-order effects.

The Accumulated Local Effects could also be computed for arbitrary higher orders (combinations of more than two features), but as argued in the [PDP chapter](#pdp), only up to 2D makes sense, because higher interactions can't be visualized meaningfully.


**What happens with categorical features?**

Accumulated Local Effects need - by definition - an order within a feature to work, because it has to accumulate the effects in some direction.
Categorical features - by definition - don't have any order.
So no ALE plots for categorical features, I am sorry.
Just kidding.
Once you have ordered the categories and enumerate them, you can compute ALE.
But how can we meaningfully order the categories?
Some categorical features have a natural order, for example grades in school or hierarchy ranks in a company.
For those it might make sense to keep that order, but for other features like the season or color there is no obvious order.
Then there are some features like the day of the week (Monday, Tuesday, ...) which look like they have an order, but it's not clear if you want to use that order for computing ALE plots. 
Where do we start? Monday? Sunday?)
For some prediction model Mondays might be more similar to Thursdays than to Tuesdays.

The order of the categories influences the interpretation and calculation of the accumulated local effects.
One solution is to order the categories based on how similar the categories are.
For a given category, the next 'lower' and the next 'larger' categories in the ordering should be similar and the farther you move a way from the category, the more dissimilar the categories should be.
How can we compute the similarity of categories?
A solution is to measure how different the instances of different categories in their feature values of the other features.

Here a short pseudo-code for calculating ALEs for categorical features.

Goal: Compute the distances between two categories.
Input: Instances from category 1 and 2

1. For all features, do (excluding the categorical feature for which we are computing the order):
   - If the feature is numerical: Take instances from category 1, calculate the empirical cumulative probability distribution function (ecdf) of the feature. The ecdf is a function that tells us for a given feature value, how many values are smaller. Do the same for category 2. The distance is the absolute maximum point-wise distance of the two ecdf. Practically, this value is high when the distribution from one category is strongly shifted far away from the other. This measure is also known as the [Kolmogorov-Smirnov distance](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test). 
   - If the feature is categorical: Take instances from category 1 and calculate a table with the relative frequency of each category of the other feature. Do the same for instances from category 2. The distance is the sum of the absolute difference of both relative frequency tables.
2. Sum up the distances over all features

This algorithm we run for all pairs of categories.
Then we have a k times k matrix, when k is the number of categories, where each entry is the distance between two categories.
Still not enough to have a single order, because, a (dis)similarity tells you the pair-wise distances, but does not give you a one-dimensional ordering of the classes.
To kind of force this thing into a single dimension, we have to use a dimension reduction trick called multi-dimensional scaling. 
This can be solved using multi-dimensional scaling, which takes in a distance matrix and returns a distance matrix with reduced dimension.
In our case, we only want 1 dimension left, so that we have a single ordering of the categories and can compute the accumulated local effects.
After reducing it to a single ordering, we are done and can use this ordering to compute ALE.
This is not the Holy Grail how to order the factors, but one possibility.

To make is a bit clearer, here is one example:
Let's say with have two categorical features (season and weather) and one numerical (temperature).
For the first categorical feature we want to calculate the ALEs. 
The feature has the categories "Spring", "Summer", "Fall", "Winter".
The numerical feature is temperature, and the other categorical feature is the type of weather with categories "good", "misty", "rainy".
We start to compute the distance between categories "Spring" and "Summer". 
The distance is the sum of distance over the features temperature and weather.
For temperature, we take all instances with "Spring", compute the empirical cumulative distribution function and do the same for instances with "Summer" and compare both with Kolmogorov-Smirnov test statistic.
For the weather feature, we compute for all "Spring" instances the probabilities for each weather type, do the same for the "Summer"-instances and sum up the absolute distance in the probability distribution.
If "Spring" and "Summer" have very different temperatures and weather, the distance will be large.
We repeat the procedure with the other season pairs, which results in the distance matrix, which is then reduced to a single dimension using multi-dimensional scaling.

Now that we have an order, we can compute the Accumulated Local Effects. 
It works very similar to numerical features, but there are some differences.
The ALE computation is slightly difference for categorical features.
We quantify the effect of a category on the prediction as the accumulated 'jumps' between the categories.
We start at 0 for the first category (well, not for the end result, because we subtract the mean effect, but for educational reasons it's good to start like this).
The effect of the second category is the 'jump' in prediction between the first and the second category.
We quantify the jump, by first looking at all instances that have category one. 
We artificially increase their category to two and measure the difference in prediction.
But we also take all instance that already have category two, decrease their category to one and measure the difference.
So we take both directions into account.
These differences are averaged so that only one value for each jump between categories remain.
We add a 0 for the first category, take the cumulative sum over the effects of the categories and subtract the overall average of effects. 
Then we are done.


### Examples

It's time to predict [bike rentals](#bike-data) dependent on weather and day and checkout if the ALE plots really do their job as well as promised.
We fit a regression tree to predict the number of rentals on a given day and use ALE plots to analyse how the temperature on a given day, the relative humidity and the wind speed influence the predictions.
Let's see what the ALE plots tell use: 

```{r ale-bike-train}
data(bike)
library("mlr")
library("ggplot2")

set.seed(42)
bike.task = makeRegrTask(data = bike, target = "cnt")
mod.bike = mlr::train(mlr::makeLearner(cl = 'regr.ctree'), bike.task)$learner.model

pred.bike = Predictor$new(mod.bike, data = bike, y = "cnt")
```

```{r ale-bike-prep, fig.keep = FALSE, include = FALSE}
y.fun = function(X.model, newdata) {
 X.model$predict(newdata)[[1]]
}

limits = c(-1500, 800)

ale1.df = data.frame(ALEPlot(bike, pred.bike, J = "temp", pred.fun = y.fun))
ale1 = ggplot(ale1.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("temp") + scale_y_continuous(expression(hat(y)), limits = limits)
ale2.df = data.frame(ALEPlot(bike, pred.bike, J = "hum", pred.fun = y.fun))
ale2 = ggplot(ale2.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("hum") + scale_y_continuous(expression(hat(y)), limits = limits)

ale3.df = data.frame(ALEPlot(bike, pred.bike, J = "windspeed", pred.fun = y.fun))
ale3 = ggplot(ale3.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("windspeed") + scale_y_continuous(expression(hat(y)), limits = limits)
```

```{r ale-bike, fig.cap = "ALE  plots for the rental bike prediction model and different weather measurements (Temperature, humidity, wind speed). The biggest differences can be seen in the temperature: On average, the hotter the more bikes are rented, until 20C degrees, stays high for some degrees and drops again when the temperature approaches 30 degrees. The marks on the x-axis indicate the distribution of the features in the data. The overall tendencies are similar to partial dependence plots. One difference is that the effects are centered at 0. For very high temperature or humidity, the ALE and PD plot differ. The ALE plot detects much larger negative effects on the predicted bike rentals when it's very hot or very humid."}
gridExtra::grid.arrange(ale1, ale2, ale3, ncol = 3)
```

Of course we have to look how they actually compare to partial dependence plots.

Let's look at the correlation of the feature temperature, humidity and wind speed.
Since I have a mix of numerical features and categorical features, we can't only use the standard Pearson correlation coefficients, since it works only when both features are numerical.
Instead, I fit a linear model where temperature is the outcome to predict and one of the features is the input feature.
Then I measure how much variance is explained and take the square root. 
If the other feature was numerical, then the outcome is the absolute value of the standard Pearson correlation coefficient.
But it also works when the second feature is categorical. 
The measure is between 0 (no association) and 1 (temperature can be perfectly predicted from the other feature).
We compute the association of temperature, humidity and wind speed with all the other features to see how much in trouble we are potentially in, when we would use partial dependence plots. 
The higher the association (correlation), the more trouble for PD plots.
The approach might be familiar to some of you as ANOVA which stands for analysis of variance.

```{r ale-bike-cor, fig.cap="The strength of correlation between temperature, humidity and wind speed with all features, measured as the amount of variance explained, when we fit a linear model with e.g. temperature as outcome to predict and e.g. season as feature. For the temperature, we observe - not surprisingly - a high correlation with the season and the month. Humidity is correlated with the weather situation."}

mycor = function(cnames, dat) {
  x.num = dat[cnames[1]][[1]]
  x.cat = dat[cnames[2]][[1]]
  av = anova(lm(x.num ~ x.cat))
  sqrt(av$`Sum Sq`[1] / sum(av$`Sum Sq`))
}

cnames = c("temp", "hum", "windspeed")
combs = expand.grid(y = cnames, x = setdiff(colnames(bike), "cnt"))
combs$cor = apply(combs, 1, mycor, dat = bike)
combs$lab = sprintf("%.2f", combs$cor)
forder = c(cnames, setdiff(unique(combs$x), cnames))
combs$x = factor(combs$x, levels = forder)
combs$y = factor(combs$y, levels = rev(cnames))
ggplot(combs, aes(x = x, y = y, fill = cor, label = lab)) + geom_tile() + geom_label(fill = "white") + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This little analysis reveals that we might run into problems using partial dependence plots especially for the temperature feature.
When we create a partial dependence plot for temperature, we estimate the average prediction of the model for any temperature grid point (e.g. -5 degree Celsius) by using all the data. 
This means we also use instance with season summer and month August, fill in -5 degrees, get the model prediction and pretend everything is legit.
But this data instance is highly unlikely and might bias the estimation of the partial effect the temperature has.
Well, let's see how partial dependence plots handle the situation.

```{r pdp-bike-compare, fig.cap = "Partial dependence plots for the features temperature, humidity and wind speed. Compared to the ALE plots, the PDPs show a smaller drop in bike rentals for high temperature or high humidity. The PDP uses all data instances for calculating the effect of high temperature, even for instances with season 'winter' for example. The results of the ALE plot are more reliable."}
pdp = Partial$new(pred.bike, "temp", ice = FALSE) 
p1 = pdp$plot() +  scale_x_continuous('Temperature') + scale_y_continuous('Predicted number of bike rentals', limits = c(3700, 5300))
pdp$set.feature("hum")
p2 = pdp$plot() +  scale_x_continuous('Humidity') + scale_y_continuous('', limits = c(3000, 5500))
pdp$set.feature("windspeed")
p3 = pdp$plot() + scale_x_continuous('Windspeed') + scale_y_continuous('', limits = c(3000, 5500))

gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

Now let's see ALE plots in action for a categorical feature.
The month is a categorical feature for which we will analyse the effect on the predicted number of bike rentals.
Arguably the months already have some order (January to December), but let's try and see what happens if we allow ALE to first reorder the categories by similarity and compute the effects. 
The categories of the feature of interest are ordered according to the similarity of the instances in each category based on the other features.

```{r ale-bike-cat-prep, include = FALSE}
alecat1.df = data.frame(ALEPlot(bike, pred.bike, J = "mnth", pred.fun = y.fun))
alecat1.df$x.values = factor(alecat1.df$x.values, levels = unique(alecat1.df$x.values))
alecat1 = ggplot(alecat1.df) + geom_col(aes(x = x.values, y = f.values)) + 
  scale_x_discrete("Month") + scale_y_continuous(expression(hat(y)))
```

```{r ale-bike-cat, fig.cap = "ALE plots for categorical feature month. The months are ordered by their similarity to each other, based on the distributions of the other features within each month. We observe that in January, March and April, but especially in December and November have a lowering effect on the predicted bike rentals compared to the other months."}
alecat1
```

Since many of the features are related to weather, the order of the months strongly reflects how similar the months are in weather. 
All the colder months are on the left (February to April) and the warmer months on the right (October to August). 
Keep in mind that also other features were considered for the similarity, for example the probability for holidays for each month has the same weight as the temperature for calculating the similarity between the months.


Next, we look at the second-order effect of humidity and temperature on the predicted number of bike rentals.
Remember that the second-order effect is the additional interaction effect of the two features and does not include the first-order effects. 
This means that for example you won't see  in the ALE 2D plot that high humidity results in a lower number of predicted bike rentals on average.

```{r ale-bike-2d, fig.cap = 'Accumulated local effect plot for the second-order effect of humidity and temperature on the predicted number of bike rentals. Yellow color indicates above average and red color below average number of predicted bike rentals, when the first-order effects are already accounted for. The plot reveals an interaction between temperature and humidity: Hot and humid weather increases the number of bike rentals (keep in mind that both first-order effects of humidity and temperature say that the number of bike rentals are decreased when hot and humid). So for hot and humid weather, the joint effect of temperature and humidity is not the sum of the first-order effects, but actually less. For cold and humid weather, an additional negative effect on the number of predicted bike rentals is shown.'}
x = ALEPlot::ALEPlot(bike, pred.bike, J = c("hum", "temp"), pred.fun = y.fun, K = 100)
```

To emphasize the difference between the pure second-order effect (the 2D ALE plot you just saw) and the total effect, let's look at the partial dependence plot, which shows the total effect, which is the sum of the mean prediction, the two first-order effects and the second-order effect (the interaction).

```{r pdp-bike-vs-ale-2D, fig.cap = "Partial dependence plot of the effect of temperature and humidity on the predicted bike rentals. The plot combines the main effect of each of the features and their interaction effect, unlike the 2D-ALE plot which only shows the interaction."}
pdp = Partial$new(pred.bike, c("hum", "temp"))
pdp$plot() + scale_fill_gradient(low = "red", high = "yellow")
```

When you are only interested in the interaction, you should look at the second-order effects, because the total effects mingles the first-order effects into the plot.
But if you want to know the combined effect of the features, you should be looking at the total effects (like the PDP shows).
For example, if you want to the expected number of bike rentals at 30 degrees Celsius and 80 percent humidity, you can directly read it from the 2D PDP. 
When you want to know the same thing using ALE plots, than you have to look at three plots: The ALE plot for temperature, for humidity and for temperature + humidity and you also need to know the overall mean prediction.
In a scenario in which two features have no interaction, the 2D total effects plot might be misleading, because it will show some complex landscape, which is simply the product of the to first-order effects.
The second-order effect would immediately reveal, that there is no interaction.


Enough bikes for now, let's turn to a classification task.
We train a random forest to predict the probability for [cervical cancer](#cervical) for a woman dependent on some risk factors.
For the random forest we visualize the Accumulated Local Effects for two of the features:

```{r ale-cervical-prep, fig.keep = FALSE, include = FALSE}
data(cervical)
cervical.task = makeClassifTask(data = cervical, target = "Biopsy")
mod = mlr::train(mlr::makeLearner(cl = 'classif.randomForest', id = 'cervical-rf', predict.type = 'prob'), cervical.task)

pred.cervical = Predictor$new(mod, data = cervical, class = "Cancer")
ale1.df = data.frame(ALEPlot(cervical, pred.cervical, J = "Age", pred.fun = y.fun))
ale1 = ggplot(ale1.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("Age") + scale_y_continuous(expression(hat(y))) 

ale2.df = data.frame(ALEPlot(cervical, pred.cervical, J = "IUD..years.", pred.fun = y.fun))
ale2 = ggplot(ale2.df) + geom_line(aes(x = x.values, y = f.values)) + 
  scale_x_continuous("Hormonal.Contraceptives..years.") + scale_y_continuous(expression(hat(y))) 

```

```{r ale-cervical-1D, fig.cap = "ALE plots of cancer probability and the risk factors age and number of years with hormonal contraceptives. For the age feature, the partial dependence plot shows that on average the cancer probability is until 40 and increases after that. The sparseness of data points after age of 50 indicates that the model did not have many data points to learn from above that age. The number of years on hormonal contraceptives is associated with a higher cancer risk after 10 years. But again, there are not many data points in that region, which implies that we might not be able to rely on the machine learning model predictions for >10 years on contraceptives "}
gridExtra::grid.arrange(ale1, ale2, ncol = 2)
```

Now let's look at the interaction between number of pregnancies and age, as we did in the partial dependence chapter.


```{r ale-cervical-2d, fig.cap = 'Accumulated Local Effects Plot for the second-order effect of number of pregnancies and the age on the predicted risk of cancer. We see a little bit of interaction. The model shows some odd behaviour at age below 20 years and 3 pregnancies and also at age 18/19 and more than 3 pregnancies. There are not many women in the data with those constellations of age and number of pregnancies, so the model is not punished much during the training for making mistakes for those women.'}
x = ALEPlot(cervical, pred.cervical, J = which(names(cervical) %in% c("Num.of.pregnancies", "Age")), pred.fun = y.fun, NA.plot = TRUE)
```

The ALE plot shows similar interactions between Age and number of pregnancies as we have seen with [partial dependence plots](#pdp).
The age and number of pregnancies are naturally correlated, with a correlation of `r sprintf("%.2f", cor(cervical$Age, cervical$Num.of.pregnancies))`.
This means I would trust the ALE plot more than the partial dependence plot.


### Advantages

- The ALE plot **also works when features are correlated**.
Partial dependence plots fail in this scenario because they marginalize over unlikely or even physically impossible combinations of feature values.
- **ALE plots are faster to compute** than PDPs and they scale with O(n), since the maximum number of intervals is the number of instances, with one interval per instance at maximum.
The PDP needs n times the number of grid points estimations. 
For 20 grid points, this results in 20x more predictions the model has to make compared to the worst case ALE plot (one interval per instance).
- The **interpretation of ALE plots is also clear**: Conditional on a given x value, the effect of the feature on the prediction can be read from the ALE plot.
- **ALE plots are centered at zero**.
This makes their interpretation nice, because the value at each point of the ALE curve is the difference to the mean prediction.
- **The 2D ALE plot only shows the interaction**: The good thing is, when two features don't interact, the plot doesn't show anything.
But it's just a technical decision that's not inherent to the idea of ALE plots. 
It's technically possible to add the first order effects and show both.

### Disadvantages

- **ALE plots can become a bit wobbly** (many small ups and downs) with a high number of intervals. 
In this case reducing the number of intervals makes the estimates more stable, but also smooths out and hides some of the true complexity of the prediction model.
- There is **no perfect solution for setting the number of intervals**. 
If the number is too small, than the ALE plots will not be very accurate. 
If the number is too high, the curve can become wobbly.
- Unlike PDPs, **ALE plots are not accompanied by ICE curves**.
For PDPs, ICE curves are great because they can reveal effect heterogeneity in the data, meaning that the effect of a feature looks different for subsets of the data.
For ALE plot you could only check per interval if the effect differs between the instances, but each interval has different instances so it is not equivalent to ICE curves.
- **Each point of the ALE curve has a different variance, which is not in any way visualized.** 
The reason is that each estimation of a local effect in an interval uses a different number of data instances.
As a result all estimates have a different accuracy (but still the best possible estimate).
- **Second-order effect for the 2D ALE computation can be a bit annoying to interpret**, since you always have to hold in your mind the first-order effects.
It's tempting to read the heat maps as the total effect of the two features, but it is only the additional effect of the interaction.
The pure second-order effect is interesting for discovering and exploring interactions, but for the interpretation of how the effect looks like, I think it more useful when the first-order effect is integrated into the heatmap.
- The implementation of ALE plots is a lot more complex and less intuitive compared with partial dependence plots.


### Implementation and Alternatives

- Did I mention that [partial dependence plots](#pdp) together with [individual conditional expectation curves](#ice) are an alternative? =) 
Another alternative are Marginal plots (M-Plots), but they are more interesting for the historical reasons than for actual use.
- To my best knowledge, ALE plots are currently only implemented in R, once in the [ALEPlot R package](https://cran.r-project.org/web/packages/ALEPlot/index.html) by the inventor himself and once in the [iml package](https://cran.r-project.org/web/packages/iml/index.html)


[^ALE]: Apley, D. W. (n.d.). Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models, 1–36. Retrieved from https://arxiv.org/ftp/arxiv/papers/1612/1612.08468.pdf
