```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all("../")
set.seed(42)
```

## Interaction strength

The interaction strength statistic measures how strongly features interact with each other, by analyzing the amount of variance explained by the interaction.


### Some Theory

When a machine learning model makes a prediction based on two features, we can decompose the prediction into 4 parts: a constant term, a term that depends only on the first feature, one for the second feature and one for the interaction effect of the two features.  

If two features $j$ and $k$ do not interact, we can decompose the [partial dependence function](#pdp) in the following way [^Friedman2008] (assuming that the effects are centered at zero):
$$PD_{jk}(x_j,x_k)=PD_j(x_j)+PD_k(x_k)$$

where $PD_{jk}(x_j,x_k)$ is the partial dependence for both features and $PD_j(x_j)$ and $PD_k(x_k)$ the single feature partial dependence functions.

Similarly, if the feature $x_j$ has no interaction with any of the other features, we can split the prediction function $f(x)$ into a sum, where the first summand only depends on $x_j$ and the second on all other features excluding $x_j$:

$$f(x)=PD_j(x_j)+PD_{-j}(x_{-j})$$
where $PD_{-j}(x_{-j})$ is the partial dependence function that depends on all features exluding $x_j$.

This decomposition gives us a way to express the partial dependence when there is no interaction and in a next step we can measure how much the real partial dependence function (where there might or might not be interaction(s)) differs from the one without interactions.
We calculate the variance of the function the partial dependence (for measuring the interaction of two features) or of the complete function (for measuring the interaction of a feature with all others). 
The amount of the variance that is explained by the interaction (difference between real and no-interaction PD) is used as the interaction strength statistic.
This measure is 0, when there is no interaction at all and 1 if 100 percent of the variance of the 2D partial dependence is explained by the sum of the 1D partial dependence functions.
An interaction statistic of 1 between two features would mean, that knowing only one single feature is completely useless for the prediction (we are as good as random), only if we know both of them, then we are able to be better than random for the prediction.


Or in mathematical terms, the statistic for the interaction between feature $x_j$ and $x_k$, proposed by Friedman is:
$$H^2_{jk}=\sum_{i=1}^n[PD_{jk}(x_{j}^{(i)},x_k^{(i)})-PD_j(x_j^{(i)})-PD_k(x_{k}^{(i)})]/\sum_{i=1}^n{}PD_{jk}^2(x_j^{(i)},x_k^{(i)})$$


Similarly for measuring the interaction between feature $x_j$ and all other features:
$$H^2_{j}=\sum_{i=1}^n[f(x^{(i)})-PD_j(x_j^{(i)})-PD_{-j}(x_{-j}^{(i)})]/\sum_{i=1}^nf^2(x^{(i)})$$

The estimate described here is really expensive to evaluate, because it iterates over all datapoints and at each point the partial dependence has to be evaluated which is done using again all $n$ datapoints. 
In the worst case we need $n*n*2$ calls to the predict function of the original model for the 1 vs. all interaction case and $n*n*3$ in the 2-interaction case. 
We can also sample from the $n$ data points to speed up the computation. 
This has the drawback that it introduces additional variance into the partial dependence estimates, which can bias the statistic upwards (because of more bias in the enumerator) and can even cause the interaction statistic to take on values larger than one. 
So when sampling, make sure to not sample to little datapoints.

In his paper, Friedman also proposes a way to test if this statistics is significantly different from zero. 
The Null-Hypothesis is that there is no interaction. 
To generate data under the Null Hypothesis, you have to be able to adjust the model so that it produces no interaction between feature $x_j$ and $x_k$ or all others.
This is not possible for all types of models, so running this test is model-specific, not model-agnostic and as such not covered here.

The interaction strenght statistics can also be applied in a classification setting, when the predicted outcome is the probability for a class.


### Examples

We measure the interaction strength of features in a support vector machine that predicts the number of [bike rentals](#bike-data).


```{r interaction-bike, fig.cap = 'The interaction strength for each feature with all other features for a support vector machine predicting bike rentals. Overall the interaction effects between the features are very weak (below 1 percent of variance explained by each feature).'}
data(bike)
library("mlr")
library("iml")
library("ggplot2")

bike.task = makeRegrTask(data = bike, target = "cnt")
mod.bike = mlr::train(mlr::makeLearner(cl = 'regr.svm', id = 'bike-rf'), bike.task)

pred.bike = Predictor$new(mod.bike, data = bike[setdiff(colnames(bike), "cnt")])
ia = Interaction$new(pred.bike, grid.size = 50) 
plot(ia)
```

The interaction statistic can also be calculated for classification, when we deal with the partial dependence of the predicted probability.
In the next example, we analyse the interactions between features in a random forest that is trained to predict [cervical cancer risk](#cervical).

```{r interaction-cervical, fig.cap = 'The interaction strength for each feature with all other features for a random forest predicting the probability of cervical cancer. The number of diagnosed sexually transmitted diseases has the highest interaction effect with all other features., followed by the number of pregnancies.'}
data(cervical)
cervical.task = makeClassifTask(data = cervical, target = "Biopsy")
mod = mlr::train(mlr::makeLearner(cl = 'classif.randomForest', id = 'cervical-rf', predict.type = 'prob'), cervical.task)

pred.cervical = Predictor$new(mod, data = cervical, class = "Cancer")
ia = Interaction$new(pred.cervical, grid.size = 100) 
plot(ia)
```

After looking into the feature interactions of each feature with all other features, we can pick one of the features and specifically analyze all the 2-way interaction between the chosen feature with the other features:

```{r interaction2-cervical-age, fig.cap = 'The 2-way interactions between number of pregnancies with each other feature. There is a strong interaction between the number of pregnancies and the age.'}
ia = Interaction$new(pred.cervical, grid.size = 100, feature = "Num.of.pregnancies") 
plot(ia)
```



### Advantages 
- The interaction statistic has an **underlying theory** with the partial dependence decomposition.
- The statistic has a **meaningful intepretation**: The interaction is defined as the portion of variance explained by the interaction.
- Since the statistics is **dimensionless and always between 0 and 1** and ,it is comparable across features and even across models.
- The statistic detects for all kinds of interactions, meaning no special form or anything is assumed (like multiplicative or similar).
- It is, theoretically, also possible to test arbitrary **higher interactions**: For example interaction strenght between 3 or more features.
- I don't know of many ways to test for interactions, so this is nice to have. 


### Disadvantages

- The first thing you will notice: The interaction statistic takes a long time to compute, because it's **computationally expensive**.
- The computation involves estimating a marginal distributions. 
These **estimates also have some variance**, when we don't use all of the data points. 
When the variance of the estimated partial dependence in the numerator is high, the interaction statistic is biased upwards and can become even larger than 1.
- When we sample points, then the estimates will also vary from run to run. 
This means that it **becomes instable, when data are sampled**.
I recommend repeating it a few times to see if you have enough data included for the sum.
- It is unclear if an interaction is significantly bigger than 0. 
We would need to create a formal test, but this is currently not possible in a model-agnostic fashion. 
- Connected to the testing problem: It's hard to tell when an interaction statistics is large enough that we would consider it a strong interaction.
- The interaction strenght tells us how strong the interactions is but, not how it looks like. 
But that's what [partial dependence plots](#pdp) are form. 
A meaningful workflow is to measure interaction strengths and then create 2D-partial dependence plots for interactions you are interested in.
- There is no meaningful interactio measure for images or text.
- The interaction statistic works under the assumption that we can independetly shuffle features (this is the same problem as for partial dependence plots.). When the features strongly correlate, the assumption is violated and we integrate over feature combinations that are very unlikely to occure in reality.





