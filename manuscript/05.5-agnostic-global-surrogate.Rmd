```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all("../")
set.seed(42)
```


## Global Surrogate Models  {#global}

A global surrogate model - in the context of interpretable machine learning - is an interpretable model that is trained to mimick the behaviour of an opaque black box model. We can draw conclusions about the black box by interpreting the surrogate model . (Solving machine learning interpretability by using more machine learning.)


# Theory

<!-- origins -->
Surrogate model models are also used in engineering:
When, for example, an outcome of interest is difficult or expensive to measure (e.g. because it comes from an computationally costly simulation), a cheap surrogate model of the outcome is used instead. 
The difference between the surrogate models used in engineering and for interpretable machine learning is that the underlying model is a machine learning model (not a simulation) and that the surrogate model has to be interpretable. 
The purpose of surrogate models is to match the predictions of the underlying model as closely as possible while being interpretable.
You might find the idea of surrogate models under different names:
Approximation model, metamodel, response surface model, emulator ...

So, about the theory ... there is actually not much theory needed to understand surrogated models. 
We want to approximate our black box prediction function $\hat{f}(x)$ as closely as possible with the surrogate model predictions function $\hat{g}(x)$, under the constraint that $g$ is interpretable.
Any model from the [interpretable models chapter](#simple) can be used for the function $g$:

For example a linear model:

$$\hat{f}^*(x)=\beta_0+\beta_1{}x_1{}+\ldots\beta_p{}x_p$$

Or a decision tree:

$$\hat{f}^*(x)=\sum_{m=1}^Mc_m{}I\{x\in{}R_m\}$$

Fitting of surrogate models is a model-agnostic method, since it requires no information about the inner workings of the black box model, only the relation of input and predicted output. 
If the underlying machine learning model would be exchanged, you could still apply the surrogate method.
It decouples the training of the black box model and the interpretation method, which indicates that it is model-agnostic.
Of course, there is the choice of the surrogate model, which dictates the interpretation.


Perform following steps to get a surrogate model:

1. Choose the training dataset. 
This could be the same dataset that was used for training the black box model or a new dataset from the same distribution.
You could even choose a subset of the data or a grid of points, depending on your application. 
1. For the chosen dataset, get the predictions $\hat{y}$ of the black box model.
1. Train the interpretable surrogate model on the data points and its predictions $\hat{y}$.
1. Measure how well the surrogate model replicates the prediction of the black box model.
1. Interpret / visualize the surrogate model.

You might find approaches for surrogate models which have some extra steps or differ a bit, but the main idea is almost always the same as described here.

<!-- other way to think about it -->
A way to measure how well the surrogate replicates the black box model is the R squared measure: 

$$R^2=1-\frac{SSE}{SST}=1 - \frac{\sum_{i=1}^n(\hat{y}^*_i-\hat{y}_i)^2}{\sum_{i=1}^n(\hat{y}_i-\bar{\hat{y}})^2}$$
where $\hat{y}^*_i$ is the prediction of the surrogate model and respectively $\hat{y}_i$ of the black box model.
The mean of the the predictions is $\bar{\hat{y}}$.
$SSE$ stands for sum of squares error and $SST$ for sum of squares total. 
The R squared measure can be interpreted as the percentage of variance that is captured by the interpretable model. 
If the R squared is close to 1 (= low $SSE$), then the interpretable model captures the behaviour of the black box model very well. 
If you are that close with the interpretable model, you might want to consider to actually replace the complex model with the interpretable model.
If the R squared is close to 0 (= high $SSE$), then the interpretable model fails to explain the black box model. \

Note that we haven't talked about the model performance of the underlying black box model, meaning how well or badly it performs at predicting the real outcome. 
For fitting the surrogate model, the performance of the underlying does not matter at all. 
The interpretation of the surrogate model is still valid, because it makes statements about the model and not about the real world.
But of course the interpretation of the surrogate model becomes irrelevant if the black box model sucks, because then the black box model itself is irrelevant.



<!-- More ideas-->
We could also build a surrogate models only based on a cluster or subset of the original data. 
By changing the distribution, we shift our focus of the explaining surrogate model (and it is not truly global any more).
Or re-weight the samples. 
When we weight the instances local to one data point we have local surrogate models, which can be used to explain individual predictions. 
Learn more about local models in the [following chapter](#lime)/


### Example
To demonstrate the surrogate models, we look at a regression and a classification example.

First we fit a support vector machine to predict the number of [bike rentals](#bike-data) on a day given weather and calendric information.
The support vector machine is not very interpretable, so we fit a surrogate with a CART decision tree as interpretable model to mimick the behaviour of the support vector machine.

```{r, message = FALSE, warning = FALSE, echo = FALSE, fig.cap = "The terminal nodes of a surrogate tree that mimicks the behaviour of a support vector machine trained on the bike rental dataset. The distribution in the nodes show that the surrogate tree predicts a higher number of bike rentals when the weather is above around 13 degrees and when the day was later in the 2 year period (cut point at 453 days)."}
library("iml")
data(bike)
bike.task = makeRegrTask(data = bike, target = "cnt")
mod.bike = mlr::train(mlr::makeLearner(cl = 'regr.svm'), bike.task)

pred.bike = Predictor$new(mod.bike, data = bike[, names(bike) != "cnt"])
tree = TreeSurrogate$new(pred.bike) 
plot(tree)


pred.tree  = predict(tree, bike)
pred.svm = getPredictionResponse(predict(mod.bike, bike.task))


r.squared.bike = 1 - var(pred.tree - pred.svm) / var(pred.svm)
```

The surrogate model has an R squared (variance explained) of `r r.squared.bike` which means it captures the underlying black box behaviour quite well, but not perfectly.
If the fit would be perfect, we could actually throw away the support vector machine and use the tree instead.

In our second example, we predict the probability for [cervical cancer](#cervical) with a random forest.
Again we fit a decision tree, using the original dataset, but with the prediction of the random forest as outcome, instead of the real classes (healthy vs. cancer) from the data.

```{r, message = FALSE, warning = FALSE, echo = FALSE, fig.cap = "The terminal nodes of a surrogate tree that mimicks the behaviour of a random forest trained on the cervical cancer dataset. The counts in the nodes show the distribution of the black box models classifications in the nodes."}
data(cervical)
cervical.task = makeClassifTask(data = cervical, target = "Biopsy")
mod.cervical = mlr::train(mlr::makeLearner(cl = 'classif.randomForest', predict.type = "prob"), cervical.task)

pred.cervical = Predictor$new(mod.cervical, data = cervical[names(cervical) != "Biopsy"])
tree.cervical = TreeSurrogate$new(pred.cervical, maxdepth = 2) 
plot(tree.cervical)
pred.tree.cervical  = predict(tree.cervical, cervical)["Cancer"]
pred.cervical = getPredictionProbabilities(predict(mod.cervical, cervical.task))

r.squared.cervical = 1 - var(pred.tree.cervical - pred.cervical) / var(pred.cervical)
```

The surrogate model has an R squared (variance explained) of `r r.squared.cervical` which means does not capture much of the underlying black box model behaviour and we should not over-interpret the tree, when drawing conclusions about the complex model.

### Advantages 
- The surrogate models method is **flexible**: Any model from the [interpretable models chapter](#simple) can be used.
This also means that you can swap not only the interpretable model, but also the underlying black box model.
Let's say you build some complex model and want to explain it to different teams in your company.
One team knows linear models really well, the other does not know stats at all, but can understand a decision tree structure without too much effort. 
Now you can simply fit two simple models on the original black box model and offer two kinds of explanations.
And if you notice that the you have a better performing black box model, you don't have to change the way you do interpretation, because you can use the same class of surrogate models.
- I'd argue that the approach is very **intuitive** and straightforward.
This means it is easy to implement, but also easy to explain to people not familiar with data science.
- We can easily **measure** how good our explanations are (in terms of fidelity to the black box predictions): 
By treating the black box model prediction as truth and measuring how far away the simple model predictions are.

### Disadvantages
- Be careful to draw **conclusions about the model, not the data**. 
- It's not clear what the **cut-off for R squared** in order to be confident that the surrogate model is close enough to the black box model.
80% of variance explained? 50%? 99%?
- We can measure how close the interpretable model is to the black box model. 
Let's assume we are not very close, but close enough. 
Now we do some interpretation. 
It could be, that the interpretable model is very close for a part of the data, but diverges in other areas. 
In this case the interpretation for the simple model would not be equally good for all data points.
- Depending on the interpretable model you choose as a surrogate, the model will come with all its advantages and disadvantages. 
- If one argues that there are no real interpretable models per se (like not even linear models or decision trees) or that it is a danger to think something is interpretable, then this technique is not useful.
