```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all("../")
set.seed(42)
```


## Global Surrogate Models  {#global}

Global surrogate models - or solving machine learning model issues by adding adding more machine learning models - are simple/interpretable models that are trained on the predictions of the more complex model to mimic its behaviour. The simpler model can then be interpreted to draw some conclusions about the underlying complex model. 



Other names: oracle, surrogate model, mimic, emulation, emulated model, distilled model. 
No real magic to it. 


Arbitrary $\hat{f}(x)$ becomes some interpretable $\hat{f}^*(x)$.
We treat the black box model as data generating process instead of the real world. 
If our original black box model sucks (performance wise), then of course, the simple model will also be garbate, even if the simple model would have been great when trained on the original data. 

We could also build a surrogate models only based on a cluster or subset of the original data. 
By changing the distribution, we shift our focus of the explaining surrogate model (and it is not truly global any more).
Or re-weight the samples. 
When we weight the instances local to one data point we have local surrogate models. 

If you are very close with the interpretable model, you might want to consider to actually replace the complex model with the interpretable model.

- With linear model
$$\hat{f}^*(x)=\beta_0+\sum_{j=1}^p\beta_j{}x_j{}$$
- With decision tree

$$\hat{f}^*(x)=\sum_{m=1}^Mc_m{}I\{x\in{}R_m\}$$


### Example
TODO: Add variance explained
Bike example with decision tree.
```{r, message = FALSE, warning = FALSE, echo = FALSE, fig.cap = "The terminal nodes of a surrogate tree that mimicks the behaviour of a support vector machine trained on the bike rental dataset. The distribution in the nodes show that the surrogate tree predicts a higher number of bike rentals when the weather is above around 13 degrees and when the day was later in the 2 year period (cut point at 453 days)."}
library("iml")
data(bike)
bike.task = makeRegrTask(data = bike, target = "cnt")
mod.bike = mlr::train(mlr::makeLearner(cl = 'regr.svm'), bike.task)

pred.bike = Predictor$new(mod.bike, data = bike[, names(bike) != "cnt"])
tree = TreeSurrogate$new(pred.bike) 
plot(tree)


pred.tree  = predict(tree, bike)
pred.svm = getPredictionResponse(predict(mod.bike, bike.task))


r.squared.bike = 1 - var(pred.tree - pred.svm) / var(pred.svm)
```
The surrogate model has an R squared (variance explained) of `r r.squared.bike` which means it captures the underlying black box behaviour quite well, but not perfectly.

Cervical example with decision tree.
```{r, message = FALSE, warning = FALSE, echo = FALSE, fig.cap = "The terminal nodes of a surrogate tree that mimicks the behaviour of a random forest trained on the cervical cancer dataset. The counts in the nodes show the distribution of the black box models classifications in the nodes."}
data(cervical)
cervical.task = makeClassifTask(data = cervical, target = "Biopsy")
mod.cervical = mlr::train(mlr::makeLearner(cl = 'classif.randomForest', predict.type = "prob"), cervical.task)

pred.cervical = Predictor$new(mod.cervical, data = cervical[names(cervical) != "Biopsy"])
tree.cervical = TreeSurrogate$new(pred.cervical, maxdepth = 2) 
plot(tree.cervical)
pred.tree.cervical  = predict(tree.cervical, cervical)["Cancer"]
pred.cervical = getPredictionProbabilities(predict(mod.cervical, cervical.task))

r.squared.cervical = 1 - var(pred.tree.cervical - pred.cervical) / var(pred.cervical)
```

The surrogate model has an R squared (variance explained) of `r r.squared.cervical` which means it imitates the underlying black box model poorly and we should not over-interpret the tree, when drawing conclusions about the complex model.

### Advantages 
- Highly modular approach: Any model from the simple models can be used. TODO LINK 
This also means that you can simply swap models. 
Let's say you build some complex model and want to explain it to different teams in your company.
One team knows linear models really well, the other does not know stats at all, but can understand a decision tree structure without too much effort. 
Now you can simply fit two simple models on the original black box model and offer two kinds of explanations.
- Easy to understand what happens here
- Mistake we are making can be easily measured: 
Either in drop of performance between original model and surrogate model on the real data (see feature importance LINK) or by treating the black box model prediction as truth and measuring how far away the simple model predictions are. 
between 

### Disadvantages
- Be careful to draw conclusions about the model, not the data. 
- Not clear what the cut-off is for an interpretable model being close enough. 
80% of variance explained? 50%? 99%?
- We can measure how close the interpretable model is to the black box model. 
Let's assume we are not very close, but close enough. 
Now we do some interpretation. 
It could be, that the interpretable model is very close for a part of the data, but diverges in other areas. 
In this case the interpretation for the simple model would not be equally good for all data points.
- Dependig on the simple model you choose as a surrogate, the model will come with all its advantages and disadvantages. 
