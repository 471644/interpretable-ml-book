```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all("../")
set.seed(42)
```


## Global Surrogate Models  {#global}

A global surrogate model - in the context of interpretable machine learning - is an interpretable model that is trained to mimick the behaviour of an opaque black box model.  We can draw conclusions from the interpretable surrogate model about the black box. (Solving machine learning interpretability by using more machine learning.)


# Theory

<!-- origins -->
Surrogate model models are used in engineering, when an outcome of interest is difficult or expensive to measure (e.g. because it comes from an computationally costly simulation), so a cheap surrogate model of the outcome is used instead. 
The difference between the surrogate models used in engineering and for interpretabl machine learning is that the underlying model is a machine learning model (not a simulation) and that the surrogate model has to be interpretable. 
The purpose of surrogate models is to make the interpretation easier. 
Other names which talk about the same principle as surrogate models:
Approximation model, metamodel, response surface model, emulator. 


So, about the theory ... there is actually not much theory needed to understand surrogated models. 
We want to approximate our black box prediction function $\hat{f}(x)$ as closely as possible with the surrogate model predictions function $\hat{f}^*(x)$, under the constraint that the replacement is interpretable.

- With linear model
$$\hat{f}^*(x)=\beta_0+\sum_{j=1}^p\beta_j{}x_j{}$$

- With decision tree

$$\hat{f}^*(x)=\sum_{m=1}^Mc_m{}I\{x\in{}R_m\}$$

<!-- other way to think about it -->
We kind of go a level more abstract: We treat the black box model as the data generating process instead of the real world. 
If our original black box model sucks (performance wise), then of course, the simple model will also be garbate, even if the simple model would have been great when trained on the original data. 
If you are very close with the interpretable model, you might want to consider to actually replace the complex model with the interpretable model.

<!-- More ideas-->
We could also build a surrogate models only based on a cluster or subset of the original data. 
By changing the distribution, we shift our focus of the explaining surrogate model (and it is not truly global any more).
Or re-weight the samples. 
When we weight the instances local to one data point we have local surrogate models. 



On which data do you train the model?
Grid or training data is possible. 
Difference is in the emphasis of the regions.
If one ones to use a grid, then you you can dive deep into the field of design of experiments, which is occupied with designing grids (different distributions, random, rigid, quasi-random, adaptive, ...).


Algorithm:

1. Choose training data points
1. For those training data points, get predictions $\hat{y}$ of the black box model
1. Train the interpretable model on chosen data points and their predictions
1. Interpret the interpretable surrogate model


It is a model-agnostic method, since it requires no information about the inner workings of the black box model, only the relation of input and predicted output. 
Of course, there is the choice of the surrogate model, which dictates the interpretation.


### Example
Bike example with decision tree.
```{r, message = FALSE, warning = FALSE, echo = FALSE, fig.cap = "The terminal nodes of a surrogate tree that mimicks the behaviour of a support vector machine trained on the bike rental dataset. The distribution in the nodes show that the surrogate tree predicts a higher number of bike rentals when the weather is above around 13 degrees and when the day was later in the 2 year period (cut point at 453 days)."}
library("iml")
data(bike)
bike.task = makeRegrTask(data = bike, target = "cnt")
mod.bike = mlr::train(mlr::makeLearner(cl = 'regr.svm'), bike.task)

pred.bike = Predictor$new(mod.bike, data = bike[, names(bike) != "cnt"])
tree = TreeSurrogate$new(pred.bike) 
plot(tree)


pred.tree  = predict(tree, bike)
pred.svm = getPredictionResponse(predict(mod.bike, bike.task))


r.squared.bike = 1 - var(pred.tree - pred.svm) / var(pred.svm)
```
The surrogate model has an R squared (variance explained) of `r r.squared.bike` which means it captures the underlying black box behaviour quite well, but not perfectly.

Cervical example with decision tree.
```{r, message = FALSE, warning = FALSE, echo = FALSE, fig.cap = "The terminal nodes of a surrogate tree that mimicks the behaviour of a random forest trained on the cervical cancer dataset. The counts in the nodes show the distribution of the black box models classifications in the nodes."}
data(cervical)
cervical.task = makeClassifTask(data = cervical, target = "Biopsy")
mod.cervical = mlr::train(mlr::makeLearner(cl = 'classif.randomForest', predict.type = "prob"), cervical.task)

pred.cervical = Predictor$new(mod.cervical, data = cervical[names(cervical) != "Biopsy"])
tree.cervical = TreeSurrogate$new(pred.cervical, maxdepth = 2) 
plot(tree.cervical)
pred.tree.cervical  = predict(tree.cervical, cervical)["Cancer"]
pred.cervical = getPredictionProbabilities(predict(mod.cervical, cervical.task))

r.squared.cervical = 1 - var(pred.tree.cervical - pred.cervical) / var(pred.cervical)
```

The surrogate model has an R squared (variance explained) of `r r.squared.cervical` which means it imitates the underlying black box model poorly and we should not over-interpret the tree, when drawing conclusions about the complex model.

### Advantages 
- Highly modular approach: Any model from the simple models can be used. TODO LINK 
This also means that you can simply swap models. 
Let's say you build some complex model and want to explain it to different teams in your company.
One team knows linear models really well, the other does not know stats at all, but can understand a decision tree structure without too much effort. 
Now you can simply fit two simple models on the original black box model and offer two kinds of explanations.
- Easy to understand what happens here
- Mistake we are making can be easily measured: 
Either in drop of performance between original model and surrogate model on the real data (see feature importance LINK) or by treating the black box model prediction as truth and measuring how far away the simple model predictions are. 
between 

### Disadvantages
- Be careful to draw conclusions about the model, not the data. 
- Not clear what the cut-off is for an interpretable model being close enough. 
80% of variance explained? 50%? 99%?
- We can measure how close the interpretable model is to the black box model. 
Let's assume we are not very close, but close enough. 
Now we do some interpretation. 
It could be, that the interpretable model is very close for a part of the data, but diverges in other areas. 
In this case the interpretation for the simple model would not be equally good for all data points.
- Dependig on the simple model you choose as a surrogate, the model will come with all its advantages and disadvantages. 
- If one argues that there are no real interpretable models per se (like not even linear models or decision trees) or that it is a danger to think something is interpretable, then this technique is not useful.
