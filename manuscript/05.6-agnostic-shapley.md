    devtools::load_all("../")

    ## Loading iml.book

    ## Loading required package: knitr

    ## Warning: package 'knitr' was built under R version 3.4.3

    ## Warning: replacing previous import 'BBmisc::isFALSE' by
    ## 'backports::isFALSE' when loading 'mlr'

    ## Warning in as.POSIXlt.POSIXct(Sys.time()): unknown timezone 'zone/tz/2018c.
    ## 1.0/zoneinfo/Europe/Berlin'

    ## Warning: package 'tm' was built under R version 3.4.3

    ## Loading required package: NLP

    ## Loading required package: ParamHelpers

    ## 
    ## Attaching package: 'dplyr'

    ## The following objects are masked from 'package:stats':
    ## 
    ##     filter, lag

    ## The following objects are masked from 'package:base':
    ## 
    ##     intersect, setdiff, setequal, union

    ## 
    ## Attaching package: 'ggplot2'

    ## The following object is masked from 'package:NLP':
    ## 
    ##     annotate

    ## Warning: package 'tidyr' was built under R version 3.4.3

    ## Warning: package 'partykit' was built under R version 3.4.3

    ## Loading required package: grid

    ## Loading required package: libcoin

    ## Warning: package 'libcoin' was built under R version 3.4.3

    ## Loading required package: mvtnorm

    ## Warning: package 'pre' was built under R version 3.4.3

Shapley Value Explanations
--------------------------

Predictions can be explained by assuming that each feature is a 'player'
in a game where the prediction is the payout. The Shapley value - a
method from coalitional game theory - tells us how to fairly distribute
the 'payout' among the features.

### The general idea

Assume the following scenario:

You trained a machine learning model to predict apartment prices. For a
certain apartment it predicts 300.000 € and you need to explain this
prediction. The apartment has a size of 50 m<sup>2</sup>, is located on
the 2nd floor, with a park nearby and cats are forbidden (see Figure
@ref(fig:shapley-instance)). ![The predicted price for our apartment is
300.000€. It's a 50 square meter apartment on the second floor. There is
a park nearby and cats are forbidden. Our goal is to explain how each of
these features values contributed towards the predicted price of
300k€.](./images/shapley-instance.png) The average prediction for all
apartments is 310.000€. How much did each feature value contribute to
the prediction compared to the average prediction?

The answer is easy for linear regression models: The effect of each
feature is the weight of the feature times the feature value minus the
average effect of all apartments: See also Chapter @ref(limo). This
works only because of the linearity of the model. For more complex model
we need a different solution. For example LIME (see Chapter @ref(lime))
suggests local models to estimate effects.

A different solution comes from cooperative game theory: The Shapley
value, coined by @shapley1953value, is a method for assigning payouts to
players depending on their contribution towards the total payout.
Players cooperate in a coalition and obtain a certain gain from that
cooperation.

Players? Game? Payout? What's the connection to machine learning
prediction and interpretability? The 'game' is the prediction task for a
single instance of the dataset. The 'gain' is the actual prediction for
this instance minus the average prediction of all instances. The
'players' are the feature values of the instance, which collaborate to
receive the gain (= predict a certain value). In our apartment example
from Figure @ref(fig:shapley-instance), the feature values
'park-allowed', 'cat-forbidden', 'area-50m<sup>2</sup>' and 'floor-2nd'
worked together to achieve the prediction of 300.000€. Our goal is to
explain the difference of the actual prediction (300.000€) and the
average prediction (310.000€): a difference of -10.000€.

The answer might be: The 'park-nearby' contributed 30.000€;
'size-50m<sup>2</sup>' contributed 10.000€; 'floor-2nd' contributed 0€;
'cat-forbidden' contributed -50.000€. The contributions add up to
-10.000€: the final prediction minus the average predicted apartment
price.

**How do we calculate the Shapley value for one feature?**

The Shapley value is the average marginal contribution of a feature
value over all possible coalitions. All clear now? Have a look at Figure
@ref(fig:shapley-instance-intervened) to learn how to compute the
contribution of a feature value to a coalition. ![We assess the
contribution of the 'cat-forbidden' feature value when added to a
coalition of 'park-nearby', 'size-50m<sup>2</sup>'. We simulate that
only 'park-nearby', 'cat-forbidden' and 'size-50m<sup>2</sup>' are in a
coalition by randomly drawing the value for the floor feature. Then we
predict the price of the apartment with this combination (301.000€). In
a second step we remove 'cat-forbidden' from the coalition by replacing
it with a random value of the cat allowed/forbidden feature from the
randomly drawn apartment. In the example it was 'cat-allowed', but it
could have been 'cat-forbidden' again. We predict the apartment price
for the coalition of 'park-nearby' and 'size-50m<sup>2</sup>'
(320.000€). The contribution of 'cat-forbidden' was 301.000€ - 320.000€
= -19.000€. This estimation depends on the sampled non-participating
feature values and we get better estimates by repeating this procedure.
This figure shows the computation of the marginal contribution for only
one coalition. The Shapley value is the weighted average of marginal
contributions over all
coalitions.](./images/shapley-instance-intervention.png) We repeat this
computation for all possible coalitions. The computation time increases
exponentially with the number of features, so we have to sample from all
possible coalitions. The Shapley value is the average over all the
marginal contributions. Figure @ref(fig:shapley-coalitions) shows all
coalitions for computing the Shapley value of the 'cat-forbidden'
feature value. When we repeat the Shapley value for all feature values,
we get the complete distribution of the prediction (minus the average)
among the feature values. ![All coalitions of feature values that are
needed to assess the Shapley value for 'cat-forbidden'. The first row
shows the coalition without any feature values. The 2nd, 3rd and 4th row
show different coalitions - separated by '|' - with increasing coalition
size. For each of those coalitions we compute the predicted apartment
price with and without the 'cat-forbidden' feature value and take the
difference to get the marginal contribution. The Shapley value is the
(weighted) average of marginal contributions. We replace the feature
values of features that are not in a coalition with random feature
values from the apartment dataset to get a prediction from the machine
learning model.](./images/shapley-coalitions.png)

### Examples and Interpretation

The interpretation of the Shapley value *ϕ*<sub>*i**j*</sub> for feature
*j* and instance *i* is: the feature value *x*<sub>*i**j*</sub>
contributed *ϕ*<sub>*i**j*</sub> towards the prediction for instance *i*
compared to the average prediction for the dataset.

The Shapley value works for both classification (if we deal with
probabilities) and regression.

We use the Shapley value to analyse the predictions of a Random Forest
model predicting cervical cancer. The dataset is described in Chapter
@ref(cervical). Table @ref(tab:shapley-cervical-instance) shows the
feature values for the cancer risk factors of a woman in the dataset.
Figure @ref(fig:shapley-cervical-plot) visualizes the Shapley values,
that are also listed in Table @ref(tab:shapley-cervical-table).

<table>
<caption>Risk factors (feature values) of one woman in the cervical cancer dataset with a predicted 0.43 cancer probability.</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">326</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Age</td>
<td align="right">20.00</td>
</tr>
<tr class="even">
<td>Number.of.sexual.partners</td>
<td align="right">1.00</td>
</tr>
<tr class="odd">
<td>First.sexual.intercourse</td>
<td align="right">19.00</td>
</tr>
<tr class="even">
<td>Num.of.pregnancies</td>
<td align="right">1.00</td>
</tr>
<tr class="odd">
<td>Smokes</td>
<td align="right">0.00</td>
</tr>
<tr class="even">
<td>Smokes..years.</td>
<td align="right">0.00</td>
</tr>
<tr class="odd">
<td>Hormonal.Contraceptives</td>
<td align="right">1.00</td>
</tr>
<tr class="even">
<td>Hormonal.Contraceptives..years.</td>
<td align="right">0.25</td>
</tr>
<tr class="odd">
<td>IUD</td>
<td align="right">0.00</td>
</tr>
<tr class="even">
<td>IUD..years.</td>
<td align="right">0.00</td>
</tr>
<tr class="odd">
<td>STDs</td>
<td align="right">1.00</td>
</tr>
<tr class="even">
<td>STDs..number.</td>
<td align="right">1.00</td>
</tr>
<tr class="odd">
<td>STDs..Number.of.diagnosis</td>
<td align="right">1.00</td>
</tr>
<tr class="even">
<td>STDs..Time.since.first.diagnosis</td>
<td align="right">1.00</td>
</tr>
<tr class="odd">
<td>STDs..Time.since.last.diagnosis</td>
<td align="right">1.00</td>
</tr>
</tbody>
</table>

![Feature value contributions for woman 326 in the cervical cancer
dataset. With a prediction of 0.43, this woman's cancer probability is
0.41 above the average prediction of 0.03. The feature value that
increased the probability the most is the number of diagnosed STDs. The
feature contributions sum up to the difference of actual and average
prediction (0.41).](images/shapley-cervical-plot-1.png)

<table>
<caption>The Shapley values (phi) for data point 326 from the cervical cancer dataset; explaining the contributions towards the prediction of 0.43 minus 0.03 (= 0.41).</caption>
<thead>
<tr class="header">
<th align="left">feature</th>
<th align="right">phi</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Age</td>
<td align="right">0.0306667</td>
</tr>
<tr class="even">
<td align="left">First.sexual.intercourse</td>
<td align="right">0.0610000</td>
</tr>
<tr class="odd">
<td align="left">Hormonal.Contraceptives</td>
<td align="right">0.0110000</td>
</tr>
<tr class="even">
<td align="left">Hormonal.Contraceptives..years.</td>
<td align="right">0.0140000</td>
</tr>
<tr class="odd">
<td align="left">IUD</td>
<td align="right">0.0046667</td>
</tr>
<tr class="even">
<td align="left">IUD..years.</td>
<td align="right">-0.0003333</td>
</tr>
<tr class="odd">
<td align="left">Num.of.pregnancies</td>
<td align="right">0.0100000</td>
</tr>
<tr class="even">
<td align="left">Number.of.sexual.partners</td>
<td align="right">0.0263333</td>
</tr>
<tr class="odd">
<td align="left">STDs</td>
<td align="right">0.1146667</td>
</tr>
<tr class="even">
<td align="left">STDs..Number.of.diagnosis</td>
<td align="right">0.0940000</td>
</tr>
<tr class="odd">
<td align="left">STDs..Time.since.first.diagnosis</td>
<td align="right">0.0020000</td>
</tr>
<tr class="even">
<td align="left">STDs..Time.since.last.diagnosis</td>
<td align="right">-0.0010000</td>
</tr>
<tr class="odd">
<td align="left">STDs..number.</td>
<td align="right">0.0356667</td>
</tr>
<tr class="even">
<td align="left">Smokes</td>
<td align="right">0.0043333</td>
</tr>
<tr class="odd">
<td align="left">Smokes..years.</td>
<td align="right">-0.0013333</td>
</tr>
</tbody>
</table>

For the bike rental dataset we also train a Random Forest to predict the
number of rented bikes for a day given the weather conditions and
calendric information. The dataset is described in Chapter
@ref(bike-data). Table @ref(tab:shapley-bike-instance) displays the
feature values of the day for which we want to explain the prediction.
Figure @ref(fig:shapley-bike-plot) shows the explanations created for
the Random Forest prediction of one specific day; the raw Shapley values
are displayed in Table @ref(tab:shapley-bike-table)

<table>
<caption>Instance (day) for which to explain the predicted outcome (2425 bikes)</caption>
<thead>
<tr class="header">
<th></th>
<th align="left">285</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>season</td>
<td align="left">WINTER</td>
</tr>
<tr class="even">
<td>yr</td>
<td align="left">2011</td>
</tr>
<tr class="odd">
<td>mnth</td>
<td align="left">OKT</td>
</tr>
<tr class="even">
<td>holiday</td>
<td align="left">NO HOLIDAY</td>
</tr>
<tr class="odd">
<td>weekday</td>
<td align="left">WED</td>
</tr>
<tr class="even">
<td>workingday</td>
<td align="left">WORKING DAY</td>
</tr>
<tr class="odd">
<td>weathersit</td>
<td align="left">RAIN/SNOW/STORM</td>
</tr>
<tr class="even">
<td>temp</td>
<td align="left">17.53665</td>
</tr>
<tr class="odd">
<td>hum</td>
<td align="left">90.625</td>
</tr>
<tr class="even">
<td>windspeed</td>
<td align="left">16.62605</td>
</tr>
<tr class="odd">
<td>days_since_2011</td>
<td align="left">284</td>
</tr>
</tbody>
</table>

![Feature value contributions for instance 285. With a predicted 2425
rented bikes, this day is -2091 below the average prediction of 4516.
The feature values that had the most negative effects were the weather
situation, humidity and the time trend (years since 2011). The
temperature on that day had a positive effect compared to the average
prediction. The feature contributions sum up to the difference of actual
and average prediction (-2091).](images/shapley-bike-plot-1.png)

<table>
<caption>The Shapley values for data point 285 from the bike rental dataset.</caption>
<thead>
<tr class="header">
<th align="left">feature</th>
<th align="right">phi</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">days_since_2011</td>
<td align="right">-209.65</td>
</tr>
<tr class="even">
<td align="left">holiday</td>
<td align="right">0.55</td>
</tr>
<tr class="odd">
<td align="left">hum</td>
<td align="right">-1019.96</td>
</tr>
<tr class="even">
<td align="left">mnth</td>
<td align="right">4.96</td>
</tr>
<tr class="odd">
<td align="left">season</td>
<td align="right">32.25</td>
</tr>
<tr class="even">
<td align="left">temp</td>
<td align="right">411.82</td>
</tr>
<tr class="odd">
<td align="left">weathersit</td>
<td align="right">-747.51</td>
</tr>
<tr class="even">
<td align="left">weekday</td>
<td align="right">-47.11</td>
</tr>
<tr class="odd">
<td align="left">windspeed</td>
<td align="right">8.80</td>
</tr>
<tr class="even">
<td align="left">workingday</td>
<td align="right">2.39</td>
</tr>
<tr class="odd">
<td align="left">yr</td>
<td align="right">-147.34</td>
</tr>
</tbody>
</table>

Be careful to interpret the Shapley value correctly: The Shapley value
is the average contribution of a feature value towards the prediction in
different coalitions. The Shapley value is NOT the difference in
prediction when we would drop the feature from the model.

### The Shapley Value in Detail

This Section goes deeper into the definition and computation of the
Shapley value for the curious reader. Skip this part straight to
'Advantages and Disadvantages' if you are not interested in the
technicalities.

We are interested in the effect each feature has on the prediction of a
data point. In a linear model it is easy to calculate the individual
effects. Here's how a linear model prediction looks like for one data
instance:
$$\\hat{f}(x\_{i\\cdot}) = \\hat{f}(x\_{i1}, \\ldots, x\_{ip}) = \\beta\_0 + \\beta\_1 x\_{i1} + \\ldots + \\beta\_p x\_{ip} $$
 where *x*<sub>*i*⋅</sub> is the instance for which we want to compute
the feature effects. Each *x*<sub>*i**j*</sub> is a feature value, with
*j* ∈ {1, …*p*}. The *β*<sub>*j*</sub> are the weights corresponding to
*x*<sub>*i**j*</sub>.

The feature effect *ϕ*<sub>*i**j*</sub> of *x*<sub>*i**j*</sub> on the
prediction $\\hat{f}(x\_{i\\cdot})$ is:
$$ \\phi\_{ij}(\\hat{f}) = \\beta\_j x\_{ij} - E(\\beta\_j X\_{j}) = \\beta\_j x\_{ij} - \\beta\_j E(X\_{j})$$
 where *E*(*β*<sub>*j*</sub>*X*<sub>*j*</sub>) is the mean effect
estimate for feature *X*<sub>*j*</sub>. The effect is the difference
between the feature contribution to the equation minus the average
contribution. Nice! Now we know how much each feature contributed
towards the prediction. If we sum up all the feature effects over all
features for one instance, the result is:
$$\\sum\_{j=1}^p \\phi\_{ij}(\\hat{f}) = \\sum\_{j=1}^p (\\beta\_j x\_{ij} - E(\\beta\_j X\_{j})) =   (\\beta\_0 +\\sum\_{j=1}^p \\beta\_j x\_{ij}) - (\\beta\_0 + \\sum\_{j=1}^p E(\\beta\_j X\_{j})) = \\hat{f}(x\_{i\\cdot}) - E(\\hat{f}(X))$$
 This is the predicted value for the data point *x*<sub>*i*⋅</sub> minus
the average predicted value. Feature effects *ϕ*<sub>*i**j*</sub> can be
negative.

Now, can we do the same for any type of model? It would be great to have
this as a model-agnostic tool. Since we don't have the *β*'s from a
linear equation in other model types, we need a different solution.

Help comes from unexpected places: cooperative game theory. The Shapley
value is a solution for computing feature effects
$\\phi\_{ij}(\\hat{f})$ for single predictions for any machine learning
model $\\hat{f}$.

#### The Shapley Value

The Shapley value is defined via a value function *v**a**l* over players
in S.

The Shapley value of a feature value *x*<sub>*i**j*</sub> is it's
contribution to the payed outcome, weighted and summed over all possible
feature value combinations:
$$ \\phi\_{ij} (val) = \\sum\_{S \\subseteq \\{x\_{i1}, \\ldots, x\_{ip}\\} \\setminus \\{x\_{ij}\\}} \\frac{|S|!\\left(p-|S| - 1\\right)!}{p!} \\left(val\\left(S \\cup \\{x\_{ij}\\}\\right) - val(S)\\right)$$
 where *S* is a subset of the features used in the model,
*x*<sub>*i*⋅</sub> is the vector feature values of instance *i* and *p*
the number of features. *v**a**l*<sub>*x*<sub>*i*</sub></sub>(*S*) is
the prediction for feature values in set *S*, marginalised over features
not in *S*:
$$val\_{x\_i}(S) = \\int \\hat{f}(x\_{i1}, \\ldots, x\_{ip})d\\mathbb{P}\_{X\_{i\\cdot} \\notin S}   - E\_X(\\hat{f}(X)) $$
 You actually do multiple integrations, for each feature not in *S*. One
concrete example: The machine learning model works on 4 features
{*x*<sub>*i*1</sub>, *x*<sub>*i*2</sub>, *x*<sub>*i*3</sub>, *x*<sub>*i*4</sub>}
and we evaluate $\\hat{f}$ for the coalition *S* consisting of feature
values *x*<sub>*i*1</sub> and *x*<sub>*i*3</sub>:
$$val\_{x\_i}(S) = val\_{x\_i}(\\{x\_{i1}, x\_{i3}\\}) = \\int\_{\\mathbb{R}}  \\int\_{\\mathbb{R}} \\hat{f}(x\_{i1}, X\_{2}, x\_{i3}, X\_{4})d\\mathbb{P}\_{X\_2,X\_4}  - E\_X(\\hat{f}(X))$$
 This looks similar to the linear model feature effects!

Don't get confused by the many uses of the word 'value': The feature
value is the numerical value of a feature and instance; the Shapley
value is the feature contribution towards the prediction; the value
function is the payout function given a certain coalition of players
(feature values).

The Shapley value is the only attribution method that satisfies the
following properties (which can be seen as a definition of a fair
payout):

1.  **Efficiency**:
    $\\sum\_{j=1}^p \\phi\_{ij} = \\hat{f}(x\_i) - E\_X(\\hat{f}(X))$.
    The feature effects have to sum up to the difference of prediction
    for *x*<sub>*i*⋅</sub> and the average.
2.  **Symmetry**: If
    *v**a**l*(*S* ∪ {*x*<sub>*i**j*</sub>}) = *v**a**l*(*S* ∪ {*x*<sub>*i**k*</sub>})
    for all
    *S* ⊆ {*x*<sub>*i*1</sub>, …, *x*<sub>*i**p*</sub>}\\{*x*<sub>*i**j*</sub>, *x*<sub>*i**k*</sub>},
    then *ϕ*<sub>*i**j*</sub> = *ϕ*<sub>*i**k*</sub>. The contribution
    for two features should be the same if they contribute equally to
    all possible coalitions.
3.  **Dummy**: If
    *v**a**l*(*S* ∪ {*x*<sub>*i**j*</sub>}) = *v**a**l*(*S*) for all
    *S* ⊆ {*x*<sub>*i*1</sub>, …, *x*<sub>*i**p*</sub>}, then
    *ϕ*<sub>*i**j*</sub> = 0. A feature which does not change the
    predicted value - no matter to which coalition of feature values it
    is added - should have a Shapley value of 0.
4.  **Additivity**: For a game with combined payouts
    *v**a**l* + *v**a**l*′ the respective Shapley values are
    *ϕ*<sub>*i**j*</sub> + *ϕ*<sub>*i**j*</sub>′. The additivity axiom
    has no practical relevance in the context of feature effects.

An intuitive way to understand the Shapley value is the following
illustration: The feature values enter a room in random order. All
feature values in the room participate in the game (= contribute to the
prediction). The Shapley value *ϕ*<sub>*i**j*</sub> is the average
marginal contribution of feature value *x*<sub>*i**j*</sub> by joining
whatever features already entered the room before, i.e.
*ϕ*<sub>*i**j*</sub> = ∑<sub>All orderings</sub>*v**a**l*({features before j}∪*x*<sub>*i**j*</sub>)−*v**a**l*({features before j})

#### Estimating the Shapley value

All possible coalitions (sets) of features have to be evaluated, with
and without the feature of interest for calculating the exact Shapley
value for one feature value. For more than a few features, the exact
solution to this problem becomes intractable, because the number of
possible coalitions increases exponentially by adding more features.
@strumbelj2014 suggest an approximation with Monte-Carlo sampling:
$$ \\hat{\\phi}\_{ij} =  \\frac{1}{M} \\sum\_{m=1}^M \\left(\\hat{f}(x^{\*+j}) - \\hat{f}(x^{\*-j})\\right) $$
 where $\\hat{f}(x^{\* + j})$ is the prediction for *x*<sub>*i*⋅</sub>,
but with a random number of features values replaced by feature values
from a random data point *x*, excluding the feature value for
*x*<sub>*i**j*</sub> The x-vector *x*<sup>\* − *j*</sup> is almost
identical to *x*<sup>\* + *j*</sup>, but the value *x*<sub>*i**j*</sub>
is also taken from the sampled *x*. Each of those *M* new instances are
kind of 'Frankensteins', pieced together from two instances.

**Approximate Shapley Estimation Algorithm**: Each feature value
*x*<sub>*i**j*</sub>'s contribution towards the difference
$\\hat{f}(x\_{i\\cdot}) - \\mathbb{E}(\\hat{f})$ for instance
*x*<sub>*i*⋅</sub> ∈ *X*.

-   Require: Number of iterations *M*, instance of interest *x*, data
    *X*, and machine learning model $\\hat{f}$
-   For all *j* ∈ {1, …, *p*}:
-   For all *m* ∈ {1, …, *M*}:
    -   draw random instance *z* from *X*
    -   choose a random permutation of feature *o* ∈ *π*(*S*)
    -   order instance *x*:
        *x*<sub>*o*</sub> = (*x*<sub>*o*<sub>1</sub></sub>, …, *x*<sub>*o*<sub>*j*</sub></sub>, …, *x*<sub>*o*<sub>*p*</sub></sub>)
    -   order instance *z*:
        *z*<sub>*o*</sub> = (*z*<sub>*o*<sub>1</sub></sub>, …, *z*<sub>*o*<sub>*j*</sub></sub>, …, *z*<sub>*o*<sub>*p*</sub></sub>)
    -   construct two new instances
        -   *x*<sup>\* + *j*</sup> = (*x*<sub>*o*<sub>1</sub></sub>, …*x*<sub>*o*<sub>*j* − 1</sub></sub>, *x*<sub>*o*<sub>*j*</sub></sub>, *z*<sub>*o*<sub>*j* + 1</sub></sub>, …, *z*<sub>*o*<sub>*p*</sub></sub>)
        -   *x*<sup>\* − *j*</sup> = (*x*<sub>*o*<sub>1</sub></sub>, …*x*<sub>*o*<sub>*j* − 1</sub></sub>, *z*<sub>*o*<sub>*j*</sub></sub>, *z*<sub>*o*<sub>*j* + 1</sub></sub>, …, *z*<sub>*o*<sub>*p*</sub></sub>)
    -   $\\phi\_{ij}^{(m)} = \\hat{f}(x^{\* + j}) - \\hat{f}(x^{\* - j})$
-   $\\phi\_{ij}(x) = \\frac{1}{M}\\sum\_{m=1}^M\\phi\_{ij}^{(m)}$

First, select an instance of interest *i*, a feature *j* and the number
of samples *M*. For each sample, a random instance from the data is
chosen and the order of the features is mixed. From this instance, two
new instances are created, by combining values from the instance of
interest *x* and the sample. The first instance *x*<sup>\* + *j*</sup>
is the instance of interest, but where all values in order before and
including feature *j* are replaced by feature values from the sample.
The second instance *x*<sup>\* − *j*</sup> is similar, but has all the
values in order before, but excluding feature *j*, replaced by features
from the sample. The difference in prediction from the black box is
computed:
$\\phi\_{ij}^{(m)} = \\hat{f}(x^{\* + j}) - \\hat{f}(x^{\* - j})$. All
these differences are averaged and result in
$\\phi\_{ij}(x) = \\frac{1}{M}\\sum\_{m=1}^M\\phi\_{ij}^{(m)}$.
Averaging implicitly weighs samples by the probability distribution of
*X*.

That's not the only way to compute the Shapley value: For example,
@Lundberg2016 propose a computation method that includes weight kernels
and regularised linear regression.

### Advantages

-   The difference between the prediction and the average prediction is
    fairly distributed among the features values of the instance - the
    shapley efficiency property. This property sets the Shapley value
    apart from other methods like LIME (Chapter @ref(lime)). LIME does
    not guarantee to perfectly distribute the effects. It might make the
    Shapley value the only method to deliver a full explanation. In
    situations that demand explainability by law - like EU's "right to
    explanations" - the Shapley value might actually be the only
    compliant method. I am not a lawyer, so this reflects only my
    intuition about the requirements.
-   The Shapley value allows contrastive explanations: Instead of
    comparing a prediction with the average prediction of the whole
    dataset, you could compare it to a subset or even to a single
    datapoint.
-   The Shapley value is the only explanation method with a solid
    theory. The axioms - efficiency, symmetry, dummy, additivity - give
    the explanation a reasonable foundation. Methods like LIME assume
    linear behaviour of the machine learning model locally but there is
    no theory why this should work or not.
-   It's mind-blowing to explain a prediction as a game played by the
    feature values.

### Disadvantages

-   The Shapley value needs a lot of computation time. In 99.9% of the
    real world problems the approximate solution - not the exact one -
    is feasible. An accurate computation of the Shapley value is
    potentially computational expensive, because there are
    2<sup>*k*</sup> possible coalitions of features and the 'absence' of
    a feature has to be simulated by drawing random samples, which
    increases the variance for the estimate *ϕ*<sub>*i**j*</sub>. The
    exponential number of the coalitions is handled by sampling
    coalitions and fixing the number of samples *M*. Decreasing *M*
    reduces computation time, but increases the variance of
    *ϕ*<sub>*i**j*</sub>. It is unclear how to choose a sensitive *M*.
-   The Shapley value can be misinterpreted: The Shapley value
    *ϕ*<sub>*i**j*</sub> of a feature *j* is not the difference in
    predicted value after the removal of feature *j*. The interpretation
    of the Shapley value is rather: Given the current set of feature
    values, the total contribution of feature value *x*<sub>*i**j*</sub>
    to the difference in the actual prediction and the mean prediction
    is *ϕ*<sub>*i**j*</sub>.
-   The Shapley value is the wrong explanation method if you seek sparse
    explanations. Humans prefer selective explanations , like LIME
    produces, so especially for explanations facing lay-persons, LIME
    might be the better choice for feature effects computation.
-   The Shapley value returns a simple value per feature, and not a
    prediction model like LIME. This means it can't be used to make
    statements about changes in the prediction for changes in the input
    like: "If I would earn 300 € more per year, my credit score should
    go up by 5 points."
