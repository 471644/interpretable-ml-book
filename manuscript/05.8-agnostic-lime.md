

## Local Surrogate Models (LIME) {#lime}
Local surrogate models are interpretable models used to explain individual predictions of black box machine learning models.
Local interpretable model-agnostic explanations (LIME) (Ribeiro, M.T., Singh, S. and Guestrin, C., 2016[^Ribeiro2016lime]) is a paper in which the authors propose a concrete implementation of local surrogate models.
Surrogate models are trained to approximate the predictions of the underlying black box model.
Instead of trying to fit a global surrogate model, LIME focuses on fitting local surrogate models to explain why single predictions were made.


The idea is quite intuitive.
First of all, forget about the training data and imagine you only have the black box model where you can input data points and get the models predicted outcome.
You can probe the box as often as you want.
Your goal is to understand why the machine learning model made a certain prediction.
LIME tests out what happens to the model's predictions when you feed variations of your data into the machine learning model.
LIME generates a new dataset consisting of perturbed samples and the associated black box model's predictions.
On this dataset LIME then trains an interpretable model weighted by the proximity of the sampled instances to the instance of interest.
The interpretable model can basically be anything from [this chapter](#simple), for example [LASSO](#lasso) or a [decision tree](#tree).
The learned model should be a good approximation of the machine learning model locally, but it does not have to be so globally.
This kind of accuracy is also called local fidelity.

Mathematically, local surrogate models with interpretability constraint can be expressed as follows:

$$\text{explanation}(x)=\arg\min_{g\in{}G}L(f,g,\pi_x)+\Omega(g)$$
The explanation model for instance x is the model g (e.g. linear regression model) that minimizes loss L (e.g. mean squared error), which measures how close the explanation is to the prediction of the original model f (e.g. an xgboost model), while the model complexity $\Omega(g)$ is kept low (e.g. favor fewer features).
G is the family of possible explanation, for example all possible linear regression models.
The proximity measure $\pi_x$ defines how large the neighbourhood is around instance x that we consider for the explanation.
In practice, LIME only optimizes the loss part.
The  user has to determine the complexity, e.g. by selecting the maximum number of features that the linear regression model may use.

The recipe for fitting local surrogate models:

- Choose your instance of interest for which you want to have an explanation of its black box prediction.
- Perturb your dataset and  get the black box predictions for these new points.
- Weight the new samples by their proximity to the instance of interest.
- Fit a weighted, interpretable model on the dataset with the variations.
- Explain prediction by interpreting the local model.

In the current implementations ([R](https://github.com/thomasp85/lime) and [Python](https://github.com/marcotcr/lime)) for example linear regression can be chosen as interpretable surrogate model.
Upfront you have to choose $K$, the number of features that you want to have in your interpretable model.
The lower the $K$, the easier the model is to interpret, higher $K$ potentially creates models with higher fidelity.
There are different methods for how to fit models with exactly $K$ features. A solid choice is [Lasso](#lasso).
A Lasso model with a high regularisation parameter $\lambda$ yields a model with only the intercept.
By refitting the Lasso models with slowly decreasing $\lambda$, one after each other, the features are getting weight estimates different from zero.
When $K$ features are in the model, you reached the desired number of features. 
Other strategies are forward or backward selection of features.
This means you either start with the full model (=containing all features) or with a model with only the intercept and then testing which feature would create the biggest improvement when added or removed, until a model with $K$ features is reached.
Other interpretable models like decision trees are also possible.

How do you get the variations of the data?
This differs depending on the type of data, which can be either text, an image or tabular data. 
For text and image the solution is turning off and on single words or super-pixels.
In the case of tabular data, LIME creates new samples by perturbing each feature individually, by drawing from a normal distribution with mean and standard deviation from the feature.

### LIME for Tabular Data
Tabular data means any data that comes in tables, where each row represents an instance and each column a feature.
LIME sampling is not done around the instance of interest, but from the training data's mass centre, which is problematic.
But it increases the likelihood that the outcome for some of the sampled points predictions differ from the data point of interest and that LIME can learn at least some explanation.

It's best to visually explain how the sampling and local model fitting works:

![How LIME sampling works: A) The black box model predicts one of two classes given feature x1 and x2. Most data points have class 0 (darker colour), and the ones with class 1 are grouped in an upside-down V-shape (lighter colour). The plot displays the decision boundaries learned by a machine learning model. In this case it was a Random Forest, but it does not matter, because LIME is model-agnostic and we only care about the decision boundaries. B) The yellow point is the instance of interest, which we want to explain. The black dots are data sampled from a normal distribution around the means of the features in the training sample. This needs to be done only once and can be reused for other explanations. C) Introducing locality by giving points near the instance of interest higher weights. D) The colours and signs of the grid display the classifications of the locally learned model form the weighted samples. The white line marks the decision boundary (P(class) = 0.5) at which the classification of the local model changes.](images/lime-fitting-1.png)



As always, the devil's in the detail.
The definition of a meaningful neighbourhood around a point is difficult.
LIME currently uses an exponential smoothing kernel to define the neighbourhood.
A smoothing kernel is a function that takes two data instances and returns a proximity measure.
The kernel width determines how large the neighbourhood is: 
A small kernel width means that an instance must be very close to impact the local model, a larger width means that instances that are farther away also influence the model.
If you look at [LIME's Python implementation (file lime/lime_tabular.py)](https://github.com/marcotcr/lime/tree/ce2db6f20f47c3330beb107bb17fd25840ca4606) you will see that it uses an exponential smoothing kernel (on the normalized data) and the kernel width is 0.75 times the square root of the number of columns of the training data.
It looks like an innocent line of code, but it's like an elephant sitting in your living room next to the good porcelain you got from your grandparents.
The big problem is that we don't have a good way to find the best kernel or the optimal width.
And where the hell does the 0.75 even come from?
In certain scenarios, you can easily flip your explanation by changing the kernel width, as shown in the following figure:


![Explanation of the prediction of instance x = 1.6. The predictions of the black box model dependent on a single feature is represented as a black line and the distribution of the data is indicated with rugs. Three local surrogate models with different kernel widths are computed. The resulting linear regression model relies heavily on the kernel width: Has the feature a negative, positive or no effect for x = 1.6? Decide for yourself, I don't know the answer.](images/lime-fail-1.png)

The example showed only one feature.
In high-dimensional feature spaces it gets much worse.
It's also very unclear whether the distance measure should treat all features equally.
Is one distance unit for feature x1 the same as one unit for feature x2?
Distance measures are quite arbitrary and distances in different dimensions (aka features) might not be comparable at all.


#### Example
Let's look at a concrete example.
We go back to the [bike rental data](#bike-data) and turn the prediction problem into a classification:
After accounting for the trend that the bike rental became more popular over time we want to know on a given day if the number of rented bikes will be above or below the trend line.
You can also interpret 'above' as being above the mean bike counts, but adjusted for the trend.



















