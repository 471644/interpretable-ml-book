## Example-based explanations {#example}

Example-based explanations are methods that return instances from the dataset to explain the behaviour of a black box machine learning model.

*Keywords: example-based explanations, case-based reasoning, CBR, solving by analogy, intra-domain analogy*

Difference to model-agnostic methods:
The methods always yield an instance, not a summary; 
The methods might be applied before fitting a model to explain the dataset, by which the behaviour of the model can be understood better.
Example-based explanations are only meaningful if we can represent an example in a humanly understandable way.
It works very well for images, because we can simply visualize images.
It's harder with tabular data, where an instance can consist of hundreds or thousands of features.
Listing the values of all those features to describe an instance is nonsense.
It works well if there are just a handful of features or if we have a way to summarize an instance.

Example-based explanations helps the humans to construct mental models of the machine learning model. 
Also it helps to understand the complex data distributions.


What are example-based explanations?
Let's start with a few real world exampels [^cbr]:

A physician treats a patient with unusual symptoms.
The patients symptoms remind her of another patient she had years ago with similar symptoms and sends him to a specialist for further examination.

A data scientist gets a new client with a new task: 
Analyzing the risk factors that lead to a breakdown of machines. 
The data scientist remembers a similar task he solved and re-uses parts of the code from the old project, because he thinks that the client might want to have the same analysis as the other client.



These are examples how we humans think in examples.
The structure his: A is similar to B, B caused Y to happen, so I predict A to cause Y as well.
We humans love these explanations, because they make things seem plausible.
In an implicit way, some machine learning approaches work example-based.
Decision trees group data points together into a node, when they are similar in the features that are important for predicting the target.
The same is true for a random forest.
Predicting the outcome for a new data point accounts to finding the data points that are similar (in the same terminal node) and predicting the average of those points (instead of returning all points).
The k-nearest neighbours algorithm works directly with example-based predictions. 
knn predicts new data points by asking what the k nearest data points predicted.
Explaining the prediction of a knn can be done by returning the k neighbours.
Only meaningful if single data points can be interpreted, i.e. they don't have too many features and the features are interpretable.
Hundreds of abstract measurements would not pass this test for example.



This chapter covers: 
- k-nearest neighbours as a machine learning model that is based on examples.
- Prototypes: Which instances from the data are stereotypical for certain classes?
- Criticisms: Which instances from the data are a bit unusual for certain classes?
- Counterfactuals: How would the instance look like if we want to flip the prediction?
- Adverserial examples: Same as counterfactuals, but with the purpose to maliciously fooling the machine learning algorithm.
- Archetypes?
- Anchors?




### Prototypes and criticisms

A prototype is a typical data point for a class. 
In the context of interpretabl machine learning, a prototype is a typical data point given the predicted class.
A criticism is the opposite: A data point that is atypical for a predicted class.



Bayesian Case Model [^bcm] (data only) and [^critique] (on ML model)



### Criticisms

### Counterfactuals and adverserial examples

Difference here: Data points are not from training data, but artificially created or modified from existing points.

### k-Nearest Neighbors {#knn}
k-nearest neighbours retrieves the k most similar data points as measured with the distance to the data point to be predicted.

k-nearest neighbours is probably the most classic method for using examples to make predictions. 


## Influence functions

### Archetypes?

### Anchors?



[^bcm]: Kim, B., Rudin, C., & Shah, J. (n.d.). The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification.

[^critique]: Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. "Examples are not enough, learn to criticize! criticism for interpretability." Advances in Neural Information Processing Systems. 2016.


[^cbr]: Aamodt, A., & Plaza, E. (1994). Case-based reasoning : Foundational issues, methodological variations, and system approaches. AI Communications, 7(1), 39â€“59.
