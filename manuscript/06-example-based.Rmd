# Example-based explanations {#example}

Example-based explanations are methods that return instances from the dataset to explain the behaviour of a black box machine learning model.

*Keywords: example-based explanations, case-based reasoning, CBR, solving by analogy, intra-domain analogy*

Difference to model-agnostic methods:
The methods always yield an instance, not a summary; 
The methods might be applied before fitting a model to explain the dataset, by which the behaviour of the model can be understood better.
Example-based explanations are only meaningful if we can represent an example in a humanly understandable way.
It works very well for images, because we can simply visualize images.
It's harder with tabular data, where an instance can consist of hundreds or thousands of features.
Listing the values of all those features to describe an instance is nonsense.
It works well if there are just a handful of features or if we have a way to summarize an instance.

Example-based explanations helps the humans to construct mental models of the machine learning model. 
Also it helps to understand the complex data distributions.


What are example-based explanations?
Let's start with a few real world exampels [^cbr]:

A physician treats a patient with unusual symptoms.
The patients symptoms remind her of another patient she had years ago with similar symptoms and sends him to a specialist for further examination.

A data scientist gets a new client with a new task: 
Analyzing the risk factors that lead to a breakdown of machines. 
The data scientist remembers a similar task he solved and re-uses parts of the code from the old project, because he thinks that the client might want to have the same analysis as the other client.

A firefighter sees a kitty in a burning house and decided whether he should go in and rescue it.
He remembers a few houses that were similar to this one: old wooden houses, with two levels. 
At a similar stage of the fire they were unstable and collapsed at some points. 
Based on the similarity of this case, he decides not to go in, because the risk of the house breaking down is too high. 
Fortunately the kitty jumps out of the window and nobody is harmed in the fire (happy end!).


These are examples how we humans think in examples.
The structure his: A is similar to B, B caused Y to happen, so I predict A to cause Y as well.
We humans love these explanations, because they make things seem plausible.
In an implicit way, some machine learning approaches work example-based.
Decision trees group data points together into a node, when they are similar in the features that are important for predicting the target.
The same is true for a random forest.
Predicting the outcome for a new data point accounts to finding the data points that are similar (in the same terminal node) and predicting the average of those points (instead of returning all points).
The k-nearest neighbours algorithm works directly with example-based predictions. 
knn predicts new data points by asking what the k nearest data points predicted.
Explaining the prediction of a knn can be done by returning the k neighbours.
Only meaningful if single data points can be interpreted, i.e. they don't have too many features and the features are interpretable.
Hundreds of abstract measurements would not pass this test for example.



This chapter covers interpretable models using example-based methods, methods for analysing the data to make it interpretable and interpretability methods that work on models, but return examples as explanations. 
- k-nearest neighbours as a machine learning model that is based on examples.
- Prototypes: Which instances from the data are stereotypical for certain classes?
- Criticisms: Which instances from the data are a bit unusual for certain classes?
- Counterfactuals: How would the instance look like if we want to flip the prediction?
- Adverserial examples: Same as counterfactuals, but with the purpose to maliciously fooling the machine learning algorithm.
- Archetypes?
- Anchors?




## Prototypes and criticisms

A prototype is a typical data point for a class. 
In the context of interpretabl machine learning, a prototype is a typical data point given the predicted class.
A criticism is the opposite: A data point that is atypical for a predicted class.
Bayesian Case Model [^bcm] (data only) and [^critique] (on ML model)


Prototypes are already nice, but they may not be sufficient to explain the data.
They are only sufficient when the data distribution within the classes is very homogenuous.
In real world datasets the situation is often more complicated.
The purpose of criticisms is to deliver insights together with prototypes, and especially where prototypes don't tell the full story of a data subset.


#### Approach: MMD-critic

Measure the maximum mean discrepancy (MMD) between to sample datasets.
MMD and its associated witness function is used in MMD critique for selecting data points that are either prototypes or criticisms.
MMD-critic measures the similarity between the training data and potential prototypes and selects prototypes that maximizes the similarity.
Samples that are not explained well by the prototypes are selected as criticisms.

Side effect: The MMD-critic approach can also be used to build a neares-prototype classifier, that classifies new data points according to the prototype that is closest to this point.


The main idea of MMD-critic:
We compare the density distributions of the prototypes and the actual data. 
Good prototypes are at points of the dataset with a high density, because then they are representative for the data.


Selecting prototypes create a density distribution of prototypes.
The goal is to minimize the discrepancy between this distribution of the prototypes and the data distribution. 
Once these prototypes are found, we can choose the points from the data distribution, where the prototypes and the data are the most different. 
Let's say we have a image dataset for classifying animals and find a bunch of prototypes, including two prototypes for the dog class, one prototype shows dogs inside the house and the other is representative for dogs in the park. 
But there are a few outliers, that are not well covered by theses prototypes, and these are dogs that are dressed in costumes. 
They might be selected as criticisms.

Prototypes and criticisms are actual data points.

Algorithm:

1. select the number of prototypes and criticisms you want
1. select prototypes using greedy search
1. selecte criticisms using greedy search.

greedy search works, because the loss function is submodular, that means that the greedy method will at least get 63% of the optimal solution.

```{r, fig.cap = "Illustration for criticisms and prototypes of a data set."}
set.seed(1)
dat1 = data.frame(x1 = rnorm(20, mean = 4, sd = 0.3), x2 = rnorm(20, mean = 1, sd = 0.3))
dat2 = data.frame(x1 = rnorm(30, mean = 2, sd = 0.2), x2 = rnorm(30, mean = 2, sd = 0.2))
dat3 = data.frame(x1 = rnorm(40, mean = 3, sd = 0.2), x2 = rnorm(40, mean = 3))
dat4 = data.frame(x1 = rnorm(7, mean = 4, sd = 0.1), x2 = rnorm(7, mean = 2.5, sd = 0.1))

dat = rbind(dat1, dat2, dat3, dat4)
dat$type = "data"
dat$type[c(7, 23, 77)] = "prototype"
dat$type[c(81,95)] = "criticism"

ggplot(dat, aes(x = x1, y = x2)) + geom_point() +
  geom_point(data = filter(dat, type!='data'), aes(color = type, shape = type), size = 6, alpha = 0.8) + 
  scale_shape_discrete(solid = TRUE)

```

Now we only need a function that can tell us how close the distribution of the prototypes is to the distribution of the data (the closer the better) and a way to find criticisms using the same function.
We want to test whether the distributions are different, based on samples from them.


So now, a bit of theory.
The function we are looking for is called maximum mean discrepancy, which measures the difference between two distributions. 


I'lls spare you with the theoretical construct and jump right into how we can actually measure it.


$$MMD(G,P,Q)=sup_{g\in{}G}\left(E_{X\sim{}P}[g(X)]-E_{Y\sim{}}Q[g(Y)]\right)$$
F is a reproducing kernel Hilbert space (RKHS) with kernel function k.
A choice for the kernel is the radial basis function kernel.

$$f(x)=E_{X'\sim{}P}[k(x,X')]-E_{X'\sim{}Q}[k(x,X')]$$
g(x) is also known as the witness function. 
They call it f(x), but since in this book f(x) is reserved for the relationship between features and target in supervised learning, I am using g here.

It measures the maximum discrepancy between two expectations in F.
The witness function is positive when Q underfits the density of P and negative if Q overfits P.


Aaaaand now you can forget about this, because we can empirically estimate this with data samples (measuring MMD squared here):

$$MMD^2=\frac{1}{m}\sum_{i,j=1}^m{}k(x_i,x_j)-\frac{2}{mn}\sum_{i,j=1}^{m,n}k(x_i,y_j)+\frac{1}{n^2}\sum_{i,j=1}^n{}k(y_i,y_j)$$

k measures the similarity of two points.
m samples from Q as $z_i$.
n samples from P as $x_i$.

The goal is to minimize MMD2, but for convenenience it is turned into a maximisation problem with some additivie bias (not so important here).

The witness function g is approximated as:

$$g(x)=\frac{1}{n}\sum_{i=1}^nk(x,x_i)-\frac{1}{m}\sum_{j=1}^mk(x,z_j)$$

Criticisms are searched for so that the points are where there are no prototypes, but many data points still. 
An additional term in the optimization function enforces diversity in the points, which is needed so that the points are not all from the same pile, but different types of criticisms.

How it can be used:

1. Find prototypes and criticisms
1. Train machine learning model as usual
1. Predict the prototypes and criticisms
1. Analyse the predictions: In which cases was the algorithm wrong? Now you have an interpretable set of examples, where you know that it represents the data well and it can help you find the weaknesses of the machine learning model.


Another way in which it can be used: 

Build an intepretable classifier with it.
Nearest prototype based classifier.
Criticisms are not used for this.
Interpretability comes from retrieving the closest prototype and showing it to the user.


### Examples:

```{r, prototypes-and-criticisms, fig.cap = "Prototypes and criticisms for two types of dog breeds from the ImageNet dataset. Prototypes mostly show the face of the dog, whereas criticisms are without the dogs face or in other coloring. Right hand side dog criticisms show the dog in a costume."}
knitr::include_graphics("images/proto-critique.png")
```

```{r, prototypes-and-criticisms, fig.cap = "Prototypes and criticisms for a digit recognition dataset. Interestingly the number of prototypes differ per class. Criticisms include digits drawn with unusually thick or thin lines amongst others. Or also unrecognizable digits."}
knitr::include_graphics("images/proto-critique2.png")
```


### Code

https://github.com/BeenKim/MMD-critic

## Counterfactuals and adverserial examples

Difference here: Data points are not from training data, but artificially created or modified from existing points.

## k-Nearest Neighbors {#knn}
k-nearest neighbours retrieves the k most similar data points as measured with the distance to the data point to be predicted.

k-nearest neighbours is probably the most classic method for using examples to make predictions. 


## Influence functions

## Archetypes?

## Anchors?



[^bcm]: Kim, B., Rudin, C., & Shah, J. (n.d.). The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification.

[^critique]: Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. "Examples are not enough, learn to criticize! criticism for interpretability." Advances in Neural Information Processing Systems. 2016.


[^cbr]: Aamodt, A., & Plaza, E. (1994). Case-based reasoning : Foundational issues, methodological variations, and system approaches. AI Communications, 7(1), 39–59.
