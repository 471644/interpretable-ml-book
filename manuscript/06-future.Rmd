# A look into the crystal ball

What will the future of machine learning and interpretability bring?
This chapter is a highly speculative and highly subjective view on the future. 
We take a look into the crystal ball and see where the field is going.
This is an extrapolation of trends we are already seing.
Plus the general observation of acceleration of science and economics which plays into my views.
I asked: 

- Where is machine learning going?
- How will interpretability evolve
- Where is this going in the near future
- What research gaps and application gaps are still there?

![](images/prophet.png)


## First: Where will ML go? 
Because without ML, there is no interpretable ML needed of course. 
You can already do a lot with machine learning. 
But from my experience it is quite hard to integrate it. 
Not because it is not possible, but it simply requires time. 
Even with the hype now, we see a lot of companies hiring data scientists, machine learning specialists, statisticians and so on. 
Big companies open owen "AI labs", "Machine Learning Units", ...
But from my own experience and from others:
They do not even have data in the form needed,
or there is a great misunderstanding what can be done. 


![](images/enrollment.png)

**Machine learning has to and will grow up**
There needs to be a lot of eductions how problems have to be formulated to have it as machine learning problem. 
And so, many highly paid data scientists do some Excel shit, or classical business intelligence with reporting and SQL queries. 
But a few are already starting to do the sexy stuff, with the big Internet companies in the lead. 
My predictions



![](images/strong.png)

**Machine learning will sweep through many industries and business**
Many problems will be reformulated as prediction problems. 
Machine Learning is a form of automation or at least can be part of automation.
We see more regulation coming, for example the GDPR, which will make it more demanding to use machine learning. 


![](images/mri.png)

**Interpretability will increase the adoption of machine learning**
In many areas and sectors, interpretability will be the key for adoptions of machine learning. 
Many people I spoke to are not using machine learning because they cannot explain the model to others. 
This is more annecdotal evidence.
But I believe that interpretability will solve this issue and make machine learning attractive for organisations and people that demand some transparency.


## How interpretability will evolve


### The trend will be model-agnostic methods
I believe model-agnostic methods is the way to go. 
Intrinsically interpretable methods will have a subordinate role, mostly as surrogate model, but not modelling the data itself. 

What data scientists and statisticians currently do: Model the data. 
What they will do in the future: Model the black box. 

### Analysis of models, instead of data
Confidence intervals
p-values and tests for pdps, for local models, 
Error analysis for feature importance in the black box feature space. 

Complete separation of optimization for predicition and optimization for interpretation. 

We will get benchmarks and measures for how good interpretability is. 


### AutoML + IML

![](images/wise.png)


Trends we already have: 
AutoML: Automatic tuning of machine learning models. 
Automatic feature engineering. 
Automatic feature selection.
With a separation of interpretation method and underlying model, we can optimize the underlying model very well and completely automatically.
The trend we already see is to combine automatic machine learning with model-agnostic interpretability methods.
AutoML + IML will be the norm

Automated data scientist. 


## Where all of this is going: The automated data scientist

![](images/scientist.png)

Automated scientist: Generation of hypothesis, testing, interpretation of results. 
Active learning 

Accelerating automation: Automation of the scientific process, automation of knowledge generation. 
Revolution of science (after the data revolution in which we are now).


## What we need to work on: Directions for future research

- Definition of interpretability
- Interpretability benchmarks
- More work on local surrogate models
- Testing for models
- Variance of models
- Special interpretability tools for time series, text
- Methods for unsupervised machine learning. In theory all the existing methods should work though, because the structure is - after the model is trained - the same: predict of a column. 
- 

For application: 
- The methods in this book are very general purpose, for many applications you might want to adapt some techniques or invent something new!
- I also believe that model-agnostic methods are more useful here. 
- Needs a lot of testing or expertise what kind of explanations are most useful for which people and in which scenarios
- 



