# A look into the crystal ball


![](images/prophet.png)

What will the future of interpretable machine learning bring?
This chapter is a highly speculative play of thought and subjective guess what the future might bring.
I opened the book with the [short story chapter](#storytime) which is the pessimistic view, and I want to conclude the book with a more optimistic outlook. 
Choose for yourself, what you think more likely.

The premises that underly my speculations:

1. **Digitize everything: Whatever information can be digitized will be digitized.**
Think about electronic cash and storing da transactions digitally. 
Think about e-books, music and videos that are streamed. 
Think about all the sensory data about our environment, human and animal movements, industrial production processes and so on.
The reasons for digitization are: cheap computers, cheap sensors, cheap storage, financial advantage.
1. **Automate everything: If a task is automatable and the time and cost of automation is smaller than the cost of task over time, the task will be automated.**
Before digitization we already have some automation: The invention of the weaving machine or the steam machine are already automations of tasks.
Digitization brings automation to another level. 
Simply the fact that you can write a for-loop in programming, write excel macros, automate e-mail replies and so on show how much of your personal life can be automated. 
Ticket machines automate the process of buying train tickets (no cashier needed any longer), washing machines automate a part of laundry, standing orders automate money transactions andd so on.
Automating tasks frees up time and money, so there is a huge economic and personal incentive to automize stuff.
We are observing currently the automation of machine translation, video subbing and so on. 
1. **Imperfect goal specification: We are not (and never will be?) able to specify goals with all its contraints.**
Because we either don't know all the constraints or because we can't measure them really. 
Think about the genie in a bottle, that always takes your wished literally, but with a bad interpretation.
"I wish to be the richest person in the world" -> You become the richest person, but as a side effect, the currency you are holding crashes due to inflation. 
"I wish to be happy for the rest of my life" -> Next 5 minutes you feel like very happy, then you die. 
Some real world examples: 
Let's look at stock corporations as a real world example for imperfect goal specification: A stock corporation has the simple goal to make money for its shareholders. 
But this goal does not capture the goal with all its constraints that we are really aiming for. 
For example we don't like a corporation to kill people to make money, or to poison rivers or to simply print its own money. 
We invented laws, regulations, punishments, compliance procedures and more to pach up the imperfect goal specification - and it is a real struggle.
Another example you can live through yourself: [Paperclips](http://www.decisionproblem.com/paperclips/index2.html), a game in which you play a machine with the goal to produce as many paperclips as possible. 
WARNING: It's highly addictive.
I don't want to spoil it too much, but let's say things get out of control really quickly.
In machine learning the imperfection in problem specification are: 
the data abstraction might not be perfect (wrong population, wrong operationalization of variables, ...), 
the loss function might not cover all constraints, we don't know how to involve constraints, etc..

Digitization fuels automation. 
Imperfect goal specification conflicts with automation.
This conflict is mediated by interpretability methods.


Based on those premises and extrapolation of some already observable trends, I will share my speculations about: 

- The future of machine learning.
- How interpretability evolves.
- The bigger trend
- Research and application gaps.


Let's take a look into the crystal ball and see where the field is going!



## First: Where will ML go? 
Without machine learning, there is no interpretable machine learning, of course.
Therefore we have to see where machine learning is heading, before we talk about interpretability.
You can already do a lot with machine learning. 


Let's start with a less optimistic observation:
But from my experience it is quite hard to integrate it into existing processes and products.
Not because it is not possible, but it simply requires time. 
Even with the hype now, we see a lot of companies hiring data scientists, machine learning specialists, statisticians and so on. 
Big companies open up "AI labs", "Machine Learning Units" and hire "Data Scientists", "Machine Learning Experts", ""
But from my own experience and from others:
They do not even have data in the form needed,
or there is a great misunderstanding what can be done. 
This leads to my first prediction, which is an extrapolation of trends.


![](images/enrollment.png)

**Machine learning will grow up slowly but steadily**. 
This is a trend observation.
Meaning that it transitions from academia into business processes, products and real world applications.
I believe there needs to be a lot of eductions how problems have to be formulated to have it as machine learning problem. 
And so, many highly paid data scientists do some Excel shit, or classical business intelligence with reporting and SQL queries. 
But a few are already starting to do the sexy stuff, with the big Internet companies in the lead. 
We have yet to find good ways to encapsulate machine learning, educated, built easier to use tools and so on. 
I believe machine learning will become a lot easier, we already see that it becomes automated and available through cloud services ("Machine Learning as a service" - just to sprinkle some buzz words.). 
But once machine learning is matured - and I believe we already see the first steps - my next prediction is: 

![](images/strong.png)

**Machine learning will fuel many applications**
From the principle "Whatever can be automated will be automated", I conclude that, whenever possible, 
tasks will be reformulated as prediction problems and solved with machine learning. 
Machine Learning is a form of automation or at least can be part of automation.
We see more regulation coming, for example the GDPR, which will make it more demanding to use machine learning. 
Many easy to do tasks for humans will be replaced by machine learning (here just a few examples):
- Automation of sorting/deciding on/filling out documents. 
This will hit the legal industry, the big consultancies. 
Applications like compliance.
- Data-driven decision like loan applications, 
- Quality checks in production: 
Image recognition based mainly. 

The breakthrough for machine learning will not only come through better computers/more data/better software:


![](images/mri.png)

**Interpretability tools catalyzes the adoption of machine learning**
From the  premise that the goal of a machine learning model can never be satisfyingly specified, it follows that it will need interpretable machine learning to make nachine learning accepted.
In many areas and sectors, interpretability will be the key for adoptions of machine learning. 
Many people I spoke to are not using machine learning because they cannot explain the model to others. 
This is more annecdotal evidence.
But I believe that interpretability will solve this issue and make machine learning attractive for organisations and people that demand some transparency.
Many industries require interpretability, either because of legal requirements, risk aversity or wish for understanding of underlying insights into the problem.
Using machine learning requires some trust, which interpretability can generate. 
Machine learning automizes the modeling process and by this moves the human a bit further away from the data and the problem: 
This increases the risk that problems with the experimental design, choice of training distribution, sampling process, data coding, feature engineering or so on happened. 
With interpretability tools these problems can be detected more easily.



## How interpretability will evolve
Now let's take a look where machine learning interpretability might lead us.
Which techniques will be the important ones?
Where is the field heading?
Whats the end game here?


![](images/detective.png)

### The focus will be on model-agnostic interpretability tools 
It is way easier to automate interpretability, when it is decoupled from the model fitting. 
The benefits of model-agnostic:
Decoupling of predicting and interpreting.
Can easily switch out the underlying machine learning model. 
Can easily switch out the explanation method.
Because of this, model-agnostic methods will scale much better. 
They do not lock you in with an not-so-good performing model.
That's why I believe in that in the long run model-agnostic methods will be more dominantly used.
Intrinsically interpretable methods will have a subordinate role, mostly as surrogate model, but not modelling the data itself. 

What data scientists and statisticians currently do: Model the data. 
What they will do in the future: Model the black box. 


![](images/doctor.png)

### Analysis of models, instead of data
Fitting classical statistical models requires a person looking at distributions, choosing the right kind of model and so on. 
Machine learning does not require any of this. 
So, from the premise of automation, it follows that the "getting insights about the real world" will also be automated as much as possible. 
And since digitization is happening, it becomes easier and easier. 
Confidence intervals
p-values and tests for pdps, for local models, 
Error analysis for feature importance in the black box feature space. 

Complete separation of optimization for predicition and optimization for interpretation. 

We will get benchmarks and measures for how good interpretability is. 

Statisticians are already doing this: 
When we want to know if X has an influence on Y and we have to adjust for Z, then we fit a linear model of Y dependent on X and Z. 
Then we take this model and interpret the weights. 
We also have some tests attached to it. 

What if we had the tools to analyse ANY model (random forests, support vector machines, etc.)?
Because the current flaw in statistics is that we have to put in strong assumptions to fit a linear model. 
Those assumptions are often unrealistic. 
And the current habit is to not test the models on an unseen dataset. 
So usually not optimized for performance. 
I say we have to develop tools to analyse black box models, test them , extract knowledge. 
We need confidence intervals, statistical tests and so on. 
Confidence intervals maybe with bootstrap samples, where we refit the model. 
A lot of research has to go into this. 
Computationally expensiv, but in a few years no problem at all. 




![](images/wise.png)

### AutoML + IML (merging of statistics and machine learning)
That's the complete automation of both steps.
While also letting a human look into the loop (through interpretability tools) and see if any goal missspecification happened.
Trends we already have: 
AutoML: Automatic tuning of machine learning models. 
Automatic feature engineering. 
Automatic feature selection.
With a separation of interpretation method and underlying model, we can optimize the underlying model very well and completely automatically.
The trend we already see is to combine automatic machine learning with model-agnostic interpretability methods.
AutoML + IML will be the norm

Automated data scientist. 


### The automated data scientist

![](images/scientist.png)
Taking this whole concept of automating modelling and explanation creation a step further: The automated data scientist.
**The goal of the data scientists is - of course - to automate themselves out of the job!**


**Same will happen in science (to a degree)**
Automated scientist: Generation of hypothesis, testing, interpretation of results. 
Active learning 

Accelerating automation: Automation of the scientific process, automation of knowledge generation. 
Revolution of science (after the data revolution in which we are now).

Think biological experiments


## What we need to work on: Directions for future research

- Definition of interpretability:
We currently have no good definition of interpretability. 
Ideally, we could simply measure interpretability in the same way we can measure the predictive performance.
- Interpretability benchmarks:
If we could quantify interpretability, we could benchmark different methods against each other and would have means to make and test improvements.
- More work on local surrogate models
- Testing for models
- Variance of models
- Special interpretability tools for time series, text
- Methods for unsupervised machine learning. In theory all the existing methods should work though, because the structure is - after the model is trained - the same: predict of a column. 
- 

For application: 
- The methods in this book are very general purpose, for many applications you might want to adapt some techniques or invent something new!
- I also believe that model-agnostic methods are more useful here. 
- Needs a lot of testing or expertise what kind of explanations are most useful for which people and in which scenarios
- 



