# A Look Into The Crystal Ball


![](images/prophet.png)

What will the future of interpretable machine learning bring?
This chapter is a highly speculative mental exercise and subjective guess how interpretable machine learning will evolve.
I opened the book with the rather pessimistic [short stories](#storytime) and I want to conclude with a more optimistic outlook. 
It's up to all of us, which future will come true.

I based my "predictions" on three premises:

1. **Digitization: Any (interesting) information will be digitized.**
Think about electronic cash and storing transactions digitally. 
Think about e-books, music and videos. 
Think about all the sensory data about our environment, human and animal movements, industrial production processes and so on.
The drivers of digitization are: cheap computers/sensors/storage, scaling effects (winner takes it all), new business models, modular value chains, cost pressure, etc..
1. **Automation: If a task is automatable and the cost of automation is smaller than the cost of doing the task over time, the task will be automated.**
We already had some automation before the advent of the computer: The weaving machine or the steam machine are already automations of tasks.
But computers and digitization brings automation to another level. 
Simply the fact that you can write a for-loop in programming, write excel macros, automate e-mail replies and so on show how much and individual can automated. 
Ticket machines automate the process of buying train tickets (no cashier needed any longer), washing machines automate laundry, standing orders automate money transactions and so on.
Automating tasks frees up time and money, so there is a huge economic and personal incentive to automate things.
We are observing currently the automation of machine translation, video subbing and so on. 
1. **Misspecification: We are not (and never will be?) able to perfectly specify goals with all its constraints.**
Think about the genie in a bottle, that always takes your wishes literally:
"I wish to be the richest person in the world!" -> You become the richest person, but as a side effect, the currency you are holding crashes due to inflation. 
"I wish to be happy for the rest of my life!" -> The next 5 minutes you feel very, very happy, then the genie kills you. 
"I wish for world peace!" -> The genie kills all humans. 
We misspecify the goals because we either don't know all the constraints or because we can't measure them. 
I will give you some real world examples: 
Let's look at stock corporations as an example for imperfect goal specification: A stock corporation has the simple goal to make money for its shareholders. 
But this goal does not capture the goal with all its constraints that we are really aiming for. 
For example we don't like a corporation to kill people to make money, or to poison rivers or to simply print its own money. 
We invented laws, regulations, punishments, compliance procedures and more to patch up the imperfect goal specification.
Another example you can live through yourself: [Paperclips](http://www.decisionproblem.com/paperclips/index2.html), a game in which you play a machine with the goal to produce as many paperclips as possible. 
WARNING: It's highly addictive.
I don't want to spoil it too much, but let's say things get out of control really quickly.
In machine learning the imperfections in problem specification are: 
The data abstraction might not be perfect (wrong population, wrong operationalization of variables, ...), 
the loss function might not cover all constraints, we don't know how to involve constraints,
the training data does not have the same distribution as the application data and so on.

The digitization fuels automation. 
Imperfect goal specification conflicts with automation.
This conflict is mediated by interpretability methods.

The scene for our predictions are set, the crystal ball is ready, now let's take a look where the field might be going!


## First: Where will ML go? 

Without machine learning, there is no interpretable machine learning.
Therefore we have to see where machine learning is heading, before we talk about interpretability.
Machine learning (or "AI") carries a lot of promises and expectations.

Let's start with a less optimistic observation:
While academia is developing a lot of fancy machine learning tools, from my experience it is quite hard to integrate it into existing processes and products.
Not because it is not possible, but simply because it requires time for companies and institutions to catch up. 
In the rush of the AI hype, companies open up "AI labs", "Machine Learning Units" and hire "Data Scientists", "Machine Learning Experts", "AI engineers", ...
But from my own experience and from others:
They sometimes do not even have data in the form needed,
they have not the slightest clue what the data scientists can or should do, 
nobody knows how to integrate this new species of people into existing structures.
This leads to my first prediction:


![](images/enrollment.png)

**Machine learning will grow up slowly but steadily**.

Digitization is pressing forward, and the lure of automation always there.
So even if the path of adoption is slow and stony, machine learning transitions from academia into business processes, products and real world applications.

I believe there needs to be a lot of education how problems have to be formulated as machine learning problem. 
And so, many highly paid data scientists do some Excel calculation, or classical business intelligence with reporting and SQL queries, instead of machine learning.
But a few companies are already starting to apply machine learning successfully, with the big Internet companies in the lead. 
We have yet to find good ways to encapsulate machine learning, educate people and built easier to use tools and so on. 
I believe that  machine learning will become a lot easier to apply: We already see that it becomes automated and available through cloud services ("Machine Learning as a service" - just to sprinkle some buzz words.). 
But once machine learning is matured - and this baby already made its first steps - my next prediction is: 

![](images/strong.png)

**Machine learning will fuel many applications.**

From the principle "Whatever can be automated will be automated", I conclude that, whenever possible, 
tasks will be reformulated as prediction problems and solved with machine learning. 
Machine Learning is a form of automation or at least can be part of automation.
We see more regulation coming, for example the GDPR, which will make it more demanding to use machine learning, but I do not believe that it will offset the gains through machine learning. 
Many easy to do tasks for humans will be replaced by machine learning (here just a few examples):

- Automation of sorting/deciding on/filling out documents (e.g. in insurance companies, the legal industry, consultancies).
- Data-driven decision like loan applications, 
- Quality checks in production: 
Image recognition based mainly. 

The breakthrough for machine learning will not only come through better computers/more data/better software:


![](images/mri.png)

**Interpretability tools catalyze the adoption of machine learning.**

From the premise that the goal of a machine learning model can never be perfectly specified, it follows that interpretable machine learning is needed to make machine learning accepted and to "patch up" resulting issues.
In many areas and sectors, interpretability will be the key for adoption of machine learning. 
Some annecdotal evidence: Many people I spoke to are not using machine learning because they cannot explain the model to others. 
I believe that interpretability will tackle this issue and make machine learning attractive for organisations and people that demand some transparency.
Next to problem misspecification, many industries require interpretability, either because of legal requirements, risk aversity or wish for understanding of underlying insights into the problem.
Machine learning automates the modeling process and by this moves the human a bit further away from the data and the problem: 
This increases the risk that problems with the experimental design, choice of training distribution, sampling process, data coding, feature engineering or so on happened. 
With interpretability tools these problems can be detected more easily.



## How interpretability will evolve

Let's take a look at where machine learning interpretability might be going.


![](images/agnostic.png)

**The focus emphasis will be on model-agnostic interpretability tools.**

It is way easier to automate interpretability, when it is decoupled from the model fitting. 
The benefits of model-agnostic:
We can easily switch out the underlying machine learning model. 
We can as easily switch out the interpretability method.
Because of this, model-agnostic methods will scale much better. 
They do not lock you in with an not-so-good performing model.
That's why I believe in that in the long run model-agnostic methods will be more dominantly used.
Intrinsically interpretable methods will also have their place though.


![](images/wise.png)

**Machine learning will be automatic and so will be interpretability.**

An already visible trend is to completely automate the model fitting: 
That includes the automated feature engineering and selection, automated fitting of models with hyper parameter optimization, comparison of different model and ensembling or stacking of the models. 
The result is the best possible prediction. 
When we use model-agnostic interpretation methods, then we can automatically apply them on whatever model comes out of the automated machine learning process.
In a way, we can also automate this second step: Computer feature importance, partial dependence plots, fit a surrogate model and so on. 
Nobody stops you from computing all of this stuff automatically (besides computational resources).
The humans will be needed for the actual interpretation.
Imagine: You only upload a dataset, specify the prediction goal and, with the click of a button - the best prediction model is fitted and the program spits out all the interpretations of the model. 
Solutions for this already exist and I argue that this is the end game for machine learning. 
When these systems are mature enough, for many applications it will be enough to do this. 
The same as now everyone can build websites without knowing HTML, CSS and Javascript:
You don't have to know HTML to build a website, but there are still web developers.
While also letting a human look into the loop (through interpretability tools) and see if any goal missspecification happened.
But I think we can also take this a step further:


![](images/doctor2.png)

**Don't analyse data, analyse black box models.**

Unpopular opinion: The raw data itself is always useless. 
Theoretically, I don't care about the data. 
I care about the distilled knowledge contained in the data. 
Fitting a machine learning to predict the data is the ultimate distillation for my task (assuming the task is captured in the loss function).
I want to be able to throw away the data and work only on the model. 
Because I can ask the model as many things as I want, the model automatically detects if and how features are relevant for the prediction (many models have built-in feature selection), the model (when fitted flexibly) can detect automatically how the relationships are represented best.
I want a sound model that approximates reality. 

To answer questions like: 
- Was website variant A or B better?
- What are the most relevant factors for the prognosis of this cancer?
- How should this item be categorized?

Simply hypothesis tests like student t-test, analysis of variance or the correlation (the standardised linear regression coefficient is the same as Pearson's correlation coefficient) are already based on models of the data (because they assume distributions).
So what I am telling here is actually not new at all. 
What is new?
That we don't make any assumptions about the distributions. 
Assumptions suck!
They are usually wrong, difficult to check and hard to automate. 
The machine learning approach is more sexy: We don't do any assumptions, we just let it approximate reality as closely as possible (as done in a CV loop) and BOOM! we have our model of reality.
I argue that we should develop all the tools we have in statistics for answering questions (hypothesis tests, correlation measures, interaction measures, visualization tools, confidence intervals, p-values, prediction intervals, probability distributions) and re-coin the for black box models.
In a way, that is already happening: 

- Let's take a classic linear model: The standardized regression coefficient is already a feature importance measure. 
With the [permutation feature importance measure](#feature-importance), we have a tool that can do it for any model. 
- In a linear model the coefficients tell us the influence of a feature for the predicted outcome. 
The generalized version of this is the [partial dependence plot](#pdp).
- Testing whether A or B is better: 
We can also use partial dependence for this, by forcing data points to be A or B. 
What we don't have yet is a statistical significance  measure for arbirtrary model (to my best knowledge).

Taking this whole concept of automating modelling and explanation creation a step further:


![](images/scientist.png)


**The automated data scientist.**

The goal of the data scientists is - of course - to automate themselves out of the job!
Why not, based on a model of the world, suggest new hypothesis to test?
When we automatically fit models, and - with a click - generate the interpretations for the field expert, then no data scientist was involved inbetween. 
Probably it won't be that radical, considering a comparison with web developers: 
Anyone can build a website today, without touching any HTML/CSS/Javascript. 
But there are still plenty of web developers around, doing the more sophisticated stuff.
I believe something similar will happen with machine learning.

If we go a bit further, imagine a program that not only models and interprets automatically, but also creates new hypothesis, plans and conducts experiments.
They can exlain why they suggested the next experiment, because they have an interpretable machine learning model that predicts possible experiments outcomes.
In very defined areas (like testing outcomes of chemical experiments) this will work well.


**Robots and programs that explain themselves**

More intuitive interfaces to machines and programs that make heavy use of machine learning. 
The explanations will be default.
A self-driving car that reports why it stopped abruptly. 
A credit default program that explains to a worker at the bank why a credit application was rejected ("Too many credit cards and working in unstable industry").
A robot arm that explains why it moved the cassis from the conveyor belt into the trash bin ("It detected a craze at the bottom.").

**Interpretability might boost machine intelligence research.**

I could imagine, that by doing more research on how programs and machines can explain themselves, we increase our understanding about intelligence and will be better at making intelligent machines.




## What we need to work on: Directions for future research

**The research field about interpretability has to mature**

- Definition of interpretability needed. Ideally, we could simply measure interpretability in the same way we can measure the predictive performance. Unclear if at all possible. 
- Benchmarking of methods: If we could quantify interpretability, we could benchmark different methods against each other and would have means to make and test improvements. Something like ImageNet for computer vision. Or kaggle competitions where interpretability is incorporated.
- More work on local models (also for clusters/subgroups)
- Methods for unsupervised machine learning. In theory all the existing methods should work though, because the structure is - after the model is trained - the same: predict of a column.  Make interpretability work on 
  - survival tasks
  - time series
  - text 
  - images 
  -reinforcement learning
  - ...
- Needs a lot of testing or expertise what kind of explanations are most useful for which people and in which scenarios
- The methods in this book are very general purpose, for many applications you might want to adapt some techniques or invent something new!
- What to do with features that are not interpretable?
- Incorporation of probability distribution of the features into the methods or at least think more about this. 
- Methods that correct biases in the models
- Model-specific methods: deep learning, boosting, ...
- Methods that can be used when machine learning model is only part of a bigger system (for example pre-processing of input, then machine learning, then the output is transformed by some manual if-then-else rules to ensure that certain cases can not happen). 
How should we adapt interpretability methods in those cases?

**Tools for drawing conclusions from black box models**

- Bootstrapping refi
- Incorporation of model error into the analysis of model output (e.g. pdp ignores error). 
This is important to tie the analysis of the black box model back to the real world.
Because if there is no link to the error, then the insights are about the model. 
If model is perfect image of real world, then the insights are also about the real world. 
But if not perfect, we want to quantify our insecurity about insights by looking at the model error (ideally locally, i.e. depending on where in the feature space we look).
Things like confidence intervals for partial dependence plots (based on model error) or probabilities for the Shapley value that they are different from zero (based on model error).
- Statistical theory for black box models (maybe something out there I do not know of)
- Automated finding of sub-groups
- More causal models as underlying models, because then the interpretations are also causal about the real world. [^causal]
  
  
**Breathing live into it**

- Better interfaces to the programs and robots
- Integration of the machine learning workflow into companies
- 


[^causal]: Hernán, M. A., Hsu, J., & Healy, B. (2018). Data science is science’s second chance to get causal inference right. A classification of data science tasks, 1–14.


