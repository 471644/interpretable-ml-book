# A Look Into The Crystal Ball


![](images/prophet.png)

What will the future of interpretable machine learning bring?
This chapter is a highly speculative play of thought and subjective guess what the future might bring.
I opened the book with the [short story chapter](#storytime) which is the pessimistic view, and I want to conclude the book with a more optimistic outlook. 
Choose for yourself, what you think more likely.

The premises that underly my speculations:

1. **Digitize everything: Whatever information can be digitized will be digitized.**
Think about electronic cash and storing da transactions digitally. 
Think about e-books, music and videos that are streamed. 
Think about all the sensory data about our environment, human and animal movements, industrial production processes and so on.
The reasons for digitization are: cheap computers, cheap sensors, cheap storage, financial advantage.
1. **Automate everything: If a task is automatable and the time and cost of automation is smaller than the cost of task over time, the task will be automated.**
Before digitization we already have some automation: The invention of the weaving machine or the steam machine are already automations of tasks.
Digitization brings automation to another level. 
Simply the fact that you can write a for-loop in programming, write excel macros, automate e-mail replies and so on show how much of your personal life can be automated. 
Ticket machines automate the process of buying train tickets (no cashier needed any longer), washing machines automate a part of laundry, standing orders automate money transactions andd so on.
Automating tasks frees up time and money, so there is a huge economic and personal incentive to automize stuff.
We are observing currently the automation of machine translation, video subbing and so on. 
1. **Imperfect goal specification: We are not (and never will be?) able to specify goals with all its contraints.**
Because we either don't know all the constraints or because we can't measure them really. 
Think about the genie in a bottle, that always takes your wished literally, but with a bad interpretation.
"I wish to be the richest person in the world" -> You become the richest person, but as a side effect, the currency you are holding crashes due to inflation. 
"I wish to be happy for the rest of my life" -> Next 5 minutes you feel like very happy, then you die. 
Some real world examples: 
Let's look at stock corporations as a real world example for imperfect goal specification: A stock corporation has the simple goal to make money for its shareholders. 
But this goal does not capture the goal with all its constraints that we are really aiming for. 
For example we don't like a corporation to kill people to make money, or to poison rivers or to simply print its own money. 
We invented laws, regulations, punishments, compliance procedures and more to pach up the imperfect goal specification - and it is a real struggle.
Another example you can live through yourself: [Paperclips](http://www.decisionproblem.com/paperclips/index2.html), a game in which you play a machine with the goal to produce as many paperclips as possible. 
WARNING: It's highly addictive.
I don't want to spoil it too much, but let's say things get out of control really quickly.
In machine learning the imperfection in problem specification are: 
the data abstraction might not be perfect (wrong population, wrong operationalization of variables, ...), 
the loss function might not cover all constraints, we don't know how to involve constraints, etc..

Digitization fuels automation. 
Imperfect goal specification conflicts with automation.
This conflict is mediated by interpretability methods.


Based on those premises and extrapolation of some already observable trends, I will share my speculations about: 

- The future of machine learning.
- How interpretability evolves.
- The bigger trend
- Research and application gaps.


Let's take a look into the crystal ball and see where the field is going!



## First: Where will ML go? 
Without machine learning, there is no interpretable machine learning, of course.
Therefore we have to see where machine learning is heading, before we talk about interpretability.
You can already do a lot with machine learning. 


Let's start with a less optimistic observation:
But from my experience it is quite hard to integrate it into existing processes and products.
Not because it is not possible, but it simply requires time. 
Even with the hype now, we see a lot of companies hiring data scientists, machine learning specialists, statisticians and so on. 
Big companies open up "AI labs", "Machine Learning Units" and hire "Data Scientists", "Machine Learning Experts", "AI engineers", ...
But from my own experience and from others:
They do not even have data in the form needed,
or there is a great misunderstanding what can be done. 
This leads to my first prediction, which is an extrapolation of trends.


![](images/enrollment.png)

**Machine learning will grow up slowly but steadily**. 

This is a trend observation.
Meaning that it transitions from academia into business processes, products and real world applications.
I believe there needs to be a lot of eductions how problems have to be formulated to have it as machine learning problem. 
And so, many highly paid data scientists do some Excel shit, or classical business intelligence with reporting and SQL queries. 
But a few are already starting to do the sexy stuff, with the big Internet companies in the lead. 
We have yet to find good ways to encapsulate machine learning, educated, built easier to use tools and so on. 
I believe machine learning will become a lot easier, we already see that it becomes automated and available through cloud services ("Machine Learning as a service" - just to sprinkle some buzz words.). 
But once machine learning is matured - and I believe we already see the first steps - my next prediction is: 

![](images/strong.png)

**Machine learning will fuel many applications.**

From the principle "Whatever can be automated will be automated", I conclude that, whenever possible, 
tasks will be reformulated as prediction problems and solved with machine learning. 
Machine Learning is a form of automation or at least can be part of automation.
We see more regulation coming, for example the GDPR, which will make it more demanding to use machine learning. 
Many easy to do tasks for humans will be replaced by machine learning (here just a few examples):
- Automation of sorting/deciding on/filling out documents. 
This will hit the legal industry, the big consultancies. 
Applications like compliance.
- Data-driven decision like loan applications, 
- Quality checks in production: 
Image recognition based mainly. 

The breakthrough for machine learning will not only come through better computers/more data/better software:


![](images/mri.png)

**Interpretability tools catalyzes the adoption of machine learning.**

From the  premise that the goal of a machine learning model can never be satisfyingly specified, it follows that it will need interpretable machine learning to make nachine learning accepted.
In many areas and sectors, interpretability will be the key for adoptions of machine learning. 
Many people I spoke to are not using machine learning because they cannot explain the model to others. 
This is more annecdotal evidence.
But I believe that interpretability will solve this issue and make machine learning attractive for organisations and people that demand some transparency.
Many industries require interpretability, either because of legal requirements, risk aversity or wish for understanding of underlying insights into the problem.
Using machine learning requires some trust, which interpretability can generate. 
Machine learning automizes the modeling process and by this moves the human a bit further away from the data and the problem: 
This increases the risk that problems with the experimental design, choice of training distribution, sampling process, data coding, feature engineering or so on happened. 
With interpretability tools these problems can be detected more easily.



## How interpretability will evolve

Now let's take a look where machine learning interpretability might lead us.
Which techniques will be the important ones?
Where is the field heading?
Whats the end game here?


![](images/detective.png)

**The focus will be on model-agnostic interpretability tools.**

It is way easier to automate interpretability, when it is decoupled from the model fitting. 
The benefits of model-agnostic:
Decoupling of predicting and interpreting.
Can easily switch out the underlying machine learning model. 
Can easily switch out the explanation method.
Because of this, model-agnostic methods will scale much better. 
They do not lock you in with an not-so-good performing model.
That's why I believe in that in the long run model-agnostic methods will be more dominantly used.
Intrinsically interpretable methods will have a subordinate role, mostly as surrogate model, but not modelling the data itself. 



![](images/wise.png)

**Machine learning will be automatic and so will be interpretability**

An already visible trend is to completely automate the model fitting: 
That includes the automatic comparison of different model types including hyperparameter tuning,  also automated feature engineering and selection and ensembling or stacking the models. 
The result is the best possible model. 
When we use model-agnostic interpretation methods, then we can automatically apply them on whatever comes out of the automated machine learning process.
In a way, we can also automate this second step: Computer feature importance, partial dependence plots, fit a surrogate model and so on. 
Nobody stops you from computing all this stuff automatically (besides computational resources).
The humans will be needed for the actual interpretation.
Imagine: You only upload a dataset, specify the prediction goal and, with the click of a button - the best prediction model is fitted and the program spits out all the interpretations of the model. 
Solutions for this already exist and I argue that this is the end game for machine learning. 
When these systems are mature enough, for many applications it will be enough to do this. 
The same as now everyone can build websites without knowing HTML, CSS and Javascript:
You don't have to know HTML to build a website, but there are still web developers.
While also letting a human look into the loop (through interpretability tools) and see if any goal missspecification happened.
But I think we can also take this a step further:

**Don't analyse data, analyse black box models.**

Unpopular opinion: The raw data itself is always useless. 
I don't care about the data. 
I care about the distilled knowledge about the data. 
Fitting a machine learning to predict the data is the ultimate distillation for my task (assuming the task is captured in the loss function).
I want to be able to throw away the data and work only on the model. 
Because I can ask the model as many things as I want, the model automatically detects if and how features are relevant for the prediction (many models have built-in feature selection), the model (when fitted flexibly) can detect automatically how the relationships are represented best.
The data are a burden. 
I want a sound model that approximates reality. 
The data are just the means to feed stuff into a computer. 

To answer questions like: 
- Was website variant A or B better?
- What are the most relevant factors for the prognosis of this cancer?
- How should this item be categorized?

Simply hypothesis tests like student t-test, analysis of variance or the correlation (the standardised linear regression coefficient is the same as Pearson's correlation coefficient) are already based on models of the data (because they assume distributions).
So what I am telling here is actually not new at all. 
What is new?
That we don't make any assumptions about the distributions. 
Assumptions suck!
They are usually wrong, difficult to check and hard to automate. 
The machine learning approach is more sexy: We don't do any assumptions, we just let it approximate reality as closely as possible (as done in a CV loop) and BOOM! we have our model of reality.
I argue that we should develop all the tools we have in statistics for answering questions (hypothesis tests, correlation measures, interaction measures, visualization tools, confidence intervals, p-values, prediction intervals, probability distributions) and re-coin the for black box models.
In a way, that is already happening: 

- Let's take a classic linear model: The standardized regression coefficients is a feature importance measures. 
With the permutation feature importance measure from CHAPTER FEATURE IMP, we have a tool that can do it for any model. 
- In a linear model the coefficients tell us the influence of a feature for the predicted outcome. 
The generalized version of this is the partial dependence plot (CITE)
- Testing wheter A or B is better: 
We can also use partial dependence for this, by forcing data points to be A or B. 
What we don't have yet is a statistical significance  measure for arbirtrary model (to my best knowledge).
- ...


![](images/scientist.png)

**No more need for data scientists**

A big part of classical data analysis is modeling some target outcome given income features and measure if some feature has a significant impact on the outcome. 
An example are clinical studies where the outcome is patient survival and one of the model features is the treatment the patient received (either placebo or the treatment to be tested).
Classically a scientist would choose an appropriate model class, depending on the distribution of the outcome, create new features, fit the model, measure model fit (usually on the training data), analyse the residuals to evaluate the adequacy of the model. 
Because the current flaw in statistics is that we have to put in strong assumptions to fit a linear model. 
Those assumptions are often unrealistic. 
And the current habit is to not test the models on an unseen dataset. 
So usually not optimized for performance. 
Then there is the machine learning approach, where you simply throw a bunch of models on the problem, cross-validate the shit out of it and choose the best model, without a look into the model.
From the premise of automation, it follows that the "getting insights about the real world" will also be automated as much as possible, which means the machine learning approach, which is more automated, is preferable. 
But the completely automated machine learning approach would not satisfy the goal to get an answer for the question "Is the treatment significantly better than the placebo?". 
In the future we will see approaches, that can combine this completely automated model fitting, but also answering these questions, if a feature significantly affects the outcome. 
We will have confidence intervals for predictions and effect strengths of features and p-values along with it. 
Confidence intervals maybe with bootstrap samples, where we refit the model. 
A lot of research has to go into this. 
Computationally expensiv, but in a few years no problem at all. 

By having a model of the outcome, we also have: 

- Feature importances: Meaning we can ignore features that are not important for the outcome
- Features are reduced to their impact on the outcome of interest
- We can measure interaction between features


Taking this whole concept of automating modelling and explanation creation a step further: The automated data scientist.
The goal of the data scientists is - of course - to automate themselves out of the job!

Taking this further: 
Why not, based on a model of the world, suggest new hypothesis to test?
Think biological experiments.

Imagine robots that hypothesise, plan, conduct and analyse experiments. 
They can exlain why they suggested the next experiment, because they have an interpretable machine learning model that predicts possible experiments outcomes.

Imagine



**Robots and programs that explain themselves**

Easier interfaces to machines and programs that make heavy use of machine learning. 
The explanations will be default.
A self-driving car that reports why it stopped abruptly. 
A credit default program that explains to a worker at the bank why a credit application was rejected ("Too many credit cards and working in unstable industry").
A robot arm that explains why it moved the cassis from the conveyor belt into the trash bin ("It detected a craze at the bottom.").


**Interpretability will boost machine intelligence research**

Ok, this is a bold claim. 
But I could imagine, that by doing more research on how programs and machines explain themselves, we increase our understanding about intelligence and will be better at making intelligent machines.


## What we need to work on: Directions for future research

**The research field about interpretability has to mature**

- Definition of interpretability needed. Ideally, we could simply measure interpretability in the same way we can measure the predictive performance. Unclear if at all possible. 
- Benchmarking of methods: If we could quantify interpretability, we could benchmark different methods against each other and would have means to make and test improvements. Something like ImageNet for computer vision. Or kaggle competitions where interpretability is incorporated.
- More work on local models (also for clusters/subgroups)
- Methods for unsupervised machine learning. In theory all the existing methods should work though, because the structure is - after the model is trained - the same: predict of a column.  Make interpretability work on 
  - survival tasks
  - time series
  - text 
  - images 
  -reinforcement learning
  - ...
- Needs a lot of testing or expertise what kind of explanations are most useful for which people and in which scenarios
- The methods in this book are very general purpose, for many applications you might want to adapt some techniques or invent something new!
- What to do with features that are not interpretable?
- Incorporation of probability distribution of the features into the methods or at least think more about this. 
- Methods that correct biases in the models

**Tools for drawing conclusions from black box models**

- Bootstrapping refi
- Incorporation of model error into the analysis of model output (e.g. pdp ignores error). 
This is important to tie the analysis of the black box model back to the real world.
Because if there is no link to the error, then the insights are about the model. 
If model is perfect image of real world, then the insights are also about the real world. 
But if not perfect, we want to quantify our insecurity about insights by looking at the model error (ideally locally, i.e. depending on where in the feature space we look).
Things like confidence intervals for partial dependence plots (based on model error) or probabilities for the Shapley value that they are different from zero (based on model error).
- Statistical theory for black box models (maybe something out there I do not know of)
- Automated finding of sub-groups
- More causal models as underlying models, because then the interpretations are also causal about the real world. [^causal]
  
  
**Breathing live into it**

- Better interfaces to the programs and robots
- Integration of the machine learning workflow into companies
- 


[^causal]: Hernán, M. A., Hsu, J., & Healy, B. (2018). Data science is science’s second chance to get causal inference right. A classification of data science tasks, 1–14.


