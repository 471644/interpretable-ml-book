# Counterfactual explanations

A counterfactual states what would contrary to the fact we observed.
Counterfactual explanations can be used to explain single predictions by asking:
What values in the features would have to change to change the prediction.





Usually used in classification: How to flip the classification.
Easily transferable to regression by introducing a threshold, which when breached, counts like flipping the prediction.


### What are counterfactual explanations?

Unconditional counterfactual explanation.

Asking: What is the smallest change to the input features that would produce the desired outcome.
Whereby "desired" means the output that we are analysing, which can also have a negative meaning in the world, like the label "sick". 


Counterfactual explanations is a model-agnostic method, since it works only with the model input and output.

Arguably it could also be in the [model-agnostic chapter](#agnostic), since the output is both a summary for a feature ("change feature A and B to get prediction change") and a new instance ("starting from instance X, change A and B to get instance X'").




Example:
Person is a applying for a loan and gets rejected by the (machine learning powered) bank's algorithm.
There are two possible outcomes: "approved" and "rejected".
What is the smallest change to the features that were factored in that would change the prediction from rejected to approved?
Fortunately, the application gives feedback.
If the person would earn 10.000 Euros more, he would get the loan.


Another example with regression:
A website automatically estimates the rent of a flat.
You want to rent out your flat, but are not sure what to charge for it, so you decide to use the service.
After entering all the details into the form about the size, the location,  if pets are allowed and so on.
The website's algorithm tells you that you can charge 900 Euro.
You are a bit setback because you expected 1000 Euro or maybe more, and don't understand the decision.
You kind of trust the website though, so you think it must be right. 
Since nothing prohibits you from trying out different setting to see how the outcome changes. 
You find out that the appartment would sell for over 1000 Euros, if it were 15 square meters larger.
Interesting, but useless knowledge, because you can't increase the size of the appartment.
Finally, by tweaking only the things under your control (age of built-in kitchen, pets allowed y/n, type of floor, etc.), 
you find out that if you allow pets and improve the isolation by replacing the windows with ones that have better isolation, you can charge 1000 Euros.


Counterfactuals create [human-friendly explanations](#good-explanation), because they are contrastive to the current situation and they are selective, because they focus on a small set of feature changes.


Rashomon effect.
There is usually more than one counterfactual explanation.

As Wachter et. al argue [^wachter] this is especially useful for GDPR right to explanation.


Desired: The changes should be minimal and the prediction should be flipped (in classification setting) or cross a user-defined threshold (in a regression setting).

One thing is the concept, the other thing is how to generate the explanations.

### How do I generate them?


Naive method: Randomly change values of instance x and see if the outcome changes in the desired way:

Other approach: Define a loss function that encourages sparseness and optimize the loss.


Other approach: Search (Growing spheres)


Special scenario: We don't know the distribution of the data.


$$argmin_{x'}max_{\lambda}\lambda*(f_w(x')-y')^2+d(x_i,x')$$

where d is a distance function, like the Euclidean distance, which measure how different the counterfactual and the original data point are. 
The distance function d is the L1-norm of the manhatten distance, weighted by the inverse median absolute deviation. 

$$MAD_j = median_{i\in{}\{1,\ldots,n\}}(|x_{i,j}-median_{i\in{}\{1,\ldots,n\}}(x_{l,j})|)$$

And as distance function:

$$d(x,x')=\sum_{j=1}^P\frac{|x_{k} - x'_k|}{MAD_k}$$

The Manhatten norm has the advantage of introducing sparsity to the explanations that are generated.
And it is more robust to outliers.


Use some optimizer, like ADAM to find a local minimum.
For the optimization, the loss has to be initialized by some x'. 
This is done repeatedly with different x', which might also lead to different results.

### Examples


### Advantages

- No need to access the data or the code
- Can be done via API
- Authors argue it is useful for balance betweeen transparancey and the privacy and trade secrets of a company.
- Can also be used without machine learning.
- Rashomon
- Easy to implement
- Sparsity of explanation
- Especially useful in critical explanations: Why was something not granted, or someone rejected?

### Disadvantages

- Rashomon
- No code available

<!-- 
## Counterfactuals and adverserial examples

What are counterfactuals?
What are adverserial examples?

Counterfactuals and adverserial examples in real life. 
Maybe include images.


Other ways ml algorithms can be attacked (keep it short)
- poison the training data set

### Theory

- Sandra Waechter Paper?
- Growing Spheres Paper?
- Something from 


### Examples

If available: Add code example for flipping bike classification 


### Advantages
- You don't have to know anything about the inner workings for creating counterfactuals. 

### Disadvantages
- Don't reveal much about inner workings.




Some links http://www.cleverhans.io/security/privacy/ml/2016/12/16/breaking-things-is-easy.html

-->



[^wachter]: Wachter, S., Mittelstadt, B., & Russell, C. (2017). Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR, (1), 1â€“47. https://doi.org/10.2139/ssrn.3063289