
{$$}\arg\min_{x'}\max_{\lambda}L(x,x',y',\lambda){/$$}

The function d for measuring the distance between instance x and counterfactual x' is the Manhattan distance weighted feature-wise with the inverse median absolute deviation (MAD).

{$$}d(x,x')=\sum_{j=1}^p\frac{|x_j-x'_j|}{MAD_j}{/$$}

The total distance is the sum of all p feature-wise distances, that is, the absolute differences of feature values between instance x and counterfactual x'.
The feature-wise distances are scaled by the inverse of the median absolute deviation of feature j over the dataset defined as:


{$$}MAD_j=median_{i\in{}\{1,\ldots,n\}}(|x_{i,j}-median_{l\in{}\{1,\ldots,n\}}(x_{l,j})|){/$$}

The median of a vector is the value at which half of the vector values are greater and the other half smaller.
The MAD is the equivalent of the variance of a feature, but instead of using the mean as the center and summing over the square distances, we use the median as the center and sum over the absolute distances.
The proposed distance function has the advantage over the Euclidean distance that it introduces sparsity.
This means that two points are closer to each other when less features are different.
And it is more robust to outliers.
Scaling with the MAD is necessary to bring all the features to the same scale - it shouldn't matter whether you measure the size of an apartment in square meters or square feet.

The recipe for producing the counterfactuals is simple:

1. Select an instance x to be explained, the desired outcome y', a tolerance {$$}\epsilon{/$$} and a (low) initial value for {$$}\lambda{/$$}.
1. Sample a random instance as initial counterfactual.
1. Optimize the loss with the initially sampled counterfactual as starting point.
1. While {$$}|\hat{f}(x')-y'|>\epsilon{/$$}:
  1. Increase {$$}\lambda{/$$}.
  1. Optimize the loss with the current counterfactual as starting point.
  1. Return the counterfactual that minimizes the loss.
1. Repeat steps 2-3 and return the list of counterfactuals or the one that minimizes the loss.

### Examples

Both examples are from the work of Wachter et. al (2017) [^wachter].

In the first example, the authors train a three-layer fully-connected neural network to predict a student's average grade of the first year at law school, based on grade point average (GPA) prior to law school, race and law school entrance exam scores.
The goal is to find counterfactual explanations for each student that answer the following question:
How would the input features need to be changed, to get a predicted score of 0?
Since the scores have been normalised before, a student with a score of 0 is as good as the average of the students.
A negative score means a below-average result, a positive score an above-average result.

The following table shows the learned counterfactuals:

| Score | GPA | LSAT | Race | GPA x' | LSAT x' |  Race x'|
| ------|--------------| --------------| --------------| -------| --------| ------- |
| 0.17 | 3.1 | 39.0 | 0 | 3.1 | 34.0 | 0|
| 0.54 | 3.7 | 48.0 | 0 | 3.7 | 32.4 | 0|
| -0.77| 3.3 | 28.0 | 1 | 3.3 | 33.5 | 0|
| -0.83| 2.4 | 28.5 | 1 | 2.4 | 35.8 | 0|
| -0.57| 2.7 | 18.3 | 0 | 2.7 | 34.9 | 0|

The first column contains the predicted score, the next 3 columns the original feature values and the last 3 columns the counterfactual feature values that result in a score close to 0.
The first two rows are students with above-average predictions, the other three rows below-average.
The counterfactuals for the first two rows describe how the student features would have to change to decrease the predicted score and for the other three cases how they would have to change to increase the score to the average.
The counterfactuals for increasing the score always change the race from black (coded with 1) to white (coded with 0) which shows a racial bias of the model.
The GPA is not changed in the counterfactuals, but LSAT is.

The second example shows counterfactual explanations for predicted risk of diabetes.
A three-layer fully-connected neural network is trained to predict the risk for diabetes depending on age, BMI, number of pregnancies and so on for women with Pima heritage.
The counterfactuals answer the question: Which feature values must be changed to increase or decrease the risk score of diabetes to 0.5?.
The following counterfactuals were found:

- Person 1: If your 2-hour serum insulin level was 154.3, you would have a score of 0.51
- Person 2: If your 2-hour serum insulin level was 169.5, you would have a score of 0.51
- Person 3: If your Plasma glucose concentration was 158.3 and your 2-hour serum insulin level was 160.5, you would have a score of 0.51


### Advantages

**The interpretation of counterfactual explanations is very clear**.
If the feature values of an instance are changed according to the counterfactual, the prediction changes to the predefined prediction.
There are no additional assumptions and no magic in the background. 
This also means it is not as dangerous as methods like [LIME](#lime), where it is unclear how far we can extrapolate the local model for the interpretation.

The counterfactual method creates a new instance, but we can also summarize a counterfactual by reporting which feature values have changed.
This gives us **two options for reporting our results**.
You can either report the counterfactual instance or highlight which features have been changed between the instance of interest and the counterfactual instance.

The **counterfactual method doesn't require access to the data or the model**.
It only requires access to the model's prediction function, which would also work via a web API, for example.
This is attractive for companies which are audited by third parties or which are offering explanations for users without disclosing the model or data. 
A company has an interest in protecting model and data because of trade secrets or data protection reasons.
Counterfactual explanations offer a balance between explaining model predictions and protecting the interests of the model owner.

The method **works also with systems that don't use machine learning**.
We can create counterfactuals for any system that receives inputs and returns outputs.
The system that predicts apartment rents could also consist of handwritten rules, and counterfactual explanations would still work.

**The counterfactual explanation method is relatively easy to implement**, since it's essentially a loss function that can be optimized with standard optimizer libraries. 
Some additional details must be taken into account, such as limiting feature values to meaningful ranges (e.g. only positive apartment sizes).

### Disadvantages

**For each instance you will usually find multiple counterfactual explanations (Rashomon effect)**.
This is inconvenient - most people prefer simple explanations to the complexity of the real world.
It is also a practical challenge.
Let's say we generated 23 counterfactual explanations for one instance. 
Are we reporting them all?
Only the best?
What if they are all relatively "good", but very different?
These questions must be answered anew for each project.
It can also be advantageous to have multiple counterfactual explanations, because then humans can select the ones that correspond to their previous knowledge.

There is **no guarantee that for a given tolerance {$$}\epsilon{/$$} a counterfactual instance is found**.
That's not necessarily the fault of the method, but rather depends on the data.

The proposed method **doesn't handle categorical features** with many different levels well. 
The authors of the method suggested running the method separately for each combination of feature values of the categorical features, but this will lead to a combinatorial explosion if you have multiple categorical features with many values.
For example, 6 categorical features with 10 unique levels would mean 1 million runs.
A solution for only categorical features was proposed by Martens et. al (2014)[^martens].
A good solution would be to use an optimizer that solves problems with a mix of continuous and discrete inputs.

The counterfactuals method **lacks a general software implementation**.
And a method is only useful if it is implemented. 
Fortunately, it should be easy to implement and hopefully I can remove this statement here soon.

### Software and Alternatives {#example-software}

- Unfortunately there is currently no software available  for counterfactual explanations.
- A very similar approach was proposed by Martens et. al (2014) for explaining document classifications.
In their work, they focus on explaining why a document was or was not classified as a particular class.
The difference to the method presented in this chapter is that Martens et. al focus on text classifiers, which have word occurrences as inputs.
- An alternative way to search counterfactuals is the Growing Spheres algorithm by Laugel et. al (2017)[^spheres].
The method first draws a sphere around the point of interest, samples points within that sphere, checks whether one of the sampled points yields the desired prediction, contracts or expands the sphere accordingly until a (sparse) counterfactual is found and finally returned.
They don't use the word counterfactual in their paper, but the method is quite similar.
They also define a loss function that favors counterfactuals with as few changes in the feature values as possible.
Instead of directly optimizing the function, they suggest the above-mentioned search with spheres.
![An illustration of growing spheres and selecting sparse counterfactuals by Laugel et. al (2017).](images/spheres.png)
- Anchors by Ribeiro et. al 2018 [^anchors] are the opposite of counterfactuals.
Anchors answer the question: 
Which features are sufficient to anchor a prediction, i.e. changing the other features can't change the prediction?
Once we have found features that serve as anchors for a prediction, we will no longer find counterfactual instances by changing the features not used in the anchor.
![Examples for anchors by Ribeiro et. al (2018).](images/anchors.png)

[^martens]: Martens, D., & Provost, F. (2014). Explaining Data-Driven Document Classifications. MIS Quarterly, 38(1), 73–99. http://doi.org/10.25300/MISQ/2014/38.1.04

[^anchors]: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin (2018). "Anchors: High-precision model-agnostic explanations." AAAI Conference on Artificial Intelligence.


[^spheres]: Laugel, T., Lesot, M.-J., Marsala, C., Renard, X., & Detyniecki, M. (2017). Inverse Classification for Comparison-based Interpretability in Machine Learning. Retrieved from http://arxiv.org/abs/1712.08443

[^wachter]: Wachter, S., Mittelstadt, B., & Russell, C. (2017). Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR, (1), 1–47. https://doi.org/10.2139/ssrn.3063289
