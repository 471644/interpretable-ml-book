```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)

opts_chunk$set(
  fig.width = 10)
```



## Adversarial examples

<!-- TODO: -->
<!-- - Wait for permission to use images (szegedy  left)-->
<!-- - Iterate over text (heavy edit of sentences) in editor -->
<!-- - extend the theory for all papers a bit -->
<!-- - Print, read and edit -->
<!-- - Double deepl translation -->
<!-- - Spell check -->
<!-- - Last read in HTML form -->

Adversarial examples are instances with small perturbations designed to fool a machine learning model into giving a wrong prediction.
I recommend reading the chapter about [counterfactual explanations](#counterfactual) first, since the concepts are very similar.
Adversarial examples are counterfactual examples with a focus on fooling instead of interpreting a system.

*Keywords: adversarial examples, adversarial machine learning, counterfactuals, evasion attacks, machine learning security*

Why do we care about adversarial examples beyond explaining single predictions?
Are they not only some curious by-products of machine learning models, without practical relevance?
The answer is a clear "No!".
Adversarial examples make machine learning systems vulnerable to attacks, like in the following scenarios.
A self-driving car crashes into another car because it doesn't recognize the stop sign and assumes the right over left rule holds.
Someone had sticked an image over the sign, which looks like a stop sign with a little dirt to humans, but was designed to look like a no parking sign to machine learning models.
A spam detector missclassifies spam as valid mail.
The spam mail was designed to resemble a normal mail, but with the intend to scam the recipient.
A machine-learning powered scanner scans packages at the airport for weapons.
A knife is designed to evade the detection by making the system think it is an umbrella.

Let's have a look at some ways of creating adversarial examples.

### Examples

There are many ways of creating these adversarial examples.
Most approaches suggest to minimize the distance of adversarial example and instance to be manipulated, while moving the prediction to the desired (wrong) outcome.
Some methods require access to the gradients of the model (which only works for gradient based models) and some methods work with input features and predictions (which makes those methods model-agnostic).
The examples in this section focus on image classifiers using deep neural networks, because there is a lot of research going on and they are easy to fool.
Adversarial examples for images are input samples that were intentionally perturbed (some pixels) aimed to fool the model during test time.
Through the examples you will see that deep convolutional neural networks for object recognition are vulnerable to adversarial examples, that look harmles to humans.
An optical illusion for machines.
A possible source for deep neural networks being vulnerable to adversarial examples is that data points that are close in input space can be very different in the deep embedded space (think of the embedding vectors of an image or text).

<!-- <!-- WAITING FOR PERMISSION TO USE IMAGE -->
<!-- #### Something's Wrong With My Dog -->

<!-- Szegedy et. al (2013)[^szegedy] in their work "Intriguing Properties of Neural Networks" used box-constrained L-BFGS optimization to find adversarial examples for deep neural networks. -->

<!-- ```{r adversarial-ostrich, fig.cap = "Adversarial examples for AlexNet by Szegedy et. al (2013). All images shown in the left column are correctly classified. The middle column shows the (magnified) perturbation that is added to the images to produce the images in the right column, which are all (wrongly) categorized as 'Ostrich'."} -->
<!-- knitr::include_graphics("images/adversarial-ostrich.png") -->
<!-- ``` -->

<!-- These adversarial examples were produced by minimizing the following function respective to r: -->

<!-- $$loss(\hat{f}(x+r),l)+c\cdot|r|$$ -->

<!-- Where x is an image (represented as vector of pixels), r are the changes to the pixels to create an adversarial image (x+r results in a new image), l is the desired outcome class and parameter c for balancing distance of images and distance between predictions. -->
<!-- The first term is the the distance between the predicted outcome of the adversarial example and the desired class l, the second term measures the distance between the adversarial example and the original image. -->
<!-- This formulation is almost identical to the loss function optimized by [counterfactuals](#counterfactual):  -->
<!-- There are additional constraints on r, so that the pixel values stay between 0 and 1.  -->
<!-- The propose to solve this with a box-constrained L-BFGS, an optimizer algorithm that works with the gradients. -->

**Perturbed Panda**

One method for creating adversarial images is called "gradient sign method" by Goodfellow et. al (2014)[^goodfellow].
The gradient sign method uses the the gradient of the underlying model to find adversarial examples.
The original image x is changed pixel-wise by adding or subtracting to each pixel a small $\epsilon$. 
Whether we add or subtract $\epsilon$ depends if the sign of the gradient for a pixel is positive or negative.


```{r adversarial-panda, fig.cap = "Making a Panda look like a gibbon for a neural network by Goodfellow et. al (2014). By changing each of the original panda pixels (left image) in the direction of the gradient with respect to the input pixels towards the targeted outcome 'Gibbon' (middle image, which would be classified as nematode) the authors create an adversarial example that is classified as gibbon but looks like a panda to humans."}
knitr::include_graphics("images/adversarial-panda.png")
```



Adversarial examples are created with the following formula:

$$x'=x+\epsilon\cdot{}sign(\bigtriangledown_x{}J(\theta,x,y))$$


where $\bigtriangledown_x{}J$ is the gradient of the loss function of the model with respect to the original input pixels x, y the true label for x and $\theta$ the model parameters.
The gradient of the loss J is calculated with respect to the input pixels towards the desired adversarial outcome y.
From the gradient vector (which is as long as the vector of input pixels) we only need the sign:
This sign of the gradient is positive if an increase of the pixel intensity increases the score for the adversarial class y and negative if decreasing the pixel intensity increases the score.
I expected that these adversarial examples were rather specific for a single image classifier, given a particular neural network architecture.
But it turns out that you can take this same adversarial examples to a different architecture trained on the same task and it is likely to be an adversarial example for this network as well.
The vulnerability comes from the linear nature of neural networks.
Especially neural network architectures that behave more linearly like LSTMs, maxout networks and networks with ReLU or other linear machine learning algorithms like logistic regression are vulnerable to the gradient sign method.
Why does it work?
Extrapolation is the vulnerability. 
Linearity is a problem when we extrapolate, meaning we go into areas with the input feature values that are outside of the data distribution. 
Through the linearity the model still changes the prediction, because linear effects have no saturation effect and so there is an opportunity to fool the model.
The models are overly confident at points that never appeared at the dataset.

Goodfellow et. al (2014) propose to defend against adversarial examples by adding adversarial examples to the training data to get learn robust prediction models.


**A Jellyfish ... No, wait. A BATHTUB! **

The approach presented by Goodfellow et. al (2014) requires changing a lot of pixels, even if only by a bit.
But what if you were only allowed to change a single pixel, would you be able to fool a machine learning model?
Su et. al (2017) [^1pixel] showed that it is indeed possible to fool image classifiers by changing a single pixel.

```{r adversarial-1pixel, fig.cap = "A neural network trained to classify the ImageNet dataset is fooled by adversarial examples. Instead of the original class (in black letters), the network predicts the wrong class (in blue letters) by changing a single pixel (marked with red circles). Work by Su et. al (2017)."}
knitr::include_graphics("images/adversarial-1pixel.png")
```

Similar to counterfactuals, they search for a modified example x' which is close to the original image, but changes the prediction to an adversarial example.
Closeness to the original image is defined differently though: 
Only one pixel is allowed to change.
For searching which pixel should be changed and by how much each of its colors should be changed, they use differential evolution.
In evolutionary approaches, the real evolutionary process is mimicked:
A population of individuals called candidates solutions recreates generation by generation.

Differential evolution does not have many assumptions. 
It doesn't need access to the models gradient.
A candidate solution consists of five elements: the x- and y-coordinates and the red, green and blue (RGB) values.
So each candidate is basically a pixel modification.
It starts out with 400 candidate solution and we create children at each generation, by using the following formula:

$$x_{i}(g+1)=x_{r1}(g)+F(x_{r2}(g)+x_{r3}(g))$$
where each xi is one element of a candidate solution (either x-coordinate, y-coordinate, red, green or blue), g is the current generation, F is a scaling parameter (set to 0.5) and r1, r2 and r3 are different random numbers.
So each new child candidate solution is again a pixel with the five attributes for location and color and each of those attributes is a mixture of three random parent pixels.

Creating childrens is stopped when one of the candidate solutions is already an adversarial example, meaning it is classified as the wrong class, or when the number of maximum iterations is reached as set by the user.


**Everything is a toaster!**

One of my favorite examples brings adversarial examples into physical reality:
Brown et. al (2017)[^toaster] designed a sticker that you can print out and stick to objects to make them look like toasters to a machine. 
Briliant work!

```{r adversarial-toaster, fig.cap = "A sticker, when added to a scene, makes a VGG16 classifier trained on ImageNet categorize an image of the scene as toaster. Work by Brown et. al (2017)."}
knitr::include_graphics("images/adversarial-toaster.png")
```

This method differs from previously presented methods for adversarial examples, because the constraint that the adversarial image has to be very close to the original image is lifted.
Instead, the method creates replaces a part of the image completely with a patch, which is allowed to take on any shape. 
This patch is optimized over various images, with different positions of the path on the images, sometimes shifted, sometimes bigger or smaller and rotated to make it work for many situations.
In the end, this optimized image can be printed out and used to fool image classifiers in the wild.


**Never bring your 3D-printed fake-rifle-turtle to a gun fight.**

The next method is literally adding another dimension to the to the toaster:
Athalye et. al (2018)[^turtle] created a 3D-printed turtle that looks like a riffle to a deep neural network from almost all possible angles.
Yes, you read that correctly. 
A physical object that looks like a turtle to humans looks like a damn riffle to the computer!
The missclassification works also when the turtle-riffle is rotated or put into a different location of the image.

```{r adversarial-turtle, fig.cap = "A 3D-printed turtle that is recognized as a riffle by TensorFlow’s standard pretrained InceptionV3 classifier. Work by Athalye et. al (2018)"}
knitr::include_graphics("images/adversarial-turtle.png")
```

The authors found a way to create an adversarial example in 3D for a 2D classifier, which is adversarial over most possible transformations, like all the ways of turning the turtle, zooming in and so on.


**The blindfolded thief and deceiver**

Imagine the following scenario:
I give you access to my great machine learning model via Web-API.
From the convenience of your couch, you can send data and my service sends you the prediction for it.
Most adversarial attacks are not designed to work in that scenario, since they require access to the gradient of the underlying deep neural network to find adversarial examples.
But it is possible to even create adversarial examples without internal model information and without access to the training data as shown by Papernot and colleagues (2017)[^papernot].
This kind of zero-knowledge attack is called a black box attack.

How it works:

1. Start with a few data points, which don't have to be training samples, but should be some realistic data for the classifier (if the to be attacked classifier is a digit classifier, start with images of digits.)
1. Get predictions from the black box.
1. Train a surrogate model, which can be for example a neural network. 
1. Create synthetic inputs for the model using a heuristic that tells us in which feature direction the model's output varies the most around an initial set of data points.
1. Repeat steps 2 to 4 for a pre-defined number of epochs.
1. Create adversarial examples for the surrogate model using the fast gradient method (or something similar).
1. Attack original model with adversarial examples.

The goal of the surrogate model is to approximate the decision boundaries of the black box model, but not necessarily achieving the same accuracy.

The authors testes this approach by attacking a mult-class deep neural network classifier trained on different cloud machine learning services.
These work by uploading images and labels.
The software automatically builds a prediction model - sometimes with unknown algorithm - and deploys it.
This prediction model then can be tested by sending images, but the model itself can't be acessed or downloaded.
On different of those services, the authors managed to find adversarial examples, with up to 84.24% missclassified by one one of the services. 

The method even works if the black box model to be fooled is some different machine learning model and not a neural network. 
Even a decision tree, which is not even differentiable.
And for the surrogate model trained on the synthetic data, they used a logistic regression model. 
The stuff still worked.


### The Cybersecurity Perspective

<!-- defense is difficiult yet important -->
Machine learning deals with known unkowns: classifying data unknown data points from know distribution.
Defending against machine learning model attacks deals with unknown unknowns: classifying never seen data points that are weird.
Since machine learning is getting integrated into more and more system, like in autonomuous vehicles or medical devices, they also become entry points for attacks.
Even if the predictions of a machine learning model are 100% correct on a dataset, there will still be adversarial examples to fool it.
Naturally, defending machine learning models against cyber attacks becomes part the field of cyber security.

<!-- cybersecurity -->
Biggio et. al (2017)[^adversarial] give a nice review of ten years of research on adversarial machine learning, on which this section is based.
Cyber security is an arms-race in which attackers and defenders repeatedly try to outsmart each other. 


**In cyber security, there are three golden rules for defending against attacks: 1) know your adversary 2) be proactive and 3) protect yourself.**

Knowing the adversary depends on the application of the model.
For mail providers people who try to scam other people for their money are adversary agents.
The mail providers want to protect their customers, so that they keep using their mail program, the attackers want to trick people into giving them money.
Knowing the adversary means that we also know what they are after, which makes it easier to defend against them.
Let's say you wouldn't know that these spammers exist and the only misuse of the mails is sending pirated material, than the defense would be very different (e.g. focusing on scanning the attachments for copyrighted material instead of analysing the text).

Being proactive means to actively test and spot weaknesses of the system.
You are proactive when you actively try to fool the model with adversarial examples and then defend against them (more about this later).
Also, using interpretability methods to understand which features are important and how feature influence the prediction is a proactive step to understanding weaknesses of the machine learning model
Looking at feature importance values gives a quick overview which features influence the model prediction the most. 
ICE curves simulate some uni-variate attacks, by trying out variations of single features and measuring the output.
Analysing miss-classifications with counterfactuals, LIME and Shapley value might reveal further weaknesses of the machine learning model. 
Do you, as a developer of a model, trust your model out into this dangerous world, without having ever looked into it and even tried to understand it?
See how it behave in different scenarios, get a feel for the most important input, get the explanations for a few selected predictions.
Interpretability of machine learning models has a great role to play in adversarial machine learning.
Being reactive, the opposite of proactive, means waiting until the system was attacked and only installing some defense afterwards.

How can we protect our machine learning systems?
There are many approaches to defend against adversarial attacks.
One of the most obvious, proactive approaches is to iteratively retrain the classifier with adversarial examples, also called adversarial training.
Other approaches are based on game theory, like learning invariant transformations of the features or robust optimization (regularisation).
Another suggested method is to use multiple classifiers instead of only one and let them vote (ensemble), but this has no guarantee to work, since they might all suffer from similar adversarial examples.
Another approach that also doesn't work well is gradient masking, which constructs a model without useful gradients by using a nearest neighbor classifier instead of the original model.


We can distinguis attacks by how much an attacker knows about the system.
The attacker might have perfect knowledge (white box attack), meaning she knows everything about the ml model like the weights of a linear model;
The attacker might have partial knowledge (gray box attack), meaning she only knows for example the feature representation and the type of model that was used, but has no access to the training data or the parameters;
The attacker might have zero knowledge (black box attack), meaning she can only query the model in a black box manner but doesn't have access to the data or information about the model.
Depending on the level of information, the attacker can employ different techniques to attack the model.
As we have seen in the examples, adversarial examples can even be created in the black box attack case, so hiding information about data and model is not enough to protect against attacks.

Given the nature of the cat-and-mouse game between attackers and defenders, we will see a lot of development and innovation in this area. 
Just think about the many different types of ever evolving spam mails.
Attack methods against machine learning will be invented, defenses will be suggested to defend against the attacks.
Then new attacks will be designed that are not detected by the implemented defenses and the circle begins again.
With this chapter, I hope to sensibilize you with the problem of adversarial examples and that only by proactively studying the machine learning models we are able to discover and fix vulnerabilities.





[^szegedy]: Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., & Fergus, R. (2013). Intriguing properties of neural networks, 1–10. http://doi.org/10.1021/ct2009208

[^adversarial]: Biggio, B., & Roli, F. (2017). Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning, 32–37. Retrieved from http://arxiv.org/abs/1712.03141

[^turtle]: Athalye, A., Engstrom, L., Ilyas, A., & Kwok, K. (2017). Synthesizing Robust Adversarial Examples. Retrieved from http://arxiv.org/abs/1707.07397

[^goodfellow]: Goodfellow, I. J., Shlens, J., & Szegedy, C. (2014). Explaining and Harnessing Adversarial Examples, 1–11. http://doi.org/10.1109/CVPR.2015.7298594

[^1pixel]: Su, J., Vargas, D. V., & Kouichi, S. (2017). One pixel attack for fooling deep neural networks. Retrieved from http://arxiv.org/abs/1710.08864

[^toaster]: Brown, T. B., Mané, D., Roy, A., Abadi, M., & Gilmer, J. (2017). Adversarial Patch, (Nips). Retrieved from http://arxiv.org/abs/1712.09665

[^papernot]: Papernot, Nicolas, et al. "Practical black-box attacks against machine learning." Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM, 2017.