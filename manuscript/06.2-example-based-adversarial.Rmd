<!-- 
Counterfactuals are not only good for understanding the models, but also to fool them.
Machine learning models can be easily fooled by adversarial examples.
Counterfactuals and adverserial examples in real life. 

The broader research area is also known as adversarial machine learning.

Adversarial examples for images are input samples that were intentionally perturbed (some pixels) aimed to fool the model during test time.

There are also attacks aimed at training time, but those are different.

Machine learning is a bit of an arms-race: spammer vs. a system. 
There are three golden rules : 1) know your adversary agent 2) be proactive and 4) protect yourself.

It has been shown that especially deep convolutional neural networs for object recognition are vulnerable to adversarial examples, that look harmles to humans.


Especially because we don't understand the machine learning models and what they learned, they are vulnerable to attacks and often the weakest link in a system. 

Interpretability of the models (for the builder of the system), knowing about adversarial examples helps to protect them against these examples.
To protect a system, try to proactively find holes in it. 
Make the models interpretable and find their weaknesses.

Some example: [^szegedy]

![](images/adversarial.png)
TEXT FROM THE PAPER FOR THE IMAGE:
Figure 5: Adversarial examples generated for AlexNet [9].(Left) is a correctly predicted sample, (center) dif- ference between correct image, and image predicted incorrectly magnified by 10x (values shifted by 128 and clamped), (right) adversarial example. All images in the right column are predicted to be an “ostrich, Struthio camelus”. Average distortion based on 64 examples is 0.006508. Plase refer to http://goo.gl/huaGPb for full resolution images. The examples are strictly randomly chosen. There is not any postselection involved.


There are even attacks that flip a classification by changing a single (!) pixel of the image:

TODO: Add 1pix-attack image from the paper and Cite

And my favorite example: Stickers you can print out and stick to objects to make them look like toasters to the machines.
TODO: Add image from toaster-sticker paper


Adversarial examples not only occur in image recognition, but also in text.
One of the frist adversarial examples hit spam filtering models in the early 2000s.



TODO: WRITE ABOUT COUNTERMEASURES


TODO: WRITE ABOUT THE INTERPRETABLE vs. NON-INTERPREABLE ISSUE FOR SECURITY (i.e. parallel to the open vs. closed source software).


Other ways ml algorithms can be attacked (keep it short)
- poisoning attacks:  the training data set
- evasion attacks: during test time, covered in this chapter.
- 



Some links http://www.cleverhans.io/security/privacy/ml/2016/12/16/breaking-things-is-easy.html


[^szegedy]: Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., & Fergus, R. (2013). Intriguing properties of neural networks, 1–10. http://doi.org/10.1021/ct2009208

-->