```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)

opts_chunk$set(
  fig.width = 10)
```



## Adversarial Examples

<!-- TODO: -->
<!-- - Wait for permission to use images (szegedy  left)-->
<!-- - extend the theory for all papers a bit -->
<!-- - Print, read and edit -->
<!-- - Double deepl translation -->
<!-- - Spell check -->
<!-- - Last read in HTML form -->

An adversarial example is an instance with small feature perturbations designed to fool a machine learning model into giving a wrong prediction.
I recommend reading the chapter about [counterfactual explanations](#counterfactual) first, since the concepts are very similar.
Adversarial examples are counterfactual examples with a focus on fooling instead of interpreting a system.

*Keywords: adversarial examples, adversarial machine learning, counterfactuals, evasion attacks, machine learning security*

Why do we care about adversarial examples beyond explaining single predictions?
Aren't they only curious by-products of machine learning models without practical relevance?
The answer is a clear "No!".
Adversarial examples make machine learning models vulnerable to attacks, like in the following scenarios.
A self-driving car crashes into another car because it doesn't recognize the stop sign and assumes that the right over left rule holds.
Someone had put an image over the sign, which looks like a stop sign with a little dirt to humans, but was designed to look like a no parking sign to the cars sign recognition machine learning model.
A spam detector misclassifies spam as valid mail.
The spam mail was designed to resemble a normal mail, but with the intend to scam the recipient.
A machine-learning powered scanner scans packages at the airport for weapons.
A knife is designed to evade the detection by making the system think it is an umbrella.

Let's have a look at some ways of creating adversarial examples.

### Methods and Examples

There are many techniques for creating adversarial examples.
Most approaches suggest minimizing the distance of the adversarial example and the instance to be manipulated, while moving the prediction to the desired (wrong) outcome.
Some methods require access to the gradients of the model, which obviously only works for gradient based models like neural networks, some methods only need access to the prediction function, which makes those methods model-agnostic.
The methods in this section focus on image classifiers using deep neural networks, because there is a lot of research going on and the visualization of the adversarial examples as images is very instructive.
Adversarial examples for images are images with intentionally perturbed pixels aiming to fool the model during test time.
The examples demonstrate impressively how easily deep neural networks for object recognition can be fooled by images that look harmless to humans.
If you haven't seen those examples already, they might surprise you, because the changes in predictions are incomprehensible for a human observer.
Adversarial examples are like optical illusion for machines.

<!-- <!-- WAITING FOR PERMISSION TO USE IMAGE -->
<!-- #### Something's Wrong With My Dog -->

<!-- Szegedy et. al (2013)[^szegedy] in their work "Intriguing Properties of Neural Networks" used box-constrained L-BFGS optimization to find adversarial examples for deep neural networks. -->

<!-- ```{r adversarial-ostrich, fig.cap = "Adversarial examples for AlexNet by Szegedy et. al (2013). All images shown in the left column are correctly classified. The middle column shows the (magnified) perturbation that is added to the images to produce the images in the right column, which are all (wrongly) categorized as 'Ostrich'."} -->
<!-- knitr::include_graphics("images/adversarial-ostrich.png") -->
<!-- ``` -->

<!-- These adversarial examples were produced by minimizing the following function respective to r: -->

<!-- $$loss(\hat{f}(x+r),l)+c\cdot|r|$$ -->

<!-- Where x is an image (represented as vector of pixels), r are the changes to the pixels to create an adversarial image (x+r results in a new image), l is the desired outcome class and parameter c for balancing distance of images and distance between predictions. -->
<!-- The first term is the distance between the predicted outcome of the adversarial example and the desired class l, the second term measures the distance between the adversarial example and the original image. -->
<!-- This formulation is almost identical to the loss function optimized by [counterfactuals](#counterfactual):  -->
<!-- There are additional constraints on r, so that the pixel values stay between 0 and 1.  -->
<!-- The propose to solve this with a box-constrained L-BFGS, an optimizer algorithm that works with the gradients. -->

**Perturbed panda: Fast gradient sign method**

Goodfellow et. al (2014)[^goodfellow] invented the fast gradient sign method for creating adversarial images.
The gradient sign method uses the gradient of the underlying model to find adversarial examples.
The original image x is manipulated by adding or subtracting a small perturbation $\epsilon$ to each pixel. 
Whether we add or subtract $\epsilon$ depends whether the sign of the gradient for a pixel is positive or negative.
Adding perturbations in the direction of the gradient means changing on purpose the image in a way that the model classification will go wrong.


```{r adversarial-panda, fig.cap = "Goodfellow et. al (2014) making a Panda look like a gibbon to a neural network. By adding small perturbations (middle image) to the original panda pixels (left image) the authors create an adversarial example that is classified as gibbon (right image) but looks like a panda to humans."}
knitr::include_graphics("images/adversarial-panda.png")
```



The following formula describes the core of the fast gradient sign method:

$$x'=x+\epsilon\cdot{}sign(\bigtriangledown_x{}J(\theta,x,y))$$


where $\bigtriangledown_x{}J$ is the gradient of the loss function of the model with respect to the original input pixel vector x, y the true label vector for x and $\theta$ the model parameters.
From the gradient vector (which is as long as the vector of input pixels) we only need the sign:
The sign of the gradient is positive (+1) if an increase of the pixel intensity increases the loss (the error the model makes) and negative (-1) if decreasing the pixel intensity increases the loss.
The vulnerability arises when a neural network handles a relationship between an input pixel intensity and the class score linearly.
Especially neural network architectures that favor linearity like LSTMs, maxout networks, networks with ReLU activation units or other linear machine learning algorithms like logistic regression are vulnerable to the gradient sign method.
The attack works through extrapolation.
Linearity between input pixel intensity and the class scores introduces vulnerability from through outliers, meaning by moving pixel values into areas outside the data distribution, the model can be fooled.
I expected that these adversarial examples were rather specific for each neural network architecture.
But it turns out that you can re-use adversarial examples for fooling networks with a different architecture trained on the same task.

Goodfellow et. al (2014) proposed to protect neural networks by adding adversarial examples to the training data to learn robust prediction models.


**A Jellyfish ... No, wait. A BATHTUB: 1-pixel attacks**

The approach presented by Goodfellow and colleagues (2014) requires changing a lot of pixels, even if only by a bit.
But what if you were only allowed to change a single pixel?
Would you be able to fool a machine learning model?
Su et. al (2017) [^1pixel] showed that it is indeed possible to fool image classifiers by changing a single pixel.

```{r adversarial-1pixel, fig.cap = "By deliberately changing a single pixel (marked with red circles) a neural network trained on ImageNet is fooled to predict the wrong class (in blue letters) instead of the original class (in black letters). Work by Su et. al (2017)."}
knitr::include_graphics("images/adversarial-1pixel.png")
```

Similar to counterfactuals, the 1-pixel attack searches for a modified example x' which is close to the original image x, but for which the prediction is changed to an adversarial outcome.
The definition of closeness differs though: Only a single pixel is allowed to change.
The 1-pixel attack uses differential evolution for finding which pixel to change and in what way.
Differential evolution is loosely inspired by the biological evolution of species.
A population of individuals called candidates solutions recreates generation by generation through recombination until a solution is found.
Each candidate solution encodes a pixel modification and is represented by vector of five elements: the x- and y-coordinates and the red, green and blue (RGB) values.
The search begins with, for example, 400 candidate solutions (= pixel modification suggestions) and creates a new generation of candidate solutions (children) from the parent generation by using the following formula:

$$x_{i}(g+1)=x_{r1}(g)+F(x_{r2}(g)+x_{r3}(g))$$
where each $x_i$ is one element of a candidate solution (either x-coordinate, y-coordinate, red, green or blue), g is the current generation, F is a scaling parameter (set to 0.5) and r1, r2 and r3 are different random numbers.
Each new child candidate solution is, again, a pixel with the five attributes for location and color and each of those attributes is a mixture of three random parent pixels.

Creating children is stopped when one of the candidate solutions is an adversarial example, meaning it is classified as the wrong class, or when the number of maximum iterations is reached as set by the user.


**Everything is a toaster: Adversarial patch**

One of my favorite examples brings adversarial examples into physical reality.
Brown et. al (2017)[^toaster] designed a printable sticker which you can stick next to objects to make them look like toasters to an image classifier.
Brilliant work!

```{r adversarial-toaster, fig.cap = "A sticker which makes a VGG16 classifier trained on ImageNet categorize an image of a banana as a toaster. Work by Brown et. al (2017)."}
knitr::include_graphics("images/adversarial-toaster.png")
```

This method differs from the previously presented methods for adversarial examples, because the constraint that the adversarial image has to be very close to the original image is lifted.
Instead, the method replaces a part of the image completely with a patch, which is allowed to take on any shape. 
The image of the  patch is optimized over various background images, with different positions of the path on the images, sometimes shifted, sometimes bigger or smaller and rotated to make the patch work for many situations.
In the end, this optimized image can be printed out and used to fool image classifiers in the wild.


**Never bring your 3D-printed fake-rifle-turtle to a gun fight: Robust adversarial examples**

The next method is literally adding another dimension to the toaster:
Athalye et. al (2018)[^turtle] 3D-printed a turtle that was designed to look like a riffle to a deep neural network from almost all possible angles.
Yes, you read that correctly. 
A physical object that looks like a turtle to humans looks like a damn riffle to the computer!

```{r adversarial-turtle, fig.cap = "A 3D-printed turtle that is recognized as a riffle by TensorFlow’s standard pre-trained InceptionV3 classifier. Work by Athalye et. al (2018)"}
knitr::include_graphics("images/adversarial-turtle.png")
```

The authors found a way to create an adversarial example in 3D for a 2D classifier, which is adversarial over most possible transformations, like all the ways of turning the turtle, zooming in and so on.


**The blindfolded adversary: Black box attack**

Imagine the following scenario:
I give you access to my great machine learning model via Web-API.
You can get predictions from the model, but you have no access to the model parameters.
From the convenience of your couch, you can send data and my service sends you the respective predictions.
Most adversarial attacks are not designed to work in that scenario, since they require access to the gradient of the underlying deep neural network to find adversarial examples.
Papernot and colleagues (2017)[^papernot] showed that it is possible to create adversarial examples without internal model information and without access to the training data.
This kind of zero-knowledge attack is called black box attack.

How it works:

1. Start with a few images, which don't have to be from the training data, but should be from the same domain (e.g. if the classifier to-be-attacked is a digit classifier, use images of digits.)
1. Get predictions for the current set of images from the black box.
1. Train a surrogate model on the current set of images (for example a neural network).
1. Create new, synthetic images by using a heuristic that explores in which feature direction the model outputs vary most around the current set of images.
1. Repeat steps 2 to 4 for a pre-defined number of epochs.
1. Create adversarial examples for the surrogate model using the fast gradient method (or something similar).
1. Attack the original model with adversarial examples.

The goal of the surrogate model is to approximate the decision boundaries of the black box model, but not necessarily achieving the same accuracy.

The authors tested this approach by attacking mult-class deep neural network classifiers trained on different cloud machine learning services.
These services train image classifiers on images and labels uploaded by the user.
The software automatically trains the model - sometimes with an algorithm unknown to the user - and deploys it.
The classifier then gives predictions for uploaded images, but the model itself can't be accessed or downloaded.
For different providers, the authors managed to find adversarial examples, with up to 84.24% misclassified by one one of the services. 

The method even works if the black box model to be fooled is some different machine learning model and not a neural network like a decision tree, which is not even differentiable.


### The Cybersecurity Perspective

<!-- defense is difficult yet important -->
Machine learning deals with known unknowns: classifying unknown data points from  a known distribution.
Defending against attacks deals with unknown unknowns: classifying never seen data points from an unknown distribution.
Since machine learning is getting integrated into more and more system, like in autonomous vehicles or medical devices, they also become entry points for attacks.
Even if the predictions of a machine learning model are 100% correct on a dataset, there will still be adversarial examples to fool it.
Naturally, defending machine learning models against cyber attacks becomes part of the field of cyber security.

<!-- cybersecurity -->
Biggio et. al (2017)[^adversarial] give a nice review of ten years of research on adversarial machine learning, on which this section is based.
Cyber security is an arms-race in which attackers and defenders repeatedly try to outsmart each other. 


**In cyber security, there are three golden rules for defending against attacks: 1) know your adversary 2) be proactive and 3) protect yourself.**

Knowing the adversary depends on the application of the model.
People who try to scam other people for their money are adversary agents for the system which are defended by the mail providers.
The mail providers want to protect their customers, so that they keep using their mail program, the attackers want to trick people into giving them money.
Knowing the adversary means that we also know what they are after, which makes it easier to defend against them.
Let's say you wouldn't know that these spammers exist and the only misuse of the mails is sending pirated material, than the defense would be very different (e.g. focusing on scanning the attachments for copyrighted material instead of analysing the text for spam indicators).

Being proactive means to actively test and spot weaknesses of the system.
You are proactive when you actively try to fool the model with adversarial examples and then defend against them (more about this later).
Also, using interpretability methods to understand which features are important and how features influence the prediction is a proactive step to understanding weaknesses of a machine learning model.
Do you, as the developer of a model, trust your model out into this dangerous world, without having ever looked into it beyond the prediction performance on a test dataset?
Have you analyzed how the model behaves in different scenarios, identified the most important inputs, inspected prediction explanations for a few examples?
Have you tried to find adversarial inputs?
Interpretability of machine learning models has a great role to play in cyber security.
Being reactive, the opposite of proactive, means waiting until the system was attacked and only afterwards understanding the problem and installing some defenses.

How can we protect our machine learning systems?
One of the most obvious, proactive approaches is to iteratively retrain the classifier with adversarial examples, also called adversarial training.
Other approaches are based on game theory, like learning invariant transformations of the features or robust optimization (regularisation).
Another suggested method is to use multiple classifiers instead of only one and letting them vote (ensemble), but this has no guarantee to work, since they might all suffer from similar adversarial examples.
Another approach that also doesn't work well is gradient masking, which constructs a model without useful gradients by using a nearest neighbor classifier instead of the original model.


We can distinguish attacks by how much an attacker knows about the system.
The attacker might have perfect knowledge (white box attack), meaning she knows everything about the model like the weights of a linear model;
The attacker might have partial knowledge (gray box attack), meaning she only knows, for example, the feature representation and the type of model that was used, but has no access to the training data or the parameters;
The attacker might have zero knowledge (black box attack), meaning she can only query the model in a black box manner but doesn't have access to the training data or information about the model parameters.
Depending on the level of information, the attacker can employ different techniques to attack the model.
As we have seen in the examples, adversarial examples can even be created in the black box case, so hiding information about data and model is not enough to protect against attacks.

Given the nature of the cat-and-mouse game between attackers and defenders, we will see a lot of development and innovation in this area. 
Just think about the many different types of ever evolving spam mails.
New attack methods against machine learning models will be invented and new defenses will be suggested to defend against these new attacks.
More powerful attacks will be designed to evade the latest defenses and so on, ad infinitum.
With this chapter, I hope to sensitize you of the problem of adversarial examples and that only by proactively studying the machine learning models we are able to discover and fix vulnerabilities.





[^szegedy]: Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., & Fergus, R. (2013). Intriguing properties of neural networks, 1–10. http://doi.org/10.1021/ct2009208

[^adversarial]: Biggio, B., & Roli, F. (2017). Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning, 32–37. Retrieved from http://arxiv.org/abs/1712.03141

[^turtle]: Athalye, A., Engstrom, L., Ilyas, A., & Kwok, K. (2017). Synthesizing Robust Adversarial Examples. Retrieved from http://arxiv.org/abs/1707.07397

[^goodfellow]: Goodfellow, I. J., Shlens, J., & Szegedy, C. (2014). Explaining and Harnessing Adversarial Examples, 1–11. http://doi.org/10.1109/CVPR.2015.7298594

[^1pixel]: Su, J., Vargas, D. V., & Kouichi, S. (2017). One pixel attack for fooling deep neural networks. Retrieved from http://arxiv.org/abs/1710.08864

[^toaster]: Brown, T. B., Mané, D., Roy, A., Abadi, M., & Gilmer, J. (2017). Adversarial Patch, (Nips). Retrieved from http://arxiv.org/abs/1712.09665

[^papernot]: Papernot, Nicolas, et al. "Practical black-box attacks against machine learning." Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM, 2017.
