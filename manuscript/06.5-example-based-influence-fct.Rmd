# Influential Instances

Which were the most influential data points for building the model?

Data instances can have extreme effects on the learning process of the model.



What's an outlier?
A data point that lies outside of the mass of the probability distribution (an unlikely point).
Not every outlier is necessarily an influential point. 
An influential point is a special outlier: An outlier that  influences the learned model (when comparing the models learned with and without the influential points).
Another term is leverage points in terms of linear regression. 
Leverage refers to the potential to greatly influence the model, but it doesn't have to.
Leverage points are extreme in some feature, but not necessarily in y.

TODO: Example with regression curve (with and without influential point).
TODO MAYBE: Example with decision tree (with and without influential point)



Identifying influential instances in linear models: Cooks distance.
For both measures, we refit the model repeatedly while leaving out certain instances.
The model with an instance is compared to the one without and the model difference for the weights is compared.


Linear model, GLM there is XXX.
In linear regression, an influential instance is an outlier that greatly affects the estimated slope of the regression line.

Cooks Distance is defined as:

$$D_i=\frac{\sum_{i=1}^n(\hat{y}_j-\hat{y}_{j(i)})^2}{p\cdot{}MSE}$$

where the numerator is the squared difference between prediction of model with and without the i-th instance, summed over the dataset.
The denominator is the number of features p times the mean squared error.
The Cooks distance tells us how much the predicted output of a linear model changes when we remove the i-the instance in the training.

TODO: Plot with Cooks Distance 



Cooks Distance is model-agnostic?
Refitting the model is very costly, but possible.


Model-agnostic, there is influence functions by Koh and Liang (2017)[^koh].
Idea: For a prediction of a instance, trace it back through the learning algorithm to the training data.
It helps for understanding the model behaviour, debugging the model and detecting errors with the dataset.
It also helps to identify [adversarial examples](#adversarial) which exploit vulnerabilities of a machine learning model.



A learning algorithm (the algorithm that produces a linear model or a random forest) is - in its essence - a function that takes in data and spits out a model, which can be described by its parameters:

$$\text{Learner}:X\rightarrow{}\text{Model}(\theta)$$

And a model is a function that projects from to data to some output y:

$\text{Model}:X\rightarrow{}y$





[^koh]: Koh, P. W., & Liang, P. (2017). Understanding Black-box Predictions via Influence Functions. Retrieved from http://arxiv.org/abs/1703.04730