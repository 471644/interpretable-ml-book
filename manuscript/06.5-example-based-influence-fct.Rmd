## Influential Instances

```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(43)

influence.matrix.filename = "../data/influence-df.RData"
```

<!-- Intro text -->
An influential instance is an instance whose deletion from the training data in the model fitting step considerably changes the prediction model.
Identifying influential training instances can be used to 'debug' the model and understand the global model behaviour and single predictions better.

*Keywords: Influential points, influence function, leave-one-out analysis, Cook's distance, deletion diagnostics*

Before we start digging into some methods how to find influential instance, we have to talk about the difference between outlier and an influential instances.

**Outlier**

You probably have already heard about outliers before.
An outlier is an instance which is far away from other instances in the dataset.
"Far away" can for example be measured by the Euclidean distance.
In a dataset of newborn body measurements, a baby that weights 6kg would be considered an outlier.
In a dataset of bank accounts, which are mostly checking accounts, a dedicated loan account, with a large negative balance and not so many movements compared to the checking accounts would be considered an outlier.
The following images shows an outlier:

```{r outlier, fig.cap = "The feature follows a Gaussian distribution with the exception of the outlier x=8."}
n = 50
x = rnorm(mean = 1, n = n)
x = c(x, 8)
y = rnorm(mean = x, n = n)
y = c(y, 7.2)
df = data.frame(x, y)
ggplot(df) + geom_histogram(aes(x = x)) + my_theme() + 
  scale_x_continuous("feature x") + 
  scale_y_continuous("count")
```


Outliers might or might not be interesting data points (like criticisms or archetypes).
Outliers can be influential for the model fit, but don't have to be. 
But when we want to know what really impacts the model and its predictions, we have to look at influential instances

**Influential instance**
An influential instance or point is a data instance which has a strong effect on the fitted model.
Influential is defined as how much the model is different when the model is refitted when the data point is removed.
An outlier can, but doesn't have to be an influential point.
This can be either measured as the influence on some model parameters or on the predictions of the model.
Influential instances can be and often are outliers.
The concepts are not the same.
Not every outlier is necessarily an influential point. 
An influential point is a special outlier: An outlier that  influences the learned model (when comparing the models learned with and without the influential points).

```{r influential-point, fig.cap = "The different influence on a fit of a linear model by adding an outlier or adding an influential data point. Adding a non-influential outlier doesn't change the fitted slope much. Adding an influential point changes the fitted slope drastically."}


df1 = df
df2  = df[1:n,]
df3 = rbind(df2, data.frame(x = 8, y = 0))
df1$regression_model = "with outlier"
df2$regression_model = "without outlier/influential point"
df3$regression_model = "with influential point"
df.all = rbind(df1, df2, df3)


text.dat = data.frame(x = c(8, 8), y = c(7.2, 0), lab = c("Outlier \u2192", "Influential point & outlier \u2192"), regression_model = "with outlier")

ggplot(df.all, mapping = aes(x = x, y = y, group = regression_model, linetype = regression_model)) + 
  geom_point(size = 2) + 
  geom_smooth(method='lm',formula=y~x, se = FALSE, aes(color = regression_model), fullrange = TRUE) + 
  my_theme() + 
  geom_label(data = text.dat, aes(label = lab), hjust = 1, nudge_x = -0.2, vjust = 0.3) 

```

**How can we use the concept of an influential instance for understanding the model? Some motivation.**

A model is created by a learner, which learns the model from the training data.
The big idea behind influential instances for interpretability is to trace predictions back were it all began: the training data.
A learner, which is the algorithm that produces the machine learning model is - in its essence - a function that takes in training data consisting of features X and the target y and spits out a machine learning model.
The learner of a decision tree is an algorithm that chooses the split features and the values at which to split. 
A learner for a neural network uses the backpropagation to find the best weights.


```{r learner, "A learner learns a model from labeled training data. The model is used to make predictions."}
knitr::include_graphics("images/learner.png")
```


Instead of looking at the model itself, or how the prediction changes when we change the inputs (to the prediction model, not to the training process), we ask how the model and the predictions would change when we remove instances from the training data in the training process.
Unlike most other methods, we don't treat the model as fixed, but as a function of the training data and outcome.
We can ask globally: Which were the most influential instances for the model parameters or the overall predictions?
We can ask locally: Which were the most influential instances for the prediction of a particular instance?
Great! Now everyone is aware that the model is a product of the data, but how, exactly, can we trace a prediction back to the training data?
We have two means of measuring the influence:
First, we can delete the instance from the training data and refit the model on the reduced training dataset and observe the difference in either model parameters or the predictions (either locally or globally).
The other option is to upweight a data point 
Influence of a data point can be measured by how much the prediction changes, when we remove or upweight a certain point.
First we learn about the approach of deleting single instances and later in the chapter about upweighting instances.


### Identifying Influential Instances with deletion diagnostics


When facing a problem, we should always look if other people have already solved a similar problem. 
In the case of influential instance, statistician have already done a lot, especially in the area of (generalized) linear regression models
For measuring the effect of deleting an instance on the model parameters, we have the DFBETA measure, for the effect on the predictions, we have Cook's distance.
For both measures, we have to refit the model repeatedly while leaving out certain instances.
The model with an instance is compared to the one without and the model difference for the weights is compared.

**Cook's distance** measures globally (across the whole dataset) how influential a training instance was for all predictions. 
The measure was invented for linear regression models, approximations for generalized linear regression models exist.
Cooks distance Di for a data point is defined as the sum of the changes in the predicted outcome when the i-th instance is removed from the model training.

$$D_i=\frac{\sum_{i=1}^n(\hat{y}_j-\hat{y}_{j}^{(-i)})^2}{p\cdot{}MSE}$$

where the numerator is the squared difference between prediction of model with and without the i-th instance, summed over the dataset.
The denominator is the number of features p times the mean squared error.
The Cooks distance tells us how much the predicted output of a linear model changes when we remove the i-the instance in the training.
Cook's distance refers to how far, on average, predicted y-values will move if the observation in question is dropped from the data set.
Interestingly, Cook's distance is usually not seen outside the context of linear models and generalized linear models, but the idea behind it does not require to be used on linear models but is actually model agnostic.
Note that Cook's distance is simply the difference between the sum of predicted outcomes with full data and predicted without the i-th instance in training.
The denominator is the same for all instances to be removed and only scales the distance.

**DFBETA** measures the difference in the model parameters after deletion:

$$DFBETA_{i}=\beta-\beta^{(-i)}$$

where $\beta$ is the vector with the model parameters with all data points, and $\beta_{(i)}$ the model parameters when the model is fit without instance i.
This obviously only works for models with weight parameters like logistic regression, neural networks etc..
For models that don't have weight parameters, like decision trees, tree ensembles, support vector machines, Cook's distance is more useful.

We are doing machine learning and don't want to restrict ourselves to linear models.
Can we use Cook's distance and DFBETA for any model?
DFBETA requires model parameters, so an approach a la Cook's distance is more general, so we will look at the later, but all ideas apply to measuring the influence of deletion on model parameters as well.
The idea to measure influence of deletion on predictions amounts to a simple difference of the prediction before and after deletion.
We can easily break it down locally to explain for a single prediction for a particular instance j, what the influence of the i-th training instance was. 
Let's define influence very simply as:

$$\text{Influence}_{j}^{(-i)}=\left|\hat{y}_j-\hat{y}_{j}^{(-i)}\right|$$

This formula is basically the a single summand from the numerator of Cook's distance. 
The denominator of Cook's distance is the same for all deleted instances, so in terms of ranking we don't need the denumerator anyways.

When we want to see the average influence of instance i on the whole dataset, we take the mean over the predictions of the whole dataset.

$$\text{Influence}^{(-i)}=\frac{1}{n}\sum_{j=1}^{n}\left|\hat{y}_j-\hat{y}_{j}^{(-i)}\right|$$



**Example with deletion influence**

In the following example, we train a support  vector machine to predict [cervical cancer](#cervical) given risk factors and we measure which training points were the most influential overall and for single predictions.
Since predicting cancer is a classification problem, we measure the influence on the predicted probability for cancer.
An instance is influential when the predicted probability changes a lot (increase / decrease) on average in the dataset when the instance is removed from the model training.
Measuring influence for all `r nrow(cervical)` training instances requires to fit the model once on all training data and refit it `r nrow(cervical)` times (= size of training data) with each time one of the instances removed.

```{r influence, eval = !file.exists(influence.matrix.filename)}
#'@param predicted The predicted outcome of a model
#'@param predicted.without The predicted outcome of a model with a data point removed
influence.v = function(predicted, predicted.without) {
  predicted - predicted.without
}

influence.matrix = matrix(NA, ncol = nrow(cervical), nrow = nrow(cervical))

lrn = makeLearner("classif.svm", predict.type = "prob")
tsk = makeClassifTask(data = cervical, target = "Biopsy")
mod = train(lrn, tsk)
predicted.orig = getPredictionProbabilities(predict(mod, newdata = cervical))
cs = lapply(1:nrow(cervical), function(to.remove.index) {
  mod.2 = train(lrn, tsk, subset = setdiff(1:nrow(cervical), to.remove.index))
  predict.removed = getPredictionProbabilities(predict(mod.2, newdata = cervical))
  influence.v(predicted.orig, predict.removed)
})

# Column: Removed instance, row: influenced instance
influence.df = data.frame(cs)
influence.df = as.matrix(influence.df)
diag(influence.df) = NA
save(influence.df, predicted.orig, file = influence.matrix.filename)
```


The following table shows the instance with the highest influence of `r df[1,"influence"]`. 

```{r}
load(influence.matrix.filename)
df = data.frame(influence = colMeans(abs(influence.df), na.rm = TRUE), id = 1:nrow(cervical))
df = df[order(df$influence, decreasing = TRUE),]
kable(t(cervical[df$id[1],]))
```
An influence of `r sprintf('%.3f', abs(df[1,"influence"]))` means when we remove the `r df$id[1]`-th instance the predicted probability changes on average by an absolute `r sprintf('%.2f', 100 * df[1,"influence"])`%.
This is quite substantial because the average predicted probability for cancer is `r sprintf('%.3f', 100 *mean(predicted.orig))`%.
The mean of influence over all possible deletion is `r sprintf('%.3f', 100 * mean(abs(df$influence)))`%.
Now we have the most influential data points. 
This is already useful for debugging the data:
Is there some problematic instance?
These are the first points to check again if they have any mistakes in them, because any mistake in them influences by definition the model a lot.

But otherwise, simply printing the instances is not, not very useful. 
This again shows that all the methods that yield instances as output are only meaningful when we have a good way of representing this instance.
But we can do other things to better understand what types of instances are influential.
What is the structure common to the influential points that makes them influential?
We can turn this meta question into a regression problem and model the influence between the overall influence per instance and their features as regression problem:
Which feature values make an instance more influential?
For this we can use any method from the [interpretable machine learning models chapter](#simple).
For this example I choose a decision tree (following figure), which shows that training data for women of age 35 and above are the most influential for training the support vector machine.
From all the women in the dataset  `r sprintf('%.3f', 100 * mean(cervical$Age >= 35))`% are older than 35.
In the [partial dependence chapter](#pdp) we have seen that after 40, there is a sharp increase in predicted cancer probability and the [feature importance](#feature-importance) has also detected age as one of the more important features.
The influence analysis says that the model becomes increasingly more unstable when predicting cancer for increasingly older women.
This is in itself valuable information. 
It means that mistakes in the data for one of those instances can have a strong effect on the model.

```{r cooks-analyzed, eval=TRUE, fig.cap = "A decision tree modeling the relationship between the influence of an instance and its features. The maximal depth of the tree is set to 2."}
df.cervical  = cbind(df, cervical)
ct = rpart(abs(influence) ~ . -id, data = df.cervical, control = rpart.control(maxdepth = 2))
ct = as.party(ct)
plot(ct, inner_panel = node_inner(ct, pval = FALSE), type='simple')
```

```{r influence-single-prepare}
i = which(predicted.orig == max(predicted.orig))
```


This analysis was global and asked which was the overall most influential instance.
Now, we pick out one of the instances, for which we want to explain the prediction by finding the most influential training data instances.
It's like asking the counterfactual:
How would the prediction for instance j change if we left out instance i from the training process.
But we don't only do this for left out instance i, but we leave out each instance one after each other and get a vector of differences.
Then we can rank all the differences by amount and choose the training instance which yield the biggest change in prediction when left out of the training and use those to explain the prediction of the model for this instance.
This gives us a list of training instances, ranked by it's influence of the `r i`-the instance.
Full disclosure: the instance to be explained was picked by choosing the instance which had the highest predicted probability of cancer (`r sprintf('%.2f', 100 * predicted.orig[i])`%), which I thought might be an interesting case to analyse more deeper.
Now, we could return the, let's say, top 10 most influential instances for the prediction of the `r i`-the instance, printed as a table.
Not very useful, because we wouldn't be able to see much.
Again it is more useful to find among those instance the relationship between features and the influence measure.
We use a decision tree for that, which is trained to predict the influence given the features, but, really, we only misuse it to find some structure and not to actually predict anything.
The following decision tree reveals what kind of training instances where the most influential for the prediction of the `r i`-th instance. 


```{r influence-single, fig.cap = sprintf("Decision tree explaining which instances were the most influential for predicting the $i-th instance. Removing women who smoked for 19 years or longer, have a very high influence on the prediction of the %i-th instance, with an average change in absolute prediction by 11.7% of cancer probability. Maximal depth of the tree is set to 2.", i, i)}
obs = influence.df[i,]
cervical.200 = cervical
cervical.200$influence = unlist(obs)
#cervical.200 = na.omit(cervical.200)
worst.case.index = which(abs(cervical.200$influence) == max(abs(cervical.200$influence), na.rm = TRUE))
ct = rpart(abs(influence) ~ ., data = cervical.200, control = rpart.control(maxdepth = 2))
ct = as.party(ct)
plot(ct, inner_panel = node_inner(ct, pval = FALSE), type='simple')
```

Instances, where women smoked for 19 years ore more have a high influence on the prediction of the `r i`-th instance.
In the data, `r sprintf('%.2f', 100 * mean(cervical$Smokes..years >= 19))`% of women smoked that long, which amounts to only `r sum(cervical$Smokes..years >= 19)` subjects.
Any error that happened in surveying the number of years smoking of one of these women will have a tremendous effect on the predicted outcome for the `r i`-th instance.

The most extreme is when we remove the `r worst.case.index`-th instance.
She has smoked for `r cervical$Smokes..years.[worst.case.index]` years, matching our results from the decision tree.
Btw. the case, for who we are checking the prediction has smoked for `r cervical$Smokes..years.[i]` years.
Then the predicted probability for the `r i`-th instance changes from `r sprintf('%.2f', 100 * predicted.orig[i])`% to `r sprintf('%.2f', 100 * (predicted.orig[i]  - cervical.200$influence[worst.case.index]))`%!!!

When looking at the other features of the influential case, we can see another possible problem.
```{r influence-most-influential}
kable(t(cervical.200[worst.case.index,]))
```

The data says that the woman is 28 years old, and has already been smoking 22 years. 
Either it's a really extreme case and she truly started smoking at 6 or this is a data error. 
I tend to believe the latter, but this is certainly a case where we should not blindly believe in the correctness of the data.

These examples demonstrated how useful identifying influential instances is for debugging models.
One problem with the proposed approach is that the model has to be refitted for each training point, which can be quite often.
In the remaining part of the chapter I will show you a method where refitting is not necessary.


### Identifying Influential Instances without Refitting

The whole refitting thing is rather slow, because if you have thousands of training samples, you have to retrain your model thousands of times.
Assuming the model takes a day to compute and you have 1000 training examples, then - without parallelization - the computation of influential instances will take almost 3 years.
Ain't nobody got time for that.

Model-agnostic, there is influence functions by Koh and Liang (2017)[^koh].
But it only works for methods where you have some loss function that you can differentiate with respect to some model parameters.
Random forests for example won't work. 
Logistic regression, neural networks and support vector machines work.
Idea: For a prediction of a instance, trace it back through the learning algorithm to the training data.


It helps for understanding the model behaviour, debugging the model and detecting errors with the dataset.
It also helps to identify [adversarial examples](#adversarial) which exploit vulnerabilities of a machine learning model.


The method is based on influence function which was invented for robust statistics.
The idea is: 
I have some estimator T (like any statistical measurement) which acts on some distribution F. 
How much does T change if we perturb F is what robust statistics deals with.
It's a measure of  stability.
Influence functions measure how a training example affects the predictions for a test example
Or, as formulated by the paper: How much would the test loss change by upweighting a certain training example?

Assuming that you can differentiad the loss twice with respect to the model parameters $\theta$ you can use this method.
Actually, it's also possible to use it when the differentiation twice is not possible and there is a trick to compute the Hessian matrix (second derivative).


How would the model parameters look like if we would remove data point z?
Thanks to influence functions from robust statistic, we can answer the question, by upweighting the z in the loss function by some small step $\epsilon$, which results in the following, new parameters:

$$\hat{\theta}_{\epsilon,z}=\arg\min_{\theta{}\in\Theta}(1-\epsilon)\frac{1}{n}\sum_{i=1}^n{}L(z_i,\theta)+\epsilon{}L(z,\theta)$$
where $\theta$ is the vector of the model parameters.
L is the loss function used to train the model and $\epsilon$ is avery small number.
$z_i$ are the training data and z is the data point for which we want to find influential instances.
The intuition behind this formula is:
How much does the loss change if we upweight a particular points $z_i$ from the training data by a little bit ($\epsilon$) and downweight the other data points respectively?

The solution is:

$$I_{up,loss}(z,z_{test})=\left.\frac{d{}L(z_{test},\hat{\theta}_{\epsilon,z})}{d\epsilon}\right|_{\epsilon=0}=-\nabla_{\theta}L(z_{test},\hat{\theta})^T{}H^{-1}_{\theta}\nabla_{\theta}L(z,\hat{\theta})$$


We look how the loss of z_test changes when we upweight a certain training data points, telling us how influential this training data points was for the prediction of z_test.
The intuition behind the parts where the L stands:
How much does the model want to go into the direction of the upweighted training data point.
A large gradient means:
A small change in the model parameter $\theta$ (for example the weights in a neural network) decrease the loss a lot, so if I upweight this training point, the parameters will change by a lot.
Some intuition behind that: 
The first term $\nabla_{\theta}L(z_{test},\hat{\theta})^T{}$ is the gradient of the test instance, 
also the last term $\nabla_{\hat{\theta}}L(z,\hat{\theta})$, but for the training instance. 
The middle part $H^{-1}_{\hat{\theta}}$ measures the similarity between the two.

More intuition about the formula:
The influence is proportionate to how large the gradients are for the training and test loss.
The higher the gradient of the training loss, the higher its influence on the parameters, the higher the influence on the test prediction.
For the gradient of the test prediction loss: The higher the gradient, the more influentiable the test instance is.
The whole construct can also be seen as a measure of similarity between the the training and the test instance.
Similarity doesn't mean here the Euclidean distance, but the similarity the model learned.

**Excursion: Intuition behind the gradient and the Hessian matrix**

Intuition behind the gradient: The gradient is the rate of change of the loss of the training (or test example) in various directions.
It tells us how much the loss changes, when we change the model parameters $\theta$ by a little bit.
Since the model usually has many parameters, the gradient also is a vector with many numbers, one for each parameter, telling for each parameter the rate and direction of change of the loss if we were to change the parameter.
A positive entry in the gradient vector means that a small increase in the corresponding model parameter increases the loss, a negative gradient means that the increase of the parameter decreases the loss.
The Hessian is the second-order derivative.
What the gradient is to the loss, is the hessian to the gradient. 
The Hessian is the rate of change of the gradient, or expressed in terms of the loss, it is the rate of change of the rate of change of the loss.
A more informal explanation is that the Hessian informs us how curved the rate of change is. 
The Hessian is a matrix because to describe the curvature of the loss and that slope depends on the direction we are looking. 
A real world example: A potato chip.
Not the irregular ones, but the ones where each chip has the same shape, which you can stack. 
If you lay such a chip on a surface, such that the ends on off the ends of the longer side touch the ground, we will see how it relates to gradients and the Hessian.
The long end points away from you, so that you could now put your fingers on the left and right elevated parts of the chip and wiggle it, but be careful not to break it.
Have a look at the next picture to see what I mean.

The chip is 3-dimensional and will be a good example.
Assume that each point on the chip describes the combiniation of two model parameters, represented as the coordinate in the direction of the long side and the coordinate in the direction of the short side.
The height of a point on the chip represents the value of the loss function.
Now, if we start on the left of the elevated short sides of the chip, the gradient of the loss (height of the chip) would be negative, since going from left to right by a small step would decrease the height/loss. 
In the middle of the chip, there is a gradient of zero and on the right a positive gradient for the direction.
But this was only one component of the gradient. 
The other component is in the longer direction of the chip. 
Coming from your body, the gradient is first positive, then zero, then negative.
Now for the Hessian: It describes how curvy the chip is or also the rate of change of the gradient. 
Since we have two parameters (two directions), we have to describe the curviness regarding the direction we are looking.
Let's again start at the left of the short sides of the chips, with the the high negative gradient. 
How curvy is the chip?
Well, that depends whether you look into the direction towards the other short end or into the direction of the long ends.
The Hessian or curviness is very high for going from short edge to short edge, because there is much curviness between the short edges. 
The curviness 
But being on a short edge and going into one 


Let's say the height of the chip is the loss, the coordinate along the back of the from back to neck is one parameter, and the coordinate of the saddle from left flank to right flank is the second parameter.
Going from left to right, the saddle first increases in height and than decreases.
A high rate of change of the gradient (rate of rate of increase of height), so there is a positive Hessian for short direction, when looking into the short direction. 
But how much does the gradient change for the short direction, when we look into the long direction?
That is, when we go to a certain coordinate on the chip, fix the position of the short side, but check how much the gradient changes, when we change the coordinate in the long direction. 
The answer is 0.

We assume the chip's height follows the following function of the position on the short side and the position on the long side.
Let's call the position on the short side s and the position on the long side l:

$$\text{Height}=s^2-l^2+2$$
The 2 is just used so that the chip starts at zero and not a negative value.

The gradient of the chip looks like this:

$$\nabla\text{Height}=\begin{pmatrix}\frac{d\text{Height}}{ds}\\\frac{d\text{Height}}{dl}\end{pmatrix}=\begin{pmatrix}2s\\-2l\end{pmatrix}$$

This means that going from left to write short end, we increase the gradient and going from body pointing long end to the one long end further away, we decrease the rate at which the height increases.
This gradient depends on the position on the chip, which shows that for the chip the gradient is dependent on where we are on the chip, which means that the Hessian matrix will not only contain zeros, but there is a changing rate of change of the height.


The Hessian for our chip looks like this: 

$$H=\begin{pmatrix}\frac{d^2\text{Height}}{dsds}&\frac{d^2\text{Height}}{dsdl}\\\frac{d^2\text{Height}}{dsdl}&\frac{d^2\text{Height}}{dldl}\end{pmatrix}=\begin{pmatrix}2&0\\0&-2\end{pmatrix}$$



```{r potato-chip, fig.cap = "Created by Topntp26 - Freepik.com"}
img <- jpeg::readJPEG("images/potato-chips.jpg")
p1 <- grid::rasterGrob(img, interpolate=TRUE)

df = expand.grid(long = seq(from = -1.5, to = 1.5, length.out = 10),
  short = seq(from = -1, to = 1, length.out = 10))

## Adding to for elevation
df$height = 2 + df$short^2  - df$long^2  

p2 = ggplot(df) + 
  geom_tile(aes(x = short, y = long, fill = height)) + 
  scale_x_continuous("Short side of chip") + 
  scale_y_continuous("Long side of chip") + 
  scale_fill_continuous("Height of chip") + 
  coord_fixed()
gridExtra::grid.arrange(p1, p2, ncol = 2)
```



The H is the hessian matrix (the second derivative):

$$H_{\theta}=\frac{1}{n}\sum_{i=1}^n\nabla^2_{\hat{\theta}}L(z_i,\hat{\theta})$$
The intuition behind the Hessian matrix is: 
When I change the training points in particular direction, how does the loss change?

When we upweight this training point, Iup is the ratio of how much the loss of this single point changes compared to how much the other data points vote for the parameters of the model not to change because the loss would increase.

Computing I-up-loss has two challenges: First, it requires calculating and inverting the Hessian matrix.
Second, we want to calculate the property across all training points zi and a few test points z.test.

The problem is that computing and inverting the Hessian requires $O(np^2 + p^3)$ operations. 
This term tells us that the computation operations necessary (and therefore time) depends on the number of parameters to the cube.
Let's say the number of parameters in your model is 10, and the time necessary to compute the Hessian is 1s.
We exchange the model with a model with 100 parameters, which is 10 times as many parameters as the original model.
The computation time is not merely 10x 1s = 10s, but since it scales with the cube, it's actually 10x 10x 10x 1s = 1000s.
This is not healthy for a method that should be usably with neural networks with millions of parameters.
So an improvement is needed.
The method proposed by the authors suggests using some tricks to get the Hessian.
Scaling it up.

The key idea:
You only need the Hessian times a vector $H_{\hat{\theta}}v$ and not the hessian itself.
By applying a series of tricks (Pearlmutter trick, CG and Taylor expansion) it can be derived.

This also works when no derivative exists, for example when the model uses Hinge loss instead of some differentiable loss.
The solution is to replace the loss with some smoothed loss, but the model can still be trained with the non-smooth loss.


Influence functions also work in non-convex models, even though they formally require convexity.


Stochastics estimation of the Hessian matrix. 
Even with the conjugate gradients trick, computing the Hessian is expensive because it has to be iterated over the whole training set.
The trick is to get a sample of the training data and then compute:


(REmove formula?)

$$H_j^{-1}v=v+(I-\nabla^2_{\theta}L(z_j,\hat{\theta}))H^{-1}_{j-1}v$$
where $\nabla^2_{\theta}L(z_j,\hat{\theta})$ is the second order derivative (curviness) at example $z_j$.

TODO: Shorten details about the tricks the authors used.


How good does upscaling with $I_{up,loss}$ work? 
We can only know for certain, by comparing the loss we get with the leave-one-out retraining.
The authors did this and got pretty good results.

Two problems with this approach: 

- Lack of convexity of loss
Solution: smooth the hessian matrix so it becomes positive definite
- Lack of differentiability of loss.
Some losses cannot be differentiaed twice.
Solution: Approximate loss with differentiable version for computing the influence function.


TODO: Write a bit about how to create adversarial examples with influence functions.

### Application of Influence Instances

TODO: Shows some examples and ask authors to use images.

- Fixing training data

Given a budget for how many data points you can change the ground truth, how do you pick the ones to look at?
Best way is to only pick the influential points, because non-influential points - by definition - don't affect the model much anyways.
This works way better than choosing random misclassified or badly predicted points or choosing by the ones with the highest loss.

- Handling domain mismatch / Debugging model errors

Assume data is trained on one domain - like deciding if a cat or a dog is on an image, but all images where from inside the house - and an example from another domain comes in, e.g. showing a dog outside the house.
The model can make heavy mistakes.
The influence function can tell how this domain mismatch will play out.
Checkout why the model makes error on certain data points by checking which inputs were most responsible.

- Poisoning attacks: Creating adversarial examples for model training

How much does the loss change if we change the image a bit
The idea is to add weight to create a synthetic image, and remove weight from the original one to create an adversarial training instance.


What is the difference to using the nearest neighbour in feature space?
Training examples with higher loss have higher influence and nearest neighbour is like taking difference of loss and hessian?



What happens when you don't have the y? 
The method requires computing the loss. 
Does that mean we can't use it for new data for which we don't know the true y?
No, because we can compute any measure based on the derivative, like the change in prediction or predicted probability.


### Advantages of identifying influential points

- Very different from other approaches, because looking at influential instances emphasizes the role the training data plays in the learning process
- Directly helps identifying problems with data points that are wrongly labeled or have high influence and are therefor to be critically evaluated.
- In theory model-agnostic at least the methods that requre refitting 
- Influence functions can also be used to create adversarial training data
- 

### Disadvantages of identifying influential points

- Cook's distance and similar are very expensive to calculate because it requires refitting. 
When your model takes an hour to fit and you have 10,000 instances, then it will take longer than a year to sequentially compute the influence of instance by leaving them out one after each other from the training.
- Influence functions are a good alternative, but only for models with differentiable parameters, like neural networks. 
Forget it for tree-based methods like random forest, boosted trees or decision trees.
- Cook's distance and the influence functions have no absolute scale on which to judge them. 
The cutoff at which we call an instance influential is therefore a bit arbitrary.
But it's still useful to rank the instances among each other.
- These diagnostics only look at the deletion of single points and not what happens when multiple points are deleted. 
Bigger groups of data instances could have some interactions with each other affecting the model fit and the prediction heavily.
There are n possibilities for deleting a single instance from the data. 
There are n times n-1 possibilities for deleting two instances from the training data. 
And you certainly don't want to start going any further, because there are just too many ways.

### Software and Alternatives

- Deletion diagnostics is very simple to implement. See example from this book.
- Cook's distance in R, though only for lm and glm. 
- Python code for influence from paper in repository. 
Unfortunately just the paper dump and no maintained Python module.
-


Good blog post, which helped me understand this paper better: http://mlexplained.com/2018/06/01/paper-dissected-understanding-black-box-predictions-via-influence-functions/#more-641



[^koh]: Koh, P. W., & Liang, P. (2017). Understanding Black-box Predictions via Influence Functions. Retrieved from http://arxiv.org/abs/1703.04730