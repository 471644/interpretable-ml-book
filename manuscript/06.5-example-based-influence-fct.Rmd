## Influential Instances

```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(43)

influence.matrix.filename = "../data/influence-df.RData"
data("cervical")
```

<!-- Intro text -->
Machine learning models are - ultimately - a product of the training data.
Deleting one of the training instances can have a big impact on the resulting model.
We call a training instance "influential" when its deletion from the training data considerably changes the model's parameters or predictions
Identifying influential training instances can be used to 'debug' a machine learning model and explain its behavior and predictions better.


*Keywords: Influential points, influence function, leave-one-out analysis, Cook's distance, deletion diagnostics, robust statistics*

This chapter will show you two approaches for identifying influential instances.
Both approaches originate from robust statistics and relate to a concept called 'influence functions'.
Robust statistics provides statistical methods that  which are less influenced by outliers or by violation of model assumptions.
Robust statistics also provides methods to measure how robust estimations (like the mean of a sample or a prediction model) from data are.
One of those methods is the influence function, which measures how strongly an estimation is influenced by a single data point.
Imagine you want to estimate the mean income of people in your city and ask ten random people on the street how much they earn.
Apart from your sample being really bad probably, how much can your mean income estimate be influenced by a single person?
The influence function can tell you how much the mean is affected when one of the income statements is an outlier. 
By the way, the answer is that your mean estimate can be really badly be influenced by a single answer, because the mean scales linearly with single answers.
A more robust choice is the median (the value at which one half of the people earns more and the other half less), because even if you make an outlier more extreme, the result will not change.

The first approach presented here tackles the question of measuring influence by simply removing an instance from the data and recalculating the statistic or the model.
This approach is also called empirical influence functions.
For the income example, we would re-calculate the mean 10 times while leaving out one of the income statements each time and measure how much the mean estimate it changes. 
A big change means that this instance was very influential.

The second approach upweights one of the data points by a small weight, which is the same as the first derivative of some model given a distribution of data.
This approach is also called "infinitesimal approach".

Before we start digging into some methods how to find influential instance, we have to talk about the difference between outlier and an influential instances.

**Outlier**

An outlier is an instance which is far away from other instances in the dataset.
"Far away" means that the distance, for example the Euclidean distance, to all the other instances is very large.
In a dataset of newborn body measurements, a baby that weights 6kg would be considered an outlier.
In a dataset of bank accounts, where most are checking accounts, a dedicated loan account (large negative balance, only few transactions) would be considered an outlier.
The following images shows an outlier:

```{r outlier, fig.cap = "The feature follows a Gaussian distribution with the exception of the outlier x=8."}
set.seed(42)
n = 50
x = rnorm(mean = 1, n = n)
x = c(x, 8)
y = rnorm(mean = x, n = n)
y = c(y, 7.2)
df = data.frame(x, y)
ggplot(df) + geom_histogram(aes(x = x)) + my_theme() + 
  scale_x_continuous("feature x") + 
  scale_y_continuous("count")
```


Outliers might or might not be interesting data points (like criticisms or archetypes).
Outliers can be influential on the model training, but don't have to be. 
Influential instances is a different concept:

**Influential instance**
An influential instance is a data instance whose removal has a strong effect on the trained model.
The more the model parameters or the predictions of a model change, when the model is retrained with an instance removed from the training data, the more influential this particular instance is. 


```{r influential-point, fig.cap = "The influence on a linear model by adding an outlier or adding an influential data point. Adding a non-influential outlier doesn't change the fitted slope much. Adding an influential point changes the fitted slope drastically."}

df2 = df[-nrow(df),]
df3 = rbind(df2, data.frame(x = 8, y = 0))

df3$regression_model = "with influential instance"
df2$regression_model = "without influential instance"
df.all = rbind(df2, df3)


text.dat = data.frame(x = c(8), y = c(0), lab = c("Influential instance"), regression_model = "with influential instance")

ggplot(df.all, mapping = aes(x = x, y = y, group = regression_model, linetype = regression_model)) + 
  geom_point(size = 2) + 
  geom_smooth(method='lm',formula=y~x, se = FALSE, aes(color = regression_model), fullrange = TRUE) + 
  my_theme() + 
  geom_label(data = text.dat, aes(label = lab), hjust = 1, nudge_x = -0.2, vjust = 0.3) +
  scale_color_discrete("Training data") + 
  scale_linetype_discrete("Training data")

```

**How can we use influential instances to understand the model? Some motivation.**

The big idea behind influential instances for interpretability is to trace predictions back were it all began: the training data.
A learner, which is the algorithm that produces the machine learning model is a function that takes in training data consisting of features X and the target y and produces a machine learning model.
The learner of a decision tree is an algorithm that chooses the split features and the values at which to split. 
A learner for a neural network uses the backpropagation to find the best weights.


```{r learner, "A learner learns a model from labeled training data. The model is used to make predictions.", out.width="\textwidth"}
knitr::include_graphics("images/learner.png")
```

We ask how the model parameters or the predictions would change when we remove instances from the training data in the training process.
This is in contrast to other interpretability approaches that analyze how the prediction changes, when we change the features, like [partial dependence plots](#pdp) or [feature importance](#feature-importance).
With influential instances, we don't treat the model as fixed, but as a function of the training data and outcome.
Influential instances help us answer questions about the global model behaviour and about single predictions:
Which were the most influential instances for the model parameters or the predictions overall?
We can ask locally: Which were the most influential instances for the prediction of a particular instance?
Great! But how, exactly, can we find those influential instances?
We have two means of measuring the influence:
Our first option is to delete the instance from the training data, re-train the model on the reduced training dataset and observe the difference in either model parameters or the predictions (either locally or globally).
The second option is to upweight a data instance using an approximation based on the gradients of the model parameters.
The deletion approach is much easier to understand and motivates the upweighting approach, so we will start with looking into the former.

### Deletion Diagnostics or Empirical Influence Functions

When facing a problem, you should always take a look if other people have already solved a similar problem. 
In the area of influential instances, statistician have already done a lot of research, especially for (generalized) linear regression models.
When you search for "influential observations", the first hits will tell you about the measures DFBETA and Cook's distance.
**DFBETA** measures the effect of deleting an instance on the model parameters.
**Cook's distance** measures the effect of deleting an instance on the overall model predictions.
For both measures, we have to retrain the model repeatedly while leaving out certain instances.
The model with an instance is compared to the one without and the difference in parameters or predictions compared.

DFBETA is defined as:

$$DFBETA_{i}=\beta-\beta^{(-i)}$$

where $\beta$ is the vector with the model parameters with all data points, and $\beta_{(i)}$ the model parameters when the model is fit without instance i.
Quite intuitive I would say.
DFBETA, obviously, only works for models with weight parameters like logistic regression, neural networks etc..
For models that don't have weight parameters, like decision trees, tree ensembles, support vector machines, Cook's distance is more useful.

Cook's distance was invented for linear regression models and approximations for generalized linear regression models exist.
The Cooks distance for a data instance is defined as the sum of the changes in the predicted outcome when the i-th instance is removed from the model training.

$$D_i=\frac{\sum_{i=1}^n(\hat{y}_j-\hat{y}_{j}^{(-i)})^2}{p\cdot{}MSE}$$

where the numerator is the squared difference between prediction of model with and without the i-th instance, summed over the dataset.
The denominator is the number of features p times the mean squared error.
The denominator is the same for all instances to be removed and only scales the distance.
Cook's distance tells us how much the predicted output of a linear model changes when we remove the i-the instance in the training.


Can we use Cook's distance and DFBETA for any machine learning model?
DFBETA requires model parameters, so this measure works only for parameterized models.
Cook's distance does not require model parameters.
Interestingly, Cook's distance is usually not seen outside the context of linear models and generalized linear models, but the idea of taking the difference of some of the model before and after removal of a particular instance is very general.

The most simplistic influence measure for the effect on the model predictions can be written as: 

$$\text{Influence}^{(-i)}=\frac{1}{n}\sum_{j=1}^{n}\left|\hat{y}_j-\hat{y}_{j}^{(-i)}\right|$$

Also, we can easily break the influence down locally to explain for the prediction of instance j what the influence of the i-th training instance was:

$$\text{Influence}_{j}^{(-i)}=\left|\hat{y}_j-\hat{y}_{j}^{(-i)}\right|$$

Here I opted to formulate the influence on the prediction.
This would also work for the difference in model parameters or the difference in the loss.
We will use these simple influence measures in the following example.

**Example with deletion influence**

In the following example, we train a support vector machine to predict [cervical cancer](#cervical) given risk factors and we measure which training instances were the most influential overall and also for a particular prediction.
Since predicting cancer is a classification problem, we measure the influence on the predicted probability for cancer.
An instance is influential when the predicted probability changes a lot (increase / decrease) on average in the dataset when the instance is removed from the model training.
Measuring influence for all `r nrow(cervical)` training instances requires to train the model once on all training data and retrain it `r nrow(cervical)` times (= size of training data) with one of the instances removed each time.

```{r influence, eval = !file.exists(influence.matrix.filename)}
#'@param predicted The predicted outcome of a model
#'@param predicted.without The predicted outcome of a model with a data point removed
influence.v = function(predicted, predicted.without) {
  predicted - predicted.without
}

influence.matrix = matrix(NA, ncol = nrow(cervical), nrow = nrow(cervical))

lrn = makeLearner("classif.svm", predict.type = "prob")
tsk = makeClassifTask(data = cervical, target = "Biopsy")
mod = train(lrn, tsk)
predicted.orig = getPredictionProbabilities(predict(mod, newdata = cervical))
cs = lapply(1:nrow(cervical), function(to.remove.index) {
  mod.2 = train(lrn, tsk, subset = setdiff(1:nrow(cervical), to.remove.index))
  predict.removed = getPredictionProbabilities(predict(mod.2, newdata = cervical))
  influence.v(predicted.orig, predict.removed)
})

# Column: Removed instance, row: influenced instance
influence.df = data.frame(cs)
influence.df = as.matrix(influence.df)
diag(influence.df) = NA
save(influence.df, predicted.orig, file = influence.matrix.filename)
```

```{r}
load(influence.matrix.filename)
df = data.frame(influence = colMeans(abs(influence.df), na.rm = TRUE), id = 1:nrow(cervical))
df = df[order(df$influence, decreasing = TRUE),]
```

The following table shows the instance with the highest influence measure (influence = `r abs(df[1,"influence"])`) on all predictions. 

```{r}
kable(t(cervical[df$id[1],]))
```

An influence of `r sprintf('%.3f', abs(df[1,"influence"]))` means that when we remove the `r df$id[1]`-th instance the predicted probability changes on average by an absolute `r sprintf('%.2f', 100 * df[1,"influence"])`%.
This is quite substantial considering the average predicted probability for cancer is `r sprintf('%.3f', 100 *mean(predicted.orig))`%.
The mean of influence measures over all possible deletions is `r sprintf('%.3f', 100 * mean(abs(df$influence)))`%.
Now know which of the data instances were most influential for the model.
This is already useful for debugging the data.
Is there some problematic instance?
Are there measurement errors?
The influential instances are the first instances to check for mistakes, because any mistake in them influences the model predictions a lot.

Apart from model debugging, can we learn anything to understand the model better?
Simply printing the top 10 most influential instances is not very useful, because that's just a table of instances with lots of features.
All the methods that yield instances as output are only meaningful when we have a good way of representing an instance.
But we can better understand what types of instances are influential by asking: 
What sets an influential instance apart from non-influential instances?
We can turn this question into a regression problem and model the influence of an instance as a function of its feature values. 
We are free to choose any model from the [interpretable machine learning models chapter](#simple).
For this example I chose a decision tree (following figure), which reveals that training data for women of age 35 and above were the most influential for training the support vector machine.
From all the women in the dataset  `r sprintf('%.3f', 100 * mean(cervical$Age >= 35))`% are older than 35.
In the [partial dependence chapter](#pdp) we have seen that after 40, there is a sharp increase in predicted cancer probability and the [feature importance](#feature-importance) has also detected age as one of the more important features.
The influence analysis tells us that the model becomes increasingly more unstable when predicting cancer for increasingly older women.
This is in itself valuable information. 
It means that mistakes in the data for one of those instances can have a strong effect on the model.

```{r cooks-analyzed, eval=TRUE, fig.cap = "A decision tree modeling the relationship between the influence of an instance and its features. The maximal depth of the tree is set to 2."}
df.cervical  = cbind(df, cervical)
ct = rpart(abs(influence) ~ . -id, data = df.cervical, control = rpart.control(maxdepth = 2))
ct = as.party(ct)
plot(ct, inner_panel = node_inner(ct, pval = FALSE), type='simple')
```

```{r influence-single-prepare}
i = which(predicted.orig == max(predicted.orig))
```


This first influence analysis was global and asked which was the overall most influential instance.
Now, we pick out one of the instances, for which we want to explain the prediction by finding the most influential training data instances.
This is like asking a counterfactual question:
How would the prediction for instance j change if we left out instance i from the training process.
We repeat this removal for all instance and get a vector of differences.
Then we can rank all the differences by amount and choose the training instance which yield the biggest change in prediction when left out of the training and use those to explain the prediction of the model for this instance.
This gives us a list of training instances, ranked by it's influence of the `r i`-the instance.
I picked the instance to be explained by choosing the instance which had the highest predicted probability of cancer (`r sprintf('%.2f', 100 * predicted.orig[i])`%), which I thought might be an interesting case to analyse more deeply.
Now, we could return the, let's say, top 10 most influential instances for the prediction of the `r i`-the instance, printed as a table.
Not very useful, because we wouldn't be able to see much.
Again it is more useful to find out what sets the influential instances apart from the non-influential by analysing their features.
We use a decision tree for that, which is trained to predict the influence given the features, but, really, we only misuse it to find some structure and not to actually predict anything.
The following decision tree reveals what kind of training instances where the most influential for the prediction of the `r i`-th instance. 


```{r influence-single, fig.cap = sprintf("Decision tree explaining which instances were the most influential for predicting the $i-th instance. Women in the data who smoked for 19 years or longer, have a very high influence on the prediction of the %i-th instance, with an average change in absolute prediction by 11.7 percentage points of cancer probability. Maximal depth of the tree is set to 2.", i, i)}
obs = influence.df[i,]
cervical.200 = cervical
cervical.200$influence = unlist(obs)
#cervical.200 = na.omit(cervical.200)
worst.case.index = which(abs(cervical.200$influence) == max(abs(cervical.200$influence), na.rm = TRUE))
ct = rpart(abs(influence) ~ ., data = cervical.200, control = rpart.control(maxdepth = 2))
ct = as.party(ct)
plot(ct, inner_panel = node_inner(ct, pval = FALSE), type='simple')
```

Instances, where women smoked for 19 years or more have a high influence on the prediction of the `r i`-th instance.
The case, for who we are checking the prediction has smoked for `r cervical$Smokes..years.[i]` years.
In the data, `r sprintf('%.2f', 100 * mean(cervical$Smokes..years >= 19))`% of women smoked that long, which amounts to only `r sum(cervical$Smokes..years >= 19)` subjects.
Any error that happened in surveying the number of years smoking of one of these women will have a tremendous effect on the predicted outcome for the `r i`-th instance.

The most extreme change in prediction happens when we remove the `r worst.case.index`-th instance.
She has smoked for `r cervical$Smokes..years.[worst.case.index]` years, confirming our findings from the decision tree.
The predicted probability for the `r i`-th instance changes from `r sprintf('%.2f', 100 * predicted.orig[i])` percentage points to `r sprintf('%.2f', 100 * (predicted.orig[i]  - cervical.200$influence[worst.case.index]))` percentage points, when we remove the `r worst.case.index`-th instance!!!


When we look closer at the other features of the influential case, we can see another possible problem.
```{r influence-most-influential}
kable(t(cervical.200[worst.case.index,]))
```

The data says that the woman is 28 years old, and has already been smoking 22 years. 
Either it's a really extreme case and she truly started smoking at 6 or this is a data error. 
I tend to believe the latter, but this is certainly a case where we should question the correctness of the data.

These examples demonstrated how useful identifying influential instances is for debugging models.
One problem with the proposed approach is that the model has to be retrained for each training point, which can be quite often.
The whole retraining can rather slow, because if you have thousands of training samples, you have to retrain your model thousands of times.
Assuming the model takes a day to train and you have 1000 training examples, then - without parallelization - the computation of influential instances will take almost 3 years.
Ain't nobody got time for that.
In the remaining part of the chapter I will show you a method that doesn't require refitting the model.


### Influence Functions

*You* : I want to know the influence of a training instance on a particular instance's prediction.  
*Research* : You can delete this training instance, retrain the model, and measure the difference in prediction.  
*You* : Great! But do you have a method that works without retraining model? It takes so much time.  
*Research* : Do you have a model with a differentiable loss and model parameters?  
*You* : I fitted a neural network with logistic loss. So yes.  
*Research* : Then you can approximate the influence with **influence functions**. 
An influence function is a measure for how much the model parameters or predictions depend on the value of one of the training instances. 
Instead of deleting the instance, the method upweights the instance in the loss by a very small step. 
This method involves approximating the loss around the current model parameters using the gradient and Hessian matrix.
Loss upweighting is similar to deleting the instance.  
*You* : Great, that's what I was looking for!  

Koh and Liang (2017)[^koh] suggested to use influence functions, a method from robust statistics, to measure how an instance influences a prediction or model parameter.
Like deletion diagnostics, influence functions trace a prediction or model parameter back to the responsible training instance.
But instead of deleting training instances, the method measures how much the model changes when the instance is upweighted.

The proposed method requires access to the gradient of the loss of the model, which only works for methods where you have some loss function that you can differentiate with respect to some model parameters.
Logistic regression, neural networks and support vector machines work, tree-based methods like random forests don't work.
Influence function help for understanding the model behaviour, debugging the model and detecting errors with the dataset.

The following section explains the math behind influence functions.

### Math Behind Influence Functions

The key idea behind influence functions is to upweight the loss of a training instance by a small step $\epsilon$, which results in new model parameters:

$$\hat{\theta}_{\epsilon,z}=\arg\min_{\theta{}\in\Theta}(1-\epsilon)\frac{1}{n}\sum_{i=1}^n{}L(z_i,\theta)+\epsilon{}L(z,\theta)$$

where $\theta$ is the vector of the original model parameters and $\hat{\theta}_{\epsilon,z}$ the parameter vector after upweighting z by a very small number $\epsilon$.
L is the loss function used to train the model, $z_i$ are the training data and z is the training instance we upweight to simulate its removal.
The intuition behind this formula is:
How much does the loss change if we upweight a particular points $z_i$ from the training data by a little bit ($\epsilon$) and downweight the other data points respectively?
How would the parameter vector look like that optimizes this newly combined loss?
The change in the parameters can be calculated as

$$I_{\text{up,params}}(z)=\left.\frac{d{}\hat{\theta}_{\epsilon,z}}{d\epsilon}\right|_{\epsilon=0}=-H_{\hat{\theta}}^{-1}\nabla_{\theta}L(y,\hat{\theta})$$

The  last term $\nabla_{\hat{\theta}}L(z,\hat{\theta})$ is the gradient of the upweighted training instance. 
The gradient is the rate of change of the loss of the training (or test example) in various directions.
It tells us how much the loss changes, when we change the model parameters $\theta$ by a little bit.
A positive entry in the gradient vector means that a small increase in the corresponding model parameter increases the loss, a negative gradient means that the increase of the parameter decreases the loss.
The first part $H^{-1}_{\hat{\theta}}$ is the inverse Hessian matrix (second derivative of the loss with respect to the model parameters).
The Hessian is the rate of change of the gradient, or expressed in terms of the loss, it is the rate of change of the rate of change of the loss.
It can be estimated using: 

$$H_{\theta}=\frac{1}{n}\sum_{i=1}^n\nabla^2_{\hat{\theta}}L(z_i,\hat{\theta})$$

A more informal explanation is that the Hessian informs us how curved the loss is at a particular point. 
The Hessian is a matrix and not only a vector, because it describes the curvature of the loss and the curviness depends on the direction we are looking. 
The actual computation of the Hessian matrix is computationally expensive if you have many parameters.
Koh and colleagues suggested some tricks to compute it efficiently, which is beyond the scope of this chapter.
The update of the model parameters, as described by the above formula, is equivalent to taking a single Newton step after forming a quadratic expansion around the estimated model parameters.

This is basically forming a quadratic expansion around the parameters $\theta$.
We can also calculate what the new parameters would be through this linear approximation:

$$\hat{\theta}_{-z}\approx\hat{\theta}-\frac{1}{n}I_{\text{up,params}}(z)$$

That's basically the original parameter following the gradient of the loss of z (because we want to decrease the loss) scaled by the curvature (= divided by the hessian) and scaled by 1 over n, because that's the weight of a single training instance.

The following figure illustrates how the upweighting works.
The x-axis shows the value of the  parameter $\theta$ and the y-axis shows the corresponding value of the loss, but only for instance z and not the other instances.
The model parameter here is 1-dimensional for demonstration purposes, but in reality it is usually high-dimensional.
If we were to pick the optimal model parameter only according to this graphic, it would mean that  we pick a model parameter that is optimal for the training instance z, starting from the current model parameter, completely ignoring the other training instances.
That's why we only move a little into the direction of improvement for instance z.
We don't actually know how the loss will truly change when we upweight z, but with the first and second derivative of the loss, we create a quadratic approximation around our actual model parameter and pretend that this is how the real loss would behave.

```{r quadratic-expansion, fig.cap="Updating the model parameter (x-axis) by forming a quadratic expansion of the loss around the current model parameter, and going 1/n into the direction where the loss for the instance z change the most. This upweighting of instance z in the loss simulates what would happen if we were to delete z and train the model on the reduced data."}
x = seq(from = -1.2, to = 1.2, length.out = 100)
y.fun = function(x) {
  -x - 2*x^2 - x^3 + 2 * x^4
}
y = y.fun(x)
expansion = function(x, x0 = 0) {
  d1 = function(x) -1 - 2*x - 3 * x^2 +  8 * x^3
  d2 = function(x)    - 2   - 6 * x   + 24 * x^2
  y.fun(x0) + d1(x0) * (x - x0) + 0.5 * (x - x0)^2*d2(x0)
}

dat = data.frame(x=x, y=y)
dat$type = "Actual loss"
dat2 = dat
dat2$type = "Quadratic expansion"
dat2$y = expansion(x)
dat3 = rbind(dat, dat2)

#pts  = data.frame(x = c(0, 2/6))

ggplot(dat3) + geom_line(aes(x = x, y = y, group = type, color = type)) + 
  geom_vline(xintercept = 0, linetype = 2) + 
  geom_vline(xintercept = 1/2, linetype = 2) + 
  scale_y_continuous("Loss for instance z", labels = NULL, breaks = NULL) + 
  scale_x_continuous("Model parameter", labels = NULL, breaks = NULL) +
  geom_point(x = 0, y = expansion(x = 0)) + 
  geom_label(x = 0, label = expression(hat(theta)), y  = expansion(x=0), vjust = 2) + 
  geom_point(x = 1/2, y = expansion(x = 1/2)) + 
  geom_label(x = 1/2, y = expansion(x = 1/2), label = expression(hat(theta)[-z]), vjust = 2) + 
  geom_segment(x = 0, xend=1/2, y=1, yend=1, arrow = arrow(length = unit(0.2, "cm"))) + 
  geom_label(x = 0.25, y = 1.1, label = expression(-frac(1,n)~I[up~theta](z)), vjust = -0.2) + 
  my_theme()

```


We don't actually have to calculate the new parameters, but we derive from the influence function a measure of influence by upweighted training instance z.
Until now, we only talked about the influence on the model parameters.

How do the predictions change when we upweight training instance z?
For that we could either calculate the new parameters and then make predictions with the newly parameterized model, or we could also calculate the influence directly, since we can get the influence on predictions by using the chain rule.
Instead of looking at the influence of upweighting training instance on model parameters, we look at how the loss of a single prediction changes.

$$\begin{align*}I_{up,loss}(z,z_{test})&=\left.\frac{d{}L(z_{test},\hat{\theta}_{\epsilon,z})}{d\epsilon}\right|_{\epsilon=0}\\&=\left.\nabla_{\theta}L(z_{test},\hat{\theta})^T\frac{d\hat{\theta}_{\epsilon,z}}{d\epsilon}\right|_{\epsilon=0}\\&=-\nabla_{\theta}L(z_{test},\hat{\theta})^T{}H^{-1}_{\theta}\nabla_{\theta}L(z,\hat{\theta})\end{align*}$$

The first line of this equation means that we measure the influence of a training instance on a test prediction as the change of the loss of the test instance $z_{test}$ when we upweight the instance z and get new parameters $\hat{\theta}_{\epsilon,z}$.
For the second line of the equation, we applied the chain rule of derivatives and get the derivative of the loss of the test instance regarding the parameters times the influence on the parameters.
In the third line, we replace the expression with the result we got for the influence on the parameters.
The first term in the third line $\nabla_{\theta}L(z_{test},\hat{\theta})^T{}$ is the gradient of the test instance with respect to the model parameters.

Having a formula is great and the scientific and accurate way of showing things.
But I believe it's very important to get some intuition what the formula means.
The formula for $I_{\text{up,loss}}$ says that the influence of the training instance z on the prediction of an instance $z_{test}$ is "how strongly the instance reacts to a change in model parameter" times "how much the parameters will change if we upweight the instance z".
Not as difficult as the formulas suggest, right?
Another way to read the formula:
The influence is proportionate to how large the gradients for the training and test loss are.
The higher the gradient of the training loss, the higher its influence on the parameters and the higher the influence on the test prediction.
For the gradient of the test prediction loss: The higher the gradient, the more influenceable the test instance is.
The whole construct can also be seen as a measure of similarity (as learned by the model) between the training and the test instance.

That's it with theory and intuition.
The next section will explain how influence functions can be applied.

### Application of Influence Instances

Influential instances have lots of applications, some already presented in this chapter.

**Understanding model behaviour**

Different machine learning models have different ways of making their predictions.
Even if they have the same performance, their come up with a prediction from the features might be very different and because of that, they might fail in different scenarios.
Understanding the particular weak points of a model through the instances that are most influential helps to form a "mental model" of the machine learning model behaviour in your head.
The following figure shows an example where a support vector machine (SVM) and a neural network were trained to distinguish dog and fish images.
The most influential instances differed a lot for one exemplary image of a fish. 
For the SVM, instances were influential when they were similar in color. 
For the neural network, instances were influential when they were conceptually similar, but also one dog image was among the most influential images.

```{r example-influence-function-svm-inception, eval = FALSE, fig.cap = "Dog or fish? For the SVM prediction (middle row) images that are close in color with the test image were the most influential. For the neural network prediction (bottom row) fish in different setting were most influential, but also a dog image (top right). Work by Koh and Liang (2017)"}
knitr::include_graphics("images/influence-functions-svm-inception.png")
```


**Handling domain mismatch / Debugging model errors**

Handling domain mismatch is closely related to understanding the model behaviour better.
Domain mismatch means that the distribution of the training and the test data are different, which can cause the model to perform poorly on the test data.
Influence function can help to identify training instances that caused the error, 
Let's again assume you built a prediction model for the clinical outcome of patients. 
All these patients come from the same hospital. 
Now you use the model in another hospital and see that it does not work well for many patients.
Of course, you assume that the two hospitals have different patients and looking at their data reveals that they are differing in many features. 
But which features are the ones that 'broke' the model?
Again, influential instances are a great way to answer the question.
You take one of the new patient, for whom the model made a wrong prediction and find the most influential instances and analyse them.
For example this might reveal that the second hospital has on average older patients and the most influential instances from the training data are the few older patients from the first hospital and the model simply lacked the data to learn to predict well for this subgroup.
The conclusion would be that the model has to train on more patients that are older to work well in the second hospital.

**Fixing training data**

Given you have a time / financial budget for how many of your training instances you can check if all features and labels are correct, how do you make an efficient selection?
The best way is to pick the most influential instances, because - by definition - they affect the model the most.
So even if you would have some instance with obviously wrong values, but if they are non-influential and the only thing you need the data for is the prediction model, it's a better choice to check the influential instances.
For example, you train a prediction model for predicting if a patient should stay at the hospital or be released early.
You really want to make sure that the model is robust and makes correct predictions, because falsely releasing a patient early can have bad consequences.
Patient records can be very messy, so you don't have perfect confidence in the quality.
But also checking patient information and possibly correcting it can be very time consuming, because once you report the ids for the patients to check, the hospital has to actually send someone to look closer into the patients records, which might be handwritten and checking data for one patient might cost an hour or so.
Given this cost, it makes sense to only check a few important data instances.
The only method I can think of that makes sense for choosing are patients that had a high influence on the prediction model.
Koh et. al (2018) showed that this type of selections works way better than choosing randomly or choosing the ones with the highest loss / that were misclassified.



### Advantages of Identifying Influential Instances

- The deletion diagnostics and influence functions approach is very different from the mostly feature-perturbation based approaches presented in the [model-agnostic chapter](#agnostic). 
Looking at influential instances emphasizes the role the training data plays in the learning process.
This makes influence function and deletion diagnostics **one of the best debugging tools**.
From the techniques presented in this book, it's the only one that directly helps to identify the instances which should be checked for errors.
- **Deletion diagnostics are model-agnostic**, meaning they can be applied to any model. 
Also influence functions based on the derivatives can be applied to a broad class of models. 
That means we can use those methods to **compare different machine learning models** and understand their different behaviours better, going beyond comparing only the predictive performance.
- We didn't touch this topic in this chapter, but **influence functions via derivatives can also be used to create adversarial training data**. 
These are instances which are manipulated in a way that the model will fail to predict specific test instances, when the model is trained on those manipulated instances.
The difference to the methods in the [adversarial examples chapter](#adversarial) is that the attack happens here during training time, also known as poisoning attacks.
If you are interested in those, read the paper by Koh and Liang (2017).
- For the empirical influence function, we looked at the difference in the prediction and for the influence function we looked at the increase in loss. 
But, really, **the approach is generalizable** to 'what happens to ... when we delete/upweight instance z?', where you can fill '...' with any function of your model can be used. 
You could analyse how much a training instance influences the overall loss of the model.
You could analyse how much a training instance influences the feature importance.
You could analyse how much a training instance influences which feature is picked for the first split when training a [decision tree](#tree).

### Disadvantages of Identifying Influential Instances

- Deletion diagnostics are very **expensive to calculate** because it requires refitting.
When your model takes an hour to fit and you have 10,000 instances, then it will take longer than a year to sequentially compute the influence of all instance by leaving them out one after each other from the training data.
But also history has proven so far that computing resources increase all the time.
A computation that was resource-wise unthinkable 20 years ago, can be done by your smartphone easily.
You can train models with thousands of of training instances and hundreds of parameters within seconds/minutes on a laptop. 
So it's not a big step to say that deletion diagnostics will even work with big neural networks easily in 10 years.
- **Influence functions are a good alternative, but only for models with differentiable parameters**, like neural networks.
It doesn't work for tree-based methods like random forest, boosted trees or decision trees.
Even if you have models with parameters and a loss function, the loss might not be differentiable. 
But for the latter problem, there is a trick you can use:
use a differentiable loss as replacement for the calculation of the influence.
For example when the underlying model uses Hinge loss instead of some differentiable loss.
The loss is replaced with some smoothed loss for the influence functions, but the model can still be trained with the non-smooth loss.
- **Influence functions with the upweighting are only approximate**, because it is only a quadratic expansion around the parameters.
This means that it can be wrong and the influence of an instance is actually higher or lower when removed.
Koh and Liang (2017) showed for a few examples that the influence calculated through influence function was close to the measure you get when the model was actually recomputed after deletion of the instance.
But there is no guarantee that the approximation is good.
- A minor issue, but there is **no clear cutoff at which we call an influence of an instance influential or non-influential**.
But it's still useful to rank the instances among each other, but it would be great to have the means to not only rank the instances, but to actually distinguish between influential and non-influential.
For example, when you identify the top 10 most influential training instance for a test instance some of them might not be influential, because actually only the top 3 or something were really influential.
- The influence measures **only consider deletions of single instances** and not what happens when multiple points are deleted. 
Bigger groups of data instances could have some interactions with each other affecting the model training and the prediction heavily.
But the problem stems from combinatorics:
There are n possibilities for deleting a single instance from the data. 
There are already n times n-1 possibilities for deleting two instances from the training data. 
And you certainly don't want to start going any further, because there are just too many ways.

### Software and Alternatives

- Deletion diagnostics are very simple to implement yourself. Have a look at [the code](https://github.com/christophM/interpretable-ml-book/blob/master/manuscript/06.5-example-based-influence-fct.Rmd) I wrote for the examples in this chapter.
- For linear models and generalized linear models many influence measures like Cook's distance are implemented in R in the `stats` package.
- Koh and Liang published the Python code for influence functions from paper [in a repository](https://github.com/kohpangwei/influence-release).
That's great!
Unfortunately it is 'only' the code to the paper and not a maintained and documented Python module.
The code is focused on the Tensorflow library, so you couldn't directly use it for black box models using other frameworks, like sci-kit learn.
- Here is a [great blog post for influence functions](http://mlexplained.com/2018/06/01/paper-dissected-understanding-black-box-predictions-via-influence-functions/#more-641), which helped me understand this paper better. 
The blog post goes a bit deeper into the mathematics behind influence function for black box models and also talks about some of the mathematical 'tricks' used to implement the method efficiently.


[^koh]: Koh, P. W., & Liang, P. (2017). Understanding Black-box Predictions via Influence Functions. Retrieved from http://arxiv.org/abs/1703.04730
















<!-- **Excursion: Intuition behind the gradient and the Hessian matrix** -->

<!-- Intuition behind the gradient: The gradient is the rate of change of the loss of the training (or test example) in various directions. -->
<!-- It tells us how much the loss changes, when we change the model parameters $\theta$ by a little bit. -->
<!-- Since the model usually has many parameters, the gradient also is a vector with many numbers, one for each parameter, telling for each parameter the rate and direction of change of the loss if we were to change the parameter. -->
<!-- A positive entry in the gradient vector means that a small increase in the corresponding model parameter increases the loss, a negative gradient means that the increase of the parameter decreases the loss. -->
<!-- The Hessian is the second-order derivative. -->
<!-- What the gradient is to the loss, is the hessian to the gradient.  -->
<!-- The Hessian is the rate of change of the gradient, or expressed in terms of the loss, it is the rate of change of the rate of change of the loss. -->
<!-- A more informal explanation is that the Hessian informs us how curved the rate of change is.  -->
<!-- The Hessian is a matrix because to describe the curvature of the loss and that slope depends on the direction we are looking.  -->
<!-- A real world example: A potato chip. -->
<!-- Not the irregular ones, but the ones where each chip has the same shape, which you can stack.  -->
<!-- If you lay such a chip on a surface, such that the ends on off the ends of the longer side touch the ground, we will see how it relates to gradients and the Hessian. -->
<!-- The long end points away from you, so that you could now put your fingers on the left and right elevated parts of the chip and wiggle it, but be careful not to break it. -->
<!-- Have a look at the next picture to see what I mean. -->

<!-- The chip is 3-dimensional and will be a good example. -->
<!-- Assume that each point on the chip describes the combiniation of two model parameters, represented as the coordinate in the direction of the long side and the coordinate in the direction of the short side. -->
<!-- The height of a point on the chip represents the value of the loss function. -->
<!-- Now, if we start on the left of the elevated short sides of the chip, the gradient of the loss (height of the chip) would be negative, since going from left to right by a small step would decrease the height/loss.  -->
<!-- In the middle of the chip, there is a gradient of zero and on the right a positive gradient for the direction. -->
<!-- But this was only one component of the gradient.  -->
<!-- The other component is in the longer direction of the chip.  -->
<!-- Coming from your body, the gradient is first positive, then zero, then negative. -->
<!-- Now for the Hessian: It describes how curvy the chip is or also the rate of change of the gradient.  -->
<!-- Since we have two parameters (two directions), we have to describe the curviness regarding the direction we are looking. -->
<!-- Let's again start at the left of the short sides of the chips, with the the high negative gradient.  -->
<!-- How curvy is the chip? -->
<!-- Well, that depends whether you look into the direction towards the other short end or into the direction of the long ends. -->
<!-- The Hessian or curviness is very high for going from short edge to short edge, because there is much curviness between the short edges.  -->
<!-- The curviness  -->
<!-- But being on a short edge and going into one  -->


<!-- Let's say the height of the chip is the loss, the coordinate along the back of the from back to neck is one parameter, and the coordinate of the saddle from left flank to right flank is the second parameter. -->
<!-- Going from left to right, the saddle first increases in height and than decreases. -->
<!-- A high rate of change of the gradient (rate of rate of increase of height), so there is a positive Hessian for short direction, when looking into the short direction.  -->
<!-- But how much does the gradient change for the short direction, when we look into the long direction? -->
<!-- That is, when we go to a certain coordinate on the chip, fix the position of the short side, but check how much the gradient changes, when we change the coordinate in the long direction.  -->
<!-- The answer is 0. -->

<!-- We assume the chip's height follows the following function of the position on the short side and the position on the long side. -->
<!-- Let's call the position on the short side s and the position on the long side l: -->

<!-- $$\text{Height}=s^2-l^2+2$$ -->
<!-- The 2 is just used so that the chip starts at zero and not a negative value. -->

<!-- The gradient of the chip looks like this: -->

<!-- $$\nabla\text{Height}=\begin{pmatrix}\frac{d\text{Height}}{ds}\\\frac{d\text{Height}}{dl}\end{pmatrix}=\begin{pmatrix}2s\\-2l\end{pmatrix}$$ -->

<!-- This means that going from left to write short end, we increase the gradient and going from body pointing long end to the one long end further away, we decrease the rate at which the height increases. -->
<!-- This gradient depends on the position on the chip, which shows that for the chip the gradient is dependent on where we are on the chip, which means that the Hessian matrix will not only contain zeros, but there is a changing rate of change of the height. -->


<!-- The Hessian for our chip looks like this:  -->

<!-- $$H=\begin{pmatrix}\frac{d^2\text{Height}}{dsds}&\frac{d^2\text{Height}}{dsdl}\\\frac{d^2\text{Height}}{dsdl}&\frac{d^2\text{Height}}{dldl}\end{pmatrix}=\begin{pmatrix}2&0\\0&-2\end{pmatrix}$$ -->



<!-- ```{r potato-chip, fig.cap = "Created by Topntp26 - Freepik.com"} -->
<!-- img <- jpeg::readJPEG("images/potato-chips.jpg") -->
<!-- p1 <- grid::rasterGrob(img, interpolate=TRUE) -->

<!-- df = expand.grid(long = seq(from = -1.5, to = 1.5, length.out = 10), -->
<!--   short = seq(from = -1, to = 1, length.out = 10)) -->

<!-- ## Adding to for elevation -->
<!-- df$height = 2 + df$short^2  - df$long^2   -->

<!-- p2 = ggplot(df) +  -->
<!--   geom_tile(aes(x = short, y = long, fill = height)) +  -->
<!--   scale_x_continuous("Short side of chip") +  -->
<!--   scale_y_continuous("Long side of chip") +  -->
<!--   scale_fill_continuous("Height of chip") +  -->
<!--   coord_fixed() -->
<!-- gridExtra::grid.arrange(p1, p2, ncol = 2) -->
<!-- ``` -->


<!-- **End of excursion** -->




