## Influential Instances

```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(43)

influence.matrix.filename = "../data/influence-df.RData"
data("cervical")
```

<!-- Intro text -->
Machine learning models are - ultimately - a product of the training data.
Deleting one of the training instances can impact the resulting model.
We call a training instance "influential" when its deletion from the training data considerably changes the model's parameters or predictions.
By identifying influential training instances we can 'debug' machine learning models and explain their behavior and predictions better.


*Keywords: Influential instances, influence function, leave-one-out analysis, Cook's distance, deletion diagnostics, robust statistics*


This chapter will talk about the difference between an outlier and an influential instances and show you two approaches for identifying influential instances.
Both approaches originate from robust statistics and relate to a concept called 'influence functions'.
Robust statistics provides statistical methods which are less influenced by outliers or by violations of model assumptions.
Robust statistics also provides methods to measure how robust estimations from data are (like a mean estimate or the the weights of a prediction model).
One of these methods is the influence function, which measures how strongly an estimation is influenced by a single data point.

Imagine you want to estimate the mean income of people in your city and ask ten random people on the street how much they earn.
Apart from your sample being probably really bad, how much can your mean income estimate be influenced by a single person?
The influence function can tell you how much the mean is affected when one of the income statements is an outlier. 
By the way, the answer is that your mean estimate can be influenced very strongly by a single answer, because the mean scales linearly with single values.
A more robust choice is the median (the value at which one half of the people earns more and the other half less), because even if the highest earning person in your sample would earn 10 times more, the resulting median would not change.

The first approach presented in this chapter tackles the question how to measure influence by simply removing an instance from the data and recalculating the statistic or the model.
For the income example, we would re-calculate the mean 10 times while leaving out one of the income statements each time and measuring how much the mean estimate changes. 
A big change means that an instance was very influential.

The second approach upweights one of the data points by a small weight, which is the same as calculating the first derivative of a statistic or model.
This approach is also called 'infinitesimal approach' or 'influence function'.

Before we start digging into these two approaches for finding influential instances, we will look into√ü the difference between an outlier and an influential instance.

**Outlier**

An outlier is an instance which is far away from the other instances in the dataset.
"Far away" means that the distance, for example the Euclidean distance, to all the other instances is very large.
In a dataset of body measurements of newborns, a newborn that weights 6kg would be considered an outlier.
In a dataset of bank accounts, where most are checking accounts, a dedicated loan account (large negative balance, only few transactions) would be considered an outlier.
The following figure shows an outlier for a 1-dimensional distribution.

```{r outlier, fig.cap = "The feature x follows a Gaussian distribution with the exception of the outlier x=8."}
set.seed(42)
n = 50
x = rnorm(mean = 1, n = n)
x = c(x, 8)
y = rnorm(mean = x, n = n)
y = c(y, 7.2)
df = data.frame(x, y)
ggplot(df) + geom_histogram(aes(x = x)) + my_theme() + 
  scale_x_continuous("feature x") + 
  scale_y_continuous("count")
```


Outliers might or might not be interesting data points (like [criticisms or prototypes](#proto)).
Outliers can be influential on the model training, but don't have to be. 
Influential instances are defined differently.

**Influential Instance**

An influential instance is a data instance whose removal has a strong effect on the trained model.
The more the model parameters or the predictions of a model change, when the model is retrained with an instance removed from the training data, the more influential this particular instance is. Whether an instance is influential for a trained model also depends on its value for the target y.
The following figure shows an influential instance for a linear model.


```{r influential-point, fig.cap = "A linear model with one featured trained once on the full data and once without the influential instance. Removing the influential instance changes the fitted slope (weight/coefficient) drastically."}

df2 = df[-nrow(df),]
df3 = rbind(df2, data.frame(x = 8, y = 0))

df3$regression_model = "with influential instance"
df2$regression_model = "without influential instance"
df.all = rbind(df2, df3)


text.dat = data.frame(x = c(8), y = c(0), lab = c("Influential instance"), regression_model = "with influential instance")

ggplot(df.all, mapping = aes(x = x, y = y, group = regression_model, linetype = regression_model)) + 
  geom_point(size = 2) + 
  geom_smooth(method='lm',formula=y~x, se = FALSE, aes(color = regression_model), fullrange = TRUE) + 
  my_theme() + 
  geom_label(data = text.dat, aes(label = lab), hjust = 1, nudge_x = -0.2, vjust = 0.3) +
  scale_x_continuous("Feature x") + 
  scale_y_continuous("Target y") + 
  scale_color_discrete("Training data") + 
  scale_linetype_discrete("Training data")

```



**How do influential instances help to understand the model? A motivation.**

The key idea behind influential instances for interpretability is to trace predictions back to were it all began: the training data.
A learner, which is the algorithm that produces the machine learning model is a function that takes in training data consisting of features X and the target y and produces a machine learning model.
For example, the learner of a decision tree is an algorithm that selects the split features and the values at which to split. 
A learner for a neural network uses backpropagation to find the best weights.


```{r learner, fig.cap = "A learner learns a model from training data (features plus target). The model makes predictions for new data.", out.width="\textwidth"}
knitr::include_graphics("images/learner.png")
```

We ask how the model parameters or the predictions would change when we remove instances from the training data in the training process.
This is in contrast to other interpretability approaches that analyze how the prediction changes when we manipulate the features of the instances to be predicted, like [partial dependence plots](#pdp) or [feature importance](#feature-importance).
With influential instances, we don't treat the model as fixed, but as a function of the training features and of the target
Influential instances help us answer questions about the global model behavior and about single predictions.
Which were the most influential instances for the model parameters or the predictions overall?
Which were the most influential instances for the prediction of a particular instance?
Great! But how, exactly, can we find those influential instances?
We have two means of measuring the influence:
Our first option is to delete the instance from the training data, retrain the model on the reduced training dataset and observe the difference in either model parameters or the predictions (either locally or globally).
The second option is to upweight a data instance using an approximation of parameter changes based on the gradients of the model parameters.
The deletion approach is easier to understand and motivates the upweighting approach, so we start with the former.

### Deletion Diagnostics

When facing a problem, it can be useful to see if other people have already solved a similar problem. 
In the area of influential instances, statistician have already done a lot of research, especially for (generalized) linear regression models.
When you search for "influential observations", the first search results are about measures like DFBETA and Cook's distance.
**DFBETA** measures the effect of deleting an instance on the model parameters.
**Cook's distance** measures the effect of deleting an instance on the model predictions.
For both measures, we have to retrain the model repeatedly while leaving out certain instances.
The model with all instances is compared to the one without in terms of the difference in parameters or predictions.

DFBETA is defined as:

$$DFBETA_{i}=\beta-\beta^{(-i)}$$

where $\beta$ is the vector with the model parameters when the model is trained on all data instances, and $\beta^{(-i)}$ the model parameters when the model is trained without instance i.
Quite intuitive I would say.
DFBETA only works for models with weight parameters, like logistic regression or neural networks, but not for models like decision trees, tree ensembles, some support vector machines and so on.

Cook's distance was invented for linear regression models and approximations for generalized linear regression models exist.
Cook's distance for a training instance is defined as the (scaled) sum of the differences in the predicted outcome when the i-th instance is removed from the model training.

$$D_i=\frac{\sum_{i=1}^n(\hat{y}_j-\hat{y}_{j}^{(-i)})^2}{p\cdot{}MSE}$$

where the numerator is the squared difference between prediction of model with and without the i-th instance, summed over the dataset.
The denominator is the number of features p times the mean squared error.
The denominator is the same for no matter which instance i is removed.
Cook's distance tells us how much the predicted output of a linear model changes when we remove the i-the instance in the training.


Can we use Cook's distance and DFBETA for any machine learning model?
DFBETA requires model parameters, so this measure works for parameterized models.
Cook's distance does not require model parameters.
Interestingly, Cook's distance is usually not seen outside the context of linear models and generalized linear models, but the idea of taking the difference of some of the model before and after removal of a particular instance is very general.
One problem of the definition of Cook's distance is the MSE, which is not meaningful for all types of predictions models (e.g. classification).

The most simplistic influence measure for the effect on the model predictions can be written as: 

$$\text{Influence}^{(-i)}=\frac{1}{n}\sum_{j=1}^{n}\left|\hat{y}_j-\hat{y}_{j}^{(-i)}\right|$$

This expression is basically the numerator of Cook's distance, with the difference of summing the absolute difference instead of the differences.
But this was a choice I made, because it makes sense for the examples later.
The general form of deletion diagnostic measures is to choose a measure (like the sum of absolute differences) and to calculate the difference of the measure for the model trained and all instances and when the influential instance is deleted.
Also, we can easily break the influence down locally to explain for the prediction of instance j what the influence of the i-th training instance was:

$$\text{Influence}_{j}^{(-i)}=\left|\hat{y}_j-\hat{y}_{j}^{(-i)}\right|$$

Here I opted to formulate the influence on the prediction.
This would also work for the difference in model parameters or the difference in the loss.
We will use these simple influence measures in the following example.

**Example with deletion diagnostics**

In the following example, we train a support vector machine to predict [cervical cancer](#cervical) given risk factors and we measure which training instances were the most influential overall and also for a particular prediction.
Since predicting cancer is a classification problem, we measure the influence as the difference in predicted probability for cancer.
An instance is influential when the predicted probability strongly increases or decreases on average in the dataset when the instance is removed from the model training.
Measuring influence for all `r nrow(cervical)` training instances of the cervical cancer training data requires to train the model once on all training data and retrain it `r nrow(cervical)` times (= size of training data) with one of the instances removed each time.

```{r influence, eval = !file.exists(influence.matrix.filename)}
#'@param predicted The predicted outcome of a model
#'@param predicted.without The predicted outcome of a model with a data point removed
influence.v = function(predicted, predicted.without) {
  predicted - predicted.without
}

influence.matrix = matrix(NA, ncol = nrow(cervical), nrow = nrow(cervical))

lrn = makeLearner("classif.svm", predict.type = "prob")
tsk = makeClassifTask(data = cervical, target = "Biopsy")
mod = train(lrn, tsk)
predicted.orig = getPredictionProbabilities(predict(mod, newdata = cervical))
cs = lapply(1:nrow(cervical), function(to.remove.index) {
  mod.2 = train(lrn, tsk, subset = setdiff(1:nrow(cervical), to.remove.index))
  predict.removed = getPredictionProbabilities(predict(mod.2, newdata = cervical))
  influence.v(predicted.orig, predict.removed)
})

# Column: Removed instance, row: influenced instance
influence.df = data.frame(cs)
influence.df = as.matrix(influence.df)
diag(influence.df) = NA
save(influence.df, predicted.orig, file = influence.matrix.filename)
```

```{r}
load(influence.matrix.filename)
df = data.frame(influence = colMeans(abs(influence.df), na.rm = TRUE), id = 1:nrow(cervical))
df = df[order(df$influence, decreasing = TRUE),]
```

The most influential instance has an  influence measure of approximately `r sprintf("%.2f", abs(df[1,"influence"]))`).

An influence of `r sprintf('%.2f', abs(df[1,"influence"]))` means that when we remove the `r df$id[1]`-th instance, the predicted probability changes on average by `r sprintf('%.1f', 100 * df[1,"influence"])` percentage points.
This is quite substantial considering the average predicted probability for cancer is `r sprintf('%.1f', 100 *mean(predicted.orig))`%.
The mean of influence measures over all possible deletions is `r sprintf('%.1f', 100 * mean(abs(df$influence)))` percentage points.
Now we know which of the data instances were most influential for the model.
This is already useful for debugging the data.
Is there some problematic instance?
Are there measurement errors?
The influential instances are the first instances to check for mistakes, because any mistake in them influences the model predictions a lot.

Apart from model debugging, can we learn anything to understand the model better?
Simply printing the top 10 most influential instances is not very useful, because that's just a table of instances with lots of features.
All the methods that yield instances as output are only meaningful when we have a good way of representing an instance.
But we can better understand what types of instances are influential by asking 
what sets an influential instance apart from non-influential instances?
We can turn this question into a regression problem and model the influence of an instance as a function of its feature values. 
We are free to choose any model from the [interpretable machine learning models chapter](#simple).
For this example I chose a decision tree (following figure), which reveals that data from women of age 35 and above were the most influential for the support vector machine.
From all the women in the dataset  `r sum(cervical$Age >= 35)` out of `r nrow(cervical)` were older than 35.
In the [partial dependence chapter](#pdp) we have seen that after 40, there is a sharp increase in predicted cancer probability and the [feature importance](#feature-importance) has also detected age as one of the more important features.
The influence analysis tells us that the model becomes increasingly more unstable when predicting cancer for higher ages.
This is in itself valuable information. 
It means that mistakes in the data for one of those instances can have a strong effect on the model.

```{r cooks-analyzed, eval=TRUE, fig.cap = "A decision tree modeling the relationship between the influence of an instance and its features. The maximal depth of the tree is set to 2."}
df.cervical  = cbind(df, cervical)
ct = rpart(abs(influence) ~ . -id, data = df.cervical, control = rpart.control(maxdepth = 2))
ct = as.party(ct)
plot(ct, inner_panel = node_inner(ct, pval = FALSE), type='simple')
```

```{r influence-single-prepare}
i = which(predicted.orig == max(predicted.orig))
```


This first influence analysis asked which was the *overall* most influential instance.
Now we pick out one of the instances, namely the `r i`-th instance, for which we want to explain the prediction by finding the most influential training data instances.
This is like asking a counterfactual question:
How would the prediction for instance `r i` change if we left out instance i from the training process.
We repeat this removal for all instances.
Then we choose the training instances which yield the biggest change in the prediction of instance `r i` when left out of the training and use those to explain the prediction of the model for this instance.
I picked the instance `r i` to be explained by choosing the instance which had the highest predicted probability of cancer (`r sprintf('%.2f', 100 * predicted.orig[i])`%), which I thought might be an interesting case to analyse more deeply.
We could return the, let's say, top 10 most influential instances for the prediction of the `r i`-the instance, printed as a table.
Not very useful, because we wouldn't be able to see much.
Again it is more useful to find out what sets the influential instances apart from the non-influential instances by analysing their features.
We use a decision tree for that, which is trained to predict the influence given the features, but, really, we only misuse it to find some structure and not to actually predict anything.
The following decision tree reveals what kind of training instances where the most influential for the prediction of the `r i`-th instance. 


```{r influence-single, fig.cap = sprintf("Decision tree explaining which instances were the most influential for predicting the %i-th instance. Data of women who smoked for 19 years or longer had a very influence on the prediction of the %i-th instance, with an average change in absolute prediction by 11.7 percentage points of cancer probability. Maximal depth of the tree is set to 2.", i, i)}
obs = influence.df[i,]
cervical.200 = cervical
cervical.200$influence = unlist(obs)
#cervical.200 = na.omit(cervical.200)
worst.case.index = which(abs(cervical.200$influence) == max(abs(cervical.200$influence), na.rm = TRUE))
ct = rpart(abs(influence) ~ ., data = cervical.200, control = rpart.control(maxdepth = 2))
ct = as.party(ct)
plot(ct, inner_panel = node_inner(ct, pval = FALSE), type='simple')
```

Instances, where women smoked for 19 years or longer have a high influence on the prediction of the `r i`-th instance.
The woman behind the `r i`-th instance has smoked for `r cervical$Smokes..years.[i]` years.
In the data, `r sprintf('%.2f', 100 * mean(cervical$Smokes..years >= 19))`% of women smoked 19 years or longer, which amounts to only `r sum(cervical$Smokes..years >= 19)` subjects.
Any error that happened in surveying the number of years smoking of one of these women will have a tremendous effect on the predicted outcome for the `r i`-th instance.

The most extreme change in prediction happens when we remove the `r worst.case.index`-th instance.
She has supposedly smoked for `r cervical$Smokes..years.[worst.case.index]` years, confirming our findings from the decision tree.
The predicted probability for the `r i`-th instance changes from `r sprintf('%.2f', 100 * predicted.orig[i])` percentage points to `r sprintf('%.2f', 100 * (predicted.orig[i]  - cervical.200$influence[worst.case.index]))` percentage points, when we remove the `r worst.case.index`-th instance!!!


When we look closer at the features of the most influential instance, we can see another possible problem.
The data says that the woman is 28 years old, and has already been smoking for 22 years. 
Either it's a really extreme case and she truly started smoking at 6 or this is a data error. 
I tend to believe the latter.
This is certainly a situation where we have to question the correctness of the data.

These examples demonstrated how useful identifying influential instances is for debugging models.
One problem with the proposed approach is that the model has to be retrained for each training point.
The whole retraining can rather slow, because if you have thousands of training samples, you have to retrain your model thousands of times.
Assuming the model takes a day to train and you have 1000 training examples, then - without parallelization - the computation of influential instances will take almost 3 years.
Ain't nobody got time for that.
In the remaining part of the chapter I will show you a method that doesn't require retraining the model.


### Influence Functions

*You* : I would like to know the influence of a training instance on a particular prediction.  
*Research* : You can delete the training instance, retrain the model, and measure the difference in prediction.  
*You* : Great! But do you also have a method for me that works without retraining model? It takes so much time.  
*Research* : Do you have a model with a loss function that is twice differentiable with respect to its model parameters?  
*You* : I trained a neural network with the logistic loss. So yes.  
*Research* : Then you can approximate the influence of the instance on the model parameters and on the prediction with **influence functions**. 
The influence function is a measure for how much the model parameters or predictions depend on a training instance. 
Instead of deleting the instance, the method upweights the instance in the loss by a very small step. 
This method involves approximating the loss around the current model parameters using the gradient and Hessian matrix.
Loss upweighting is similar to deleting the instance.  
*You* : Great, that's what I was looking for!  

Koh and Liang (2017)[^koh] suggested to use influence functions, a method from robust statistics, to measure how an instance influences model parameter or a prediction.
Like deletion diagnostics, influence functions trace the model parameters and predictions back to the responsible training instance.
But instead of deleting training instances, the method approximates how much the model changes when the instance is upweighted.

The influence functions method requires access to the gradient of the loss with respect to the model parameters, which only works for a subset of machine learning models.
Logistic regression, neural networks and support vector machines qualify, tree-based methods like random forests don't.
Influence functions help for understanding the model behavior, debugging the model and detecting errors with the dataset.

The following section explains the intuition and math behind influence functions.

### Math Behind Influence Functions

The key idea behind influence functions is to upweight the loss of a training instance by a (infinitesimally) small step $\epsilon$, which results in new model parameters:

$$\hat{\theta}_{\epsilon,z}=\arg\min_{\theta{}\in\Theta}(1-\epsilon)\frac{1}{n}\sum_{i=1}^n{}L(z_i,\theta)+\epsilon{}L(z,\theta)$$

where $\theta$ is model parameters vector and $\hat{\theta}_{\epsilon,z}$ the parameter vector after upweighting z by a very small number $\epsilon$.
L is the loss function used to train the model, $z_i$ are the training data and z is the training instance we upweight to simulate its removal.
The intuition behind this formula is:
How much does the loss change if we upweight a particular points $z_i$ from the training data by a little bit ($\epsilon$) and downweight the other data instance accordingly?
How would the parameter vector look like that optimizes this newly combined loss?
The influence function of the parameters, which is the influence of upweighting training isntance z on the parameters, can be calculated as

$$I_{\text{up,params}}(z)=\left.\frac{d{}\hat{\theta}_{\epsilon,z}}{d\epsilon}\right|_{\epsilon=0}=-H_{\hat{\theta}}^{-1}\nabla_{\theta}L(z,\hat{\theta})$$

The  last term $\nabla_{\theta}L(z,\hat{\theta})$ is the gradient of the loss for the upweighted training instance with respect to the parameters.
The gradient is the rate of change of the loss of the training instance in various directions.
It tells us how much the loss changes when we change the model parameters $\hat{\theta}$ by a bit.
A positive entry in the gradient vector means that a small increase in the corresponding model parameter increases the loss, a negative entry means that the increase of the parameter decreases the loss.
The first part $H^{-1}_{\hat{\theta}}$ is the inverse Hessian matrix (second derivative of the loss with respect to the model parameters).
The Hessian matrix is the rate of change of the gradient, or expressed in terms of the loss, it is the rate of change of the rate of change of the loss.
It can be estimated using: 

$$H_{\theta}=\frac{1}{n}\sum_{i=1}^n\nabla^2_{\hat{\theta}}L(z_i,\hat{\theta})$$

More informally, the Hessian matrix captures how curved the loss is at a particular point. 
The Hessian is a matrix and not only a vector, because it describes the curvature of the loss and the curviness depends on the direction we are looking into. 
The actual computation of the Hessian matrix is time consuming if you have many parameters.
Koh and colleagues suggested some tricks to compute it efficiently, which is beyond the scope of this chapter.
The update of the model parameters, as described by the above formula, is equivalent to taking a single Newton step after forming a quadratic expansion around the estimated model parameters.

What's the intuition behind this influence function formula?
The formula comes from forming a quadratic expansion around the parameters $\hat{\theta}$.
That means we don't actually know or it's to complex to calculate how the loss of instance z will change exactly when removed/upweighted.
That's why we approximate the function locally by using information about the steepness (= gradient) and the curviness (= Hessian matrix) at the current model parameter setting.
With this loss approximation, we can calculate what the new parameters would approximately look like, when we upweight instance z:

$$\hat{\theta}_{-z}\approx\hat{\theta}-\frac{1}{n}I_{\text{up,params}}(z)$$

The approximates parameters vector is basically the original parameter subtracting the gradient of the loss of z (because we want to decrease the loss) scaled by the curvature (= multiplied by the inverese Hessian matrix) and scaled by 1 over n, because that's the weight of a single training instance.

The following figure illustrates how the upweighting works.
The x-axis shows the value of the  parameter $\theta$ and the y-axis shows the corresponding value of the loss, with upweighted instance z.
The model parameter here is 1-dimensional for demonstration purposes, but in reality it is usually high-dimensional.
We only move a 1 over n into the direction of improvement for instance z.
We don't know how the loss will truly change when we upweight z, but with the first and second derivative of the loss, we create this quadratic approximation around our actual model parameter and pretend that this is how the real loss would behave.

```{r quadratic-expansion, fig.cap="Updating the model parameter (x-axis) by forming a quadratic expansion of the loss around the current model parameter, and going 1/n into the direction where the loss for the instance with upweighted instance z (y-axis) improves the most. This upweighting of instance z in the loss approximates the parameter changes if we were to delete z and train the model on the reduced data."}
x = seq(from = -1.2, to = 1.2, length.out = 100)
y.fun = function(x) {
  -x - 2*x^2 - x^3 + 2 * x^4
}
y = y.fun(x)
expansion = function(x, x0 = 0) {
  d1 = function(x) -1 - 2*x - 3 * x^2 +  8 * x^3
  d2 = function(x)    - 2   - 6 * x   + 24 * x^2
  y.fun(x0) + d1(x0) * (x - x0) + 0.5 * (x - x0)^2*d2(x0)
}

dat = data.frame(x=x, y=y)
dat$type = "Actual loss"
dat2 = dat
dat2$type = "Quadratic expansion"
dat2$y = expansion(x)
dat3 = rbind(dat, dat2)

#pts  = data.frame(x = c(0, 2/6))

ggplot(dat3) + 
  geom_line(aes(x = x, y = y, group = type, color = type, linetype = type)) + 
  geom_vline(xintercept = 0, linetype = 2) + 
  geom_vline(xintercept = 1/2, linetype = 2) + 
  scale_y_continuous("Loss with upweighted instance z", labels = NULL, breaks = NULL) + 
  scale_x_continuous("Model parameter", labels = NULL, breaks = NULL) +
  geom_point(x = 0, y = expansion(x = 0)) + 
  geom_label(x = 0, label = expression(hat(theta)), y  = expansion(x=0), vjust = 2) + 
  geom_point(x = 1/2, y = expansion(x = 1/2)) + 
  geom_label(x = 1/2, y = expansion(x = 1/2), label = expression(hat(theta)[-z]), vjust = 2) + 
  geom_segment(x = 0, xend=1/2, y=1, yend=1, arrow = arrow(length = unit(0.2, "cm"))) + 
  geom_label(x = 0.25, y = 1.1, label = expression(-frac(1,n)~I[up~theta](z)), vjust = -0.2) + 
  my_theme()

```


We don't actually have to calculate the new parameters, but we can use the influence function as a measure of influence of z on the parameters.

How do the *predictions* change when we upweight training instance z?
For that we could either calculate the new parameters and then make predictions with the newly parameterized model, or we can also calculate the influence directly, since we can calculate the influence of instance z on predictions by using the chain rule:

$$\begin{align*}I_{up,loss}(z,z_{test})&=\left.\frac{d{}L(z_{test},\hat{\theta}_{\epsilon,z})}{d\epsilon}\right|_{\epsilon=0}\\&=\left.\nabla_{\theta}L(z_{test},\hat{\theta})^T\frac{d\hat{\theta}_{\epsilon,z}}{d\epsilon}\right|_{\epsilon=0}\\&=-\nabla_{\theta}L(z_{test},\hat{\theta})^T{}H^{-1}_{\theta}\nabla_{\theta}L(z,\hat{\theta})\end{align*}$$

The first line of this equation means that we measure the influence of a training instance on a particular prediction $z_{test}$ as the change of the loss of the test instance when we upweight the instance z and get new parameters $\hat{\theta}_{\epsilon,z}$.
For the second line of the equation, we applied the chain rule of derivatives and get the derivative of the loss of the test instance with respect to the the parameters times the influence of z on the parameters.
In the third line, we replace the expression with the influence function for the parameters.
The first term in the third line $\nabla_{\theta}L(z_{test},\hat{\theta})^T{}$ is the gradient of the test instance with respect to the model parameters.

Having a formula is great and the scientific and accurate way of showing things.
But I believe it's very important to get some intuition what the formula means.
The formula for $I_{\text{up,loss}}$ says that the influence function of the training instance z on the prediction of an instance $z_{test}$ is "how strongly the instance reacts to a change in model parameter" times "how much the parameters will change if we upweight the instance z".
Another way to read the formula:
The influence is proportionate to how large the gradients for the training and test loss are.
The higher the gradient of the training loss, the higher its influence on the parameters and the higher the influence on the test prediction.
The higher the gradient of the test prediction, the more influenceable the test instance is.
The whole construct can also be seen as a measure of similarity (as learned by the model) between the training and the test instance.

That's it with theory and intuition.
The next section will explain how influence functions can be applied.

### Application of Influence Functions

Influence functions have lots of applications, some already presented in this chapter.

**Understanding model behavior**

Different machine learning models have different ways of making predictions.
Even if two models have the same performance, their way to come up with a prediction from the features might be very different and because of that, they might fail in different scenarios.
Understanding the particular weak points of a model through identifying influential instances helps to form a "mental model" of the machine learning model behavior in your head.
The following figure shows an example where a support vector machine (SVM) and a neural network were trained to distinguish images of dogs and fish.
The most influential instances for one exemplary image of a fish differed a lot. 
For the SVM, instances were influential when they were similar in color. 
For the neural network, instances were influential when they were conceptually similar.
But also one dog image was among the most influential images for the neural network, showing that it learned the concepts rather than the Euclidean distance in color space.

```{r example-influence-function-svm-inception, eval = FALSE, fig.cap = "Dog or fish? For the SVM prediction (middle row) images that are close in color with the test image were the most influential. For the neural network prediction (bottom row) fish in different setting were most influential, but also a dog image (top right). Work by Koh and Liang (2017)"}
knitr::include_graphics("images/influence-functions-svm-inception.png")
```


**Handling domain mismatch / Debugging model errors**

Handling domain mismatch is closely related to understanding the model behavior better.
Domain mismatch means that the distribution of the training and the test data are different, which can cause the model to perform poorly on the test data.
Influence function can help to identify training instances that caused the error, 
Let's again assume you built a prediction model for the clinical outcome of patients. 
All these patients come from the same hospital. 
Now you use the model in another hospital and see that it does not work well for many patients.
Of course, you assume that the two hospitals have different patients and looking at their data reveals that they are differing in many features. 
But which features are the ones that 'broke' the model?
Again, influential instances are a great way to answer the question.
You take one of the new patient, for whom the model made a wrong prediction and find the most influential instances and analyse them.
For example this might reveal that the second hospital has on average older patients and the most influential instances from the training data are the few older patients from the first hospital and the model simply lacked the data to learn to predict well for this subgroup.
The conclusion would be that the model has to train on more patients that are older to work well in the second hospital.

**Fixing training data**

Given you have a time / financial budget for how many of your training instances you can check if all features and labels are correct, how do you make an efficient selection?
The best way is to pick the most influential instances, because - by definition - they affect the model the most.
So even if you would have some instance with obviously wrong values, but if they are non-influential and the only thing you need the data for is the prediction model, it's a better choice to check the influential instances.
For example, you train a prediction model for predicting if a patient should stay at the hospital or be released early.
You really want to make sure that the model is robust and makes correct predictions, because falsely releasing a patient early can have bad consequences.
Patient records can be very messy, so you don't have perfect confidence in the quality.
But also checking patient information and possibly correcting it can be very time consuming, because once you report the ids for the patients to check, the hospital has to actually send someone to look closer into the patients records, which might be handwritten and checking data for one patient might cost an hour or so.
Given this cost, it makes sense to only check a few important data instances.
The only method I can think of that makes sense for choosing are patients that had a high influence on the prediction model.
Koh et. al (2018) showed that this type of selections works way better than choosing randomly or choosing the ones with the highest loss / that were misclassified.



### Advantages of Identifying Influential Instances

- The deletion diagnostics and influence functions approach is very different from the mostly feature-perturbation based approaches presented in the [model-agnostic chapter](#agnostic). 
Looking at influential instances emphasizes the role the training data plays in the learning process.
This makes influence function and deletion diagnostics **one of the best debugging tools**.
From the techniques presented in this book, it's the only one that directly helps to identify the instances which should be checked for errors.
- **Deletion diagnostics are model-agnostic**, meaning they can be applied to any model. 
Also influence functions based on the derivatives can be applied to a broad class of models. 
That means we can use those methods to **compare different machine learning models** and understand their different behaviors better, going beyond comparing only the predictive performance.
- We didn't touch this topic in this chapter, but **influence functions via derivatives can also be used to create adversarial training data**. 
These are instances which are manipulated in a way that the model will fail to predict specific test instances, when the model is trained on those manipulated instances.
The difference to the methods in the [adversarial examples chapter](#adversarial) is that the attack happens here during training time, also known as poisoning attacks.
If you are interested in those, read the paper by Koh and Liang (2017).
- For the deletion diagnostics and influence function, we looked at the difference in the prediction and for the influence function we looked at the increase in loss. 
But, really, **the approach is generalizable** to 'what happens to ... when we delete/upweight instance z?', where you can fill '...' with any function of your model can be used. 
You could analyse how much a training instance influences the overall loss of the model.
You could analyse how much a training instance influences the feature importance.
You could analyse how much a training instance influences which feature is picked for the first split when training a [decision tree](#tree).

### Disadvantages of Identifying Influential Instances

- Deletion diagnostics are very **expensive to calculate** because it requires refitting.
When your model takes an hour to fit and you have 10,000 instances, then it will take longer than a year to sequentially compute the influence of all instance by leaving them out one after each other from the training data.
But also history has proven so far that computing resources increase all the time.
A computation that was resource-wise unthinkable 20 years ago, can be done by your smartphone easily.
You can train models with thousands of of training instances and hundreds of parameters within seconds/minutes on a laptop. 
So it's not a big step to say that deletion diagnostics will even work with big neural networks easily in 10 years.
- **Influence functions are a good alternative, but only for models with differentiable parameters**, like neural networks.
It doesn't work for tree-based methods like random forest, boosted trees or decision trees.
Even if you have models with parameters and a loss function, the loss might not be differentiable. 
But for the latter problem, there is a trick you can use:
use a differentiable loss as replacement for the calculation of the influence.
For example when the underlying model uses Hinge loss instead of some differentiable loss.
The loss is replaced with some smoothed loss for the influence functions, but the model can still be trained with the non-smooth loss.
- **Influence functions with the upweighting are only approximate**, because it is only a quadratic expansion around the parameters.
This means that it can be wrong and the influence of an instance is actually higher or lower when removed.
Koh and Liang (2017) showed for a few examples that the influence calculated through influence function was close to the measure you get when the model was actually recomputed after deletion of the instance.
But there is no guarantee that the approximation is good.
- A minor issue, but there is **no clear cutoff at which we call an influence of an instance influential or non-influential**.
But it's still useful to rank the instances among each other, but it would be great to have the means to not only rank the instances, but to actually distinguish between influential and non-influential.
For example, when you identify the top 10 most influential training instance for a test instance some of them might not be influential, because actually only the top 3 or something were really influential.
- The influence measures **only consider deletions of single instances** and not what happens when multiple points are deleted. 
Bigger groups of data instances could have some interactions with each other affecting the model training and the prediction heavily.
But the problem stems from combinatorics:
There are n possibilities for deleting a single instance from the data. 
There are already n times n-1 possibilities for deleting two instances from the training data. 
And you certainly don't want to start going any further, because there are just too many ways.

### Software and Alternatives

- Deletion diagnostics are very simple to implement yourself. Have a look at [the code](https://github.com/christophM/interpretable-ml-book/blob/master/manuscript/06.5-example-based-influence-fct.Rmd) I wrote for the examples in this chapter.
- For linear models and generalized linear models many influence measures like Cook's distance are implemented in R in the `stats` package.
- Koh and Liang published the Python code for influence functions from paper [in a repository](https://github.com/kohpangwei/influence-release).
That's great!
Unfortunately it is 'only' the code to the paper and not a maintained and documented Python module.
The code is focused on the Tensorflow library, so you couldn't directly use it for black box models using other frameworks, like sci-kit learn.
- Here is a [great blog post for influence functions](http://mlexplained.com/2018/06/01/paper-dissected-understanding-black-box-predictions-via-influence-functions/#more-641), which helped me understand this paper better. 
The blog post goes a bit deeper into the mathematics behind influence function for black box models and also talks about some of the mathematical 'tricks' used to implement the method efficiently.


[^koh]: Koh, P. W., & Liang, P. (2017). Understanding Black-box Predictions via Influence Functions. Retrieved from http://arxiv.org/abs/1703.04730
















<!-- **Excursion: Intuition behind the gradient and the Hessian matrix** -->

<!-- Intuition behind the gradient: The gradient is the rate of change of the loss of the training (or test example) in various directions. -->
<!-- It tells us how much the loss changes, when we change the model parameters $\theta$ by a little bit. -->
<!-- Since the model usually has many parameters, the gradient also is a vector with many numbers, one for each parameter, telling for each parameter the rate and direction of change of the loss if we were to change the parameter. -->
<!-- A positive entry in the gradient vector means that a small increase in the corresponding model parameter increases the loss, a negative gradient means that the increase of the parameter decreases the loss. -->
<!-- The Hessian is the second-order derivative. -->
<!-- What the gradient is to the loss, is the hessian to the gradient.  -->
<!-- The Hessian is the rate of change of the gradient, or expressed in terms of the loss, it is the rate of change of the rate of change of the loss. -->
<!-- A more informal explanation is that the Hessian informs us how curved the rate of change is.  -->
<!-- The Hessian is a matrix because to describe the curvature of the loss and that slope depends on the direction we are looking.  -->
<!-- A real world example: A potato chip. -->
<!-- Not the irregular ones, but the ones where each chip has the same shape, which you can stack.  -->
<!-- If you lay such a chip on a surface, such that the ends on off the ends of the longer side touch the ground, we will see how it relates to gradients and the Hessian. -->
<!-- The long end points away from you, so that you could now put your fingers on the left and right elevated parts of the chip and wiggle it, but be careful not to break it. -->
<!-- Have a look at the next picture to see what I mean. -->

<!-- The chip is 3-dimensional and will be a good example. -->
<!-- Assume that each point on the chip describes the combiniation of two model parameters, represented as the coordinate in the direction of the long side and the coordinate in the direction of the short side. -->
<!-- The height of a point on the chip represents the value of the loss function. -->
<!-- Now, if we start on the left of the elevated short sides of the chip, the gradient of the loss (height of the chip) would be negative, since going from left to right by a small step would decrease the height/loss.  -->
<!-- In the middle of the chip, there is a gradient of zero and on the right a positive gradient for the direction. -->
<!-- But this was only one component of the gradient.  -->
<!-- The other component is in the longer direction of the chip.  -->
<!-- Coming from your body, the gradient is first positive, then zero, then negative. -->
<!-- Now for the Hessian: It describes how curvy the chip is or also the rate of change of the gradient.  -->
<!-- Since we have two parameters (two directions), we have to describe the curviness regarding the direction we are looking. -->
<!-- Let's again start at the left of the short sides of the chips, with the the high negative gradient.  -->
<!-- How curvy is the chip? -->
<!-- Well, that depends whether you look into the direction towards the other short end or into the direction of the long ends. -->
<!-- The Hessian or curviness is very high for going from short edge to short edge, because there is much curviness between the short edges.  -->
<!-- The curviness  -->
<!-- But being on a short edge and going into one  -->


<!-- Let's say the height of the chip is the loss, the coordinate along the back of the from back to neck is one parameter, and the coordinate of the saddle from left flank to right flank is the second parameter. -->
<!-- Going from left to right, the saddle first increases in height and than decreases. -->
<!-- A high rate of change of the gradient (rate of rate of increase of height), so there is a positive Hessian for short direction, when looking into the short direction.  -->
<!-- But how much does the gradient change for the short direction, when we look into the long direction? -->
<!-- That is, when we go to a certain coordinate on the chip, fix the position of the short side, but check how much the gradient changes, when we change the coordinate in the long direction.  -->
<!-- The answer is 0. -->

<!-- We assume the chip's height follows the following function of the position on the short side and the position on the long side. -->
<!-- Let's call the position on the short side s and the position on the long side l: -->

<!-- $$\text{Height}=s^2-l^2+2$$ -->
<!-- The 2 is just used so that the chip starts at zero and not a negative value. -->

<!-- The gradient of the chip looks like this: -->

<!-- $$\nabla\text{Height}=\begin{pmatrix}\frac{d\text{Height}}{ds}\\\frac{d\text{Height}}{dl}\end{pmatrix}=\begin{pmatrix}2s\\-2l\end{pmatrix}$$ -->

<!-- This means that going from left to write short end, we increase the gradient and going from body pointing long end to the one long end further away, we decrease the rate at which the height increases. -->
<!-- This gradient depends on the position on the chip, which shows that for the chip the gradient is dependent on where we are on the chip, which means that the Hessian matrix will not only contain zeros, but there is a changing rate of change of the height. -->


<!-- The Hessian for our chip looks like this:  -->

<!-- $$H=\begin{pmatrix}\frac{d^2\text{Height}}{dsds}&\frac{d^2\text{Height}}{dsdl}\\\frac{d^2\text{Height}}{dsdl}&\frac{d^2\text{Height}}{dldl}\end{pmatrix}=\begin{pmatrix}2&0\\0&-2\end{pmatrix}$$ -->



<!-- ```{r potato-chip, fig.cap = "Created by Topntp26 - Freepik.com"} -->
<!-- img <- jpeg::readJPEG("images/potato-chips.jpg") -->
<!-- p1 <- grid::rasterGrob(img, interpolate=TRUE) -->

<!-- df = expand.grid(long = seq(from = -1.5, to = 1.5, length.out = 10), -->
<!--   short = seq(from = -1, to = 1, length.out = 10)) -->

<!-- ## Adding to for elevation -->
<!-- df$height = 2 + df$short^2  - df$long^2   -->

<!-- p2 = ggplot(df) +  -->
<!--   geom_tile(aes(x = short, y = long, fill = height)) +  -->
<!--   scale_x_continuous("Short side of chip") +  -->
<!--   scale_y_continuous("Long side of chip") +  -->
<!--   scale_fill_continuous("Height of chip") +  -->
<!--   coord_fixed() -->
<!-- gridExtra::grid.arrange(p1, p2, ncol = 2) -->
<!-- ``` -->


<!-- **End of excursion** -->




