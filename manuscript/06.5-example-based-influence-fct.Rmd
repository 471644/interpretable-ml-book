# Influential Instances

```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

<!-- Intro text -->
An influential instance is a data instance whose deletion would considerably change the fitted model.
Data instances can have extreme effects on the learning process of the model.


<!-- Some intuition -->


<!-- Model as a function of the data-->

Why did the model make a particular prediction?
Unlike most other methods, we don't treat the model as fixed, but as a function of the training data and outcome.
We explain the model and the prediction with respect to the training data that was most responsible for the prediction and the model parameters.
Considering the model is a function of the features plus the outcome, we can ask which training instance were most responsible for the particular prediction.
Responsibility for a data point can be measured by how much the prediction changes, when we remove or upweight a certain point.

A learning algorithm (the algorithm that produces a linear model or a random forest) is - in its essence - a function that takes in data and spits out a model, which can be described by its parameters:

$$\text{Learner}:X\rightarrow{}\text{Model}(\theta)$$

And a model is a function that projects from to data to some output y:

$$\text{Model}:X\rightarrow{}y$$
<!-- Definitions and visualisations: outliers, leverage? and influential instances -->

**Outlier**
An outlier is a point which is far away from other data points.
What's an outlier?
A data point that lies outside of the mass of the probability distribution (an unlikely point).

```{r outlier, fig.cap = "The feature follows a Gaussian distribution with the exception of an outlier with x=8."}
n = 50
x = rnorm(mean = 1, n = n)
x = c(x, 8)
y = rnorm(mean = x, n = n)
y = c(y, 7.2)
df = data.frame(x, y)
ggplot(df) + geom_histogram(aes(x = x)) + my_theme()
```

Outliers can results because of the following reasons:

TODO: Checkout this blog post (see AA book example)


Outliers might or might not be interesting data points (for as criticisms or archetypes), but when we want to know what really impacts the model and its predictions, we have to look at influential points.

**Influential point**
An influential point is a data point which has a strong effect on the fitted model.
An outlier can, but doesn't have to be an influential point.
This can be either measured as the influence on some model parameters or on the predictions of the model.
Influential instances can be and often are outliers.
The concepts are not the same.
Not every outlier is necessarily an influential point. 
An influential point is a special outlier: An outlier that  influences the learned model (when comparing the models learned with and without the influential points).

```{r influential-point, fig.cap = "The different influence on a fit of a linear model by adding an outlier or adding an influential data point. Adding a non-influential outlier doesn't change the fitted slope much. Adding an influential point changes the fitted slope drastically."}


df1 = df
df2  = df[1:n,]
df3 = rbind(df2, data.frame(x = 8, y = 0))
df1$regression_model = "with outlier"
df2$regression_model = "without outlier/influential point"
df3$regression_model = "with influential point"
df.all = rbind(df1, df2, df3)


text.dat = data.frame(x = c(8, 8), y = c(7.2, 0), lab = c("Outlier \u2192", "Influential point & outlier \u2192"), regression_model = "with outlier")

ggplot(df.all, mapping = aes(x = x, y = y, group = regression_model, linetype = regression_model)) + 
  geom_point(size = 2) + 
  geom_smooth(method='lm',formula=y~x, se = FALSE, aes(color = regression_model)) + 
  my_theme() + 
  geom_text(data = text.dat, aes(label = lab), hjust = 1, nudge_x = -0.2, vjust = 0.3)

```

**Leverage point??**

Another term is leverage points in terms of linear regression. 
Leverage refers to the potential to greatly influence the model, but it doesn't have to.
Leverage points are extreme in some feature, but not necessarily in y.

```{r leverage-point}



```




### Identifying Influential Instances with Cook's Distance


Identifying influential instances in linear models: Cooks distance.
For both measures, we refit the model repeatedly while leaving out certain instances.
The model with an instance is compared to the one without and the model difference for the weights is compared.


Linear model, GLM there is XXX.
In linear regression, an influential instance is an outlier that greatly affects the estimated slope of the regression line.

Cooks Distance Di for a data point is defined as the sum of the changes in the predicted outcome when the i-th instance is removed from the model training.

$$D_i=\frac{\sum_{i=1}^n(\hat{y}_j-\hat{y}_{j(i)})^2}{p\cdot{}MSE}$$

where the numerator is the squared difference between prediction of model with and without the i-th instance, summed over the dataset.
The denominator is the number of features p times the mean squared error.
The Cooks distance tells us how much the predicted output of a linear model changes when we remove the i-the instance in the training.
Cook's distance refers to how far, on average, predicted y-values will move if the observation in question is dropped from the data set.
Interestingly, Cook's distance is usually not seen outside the context of linear models and generalized linear models, but the formula is not required to be used on linear models but is actually model agnostic.

### Example with Cook's Distance


```{r cooks}

data("bike")

cooks = function(predicted, predicted.without, n.features, errors) {
  sum((predicted - predicted.without)^2)/(n.features * ((1/(length(predicted) - n.features)) * sum(errors * errors)))
}

library(rpart)

cars.lm = rpart(cnt ~ ., data = bike)
predicted.orig = predict(cars.lm, newdata = bike)
cs= lapply(1:nrow(bike), function(to.remove.index) {
  cars.lm.1 = rpart(cnt ~ ., data = bike[setdiff(1:nrow(bike), to.remove.index),])
  predict.removed = predict(cars.lm.1, newdata = bike)
  error = bike$cnt - predicted.orig
  cooks(predicted.orig, predict.removed, n.features = ncol(bike) - 1, errors = error)
})

df = data.frame(cooks = unlist(cs), id = 1:nrow(bike))

ggplot(df) + geom_point(aes(x = id, y = cooks))

r.dist = cooks.distance(cars.lm)

plot(my.dist, r.dist)
cor(my.dist, r.dist)

my.dist / r.dist
```

Instead of removing a data point and aggregating over the data,  we flip it around:
For a given instance and its prediction, refit the model with iteratively each instance removed and find out the most influential data points.
As done in the following example with a decision tree.


```{r cooks-single}

cooks.matrix = matrix(NA, ncol = nrow(bike), nrow = nrow(bike))
cars.lm = rpart(cnt ~ ., data = bike)
predicted.orig = predict(cars.lm, newdata = bike)
for (to.remove.index in 1:nrow(bike)) {
  cars.lm.1 = rpart(cnt ~ ., data = bike[setdiff(1:nrow(bike), to.remove.index),])
  print(to.remove.index)
  predict.removed = predict(cars.lm.1, newdata = bike)
  error = bike$cnt - predicted.orig[to.predict.index]
  
  for (to.predict.index in 1:nrow(bike)) {
    cooks.matrix[to.remove.index, to.predict.index] = (predicted.orig[to.predict.index] - predict.removed[to.predict.index])^2
  }
}

cooks.matrix

plot(colSums(cooks.matrix))
plot(rowSums(cooks.matrix))



obs = cooks.matrix[,200]
xx = rbind(bike[200,],
  bike[order(obs, decreasing = TRUE)[1:5],])

pred = predict(cars.lm, newdata = xx)
xx$pred = pred
xx
## Examples end up in the same node, so removing them changes the tree structure and the points are influential to the data point
## 
cars.lm
```



```{r cooks-single-2}
library(mlr)

cooks.matrix = matrix(NA, ncol = nrow(bike), nrow = nrow(bike))

lrn = makeLearner("regr.svm")
tsk = makeRegrTask(data = bike, target = "cnt")
mod = train(lrn, tsk)

predicted.orig = getPredictionResponse(predict(mod, newdata = bike))
for (to.remove.index in 1:nrow(bike)) {
  print(to.remove.index)
  mod.2 = train(lrn, tsk, subset = setdiff(1:nrow(bike), to.remove.index))
  predict.removed = getPredictionResponse(predict(mod.2, newdata = bike))
  error = bike$cnt - predicted.orig[to.predict.index]
  
  for (to.predict.index in 1:nrow(bike)) {
    cooks.matrix[to.remove.index, to.predict.index] = (predicted.orig[to.predict.index] - predict.removed[to.predict.index])^2
  }
}

cooks.matrix

plot(colSums(cooks.matrix))
plot(rowSums(cooks.matrix))



obs = cooks.matrix[,200]
xx = rbind(bike[200,],
  bike[order(obs, decreasing = TRUE)[1:5],])

pred = predict(cars.lm, newdata = xx)
xx$pred = pred
xx
## Examples end up in the same node, so removing them changes the tree structure and the points are influential to the data point
## 
mod
```

When is a point influential?
Some say, when the Cooks Distance is larger than one. 
Some say, when Cooks Distance is larger than 4/N where N is the number of instances in the training data.
Some say, look at visualisations of the distribution and check if some data points clearly peak out.





Cooks Distance is model-agnostic?
Refitting the model is very costly, but possible.

### Identifying Influential Instances without Refitting

The whole refitting thing is rather slow, because if you have thousands of training samples, you have to retrain your model thousands of times.
Assuming the model takes a day to compute and you have 1000 training examples, then - without parallelization - the computation of influential instances will take almost 3 years.
Ain't nobody got time for that.

Model-agnostic, there is influence functions by Koh and Liang (2017)[^koh].
But it only works for methods where you have some loss function that you can differentiate with respect to some model parameters.
Random forests for example won't work. 
Logistic regression, neural networks and support vector machines work.
Idea: For a prediction of a instance, trace it back through the learning algorithm to the training data.
It helps for understanding the model behaviour, debugging the model and detecting errors with the dataset.
It also helps to identify [adversarial examples](#adversarial) which exploit vulnerabilities of a machine learning model.


The method is based on influence function which was invented for robust statistics.
The idea is: 
I have some estimator T (like any statistical measurement) which acts on some distribution F. 
How much does T change if we perturb F is what robust statistics deals with.
It's a measure of  stability.

Assuming that you can differentiad the loss twice with respect to the model parameters $\theta$ you can use this method.
Actually, it's also possible to use it when the differentiation twice is not possible and there is a trick to compute the Hessian matrix (second derivative).


The goal:

$$\theta_{\epsilon,z}=\arg\min_{\theta{}\in\Theta}(1-\epsilon)\frac{1}{n}\sum_{i=1}^n{}L(z_i,\theta)+\epsilon{}L(z_i,\theta)$$
where $\theta$ is the vector of the model parameters.
L is the loss function used to train the model and $\epsilon$ is avery small number.
$z_i$ are the training data and z is the data point for which we want to find influential instances.
The intuition behind this formula is:
How much does the loss change if we upweight a particular points $z_i$ from the training data by a little bit ($\epsilon$) and downweight the other data points respectively?

The solution is:

$$I_{up,loss}=\frac{\delta{}L(z_{test},\hat{\theta}_{\epsilon{}z})}{d\epsilon}=-\bigtriangledown_{\theta}L(z_{test},\hat{\theta})^T{}H^{-1}_{\hat{\theta}}\bigtriangledown_{\hat{\theta}}L(z_{test},\hat{\theta})$$
The intuition behind the parts where the L stands:
How much does the model want to go into the direction of the upweighted training data point.
A large gradient means:
A small change in the model parameter $\theta$ (for example the weights in a neural network) decrease the loss a lot, so if I upweight this training point, the parameters will change by a lot.

The H is the hessian matrix (the second derivative):

$$H_{\theta}=\frac{1}{n}\sum_{i=1}^n\bigtriangledown^2_{\hat{\theta}}L(z_i,\hat{\theta})$$
The intuition behind the Hessian matrix is: 
When I change the training points in particular direction, how does the loss change?

The notation of this I measure is basically so that this is a bit like a fraction, where the L-part is in the numerator and the H-part is in the denominator. 
So the informal way to think about the Iup measure is:
When we upweight this training point, Iup is the ratio of how much the loss of this single point changes compared to how much the other data points vote for the parameters of the model not to change because the loss would increase.

The problem is that the Hessian is not tractable (TODO: explain better).
So an improvement is needed.
The method proposed by the authors suggests using some tricks to get the Hessian.
Scaling it up.

The key idea:
You only need the Hessian times a vector $H_{\hat{\theta}}v$ and not the hessian itself.
By applying a series of tricks (Pearlmutter trick, CG and Taylor expansion) it can be derived.

This also works when no derivative exists, for example when the model uses Hinge loss instead of some differentiable loss.
The solution is to replace the loss with some smoothed loss, but the model can still be trained with the non-smooth loss.

### Application of Influence Instances

- Debugging model errors

Checkout why the model makes error on certain data points by checking which inputs were most responsible.

- Fixing training data

Given a budget for how many data points you can change the ground truth, how do you pick the ones to look at?
Best way is to only pick the influential points, because non-influential points - by definition - don't affect the model much anyways.
This works way better than choosing random misclassified or badly predicted points or choosing by the ones with the highest loss.

- Poisoning attacks: Creating adversarial examples for model training

How much does the loss change if we change the image a bit
The idea is to add weight to create a synthetic image, and remove weight from the original one to create an adversarial training instance.





What is the difference to using the nearest neighbour in feature space?
Training examples with higher loss have higher influence and nearest neighbour is like taking difference of loss and hessian?



What happens when you don't have the y? 
The method requires computing the loss. 
Does that mean we can't use it for new data for which we don't know the true y?
No, because we can compute any measure based on the derivative, like the change in prediction or predicted probability.




[^koh]: Koh, P. W., & Liang, P. (2017). Understanding Black-box Predictions via Influence Functions. Retrieved from http://arxiv.org/abs/1703.04730