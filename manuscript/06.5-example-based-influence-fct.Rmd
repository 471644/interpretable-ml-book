# Influential Instances

```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(43)
```

<!-- Intro text -->
An influential instance is a data instance whose deletion from the training data would considerably change a model, when fitted on the reduced dataset.
Identifying influential training instances can explain predictions of black box models.

*Keywords: Influential points, influence function, leave-one-out analysis, Cook's Distance, regression deletion diagnostics*

<!-- Model as a function of the data-->
Classically, identfiying influential instances is a procedure used for researching the robustness of a model and also to maybe remove these instance from the training data.
But we can also use influential instances to explain why a particular prediction was made, by tracing the decision through the model back to the training data.
We explain the model and the prediction with respect to the training data that was most responsible for the prediction and the model parameters.
Unlike most other methods, we don't treat the model as fixed, but as a function of the training data and outcome.

Considering that the model is a function of the features plus the outcome, we can ask which training instance were most responsible for the particular prediction.
Responsibility for a data point can be measured by how much the prediction changes, when we remove or upweight a certain point.

A learning algorithm (the algorithm that produces a linear model or a random forest) is - in its essence - a function that takes in data and spits out a model, which can be described by its parameters:

$$\text{Learner}:X,y\rightarrow{}\text{Model}(\theta)$$

And a model is a function that projects from to data to some prediction $\hat{y}:

$$\text{Model}:X\rightarrow{}\hat{y}$$

Why not trace a prediction by the model right back to the training points that were most responsible?
That question drives the idea behind influential data points.

Before we start into some methods how to find influential instance, we have to talk about what influential instances are and what the difference between an outlier and an influential instance is. 
You probably have already heared about outliers before.
An outlier is an instance which is far away from other instances in the dataset.
"Far away" can for example be measured by the Euclidean distance.
In a dataset of newborn stats, a baby that weights 6kg would be considered an outlier.
In a dataset of bank accounts, which are mostly checking accounts, a dedicated loan account, with a large negative balance and not so many movements compared to the checking accounts would be considered an outlier.
The following images shows an outlier:

```{r outlier, fig.cap = "The feature follows a Gaussian distribution with the exception of an outlier with x=8."}
n = 50
x = rnorm(mean = 1, n = n)
x = c(x, 8)
y = rnorm(mean = x, n = n)
y = c(y, 7.2)
df = data.frame(x, y)
ggplot(df) + geom_histogram(aes(x = x)) + my_theme()
```


Outliers might or might not be interesting data points (for as criticisms or archetypes), but when we want to know what really impacts the model and its predictions, we have to look at influential points.

**Influential point**
An influential point is a data point which has a strong effect on the fitted model.
An outlier can, but doesn't have to be an influential point.
This can be either measured as the influence on some model parameters or on the predictions of the model.
Influential instances can be and often are outliers.
The concepts are not the same.
Not every outlier is necessarily an influential point. 
An influential point is a special outlier: An outlier that  influences the learned model (when comparing the models learned with and without the influential points).

```{r influential-point, fig.cap = "The different influence on a fit of a linear model by adding an outlier or adding an influential data point. Adding a non-influential outlier doesn't change the fitted slope much. Adding an influential point changes the fitted slope drastically."}


df1 = df
df2  = df[1:n,]
df3 = rbind(df2, data.frame(x = 8, y = 0))
df1$regression_model = "with outlier"
df2$regression_model = "without outlier/influential point"
df3$regression_model = "with influential point"
df.all = rbind(df1, df2, df3)


text.dat = data.frame(x = c(8, 8), y = c(7.2, 0), lab = c("Outlier \u2192", "Influential point & outlier \u2192"), regression_model = "with outlier")

ggplot(df.all, mapping = aes(x = x, y = y, group = regression_model, linetype = regression_model)) + 
  geom_point(size = 2) + 
  geom_smooth(method='lm',formula=y~x, se = FALSE, aes(color = regression_model)) + 
  my_theme() + 
  geom_label(data = text.dat, aes(label = lab), hjust = 1, nudge_x = -0.2, vjust = 0.3)

```

<!-- **Leverage point??** -->

<!-- Another term is leverage points in terms of linear regression.  -->
<!-- Leverage refers to the potential to greatly influence the model, but it doesn't have to. -->
<!-- Leverage points are extreme in some feature, but not necessarily in y. -->

<!-- ```{r leverage-point} -->



<!-- ``` -->




### Identifying Influential Instances with Cook's Distance

Identifying influential instances in linear models: Cooks distance.
For both measures, we refit the model repeatedly while leaving out certain instances.
The model with an instance is compared to the one without and the model difference for the weights is compared.

Cook's distance measures globally (across the whole dataset) how influential a training instance was for all predictions.

Linear model, GLM there is XXX.
In linear regression, an influential instance is an outlier that greatly affects the estimated slope of the regression line.

Cooks Distance Di for a data point is defined as the sum of the changes in the predicted outcome when the i-th instance is removed from the model training.

$$D_i=\frac{\sum_{i=1}^n(\hat{y}_j-\hat{y}_{j(i)})^2}{p\cdot{}MSE}$$

where the numerator is the squared difference between prediction of model with and without the i-th instance, summed over the dataset.
The denominator is the number of features p times the mean squared error.
The Cooks distance tells us how much the predicted output of a linear model changes when we remove the i-the instance in the training.
Cook's distance refers to how far, on average, predicted y-values will move if the observation in question is dropped from the data set.
Interestingly, Cook's distance is usually not seen outside the context of linear models and generalized linear models, but the formula is not required to be used on linear models but is actually model agnostic.

TODO: Analyse for Cook's distance what the most important features were by learning a tree: cooks.dist ~ . -id

```{r cooks}

data("bike")

cooks = function(predicted, predicted.without, n.features, errors) {
  sum((predicted - predicted.without)^2)/(n.features * ((1/(length(predicted) - n.features)) * sum(errors * errors)))
}


lrn = makeLearner("regr.ranger")
tsk = makeRegrTask(data = bike, target = "cnt")
mod = train(lrn, tsk)
predicted.orig = getPredictionResponse(predict(mod, newdata = bike))
cs= lapply(1:nrow(bike), function(to.remove.index) {
  print(to.remove.index)
  mod.2 = train(lrn, tsk, subset = setdiff(1:nrow(bike), to.remove.index))
  predict.removed = getPredictionResponse(predict(mod.2, newdata = bike))
  error = bike$cnt - predicted.orig
  cooks(predicted.orig, predict.removed, n.features = ncol(bike) - 1, errors = error)
})

df = data.frame(cooks = unlist(cs), id = 1:nrow(bike))

ggplot(df) + geom_point(aes(x = id, y = cooks)) + 
  scale_y_continuous("Cook's Distance") + 
  scale_x_continuous("Index of instance")

df.bike  = cbind(df, bike)
rpart(cooks ~ . -id, data = df.bike, control = rpart.control(maxdepth = 2))



lrn = makeLearner("regr.svm")
tsk = makeRegrTask(data = bike, target = "cnt")
mod = train(lrn, tsk)
predicted.orig = getPredictionResponse(predict(mod, newdata = bike))
cs= lapply(1:nrow(bike), function(to.remove.index) {
  print(to.remove.index)
  mod.2 = train(lrn, tsk, subset = setdiff(1:nrow(bike), to.remove.index))
  predict.removed = getPredictionResponse(predict(mod.2, newdata = bike))
  error = bike$cnt - predicted.orig
  cooks(predicted.orig, predict.removed, n.features = ncol(bike) - 1, errors = error)
})

df = data.frame(cooks = unlist(cs), id = 1:nrow(bike))

ggplot(df) + geom_point(aes(x = id, y = cooks)) + 
  scale_y_continuous("Cook's Distance") + 
  scale_x_continuous("Index of instance")

df.bike  = cbind(df, bike)
rpart(cooks ~ . -id, data = df.bike, control = rpart.control(maxdepth = 2))


```

Now we have the most influential data points. 
This is already useful for debugging the data:
Is there some problematic instance?
These are the first points to check again if they have any mistakes in them, because any mistake in them influences by definition the model a lot.

```{r cooks-top}
cooks.sorted = df[order(df$cooks, decreasing = TRUE),]
bike[cooks.sorted$id[1],]
```
```{r cooks-top-expl}
library(randomForest)
to.remove.index = cooks.sorted$id[1]
cars.lm = randomForest(cnt ~ ., data = bike)
cars.lm.1 = randomForest(cnt ~ ., data = bike[setdiff(1:nrow(bike), to.remove.index),])

bike$cnt[to.remove.index]
abs.diffs = abs(predict(cars.lm, newdata = bike) - predict(cars.lm.1, newdata = bike))
which(abs.diffs > 200)

bike[c(239, 641),]

cars.lm

```

Point lies right on the edge of the threshold and removing it changes the split value which cascades down the tree.
Obviously, we could only make sense here, because we have a simple decision tree model, which we can easily investigate.
What would we do with a black box model?


Instead of removing a data point and aggregating over the data,  we flip it around:
For a given instance and its prediction, refit the model with iteratively each instance removed and find out the most influential data points.

We simply look at the absolute difference between predicted value of instance j with and without instance j included in the training of the model:

$$d_{j(i)}=|\hat{y}_j-\hat{y}_{j(i)}|$$
It's like asking the counterfactual:
How would the prediction for instance j change if we left out instance i from the training process.
But we don't only do this for left out instance i, but we leave out each instance one after each other and get a vector of differences.
Then we can rank all the differences by amount and choose the training instance which yield the biggest change in prediction when left out of the training and use those to explain the prediction of the model for this instance.

As done in the following example with a decision tree.


```{r cooks-single}

cooks.matrix = matrix(NA, ncol = nrow(bike), nrow = nrow(bike))
cars.lm = rpart(cnt ~ ., data = bike)
predicted.orig = predict(cars.lm, newdata = bike)
for (to.remove.index in 1:nrow(bike)) {
  cars.lm.1 = rpart(cnt ~ ., data = bike[setdiff(1:nrow(bike), to.remove.index),])
  predict.removed = predict(cars.lm.1, newdata = bike)
  error = bike$cnt - predicted.orig
  for (to.predict.index in 1:nrow(bike)) {
    cooks.matrix[to.remove.index, to.predict.index] = (predicted.orig[to.predict.index] - predict.removed[to.predict.index])^2
  }
}

cooks.matrix

plot(colSums(cooks.matrix))
plot(rowSums(cooks.matrix))



obs = cooks.matrix[,200]
xx = rbind(bike[200,],
  bike[order(obs, decreasing = TRUE)[1:5],])

pred = predict(cars.lm, newdata = xx)
xx$pred = pred
xx
## Examples end up in the same node, so removing them changes the tree structure and the points are influential to the data point
## 
cars.lm
```


Cook's distance is very similar in approach to the leave-one-out cross validation (LOOCV), which can be used to estimate the error of a machine learning model, but usually repeated k cross validation works better, btw..



```{r cooks-single-2}
library(mlr)

cooks.matrix = matrix(NA, ncol = nrow(bike), nrow = nrow(bike))

lrn = makeLearner("regr.svm")
tsk = makeRegrTask(data = bike, target = "cnt")
mod = train(lrn, tsk)

predicted.orig = getPredictionResponse(predict(mod, newdata = bike))
for (to.remove.index in 1:nrow(bike)) {
  mod.2 = train(lrn, tsk, subset = setdiff(1:nrow(bike), to.remove.index))
  predict.removed = getPredictionResponse(predict(mod.2, newdata = bike))
  error = bike$cnt - predicted.orig
  
  for (to.predict.index in 1:nrow(bike)) {
    cooks.matrix[to.remove.index, to.predict.index] = (predicted.orig[to.predict.index] - predict.removed[to.predict.index])^2
  }
}

cooks.matrix

plot(colSums(cooks.matrix))
plot(rowSums(cooks.matrix))



obs = cooks.matrix[,200]
xx = rbind(bike[200,],
  bike[order(obs, decreasing = TRUE)[1:5],])

pred = predict(cars.lm, newdata = xx)
xx$pred = pred
xx
## Examples end up in the same node, so removing them changes the tree structure and the points are influential to the data point
## 
mod
```


TODO: Visualize instance wise changes.

When is a point influential?
Some say, when the Cooks Distance is larger than one. 
Some say, when Cooks Distance is larger than 4/N where N is the number of instances in the training data.
Some say, look at visualisations of the distribution and check if some data points clearly peak out.


Cooks Distance is model-agnostic?
Refitting the model is very costly, but possible.

### Identifying Influential Instances without Refitting

The whole refitting thing is rather slow, because if you have thousands of training samples, you have to retrain your model thousands of times.
Assuming the model takes a day to compute and you have 1000 training examples, then - without parallelization - the computation of influential instances will take almost 3 years.
Ain't nobody got time for that.

Model-agnostic, there is influence functions by Koh and Liang (2017)[^koh].
But it only works for methods where you have some loss function that you can differentiate with respect to some model parameters.
Random forests for example won't work. 
Logistic regression, neural networks and support vector machines work.
Idea: For a prediction of a instance, trace it back through the learning algorithm to the training data.


It helps for understanding the model behaviour, debugging the model and detecting errors with the dataset.
It also helps to identify [adversarial examples](#adversarial) which exploit vulnerabilities of a machine learning model.


The method is based on influence function which was invented for robust statistics.
The idea is: 
I have some estimator T (like any statistical measurement) which acts on some distribution F. 
How much does T change if we perturb F is what robust statistics deals with.
It's a measure of  stability.
Influence functions measure how a training example affects the predictions for a test example
Or, as formulated by the paper: How much would the test loss change by upweighting a certain training example?

Assuming that you can differentiad the loss twice with respect to the model parameters $\theta$ you can use this method.
Actually, it's also possible to use it when the differentiation twice is not possible and there is a trick to compute the Hessian matrix (second derivative).


How would the model parameters look like if we would remove data point z?
Thanks to influence functions from robust statistic, we can answer the question, by upweighting the z in the loss function by some small step $\epsilon$, which results in the following, new parameters:

$$\hat{\theta}_{\epsilon,z}=\arg\min_{\theta{}\in\Theta}(1-\epsilon)\frac{1}{n}\sum_{i=1}^n{}L(z_i,\theta)+\epsilon{}L(z,\theta)$$
where $\theta$ is the vector of the model parameters.
L is the loss function used to train the model and $\epsilon$ is avery small number.
$z_i$ are the training data and z is the data point for which we want to find influential instances.
The intuition behind this formula is:
How much does the loss change if we upweight a particular points $z_i$ from the training data by a little bit ($\epsilon$) and downweight the other data points respectively?

The solution is:

$$I_{up,loss}(z,z_{test})=\left.\frac{d{}L(z_{test},\hat{\theta}_{\epsilon,z})}{d\epsilon}\right|_{\epsilon=0}=-\nabla_{\theta}L(z_{test},\hat{\theta})^T{}H^{-1}_{\theta}\nabla_{\theta}L(z,\hat{\theta})$$


We look how the loss of z_test changes when we upweight a certain training data points, telling us how influential this training data points was for the prediction of z_test.
The intuition behind the parts where the L stands:
How much does the model want to go into the direction of the upweighted training data point.
A large gradient means:
A small change in the model parameter $\theta$ (for example the weights in a neural network) decrease the loss a lot, so if I upweight this training point, the parameters will change by a lot.
Some intuition behind that: 
The first term $\nabla_{\theta}L(z_{test},\hat{\theta})^T{}$ is the gradient of the test instance, 
also the last term $\nabla_{\hat{\theta}}L(z,\hat{\theta})$, but for the training instance. 
The middle part $H^{-1}_{\hat{\theta}}$ measures the similarity between the two.

More intuition about the formula:
The influence is proportionate to how large the gradients are for the training and test loss.
The higher the gradient of the training loss, the higher its influence on the parameters, the higher the influence on the test prediction.
For the gradient of the test prediction loss: The higher the gradient, the more influentiable the test instance is.
The whole construct can also be seen as a measure of similarity between the the training and the test instance.
Similarity doesn't mean here the Euclidean distance, but the similarity the model learned.

**Excursion: Intuition behind the gradient and the Hessian matrix**

Intuition behind the gradient: The gradient is the rate of change of the loss of the training (or test example) in various directions.
It tells us how much the loss changes, when we change the model parameters $\theta$ by a little bit.
Since the model usually has many parameters, the gradient also is a vector with many numbers, one for each parameter, telling for each parameter the rate and direction of change of the loss if we were to change the parameter.
A positive entry in the gradient vector means that a small increase in the corresponding model parameter increases the loss, a negative gradient means that the increase of the parameter decreases the loss.
The Hessian is the second-order derivative.
What the gradient is to the loss, is the hessian to the gradient. 
The Hessian is the rate of change of the gradient, or expressed in terms of the loss, it is the rate of change of the rate of change of the loss.
A more informal explanation is that the Hessian informs us how curved the rate of change is. 
The Hessian is a matrix because to describe the curvature of the loss and that slope depends on the direction we are looking. 
A real world example: A potato chip.
Not the irregular ones, but the ones where each chip has the same shape, which you can stack. 
If you lay such a chip on a surface, such that the ends on off the ends of the longer side touch the ground, we will see how it relates to gradients and the Hessian.
The long end points away from you, so that you could now put your fingers on the left and right elevated parts of the chip and wiggle it, but be careful not to break it.
Have a look at the next picture to see what I mean.

The chip is 3-dimensional and will be a good example.
Assume that each point on the chip describes the combiniation of two model parameters, represented as the coordinate in the direction of the long side and the coordinate in the direction of the short side.
The height of a point on the chip represents the value of the loss function.
Now, if we start on the left of the elevated short sides of the chip, the gradient of the loss (height of the chip) would be negative, since going from left to right by a small step would decrease the height/loss. 
In the middle of the chip, there is a gradient of zero and on the right a positive gradient for the direction.
But this was only one component of the gradient. 
The other component is in the longer direction of the chip. 
Coming from your body, the gradient is first positive, then zero, then negative.
Now for the Hessian: It describes how curvy the chip is or also the rate of change of the gradient. 
Since we have two parameters (two directions), we have to describe the curviness regarding the direction we are looking.
Let's again start at the left of the short sides of the chips, with the the high negative gradient. 
How curvy is the chip?
Well, that depends whether you look into the direction towards the other short end or into the direction of the long ends.
The Hessian or curviness is very high for going from short edge to short edge, because there is much curviness between the short edges. 
The curviness 
But being on a short edge and going into one 


Let's say the height of the chip is the loss, the coordinate along the back of the from back to neck is one parameter, and the coordinate of the saddle from left flank to right flank is the second parameter.
Going from left to right, the saddle first increases in height and than decreases.
A high rate of change of the gradient (rate of rate of increase of height), so there is a positive Hessian for short direction, when looking into the short direction. 
But how much does the gradient change for the short direction, when we look into the long direction?
That is, when we go to a certain coordinate on the chip, fix the position of the short side, but check how much the gradient changes, when we change the coordinate in the long direction. 
The answer is 0.

We assume the chip's height follows the following function of the position on the short side and the position on the long side.
Let's call the position on the short side s and the position on the long side l:

$$\text{Height}=s^2-l^2+2$$
The 2 is just used so that the chip starts at zero and not a negative value.

The gradient of the chip looks like this:

$$\nabla\text{Height}=\begin{pmatrix}\frac{d\text{Height}}{ds}\\\frac{d\text{Height}}{dl}\end{pmatrix}=\begin{pmatrix}2s\\-2l\end{pmatrix}$$

This means that going from left to write short end, we increase the gradient and going from body pointing long end to the one long end further away, we decrease the rate at which the height increases.
This gradient depends on the position on the chip, which shows that for the chip the gradient is dependent on where we are on the chip, which means that the Hessian matrix will not only contain zeros, but there is a changing rate of change of the height.


The Hessian for our chip looks like this: 

$$H=\begin{pmatrix}\frac{d^2\text{Height}}{dsds}&\frac{d^2\text{Height}}{dsdl}\\\frac{d^2\text{Height}}{dsdl}&\frac{d^2\text{Height}}{dldl}\end{pmatrix}=\begin{pmatrix}2&0\\0&-2\end{pmatrix}$$



```{r potato-chip, fig.cap = "Created by Topntp26 - Freepik.com"}
img <- readJPEG("images/potato-chips.jpg")
p1 <- rasterGrob(img, interpolate=TRUE)

p1 = knitr::include_graphics("images/potato-chips.jpg")

df = expand.grid(long = seq(from = -1.5, to = 1.5, length.out = 10),
  short = seq(from = -1, to = 1, length.out = 10))

## Adding to for elevation
df$height = 2 + df$short^2  - df$long^2  

p2 = ggplot(df) + 
  geom_tile(aes(x = short, y = long, fill = height)) + 
  scale_x_continuous("Short side of chip") + 
  scale_y_continuous("Long side of chip") + 
  scale_fill_continuous("Height of chip") + 
  coord_fixed()
gridExtra::grid.arrange(p1, p2, ncol = 2)
```



The H is the hessian matrix (the second derivative):

$$H_{\theta}=\frac{1}{n}\sum_{i=1}^n\nabla^2_{\hat{\theta}}L(z_i,\hat{\theta})$$
The intuition behind the Hessian matrix is: 
When I change the training points in particular direction, how does the loss change?

When we upweight this training point, Iup is the ratio of how much the loss of this single point changes compared to how much the other data points vote for the parameters of the model not to change because the loss would increase.

Computing I-up-loss has two challenges: First, it requires calculating and inverting the Hessian matrix.
Second, we want to calculate the property across all training points zi and a few test points z.test.

The problem is that computing and inverting the Hessian requires $O(np^2 + p^3)$ operations. 
This term tells us that the computation operations necessary (and therefore time) depends on the number of parameters to the cube.
Let's say the number of parameters in your model is 10, and the time necessary to compute the Hessian is 1s.
We exchange the model with a model with 100 parameters, which is 10 times as many parameters as the original model.
The computation time is not merely 10x 1s = 10s, but since it scales with the cube, it's actually 10x 10x 10x 1s = 1000s.
This is not healthy for a method that should be usably with neural networks with millions of parameters.
So an improvement is needed.
The method proposed by the authors suggests using some tricks to get the Hessian.
Scaling it up.

The key idea:
You only need the Hessian times a vector $H_{\hat{\theta}}v$ and not the hessian itself.
By applying a series of tricks (Pearlmutter trick, CG and Taylor expansion) it can be derived.

This also works when no derivative exists, for example when the model uses Hinge loss instead of some differentiable loss.
The solution is to replace the loss with some smoothed loss, but the model can still be trained with the non-smooth loss.


Influence functions also work in non-convex models, even though they formally require convexity.


Stochastics estimation of the Hessian matrix. 
Even with the conjugate gradients trick, computing the Hessian is expensive because it has to be iterated over the whole training set.
The trick is to get a sample of the training data and then compute:


(REmove formula?)

$$H_j^{-1}v=v+(I-\nabla^2_{\theta}L(z_j,\hat{\theta}))H^{-1}_{j-1}v$$
where $\nabla^2_{\theta}L(z_j,\hat{\theta})$ is the second order derivative (curviness) at example $z_j$.

TODO: Shorten details about the tricks the authors used.






How good does upscaling with $I_{up,loss}$ work? 
We can only know for certain, by comparing the loss we get with the leave-one-out retraining.
The authors did this and got pretty good results.

Two problems with this approach: 

- Lack of convexity of loss
Solution: smooth the hessian matrix so it becomes positive definite
- Lack of differentiability of loss.
Some losses cannot be differentiaed twice.
Solution: Approximate loss with differentiable version for computing the influence function.


TODO: Write a bit about how to create adversarial examples with influence functions.

### Application of Influence Instances

TODO: Shows some examples and ask authors to use images.

- Fixing training data

Given a budget for how many data points you can change the ground truth, how do you pick the ones to look at?
Best way is to only pick the influential points, because non-influential points - by definition - don't affect the model much anyways.
This works way better than choosing random misclassified or badly predicted points or choosing by the ones with the highest loss.

- Handling domain mismatch / Debugging model errors

Assume data is trained on one domain - like deciding if a cat or a dog is on an image, but all images where from inside the house - and an example from another domain comes in, e.g. showing a dog outside the house.
The model can make heavy mistakes.
The influence function can tell how this domain mismatch will play out.
Checkout why the model makes error on certain data points by checking which inputs were most responsible.

- Poisoning attacks: Creating adversarial examples for model training

How much does the loss change if we change the image a bit
The idea is to add weight to create a synthetic image, and remove weight from the original one to create an adversarial training instance.


What is the difference to using the nearest neighbour in feature space?
Training examples with higher loss have higher influence and nearest neighbour is like taking difference of loss and hessian?



What happens when you don't have the y? 
The method requires computing the loss. 
Does that mean we can't use it for new data for which we don't know the true y?
No, because we can compute any measure based on the derivative, like the change in prediction or predicted probability.


### Advantages of identifying influential points

- Very different from other approaches, because looking at influential instances emphasizes the role the training data plays in the learning process
- Directly helps identifying problems with data points that are wrongly labeled or have high influence and are therefor to be critically evaluated.
- In theory model-agnostic at least the methods that requre refitting 
- Influence functions can also be used to create adversarial training data
- 

### Disadvantages of identifying influential points

- Cook's Distance and similar are very expensive to calculate because it requires refitting. 
When your model takes an hour to fit and you have 10,000 instances, then it will take longer than a year to sequentially compute the influence of instance by leaving them out one after each other from the training.
- Influence functions are a good alternative, but only for models with differentiable parameters, like neural networks. 
Forget it for tree-based methods like random forest, boosted trees or decision trees.
- Cook's distance and the influence functions have no absolute scale on which to judge them. 
The cutoff at which we call an instance influential is therefore a bit arbitrary.
But it's still useful to rank the instances among each other.

### Software and Alternatives

- Cook's distance in R, though only for lm and glm. But very easy to implement, see example from this book.
- Python code for influence from paper in repository. 
Unfortunately just the paper dump and no maintained Python module.
-


Good blog post, which helped me understand this paper better: http://mlexplained.com/2018/06/01/paper-dissected-understanding-black-box-predictions-via-influence-functions/#more-641



[^koh]: Koh, P. W., & Liang, P. (2017). Understanding Black-box Predictions via Influence Functions. Retrieved from http://arxiv.org/abs/1703.04730