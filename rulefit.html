<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable." />
  <meta name="github-repo" content="christophM/interpretable-ml-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable." />
  

<meta name="author" content="Christoph Molnar">


<meta name="date" content="2018-03-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="tree.html">
<link rel="next" href="other-interpretable-models.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.0/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.4/datatables.js"></script>
<link href="libs/dt-core-1.10.16/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.16/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.16/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/dt-core-bootstrap-1.10.16/css/dataTables.bootstrap.min.css" rel="stylesheet" />
<link href="libs/dt-core-bootstrap-1.10.16/css/dataTables.bootstrap.extra.css" rel="stylesheet" />
<script src="libs/dt-core-bootstrap-1.10.16/js/jquery.dataTables.min.js"></script>
<script src="libs/dt-core-bootstrap-1.10.16/js/dataTables.bootstrap.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110543840-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110543840-1');
</script>

<!-- For the Bitcoin donation button-->
<script type="text/javascript" src="https://blockchain.info/Resources/js/pay-now-button.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="what-to-expect-from-this-book.html"><a href="what-to-expect-from-this-book.html"><i class="fa fa-check"></i><b>1.1</b> What to Expect from This Book</a></li>
<li class="chapter" data-level="1.2" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>1.2</b> Storytime</a><ul>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#lightning-never-strikes-twice"><i class="fa fa-check"></i>Lightning Never Strikes Twice</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#trust-fall"><i class="fa fa-check"></i>Trust Fall</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#fermis-paperclips"><i class="fa fa-check"></i>Fermi’s Paperclips</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>1.3</b> What Is Machine Learning?</a></li>
<li class="chapter" data-level="1.4" data-path="definitions.html"><a href="definitions.html"><i class="fa fa-check"></i><b>1.4</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> Interpretability</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>2.1</b> The Importance of Interpretability</a></li>
<li class="chapter" data-level="2.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html"><i class="fa fa-check"></i><b>2.2</b> Scope of Interpretability</a><ul>
<li class="chapter" data-level="2.2.1" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#algorithm-transparency"><i class="fa fa-check"></i><b>2.2.1</b> Algorithm transparency</a></li>
<li class="chapter" data-level="2.2.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-holistic-model-interpretability"><i class="fa fa-check"></i><b>2.2.2</b> Global, Holistic Model Interpretability</a></li>
<li class="chapter" data-level="2.2.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-model-interpretability-on-a-modular-level"><i class="fa fa-check"></i><b>2.2.3</b> Global Model Interpretability on a Modular Level</a></li>
<li class="chapter" data-level="2.2.4" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-single-prediction"><i class="fa fa-check"></i><b>2.2.4</b> Local Interpretability for a Single Prediction</a></li>
<li class="chapter" data-level="2.2.5" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-group-of-prediction"><i class="fa fa-check"></i><b>2.2.5</b> Local Interpretability for a Group of Prediction</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="evaluating-interpretability.html"><a href="evaluating-interpretability.html"><i class="fa fa-check"></i><b>2.3</b> Evaluating Interpretability</a><ul>
<li class="chapter" data-level="2.3.1" data-path="evaluating-interpretability.html"><a href="evaluating-interpretability.html#approaches-for-evaluating-the-interpretability-quality"><i class="fa fa-check"></i><b>2.3.1</b> Approaches for Evaluating the Interpretability Quality</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>2.4</b> Human-style Explanations</a><ul>
<li class="chapter" data-level="2.4.1" data-path="explanation.html"><a href="explanation.html#what-is-an-explanation"><i class="fa fa-check"></i><b>2.4.1</b> What is an explanation?</a></li>
<li class="chapter" data-level="2.4.2" data-path="explanation.html"><a href="explanation.html#good-explanation"><i class="fa fa-check"></i><b>2.4.2</b> What is a “good” explanation?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Datasets</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> Bike Sharing Counts (Regression)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> YouTube Spam Comments (Text Classification)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical-data.html"><a href="cervical-data.html"><i class="fa fa-check"></i><b>3.3</b> Risk Factors for Cervical Cancer (Classification)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>4</b> Interpretable Models</a><ul>
<li class="chapter" data-level="4.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>4.1</b> Linear Model</a><ul>
<li class="chapter" data-level="4.1.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>4.1.1</b> Interpretation</a></li>
<li class="chapter" data-level="4.1.2" data-path="limo.html"><a href="limo.html#interpretation-example"><i class="fa fa-check"></i><b>4.1.2</b> Interpretation Example</a></li>
<li class="chapter" data-level="4.1.3" data-path="limo.html"><a href="limo.html#interpretation-templates"><i class="fa fa-check"></i><b>4.1.3</b> Interpretation templates</a></li>
<li class="chapter" data-level="4.1.4" data-path="limo.html"><a href="limo.html#visual-parameter-interpretation"><i class="fa fa-check"></i><b>4.1.4</b> Visual parameter interpretation</a></li>
<li class="chapter" data-level="4.1.5" data-path="limo.html"><a href="limo.html#explaining-single-predictions"><i class="fa fa-check"></i><b>4.1.5</b> Explaining Single Predictions</a></li>
<li class="chapter" data-level="4.1.6" data-path="limo.html"><a href="limo.html#cat.code"><i class="fa fa-check"></i><b>4.1.6</b> Coding Categorical Features</a></li>
<li class="chapter" data-level="4.1.7" data-path="limo.html"><a href="limo.html#the-disadvantages-of-linear-models"><i class="fa fa-check"></i><b>4.1.7</b> The disadvantages of linear models</a></li>
<li class="chapter" data-level="4.1.8" data-path="limo.html"><a href="limo.html#do-linear-models-create-good-explanations"><i class="fa fa-check"></i><b>4.1.8</b> Do linear models create good explanations?</a></li>
<li class="chapter" data-level="4.1.9" data-path="limo.html"><a href="limo.html#extending-linear-models"><i class="fa fa-check"></i><b>4.1.9</b> Extending Linear Models</a></li>
<li class="chapter" data-level="4.1.10" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>4.1.10</b> Sparse linear models</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>4.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.2.1" data-path="logistic.html"><a href="logistic.html#whats-wrong-with-linear-regression-models-for-classification"><i class="fa fa-check"></i><b>4.2.1</b> What’s Wrong with Linear Regression Models for Classification?</a></li>
<li class="chapter" data-level="4.2.2" data-path="logistic.html"><a href="logistic.html#logistic-regression"><i class="fa fa-check"></i><b>4.2.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="4.2.3" data-path="logistic.html"><a href="logistic.html#interpretation-1"><i class="fa fa-check"></i><b>4.2.3</b> Interpretation</a></li>
<li class="chapter" data-level="4.2.4" data-path="logistic.html"><a href="logistic.html#example"><i class="fa fa-check"></i><b>4.2.4</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>4.3</b> Decision Tree</a><ul>
<li class="chapter" data-level="4.3.1" data-path="tree.html"><a href="tree.html#interpretation-2"><i class="fa fa-check"></i><b>4.3.1</b> Interpretation</a></li>
<li class="chapter" data-level="4.3.2" data-path="tree.html"><a href="tree.html#interpretation-example-1"><i class="fa fa-check"></i><b>4.3.2</b> Interpretation Example</a></li>
<li class="chapter" data-level="4.3.3" data-path="tree.html"><a href="tree.html#advantages"><i class="fa fa-check"></i><b>4.3.3</b> Advantages</a></li>
<li class="chapter" data-level="4.3.4" data-path="tree.html"><a href="tree.html#disadvantages"><i class="fa fa-check"></i><b>4.3.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>4.4</b> RuleFit</a><ul>
<li class="chapter" data-level="4.4.1" data-path="rulefit.html"><a href="rulefit.html#interpretation-and-example"><i class="fa fa-check"></i><b>4.4.1</b> Interpretation and Example</a></li>
<li class="chapter" data-level="4.4.2" data-path="rulefit.html"><a href="rulefit.html#guidelines"><i class="fa fa-check"></i><b>4.4.2</b> Guidelines</a></li>
<li class="chapter" data-level="4.4.3" data-path="rulefit.html"><a href="rulefit.html#theory"><i class="fa fa-check"></i><b>4.4.3</b> Theory</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="other-interpretable-models.html"><a href="other-interpretable-models.html"><i class="fa fa-check"></i><b>4.5</b> Other Interpretable Models</a><ul>
<li class="chapter" data-level="4.5.1" data-path="other-interpretable-models.html"><a href="other-interpretable-models.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>4.5.1</b> Naive Bayes classifier</a></li>
<li class="chapter" data-level="4.5.2" data-path="other-interpretable-models.html"><a href="other-interpretable-models.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>4.5.2</b> K-Nearest Neighbours</a></li>
<li class="chapter" data-level="4.5.3" data-path="other-interpretable-models.html"><a href="other-interpretable-models.html#and-so-many-more"><i class="fa fa-check"></i><b>4.5.3</b> And so many more …</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>5</b> Model-Agnostic Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>5.1</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="5.1.1" data-path="pdp.html"><a href="pdp.html#examples"><i class="fa fa-check"></i><b>5.1.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>5.2</b> Individual Conditional Expectation (ICE) Plot</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ice.html"><a href="ice.html#example-1"><i class="fa fa-check"></i><b>5.2.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>5.3</b> Feature Importance</a><ul>
<li class="chapter" data-level="5.3.1" data-path="feature-importance.html"><a href="feature-importance.html#the-theory"><i class="fa fa-check"></i><b>5.3.1</b> The Theory</a></li>
<li class="chapter" data-level="5.3.2" data-path="feature-importance.html"><a href="feature-importance.html#example-and-interpretation"><i class="fa fa-check"></i><b>5.3.2</b> Example and Interpretation</a></li>
<li class="chapter" data-level="5.3.3" data-path="feature-importance.html"><a href="feature-importance.html#advantages-1"><i class="fa fa-check"></i><b>5.3.3</b> Advantages</a></li>
<li class="chapter" data-level="5.3.4" data-path="feature-importance.html"><a href="feature-importance.html#disadvantages-1"><i class="fa fa-check"></i><b>5.3.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>5.4</b> Local Surrogate Models (LIME)</a><ul>
<li class="chapter" data-level="5.4.1" data-path="lime.html"><a href="lime.html#lime-for-tabular-data"><i class="fa fa-check"></i><b>5.4.1</b> LIME for Tabular Data</a></li>
<li class="chapter" data-level="5.4.2" data-path="lime.html"><a href="lime.html#lime-for-text"><i class="fa fa-check"></i><b>5.4.2</b> LIME for Text</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>5.5</b> Shapley Value Explanations</a><ul>
<li class="chapter" data-level="5.5.1" data-path="shapley.html"><a href="shapley.html#the-general-idea"><i class="fa fa-check"></i><b>5.5.1</b> The general idea</a></li>
<li class="chapter" data-level="5.5.2" data-path="shapley.html"><a href="shapley.html#examples-and-interpretation"><i class="fa fa-check"></i><b>5.5.2</b> Examples and Interpretation</a></li>
<li class="chapter" data-level="5.5.3" data-path="shapley.html"><a href="shapley.html#the-shapley-value-in-detail"><i class="fa fa-check"></i><b>5.5.3</b> The Shapley Value in Detail</a></li>
<li class="chapter" data-level="5.5.4" data-path="shapley.html"><a href="shapley.html#advantages-2"><i class="fa fa-check"></i><b>5.5.4</b> Advantages</a></li>
<li class="chapter" data-level="5.5.5" data-path="shapley.html"><a href="shapley.html#disadvantages-2"><i class="fa fa-check"></i><b>5.5.5</b> Disadvantages</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>6</b> Contribute</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="rulefit" class="section level2">
<h2><span class="header-section-number">4.4</span> RuleFit</h2>
<p>The RuleFit algorithm <span class="citation">(J. H. Friedman and Popescu <a href="#ref-friedman2008predictive">2008</a>)</span> fits sparse linear models which include automatically detected interaction effects in the form of binary decision rules.</p>
The standard linear model doesn’t account for interactions between the features. Wouldn’t it be convenient to have a model that is as simple and interpretable as linear models, but that also integrates feature interactions? RuleFit fills this gap. RuleFit fits a sparse linear model with the original features and also a set of new features which are decision rules. These new features capture interactions between the original features. RuleFit generates these features automatically from decision trees. Each path through a tree can be turned into a decision rule by combining the split decisions to a rule, as in Figure <a href="rulefit.html#fig:rulefit-split">4.9</a>. The node predictions are thrown away and only the splits are used in the decision rules.
<div class="figure" style="text-align: center"><span id="fig:rulefit-split"></span>
<img src="images/rulefit.jpg" alt="4 rules can be generated from a tree with 3 terminal nodes." width="80%" />
<p class="caption">
FIGURE 4.9: 4 rules can be generated from a tree with 3 terminal nodes.
</p>
</div>
<p>Where do the decision trees come from? These are trees that are trained to predict the outcome of interest, so that the splits are meaningful for the task at hand and not arbitrary. Any algorithm that creates a lot of trees can be used for RuleFit, like a Random Forest for example. Each tree is disassembled into decision rules, which are used as additional features in a linear Lasso model.</p>
<p>The RuleFit paper uses the Boston housing data for illustration: The goal is to predict the median house value in the Boston neighbourhood. One of the rules (read: features) generated by RuleFit: “if (number of rooms <span class="math inline">\(&gt; 6.64\)</span>) and (concentration of nitric oxide <span class="math inline">\(&lt; 0.67\)</span>) then <span class="math inline">\(1\)</span> else <span class="math inline">\(0\)</span>”</p>
<p>RuleFit also comes with a feature importance measurement, which helps to identify linear terms and rules that are important for the prediction. The feature importance is calculated from the weights of the regression model. The importance measure can be aggregated for the original features (which appear once untransformed and possibly in many decision rules).</p>
<p>RuleFit also introduces partial dependence plots to plot the average change of the prediction by changing one feature. The partial dependence plot is a model-agnostic method, which can be used with any model, and it has its own part in the book, see Chapter <a href="pdp.html#pdp">5.1</a>.</p>
<div id="interpretation-and-example" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Interpretation and Example</h3>
<p>Since RuleFit estimates a linear model in the end, the interpretation is equivalent to linear models described in Chapter <a href="limo.html#limo">4.1</a>. The only difference is that the model has new features that are coming from decision rules. Decision rules are binary features: A value of 1 means that all conditions of the rule are met, otherwise the value is 0. For linear terms in RuleFit, the interpretation is the same as in linear regression models: If <span class="math inline">\(x_j\)</span> increases by one unit, the predicted outcome changes by <span class="math inline">\(\beta_j\)</span>.</p>
<p>In this example, we use RuleFit to predict the number of bike rentals on a given day (see Chapter <a href="bike-data.html#bike-data">3.1</a>). The rules generated for the bike rental prediction task can be seen in Figure <a href="rulefit.html#fig:rulefit-example">4.10</a>. The most important rule was: “days_since_2011 &gt; 428 &amp; temp &gt; 5.081651” and the associated weight is 632.6. The interpretation is: If days_since_2011 &gt; 428 &amp; temp &gt; 5.081651, then the predicted number of bike rentals increases by 632.6, given all other features values stay fixed. In total, 330 such rules were created from the original 8 features. Quite a lot! But thanks to Lasso, only 44 of the 330 got a weight different from zero.</p>
<div class="figure" style="text-align: center"><span id="fig:rulefit-example"></span>
<div id="htmlwidget-94285ba5c6903b17154e" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-94285ba5c6903b17154e">{"x":{"style":"bootstrap","filter":"none","data":[["days_since_2011 &gt; 428 &amp; temp &gt; 5.081651","temp &gt; 9.86 &amp; weathersit %in% c(\"GOOD\", \"MISTY\")","temp &lt;= 13.776651 &amp; days_since_2011 &lt;= 367","days_since_2011 &gt; 111 &amp; weathersit %in% c(\"GOOD\", \"MISTY\")","temp &lt;= 15.663466 &amp; temp &lt;= 3.945849","temp &gt; 12.758349 &amp; days_since_2011 &gt; 517","temp &lt;= 13.228349 &amp; season %in% c(\"SPRING\")","temp &gt; 8.058349 &amp; weathersit %in% c(\"GOOD\", \"MISTY\")","temp &lt;= 12.68 &amp; temp &lt;= 5.16","days_since_2011 &gt; 108 &amp; weathersit %in% c(\"GOOD\")","temp &gt; 11.583349 &amp; days_since_2011 &gt; 436","days_since_2011 &gt; 108 &amp; weathersit %in% c(\"GOOD\", \"MISTY\")","days_since_2011 &gt; 434 &amp; temp &lt;= 15.6175","temp &gt; 8.184356 &amp; days_since_2011 &lt;= 440","temp &lt;= 15.663466","days_since_2011 &gt; 430 &amp; temp &lt;= 5.691288","temp &lt;= 13.0325 &amp; days_since_2011 &lt;= 387","days_since_2011 &lt;= 428 &amp; temp &lt;= 20.160849","days_since_2011 &gt; 428 &amp; windspeed &lt;= 20.125661","temp &gt; 7.98 &amp; days_since_2011 &gt; 391","temp &gt; 13.7375 &amp; days_since_2011 &gt; 500","days_since_2011 &gt; 415 &amp; weathersit %in% c(\"MISTY\", \"RAIN/SNOW/STORM\")","temp &lt;= 11.544151 &amp; days_since_2011 &gt; 102","temp &lt;= 11.544151 &amp; season %in% c(\"SPRING\", \"SUMMER\")","temp &lt;= 12.758349 &amp; weathersit %in% c(\"RAIN/SNOW/STORM\")","temp &lt;= 13.776651 &amp; days_since_2011 &gt; 367","days_since_2011 &gt; 430 &amp; temp &lt;= 15.6175","temp &lt;= 7.98 &amp; season %in% c(\"SPRING\", \"SUMMER\")","temp &lt;= 16.0875","days_since_2011 &gt; 408 &amp; temp &gt; 11.544151","days_since_2011 &gt; 434 &amp; temp &gt; 15.6175","temp &lt;= 15.774151","temp &lt;= 16.0875 &amp; days_since_2011 &gt; 123","days_since_2011 &gt; 440 &amp; weathersit %in% c(\"GOOD\")","temp &lt;= 6.954554 &amp; season %in% c(\"SPRING\", \"SUMMER\")","days_since_2011 &gt; 428 &amp; windspeed &gt; 20.125661","days_since_2011 &lt;= 440 &amp; temp &gt; 13.7375","days_since_2011 &gt; 415 &amp; weathersit %in% c(\"GOOD\")","4.2615484 &lt;= windspeed &lt;= 24.43763025","temp &gt; 8.184356 &amp; days_since_2011 &gt; 440","0.28373825 &lt;= temp &lt;= 29.2377005","37.45835 &lt;= hum &lt;= 90.156225","temp &lt;= 11.583349","temp &gt; 6.954554 &amp; days_since_2011 &gt; 381","temp &lt;= 12.249151","temp &lt;= 12.249151 &amp; days_since_2011 &lt;= 273","temp &lt;= 12.249151 &amp; days_since_2011 &gt; 273","temp &gt; 12.249151 &amp; days_since_2011 &lt;= 436","temp &gt; 12.249151 &amp; days_since_2011 &gt; 436","temp &lt;= 13.0325","temp &lt;= 13.0325 &amp; days_since_2011 &gt; 387","temp &gt; 13.0325 &amp; days_since_2011 &lt;= 436","temp &gt; 13.0325 &amp; days_since_2011 &gt; 436","days_since_2011 &lt;= 429","days_since_2011 &lt;= 429 &amp; temp &lt;= 13.0325","days_since_2011 &lt;= 429 &amp; temp &gt; 13.0325","days_since_2011 &gt; 429 &amp; temp &lt;= 10.055849","days_since_2011 &gt; 429 &amp; temp &gt; 10.055849","temp &lt;= 11.583349 &amp; days_since_2011 &lt;= 301","temp &lt;= 11.583349 &amp; days_since_2011 &gt; 301","temp &gt; 11.583349 &amp; days_since_2011 &lt;= 436","days_since_2011 &lt;= 434","days_since_2011 &lt;= 434 &amp; temp &lt;= 13.0325","days_since_2011 &lt;= 434 &amp; temp &gt; 13.0325","days_since_2011 &gt; 434 &amp; temp &lt;= 10.6825","days_since_2011 &gt; 434 &amp; temp &gt; 10.6825","days_since_2011 &lt;= 434 &amp; temp &lt;= 14.09","days_since_2011 &lt;= 434 &amp; temp &gt; 14.09","days_since_2011 &gt; 434 &amp; temp &lt;= 15.5","days_since_2011 &gt; 434 &amp; temp &gt; 15.5","temp &lt;= 12.719151","temp &lt;= 12.719151 &amp; days_since_2011 &lt;= 91","temp &lt;= 12.719151 &amp; days_since_2011 &gt; 91","temp &gt; 12.719151 &amp; days_since_2011 &lt;= 448","temp &gt; 12.719151 &amp; days_since_2011 &gt; 448","temp &lt;= 12.230445 &amp; days_since_2011 &lt;= 123","temp &lt;= 12.230445 &amp; days_since_2011 &gt; 123","temp &gt; 12.230445 &amp; days_since_2011 &lt;= 440","temp &gt; 12.230445 &amp; days_since_2011 &gt; 440","temp &lt;= 12.230445 &amp; days_since_2011 &lt;= 111","temp &lt;= 12.230445 &amp; days_since_2011 &gt; 111","temp &gt; 12.230445 &amp; days_since_2011 &lt;= 450","temp &gt; 12.230445 &amp; days_since_2011 &gt; 450","days_since_2011 &lt;= 434 &amp; temp &lt;= 13.463349","days_since_2011 &lt;= 434 &amp; temp &gt; 13.463349","days_since_2011 &lt;= 434 &amp; temp &lt;= 11.583349","days_since_2011 &lt;= 434 &amp; temp &gt; 11.583349","days_since_2011 &gt; 434 &amp; temp &lt;= 4.494151","days_since_2011 &gt; 434 &amp; temp &gt; 4.494151","days_since_2011 &lt;= 429 &amp; temp &lt;= 13.463349","days_since_2011 &lt;= 429 &amp; temp &gt; 13.463349","days_since_2011 &gt; 429 &amp; temp &lt;= 12.68","days_since_2011 &gt; 429 &amp; temp &gt; 12.68","days_since_2011 &lt;= 428","days_since_2011 &lt;= 428 &amp; temp &lt;= 13.541651","days_since_2011 &lt;= 428 &amp; temp &gt; 13.541651","days_since_2011 &gt; 428 &amp; temp &lt;= 12.68","days_since_2011 &lt;= 430","days_since_2011 &lt;= 430 &amp; temp &lt;= 14.09","days_since_2011 &lt;= 430 &amp; temp &gt; 14.09","days_since_2011 &gt; 430 &amp; temp &lt;= 9.703349","days_since_2011 &gt; 430 &amp; temp &gt; 9.703349","temp &lt;= 11.544151 &amp; days_since_2011 &lt;= 102","days_since_2011 &lt;= 429 &amp; temp &lt;= 14.09","days_since_2011 &gt; 429 &amp; temp &lt;= 7.98","days_since_2011 &gt; 429 &amp; temp &gt; 7.98","temp &gt; 12.249151 &amp; days_since_2011 &lt;= 418","temp &gt; 12.249151 &amp; days_since_2011 &gt; 418","days_since_2011 &lt;= 434 &amp; temp &lt;= 15.6175","days_since_2011 &lt;= 434 &amp; temp &gt; 15.6175","days_since_2011 &gt; 434 &amp; season %in% c(\"SPRING\")","days_since_2011 &gt; 434 &amp; season %in% c(\"SUMMER\", \"FALL\", \"WINTER\")","days_since_2011 &gt; 429 &amp; weathersit %in% c(\"GOOD\")","days_since_2011 &gt; 429 &amp; weathersit %in% c(\"MISTY\", \"RAIN/SNOW/STORM\")","days_since_2011 &lt;= 433","days_since_2011 &lt;= 433 &amp; temp &lt;= 11.583349","days_since_2011 &gt; 433 &amp; temp &lt;= 5.20089","days_since_2011 &gt; 433 &amp; temp &gt; 5.20089","days_since_2011 &gt; 434 &amp; temp &lt;= 13.776651","days_since_2011 &gt; 434 &amp; temp &gt; 13.776651","temp &lt;= 12.758349","temp &lt;= 12.758349 &amp; days_since_2011 &lt;= 273","temp &lt;= 12.758349 &amp; days_since_2011 &gt; 273","temp &gt; 12.758349 &amp; days_since_2011 &lt;= 418","temp &gt; 12.758349 &amp; days_since_2011 &gt; 418","temp &lt;= 13.7375","temp &lt;= 13.7375 &amp; days_since_2011 &lt;= 111","temp &lt;= 13.7375 &amp; days_since_2011 &gt; 111","temp &gt; 13.7375 &amp; days_since_2011 &lt;= 500","temp &lt;= 12.249151 &amp; days_since_2011 &lt;= 301","temp &lt;= 12.249151 &amp; days_since_2011 &gt; 301","temp &gt; 12.249151 &amp; days_since_2011 &lt;= 432","temp &gt; 12.249151 &amp; days_since_2011 &gt; 432","temp &lt;= 12.719151 &amp; days_since_2011 &lt;= 111","temp &lt;= 12.719151 &amp; days_since_2011 &gt; 111","temp &gt; 12.719151 &amp; days_since_2011 &lt;= 432","temp &gt; 12.719151 &amp; days_since_2011 &gt; 432","days_since_2011 &lt;= 429 &amp; temp &lt;= 13.541651","days_since_2011 &gt; 429 &amp; temp &lt;= 10.643349","days_since_2011 &gt; 429 &amp; temp &gt; 10.643349","days_since_2011 &lt;= 430 &amp; temp &lt;= 12.719151","days_since_2011 &lt;= 430 &amp; temp &gt; 12.719151","days_since_2011 &gt; 430 &amp; temp &gt; 15.6175","temp &lt;= 11.3875","temp &lt;= 11.3875 &amp; days_since_2011 &lt;= 273","temp &lt;= 11.3875 &amp; days_since_2011 &gt; 273","temp &gt; 11.3875 &amp; days_since_2011 &lt;= 427","temp &gt; 11.3875 &amp; days_since_2011 &gt; 427","days_since_2011 &gt; 434 &amp; weathersit %in% c(\"GOOD\")","days_since_2011 &gt; 434 &amp; weathersit %in% c(\"MISTY\", \"RAIN/SNOW/STORM\")","days_since_2011 &gt; 428 &amp; temp &lt;= 5.20089","days_since_2011 &gt; 428 &amp; temp &gt; 5.20089","temp &lt;= 12.758349 &amp; temp &lt;= 5.081651","temp &lt;= 12.758349 &amp; temp &gt; 5.081651","temp &gt; 12.758349 &amp; days_since_2011 &lt;= 425","temp &lt;= 11.505","temp &lt;= 11.505 &amp; days_since_2011 &lt;= 91","temp &lt;= 11.505 &amp; days_since_2011 &gt; 91","temp &gt; 11.505 &amp; days_since_2011 &gt; 432","temp &lt;= 13.776651 &amp; days_since_2011 &lt;= 90","temp &lt;= 13.776651 &amp; days_since_2011 &gt; 90","temp &gt; 13.776651 &amp; days_since_2011 &lt;= 436","temp &gt; 13.776651 &amp; days_since_2011 &gt; 436","days_since_2011 &lt;= 428 &amp; temp &lt;= 7.285199","days_since_2011 &lt;= 428 &amp; temp &gt; 7.285199","days_since_2011 &gt; 428 &amp; temp &lt;= 5.081651","temp &gt; 13.776651 &amp; days_since_2011 &lt;= 432","temp &lt;= 10.287277","temp &lt;= 10.287277 &amp; days_since_2011 &lt;= 301","temp &lt;= 10.287277 &amp; days_since_2011 &gt; 301","temp &gt; 10.287277 &amp; days_since_2011 &lt;= 452","temp &gt; 10.287277 &amp; days_since_2011 &gt; 452","days_since_2011 &lt;= 434 &amp; temp &lt;= 4.833021","days_since_2011 &lt;= 434 &amp; temp &gt; 4.833021","days_since_2011 &lt;= 430 &amp; temp &lt;= 13.541651","days_since_2011 &gt; 430 &amp; weathersit %in% c(\"GOOD\")","temp &lt;= 13.776651 &amp; days_since_2011 &lt;= 102","temp &lt;= 13.776651 &amp; days_since_2011 &gt; 102","temp &lt;= 13.228349","temp &lt;= 13.228349 &amp; season %in% c(\"SUMMER\", \"WINTER\")","temp &gt; 13.228349 &amp; days_since_2011 &lt;= 432","temp &gt; 13.228349 &amp; days_since_2011 &gt; 432","temp &lt;= 11.505 &amp; days_since_2011 &lt;= 94","temp &lt;= 11.505 &amp; days_since_2011 &gt; 94","temp &gt; 11.505 &amp; days_since_2011 &lt;= 425","temp &gt; 11.505 &amp; days_since_2011 &gt; 425","temp &lt;= 12.68","temp &lt;= 12.68 &amp; temp &gt; 5.16","temp &gt; 12.68 &amp; days_since_2011 &lt;= 436","temp &gt; 12.68 &amp; days_since_2011 &gt; 436","temp &lt;= 13.7375 &amp; days_since_2011 &lt;= 407","temp &lt;= 13.7375 &amp; days_since_2011 &gt; 407","days_since_2011 &lt;= 428 &amp; temp &lt;= 14.09","days_since_2011 &gt; 428 &amp; weathersit %in% c(\"GOOD\")","days_since_2011 &lt;= 430 &amp; temp &lt;= 13.776651","days_since_2011 &lt;= 430 &amp; temp &gt; 13.776651","temp &gt; 12.249151 &amp; days_since_2011 &lt;= 499","temp &gt; 12.249151 &amp; days_since_2011 &gt; 499","temp &lt;= 12.249151 &amp; season %in% c(\"SPRING\", \"SUMMER\")","temp &lt;= 12.249151 &amp; season %in% c(\"WINTER\")","temp &gt; 12.249151 &amp; days_since_2011 &lt;= 396","temp &gt; 12.249151 &amp; days_since_2011 &gt; 396","days_since_2011 &lt;= 429 &amp; temp &lt;= 6.256651","days_since_2011 &lt;= 429 &amp; temp &gt; 6.256651","days_since_2011 &gt; 429 &amp; temp &lt;= 6.954554","days_since_2011 &gt; 429 &amp; temp &gt; 6.954554","days_since_2011 &lt;= 430 &amp; temp &lt;= 6.1","days_since_2011 &lt;= 430 &amp; temp &gt; 6.1","days_since_2011 &lt;= 430 &amp; temp &gt; 4.833021","days_since_2011 &gt; 430 &amp; season %in% c(\"SPRING\")","days_since_2011 &lt;= 424","days_since_2011 &lt;= 424 &amp; temp &lt;= 13.776651","days_since_2011 &lt;= 424 &amp; temp &gt; 13.776651","days_since_2011 &gt; 424 &amp; temp &lt;= 10.643349","days_since_2011 &gt; 424 &amp; temp &gt; 10.643349","temp &lt;= 13.580849","temp &lt;= 13.580849 &amp; weathersit %in% c(\"GOOD\", \"MISTY\")","temp &lt;= 13.580849 &amp; weathersit %in% c(\"RAIN/SNOW/STORM\")","temp &gt; 13.580849 &amp; days_since_2011 &lt;= 331","temp &gt; 13.580849 &amp; days_since_2011 &gt; 331","temp &lt;= 11.544151 &amp; season %in% c(\"WINTER\")","temp &lt;= 12.484151","temp &lt;= 12.484151 &amp; days_since_2011 &lt;= 72","temp &lt;= 12.484151 &amp; days_since_2011 &gt; 72","temp &gt; 12.484151 &amp; days_since_2011 &lt;= 332","temp &gt; 12.484151 &amp; days_since_2011 &gt; 332","temp &lt;= 13.776651 &amp; weathersit %in% c(\"GOOD\", \"MISTY\")","temp &lt;= 13.776651 &amp; weathersit %in% c(\"RAIN/SNOW/STORM\")","temp &gt; 13.776651 &amp; days_since_2011 &lt;= 448","temp &gt; 13.776651 &amp; days_since_2011 &gt; 448","days_since_2011 &lt;= 430 &amp; temp &lt;= 13.0325","days_since_2011 &gt; 430 &amp; temp &gt; 5.691288","days_since_2011 &lt;= 434 &amp; temp &lt;= 13.541651","days_since_2011 &lt;= 434 &amp; temp &gt; 13.541651","temp &lt;= 13.776651 &amp; days_since_2011 &lt;= 105","temp &lt;= 13.776651 &amp; days_since_2011 &gt; 105","temp &gt; 13.776651 &amp; days_since_2011 &lt;= 499","temp &gt; 13.776651 &amp; days_since_2011 &gt; 499","temp &lt;= 8.184356","temp &lt;= 8.184356 &amp; season %in% c(\"SPRING\", \"SUMMER\")","temp &lt;= 8.184356 &amp; season %in% c(\"WINTER\")","days_since_2011 &lt;= 105","days_since_2011 &lt;= 105 &amp; days_since_2011 &lt;= 68","days_since_2011 &lt;= 105 &amp; days_since_2011 &gt; 68","days_since_2011 &gt; 105 &amp; weathersit %in% c(\"GOOD\", \"MISTY\")","days_since_2011 &gt; 105 &amp; weathersit %in% c(\"RAIN/SNOW/STORM\")","days_since_2011 &lt;= 429 &amp; temp &lt;= 9.546651","days_since_2011 &lt;= 429 &amp; temp &gt; 9.546651","temp &lt;= 6.954554","temp &lt;= 6.954554 &amp; season %in% c(\"WINTER\")","temp &gt; 6.954554 &amp; days_since_2011 &lt;= 381","temp &gt; 8.184356 &amp; days_since_2011 &lt;= 452","temp &gt; 8.184356 &amp; days_since_2011 &gt; 452","temp &lt;= 15.663466 &amp; temp &gt; 3.945849","temp &gt; 15.663466 &amp; days_since_2011 &lt;= 432","days_since_2011 &lt;= 440","days_since_2011 &lt;= 440 &amp; temp &lt;= 13.7375","days_since_2011 &gt; 440 &amp; weathersit %in% c(\"MISTY\", \"RAIN/SNOW/STORM\")","temp &lt;= 9.86","temp &lt;= 9.86 &amp; season %in% c(\"SPRING\", \"SUMMER\")","temp &lt;= 9.86 &amp; season %in% c(\"WINTER\")","temp &gt; 9.86 &amp; weathersit %in% c(\"RAIN/SNOW/STORM\")","days_since_2011 &lt;= 408","days_since_2011 &lt;= 408 &amp; temp &lt;= 9.546651","days_since_2011 &lt;= 408 &amp; temp &gt; 9.546651","days_since_2011 &gt; 408 &amp; temp &lt;= 11.544151","temp &gt; 8.184356 &amp; days_since_2011 &lt;= 450","temp &gt; 8.184356 &amp; days_since_2011 &gt; 450","days_since_2011 &lt;= 424 &amp; temp &lt;= 13.463349","days_since_2011 &lt;= 424 &amp; temp &gt; 13.463349","days_since_2011 &gt; 424 &amp; weathersit %in% c(\"GOOD\")","days_since_2011 &gt; 424 &amp; weathersit %in% c(\"MISTY\", \"RAIN/SNOW/STORM\")","temp &gt; 12.758349 &amp; days_since_2011 &lt;= 339","temp &gt; 12.758349 &amp; days_since_2011 &gt; 339","temp &lt;= 10.287277 &amp; season %in% c(\"SPRING\")","temp &lt;= 10.287277 &amp; season %in% c(\"SUMMER\", \"WINTER\")","temp &gt; 10.287277 &amp; weathersit %in% c(\"GOOD\", \"MISTY\")","temp &gt; 10.287277 &amp; weathersit %in% c(\"RAIN/SNOW/STORM\")","days_since_2011 &lt;= 440 &amp; temp &lt;= 3.945849","days_since_2011 &lt;= 440 &amp; temp &gt; 3.945849","temp &lt;= 16.0875 &amp; days_since_2011 &lt;= 123","temp &gt; 16.0875 &amp; days_since_2011 &lt;= 432","temp &gt; 16.0875 &amp; days_since_2011 &gt; 432","temp &lt;= 8.058349","temp &lt;= 8.058349 &amp; season %in% c(\"SPRING\", \"SUMMER\")","temp &lt;= 8.058349 &amp; season %in% c(\"WINTER\")","temp &lt;= 12.719151 &amp; days_since_2011 &lt;= 123","temp &lt;= 12.719151 &amp; days_since_2011 &gt; 123","temp &gt; 12.719151 &amp; days_since_2011 &lt;= 499","temp &gt; 12.719151 &amp; days_since_2011 &gt; 499","temp &lt;= 12.993349","temp &lt;= 12.993349 &amp; days_since_2011 &gt; 90","temp &gt; 12.993349 &amp; days_since_2011 &lt;= 330","temp &gt; 12.993349 &amp; days_since_2011 &gt; 330","days_since_2011 &lt;= 108","days_since_2011 &lt;= 108 &amp; days_since_2011 &gt; 68","temp &lt;= 13.228349 &amp; days_since_2011 &lt;= 273","temp &lt;= 13.228349 &amp; days_since_2011 &gt; 273","temp &gt; 13.228349 &amp; days_since_2011 &lt;= 517","temp &gt; 13.228349 &amp; days_since_2011 &gt; 517","days_since_2011 &gt; 108 &amp; weathersit %in% c(\"MISTY\", \"RAIN/SNOW/STORM\")","days_since_2011 &lt;= 428 &amp; temp &gt; 20.160849","temp &lt;= 15.774151 &amp; weathersit %in% c(\"GOOD\", \"MISTY\")","temp &gt; 15.774151 &amp; days_since_2011 &lt;= 444","temp &gt; 15.774151 &amp; days_since_2011 &gt; 444","temp &lt;= 7.98","temp &lt;= 7.98 &amp; season %in% c(\"WINTER\")","temp &gt; 7.98 &amp; days_since_2011 &lt;= 391","days_since_2011 &lt;= 111","days_since_2011 &lt;= 111 &amp; days_since_2011 &gt; 68","temp &lt;= 12.758349 &amp; weathersit %in% c(\"GOOD\", \"MISTY\")","temp &gt; 12.758349 &amp; days_since_2011 &lt;= 517","temp &lt;= 4.768349","temp &lt;= 4.768349 &amp; hum &lt;= 77.9583","temp &lt;= 4.768349 &amp; hum &gt; 77.9583","temp &gt; 4.768349 &amp; days_since_2011 &lt;= 517","temp &gt; 4.768349 &amp; days_since_2011 &gt; 517","days_since_2011 &lt;= 428 &amp; temp &lt;= 13.7375","days_since_2011 &lt;= 415","days_since_2011 &lt;= 415 &amp; temp &lt;= 13.541651","days_since_2011 &lt;= 415 &amp; temp &gt; 13.541651","18.25 &lt;= days_since_2011 &lt;= 711.75"],[632.6,524.7,-489.6,469.5,-415.7,389.6,-383.5,379.5,-312.6,290.5,279.5,279.1,-271.8,-259.1,-190.3,-183.6,-173.8,-168.9,168.1,165.1,161.5,-159.5,-142.8,-130.7,-126.7,119.1,-108.4,-86.6,-68.6,67.1,62.4,-61.7,-55.9,55.4,-52.1,-50.7,-47.2,40.5,-38.1,34.1,-33.2,-19.1,-9.8,1.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[309.3,249.8,209.5,179.1,120.8,156.2,160.8,170.4,107.5,144.6,129.1,105.5,98,126.5,95.2,28.5,75.9,83.2,81.5,80.2,67,56.9,62.7,57.3,15.4,48.7,39.5,33.9,34.3,31.1,27.1,30.9,26.9,24.2,19.5,9.6,21,18.2,189.5,16.1,281.1,260.6,4.8,0.7,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[0.49,0.48,0.43,0.38,0.29,0.4,0.42,0.45,0.34,0.5,0.46,0.38,0.36,0.49,0.5,0.16,0.44,0.49,0.48,0.49,0.41,0.36,0.44,0.44,0.12,0.41,0.36,0.39,0.5,0.46,0.43,0.5,0.48,0.44,0.37,0.19,0.45,0.45,4.98,0.47,8.46,13.62,0.49,0.49,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null]],"container":"<table class=\"table table-striped table-hover\">\n  <thead>\n    <tr>\n      <th>Description<\/th>\n      <th>Weight<\/th>\n      <th>Importance<\/th>\n      <th>Std. Dev<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"scrollX":true,"columnDefs":[{"className":"dt-right","targets":[1,2,3]}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 4.10: The table shows the generated rules and weights after fitting ‘RuleFit’ on the bike dataset.
</p>
</div>
Computing the global feature importances reveals that temperature and the time trend are the most important features, see Figure <a href="rulefit.html#fig:rulefit-importance">4.11</a>. The feature importance measurement includes the importance of the raw feature term and all the decision rules the feature appears in.
<div class="figure" style="text-align: center"><span id="fig:rulefit-importance"></span>
<img src="interpretable-ml_files/figure-html/rulefit-importance-1.svg" alt="Feature importance measures for a RuleFit model predicting bike rentals. The most important features for the predictions were temperature and the time trend." width="80%" />
<p class="caption">
FIGURE 4.11: Feature importance measures for a RuleFit model predicting bike rentals. The most important features for the predictions were temperature and the time trend.
</p>
</div>
</div>
<div id="guidelines" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Guidelines</h3>
<p>In this section we will talk about the advantages and disadvantages of RuleFit and how to interpret it.</p>
<p><strong>Interpretation template</strong></p>
<p>The interpretation is analogue to linear models: The predicted outcome changes by <span class="math inline">\(\beta_j\)</span> if feature <span class="math inline">\(x_j\)</span> changes by one unit, given all other features stay the same. The weight interpretation of a decision rule is a special case: If all conditions of a decision rule <span class="math inline">\(r_k\)</span> apply, the predicted outcome changes by <span class="math inline">\(\alpha_k\)</span> (the learned weight for rule <span class="math inline">\(r_k\)</span> in the linear model). And, respectively, for classification: If all conditions of decision rule <span class="math inline">\(r_k\)</span> apply, the odds for event vs. no-event changes by a factor of <span class="math inline">\(\alpha_k\)</span>.</p>
<p><strong>The Good</strong>:</p>
<ul>
<li>RuleFit adds <strong>feature interactions</strong> automatically to linear models. Therefore it solves the problem of linear models that you have to add interaction terms manually and it helps a bit with the issue of modeling non-linear relationships.</li>
<li>RuleFit can handle both classification and regression tasks.</li>
<li>The created rules are easy to interpret, because they are binary decision rules. Either the rule applies to an instance or not. Good interpretability is only guaranteed as long as the number of conditions within a rule is not to big. A rule with 1 to 3 conditions seems reasonable to me. This translates into a maximum depth of 3 for the trees in the tree ensemble.</li>
<li>Even if there are many rules in the model, they do not apply to each instance, so for one instance only a handful of rules are important (non-zero weights). This improves local interpretability.</li>
<li>RuleFit comes with a bunch of tools, namely an importance measurement, interaction effects calculation and partial dependence plots.</li>
</ul>
<p><strong>The Bad</strong>:</p>
<ul>
<li>Sometimes RuleFit creates many rules which get a non-zero weight in the Lasso model. The interpretability degrades with higher number of features in the model. A promising solution is to force feature effects to be monotonic, meaning that an increase in a feature has to result in an increase of the predicted outcome.</li>
<li>An anecdotal drawback: The papers claim good performance of RuleFit - often close to the predictive performance of Random Forests! - yet in the few cases where I personally tried it, the performance was disappointing.</li>
<li>The end product of the RuleFit procedure is a linear model with additional fancy features (the decision rules). But since it is a linear model, the weight interpretation is still unintuitive (given all features are fixed, increasing feature <span class="math inline">\(x_j\)</span> by one unit, increases the predicted outcome by <span class="math inline">\(\beta_j\)</span>). It gets a bit more tricky if you have overlapping rules: For example one decision rule (feature) for the bike prediction could be: “temp &gt; 15” and another rule could be “temp &gt; 10 &amp; weather=‘GOOD’”. When the weather is good and the temperature is above 10 degrees, the temperature is automatically also always bigger then 15, which means in the cases where the second rule applies, the first one also always applies. The interpretation of the estimated weight for the second rule is: ‘Given all other features are fixed, the predicted number of bikes increases by <span class="math inline">\(\beta_2\)</span>’. BUT, now it becomes really clear that the ‘all other feature fixed’ is problematic, because if rule 2 applies, also rule 1 applies and the interpretation is nonsensical.</li>
</ul>
<p>The RuleFit algorithm is implemented in R by <span class="citation">Fokkema and Christoffersen (<a href="#ref-pre2017">2017</a>)</span> and you can find a <a href="https://github.com/christophM/rulefit">Python version on Github</a>.</p>
</div>
<div id="theory" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Theory</h3>
<p>Let’s dive deeper into the technicalities of the RuleFit algorithm. RuleFit consists of two components: The first component produces “rules” from decision trees and the second component fits a linear model with the original features and the new rules as input (hence the name “RuleFit”). It enables automatic integration of interactions between features into a linear model, while having the interpretability of a sparse linear model.</p>
<p><strong>Step 1: Rule generation</strong></p>
<p>How does a rule look like? The rules that the algorithm generates have a simple form: For example: “if <span class="math inline">\(x2 &lt; 3\)</span> and <span class="math inline">\(x5 &lt; 7\)</span> then <span class="math inline">\(1\)</span> else <span class="math inline">\(0\)</span>”. The rules are constructed by disassembling decision trees: Each path to a node in a tree can be turned into a decision rule. The trees used for the rules are fitted to predict the target outcome. The splits and resulting rules are optimised to predict the outcome you are interested in. Figure <a href="rulefit.html#fig:rulefit-split">4.9</a> illustrates the rule generation. You simply chain the binary decisions that lead to a certain node with a logical “AND”, and voilà, you have a rule. It is desirable to generate a lot of diverse and meaningful rules. Gradient boosting is used to fit an ensemble of decision trees (by regressing or classifying <span class="math inline">\(y\)</span> with your original features <span class="math inline">\(X\)</span>). Each resulting tree is turned into multiple rules. Not only boosted trees, but any type of ensemble of trees can be used to generate the trees for RuleFit: <span class="math display">\[f(x) = a_0 + \sum_{m=1}^M a_m f_m(X)\]</span> where <span class="math inline">\(M\)</span> is the number of trees and <span class="math inline">\(f_m(x)\)</span> represents the prediction function of the <span class="math inline">\(m\)</span>-th tree. Bagged ensembles, Random forests, AdaBoost and MART yield ensemble of trees and can be used for RuleFit.</p>
<p>From all of the trees of the ensemble, we produce the rules. See again Figure <a href="rulefit.html#fig:rulefit-split">4.9</a>. Each rule <span class="math inline">\(r_m\)</span> takes on the form: <span class="math display">\[r_m(x) = \prod_{j \in \text{T}_m} I(x_j \in s_{jm})\]</span> where <span class="math inline">\(\text{T}_{m}\)</span> is the set of features used in <span class="math inline">\(m\)</span>-th tree, <span class="math inline">\(I(\cdot)\)</span> is the indicator function, which is 1 if the feature <span class="math inline">\(x_j\)</span> is in the specified subset of values <span class="math inline">\(s_{jm}\)</span> for <span class="math inline">\(x_j\)</span> (as specified by the tree splits) and otherwise 0. For numerical features, <span class="math inline">\(s_{jm}\)</span> is one to multiple intervals in the value range of the feature <span class="math inline">\(x_j\)</span>, depending on the number of splits in that feature. In case of a single split, the <span class="math inline">\(s_{jm}\)</span> looks like one of the two cases: <span class="math inline">\(x_{s_{jm}, \text{lower}} &lt; x_j\)</span> or <span class="math inline">\(x_j &lt; x_{s_{jm}, upper}\)</span>. Further splits in that feature create more complicated intervals. For categorical features the subset <span class="math inline">\(s_{jm}\)</span> contains some specific categories of <span class="math inline">\(x_j\)</span>.</p>
<p>A made up example for the bike rental data set: <span class="math display">\[ \begin{eqnarray} r_{17}(x) &amp;=&amp; I(x_{\text{temp}} &lt; 15) \cdot  \\
            &amp; &amp; I(x_{\text{weather}} \in \{\text{good}, \text{cloudy}\}) \cdot  \\
            &amp; &amp; I(10 \leq x_{\text{windspeed}} &lt; 20)
            \end{eqnarray}\]</span> This rule will only be equal to 1 if all of the three conditions are met, otherwise 0. RuleFit extracts all possible rules from a tree, not only from the leaf nodes. So another rule that would be created is: <span class="math display">\[ \begin{eqnarray} r_{18}(x) &amp;=&amp; I(x_{\text{temp}} &lt; 15) \cdot \\
            &amp; &amp;  I(x_{\text{weather}} \in \{\text{good}, \text{cloudy}\})
            \end{eqnarray}\]</span> In total, <span class="math inline">\(K = \sum_{m=1}^M 2(t_m -1 )\)</span> rules are created from the ensemble of <span class="math inline">\(M\)</span> trees, with <span class="math inline">\(t_m\)</span> terminal nodes each. A trick that is introduced by the RuleFit authors is to fit trees with random depth, so that a lot of diverse rules are generated with different lengths. Note that we throw away the predicted value in each node and only keep the conditions that lead us to the node and create a rule from it. The weighting of the decision rules will happen in step 2 of fitting RuleFit.</p>
<p>Another way to see the first step is, that it generates a new set of features <span class="math inline">\(X&#39;\)</span> out of your original features <span class="math inline">\(X\)</span>. Those features are binary and can represent quite complex interactions of your original <span class="math inline">\(X\)</span>. The rules are chosen to maximise the prediction task at hand. The rules are automatically generated from the covariates matrix X. You can see the rules simply as new features based on your original features.</p>
<p><strong>Step 2: Sparse linear model</strong></p>
<p>You will get A LOT of rules from the first step. Since the first step is only a feature transformation function on your original data set you are still not done with fitting a model and also you want to reduce the number of rules. Next to the rules, also all your ‘raw’ features from your original dataset will be used in the Lasso linear model. Every rule and original feature becomes a feature in Lasso and gets a weight estimate. The original, raw features are added because trees suck at representing simple linear relationships between y and x. Before we put everything into Lasso to get a sparse linear model, we winsorise the original features, so that they are more robust against outliers: <span class="math display">\[l_j^*(x_j) = min(\delta_j^+, max(\delta_j^-, x_j))%\]</span> where <span class="math inline">\(\delta_j^-\)</span> and <span class="math inline">\(\delta_j^+\)</span> are the <span class="math inline">\(\delta\)</span> quantiles of the data distribution of <span class="math inline">\(x_j\)</span>. A choice of <span class="math inline">\(0.05\)</span> for <span class="math inline">\(\delta\)</span> means that every value of <span class="math inline">\(x_j\)</span> that is in the 5% lowest or 5% highest values will be set to the values at 5% or 95% respectively. As a rule of thumb, you can choose <span class="math inline">\(\delta = 0.025\)</span>. In addition, the linear terms have to be normalised so that they have the same prior influence as a typical decision rule: <span class="math display">\[l_j(x_j) = 0.4 \cdot l^*_j(x_j) / std(l^*_j(x_j))\]</span> The <span class="math inline">\(0.4\)</span> is the average standard deviation of rules with a uniform support distribution <span class="math inline">\(s_k \sim U(0,1)\)</span>.</p>
<p>We combine both types of features to generate a new feature matrix and estimate a sparse linear model with Lasso, with the following structure: <span class="math display">\[ \hat{f}(x) = \hat{\beta}_0 + \sum_{k=1}^K\hat{\alpha}_k r_k(x) + \sum_{j=1}^p\hat{\beta}_j l_j (x_j)\]</span> where <span class="math inline">\(\hat{\alpha}\)</span> are the estimated weights for the rule features and <span class="math inline">\(\hat{\beta}\)</span> for the original features. Since RuleFit uses Lasso, the loss function gets the additional constraint that forces some of the weights to get a zero estimate: <span class="math display">\[(\{\hat{\alpha}\}_1^K, \{\hat{\beta}\}_0^p) = argmin_{\{\hat{\alpha}\}_1^K, \{\hat{\beta}\}_0^p} \sum_{i=1}^n L(y_i, f(x)) + \lambda \cdot (\sum_{k=1}^K |\alpha_k| + \sum_{j=1}^p |b_j|)\]</span> The outcome is a linear model that has linear effects for all of the original features and for the rules. The interpretation is the same as with linear models, the only difference is that some features are now binary rules.</p>
<p><strong>Step 3 (optional): Feature importance</strong> For the linear terms of the original features, the feature importance is measured with the standardised predictor: <span class="math display">\[I_j = |\hat{\beta}_j| \cdot std(l_j(x_j)) \]</span> where <span class="math inline">\(\beta_j\)</span> is the weight from the Lasso model and <span class="math inline">\(std(l_j(x_j))\)</span> the standard deviation of the linear term over the data.</p>
<p>For the decision rule terms, the importance is calculated with: <span class="math display">\[I_k = |\hat{\alpha}_k| \cdot \sqrt{s_k(1 - s_k)}\]</span> where <span class="math inline">\(\hat{\alpha}_k\)</span> is the associated Lasso weight of the decision rule and <span class="math inline">\(s_k\)</span> is the support of the feature in the data, which is the percentage of data points for which the decision rule applies (where <span class="math inline">\(r_k(x)\)</span> = 0): <span class="math inline">\(s_k = \frac{1}{n}\sum_{i=1}^n r_k(x_i)\)</span></p>
<p>A feature <span class="math inline">\(x_j\)</span> occurs as a linear term and possibly also within many decision rules. How do we measure the total importance of the feature <span class="math inline">\(x_j\)</span>? The importance <span class="math inline">\(J_j(x)\)</span> of feature <span class="math inline">\(x_j\)</span> can be measured at each individual prediction: <span class="math display">\[ J_j(x) = I_l(x) + \sum_{x_j \in r_k} I_k(x) / m_k \]</span> where <span class="math inline">\(I_l\)</span> is the importance of the linear term and <span class="math inline">\(I_k\)</span> the importance of the decision rules in which <span class="math inline">\(x_j\)</span> appears, and <span class="math inline">\(m_k\)</span> is the number of features that constitute rule <span class="math inline">\(r_k\)</span>. Summing the feature importance over all instances gives us the global feature importance: <span class="math display">\[J_j(X) = \sum_{i=1}^n J_j(x_i)\]</span> It is possible to choose a subset of instances and calculate the feature importance for this group.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-friedman2008predictive">
<p>Friedman, Jerome H, and Bogdan E Popescu. 2008. “Predictive Learning via Rule Ensembles.” <em>The Annals of Applied Statistics</em>. JSTOR, 916–54.</p>
</div>
<div id="ref-pre2017">
<p>Fokkema, Marjolein, and Benjamin Christoffersen. 2017. <em>Pre: Prediction Rule Ensembles</em>. <a href="https://CRAN.R-project.org/package=pre" class="uri">https://CRAN.R-project.org/package=pre</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tree.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="other-interpretable-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/chapters/04.5-interpretable-rulefit.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
