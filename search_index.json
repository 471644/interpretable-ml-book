[
["index.html", "Explainable machine learning Preface", " Explainable machine learning Christoph Molnar 2017-07-23 Preface This book is about explainable machine learning: Making machines explain their decisions. Machine learning is being built into many products and processes of our daily lives, yet decisions made by machines don’t automatically come with an explanation. An explanation increases the trust in the decision and in the machine learning model. As the programmer of an algorithm you want to know whether you can trust the learned model. Did it learn generalizable features? Or are there some odd artifacts in the training data which the algorithm picked up? This book will give an overview over techniques that can be used to make black boxes as transparent as possible and explain decisions. In the first chapter algorithms that produce simple, explainable models are introduced together with instructions how to interpret the output. The later chapters focus on analyzing complex models and their decisions. In an ideal future, machines will be able to explain their decisions and make a transition into an algorithmic age more human. This books is recommended for machine learning practitioners, data scientists, statisticians and also for stakeholders deciding on the use of machine learning and intelligent algorithms. About me: My name is Christoph Molnar, I consider myself both a statistician and machine learner. I work as a freelance data scientist and also offer courses in explainable machine learning. If you are interested in bringing explainability to your machine learning models, feel free to contact me! Mail: christoph.molnar.ai@gmail.com Website: https://christophm.github.io/ The online version of this book (currently the only available version) is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction “Essentially, all models are wrong, but some are useful.” — Box, George E. P.; Norman R. Draper (1987). Empirical Model-Building and Response Surfaces, p. 424, Wiley. ISBN 0471810339. UNDER CONSTRUCTION "],
["who-should-read-this-book.html", "1.1 Who should read this book", " 1.1 Who should read this book This book is for everyone who wants to learn how to make machine learning models more explainable. It is a recommended reading for machine learning practitioners, statisticians, data scientists, scientists and everyone who has contact with machine learning applications. It contains one or the other formula, but it’s kept at a manageable level of math. This book is not for people who are trying to learn machine learning from scratch. If you want to learn machine learning, there are loads of books and other resources for learning the basics. "],
["outline.html", "1.2 Outline", " 1.2 Outline This book starts out with framing the problem: What aspects does explainability have? It goes on and lays out the ‘simple’ machine learning models (linear models, decision trees, decision rules) that are interpretable. The following chapter explains different methods for explaining and understanding the data better, followed by the core chapter about methods for explaining the output of any black box model. Later methods that are specific for certain model classes are shown. The book concludes with an outlook on the future. "],
["what-is-machine-learning-and-why-is-it-important.html", "1.3 What is machine learning and why is it important?", " 1.3 What is machine learning and why is it important? So what is machine learning? In general we speak of predictive models, that take some input vector (features) and map it to an output. If the output is a categorical variable people often name it classification and if it is a numerical variable, then regression. Machine learning is a set of algorithms that can learn these mappings from training data, which are pairs of input features and output variable. The machine learning algorithm learns a model by changing parameters (like linear weights) or learning structures (like trees). The algorithm is guided by a score or loss function that is minimized. A fully trained machine learning model can then be used to make predictions for new instances. Recommendation of products, identifying street signs, counting people on the street, assessing a person’s credit worthiness, detecting fraud: All these examples have in common that they can, and are increasingly, realized with machine learning models. The tasks are different, but the approach is the same: Step 1 is to collect data. This can be images with and without street signs plus the information which sign is visible or the personal data from loan applicants together with the information if they repaid their loan or not. Step 2: Feed this information into a machine learning algorithm, which produces a sign detector model or a credit worthiness model. This model can then be used in Step 3: Integrate the model into the product or process, like an self-driving car or a loan application process. There are a lot of tasks in which machines exceed humans. Even if the machine is as good as a human at a task, or slightly worse, there remains big advantages, and that is speed, reproducibility and scale. A machine learning model that has been implemented once, can do a task much faster than humans, will reliably produce the same results from the same input and can be copied endlessly. "],
["definitions.html", "1.4 Definitions", " 1.4 Definitions An Algorithm is a set of rules that a machine follows to achieve a particular goal (“Definition of Algorithm,” n.d.) Machine learning algorithm is an set of rules that a machine follows to learn how to a achieve a particular goal. The output of a machine learning algorithm is a machine learning model. (Machine learning) Model is the outcome of a machine learning algorithm. This can be a set of weights for a linear model or neural network plus the architecture. Features are the variables/information used for prediction/classification/clustering. (machine learning) Task can be classification, regression, survival analysis, clustering, outlier detection Instance One row in the dataset. -->"],
["explainability.html", "Chapter 2 Explainability ", " Chapter 2 Explainability "],
["what-is-explainability.html", "2.1 What is explainability", " 2.1 What is explainability UNDER CONSTRUCTION "],
["when-is-explainability-important.html", "2.2 When is explainability important?", " 2.2 When is explainability important? AI explainability is the ability of a machine learning system to explain or to present a decision in an understandable way for humans. Machine learning has come to a state where you have to make a trade-off: Do you simply want to know what will happen? For example if a client will churn or if medication will work well for a patient. Or do you want to know why something will happen and paying for the explainability with accuracy? In some cases you will not care why a decision was made, only the assurance that the accuracy was good on some test set is enough. But in other cases knowing the ‘why’ can help you understand more about the problem, the data and also know why a model might fail. Two sorts of problems might not need explanations, because they either are low risk (e.g. movie recommender system) or the method is already extensively studied and evaluated (e.g. optical character recognition). The necessity for explainability comes from an incompleteness in the problem formalization (Doshi-Velez and Kim 2017), meaning that for certain problems/tasks it is not enough to get the answer (the “what”), but the model also has to give an explanation (the *why**) There is a shift in many scientific disciplines Add example disciplines--> from qualitative to quantitative methods (e.g. sociology, psychology), and also towards machine learning (biology, genomics). The goal of science is to gain knowledge, but many problems can only be solved with big datasets and black box machine learning models. Explainability allows to extract additional knowledge. It is human nature wanting to understand things, to have some form of control. Machine learning models are taking over real world tasks, that demand safety measurements and testing. A self-driving car automatically detects cyclists, which is as desired. You want to bet 100% sure that the abstraction the system learned will be fail-safe, because running over cyclists is quite bad. An explanation might reveal that the most important feature learned is to recognize the two wheels of a bike and this explanation helps to think about edge cases like bikes with side bags, that partially cover the wheels. By default most machine learning models pick up biases from the training data. This can turn your machine learning models into racists (see Microsoft failed experiment: Tay) or discriminate in against other demographic, protected groups. Explainability helps you to get the ethics of your machine learning model right. From the data it might not be possible to see the biases, so it might also not be enough to implement protections against known biases. An example: The goal is to train a machine learning model to pre-sort applications for jobs in a big company. The company had some troubles in the past because they preferred to hire men over women, having the same qualifications. So for the model training the data scientist decides to remove all features that indicate the gender. But it turns out that the model picks up another, a priori unknown, bias favoring younger people over older people. This could only be uncovered because the model is explainable. Sometimes it is not possible to optimize a goal directly, but only a proxy. Consider a marketing where a e-commerce company sends some gift to some customers in order to bind them to the business. It has already done similar campaigns, so there is data to learn a model to select the best customers to send the gifts to (sending it to all would be far to expensive). Ideally the company wants to optimize the likelihood that the customers with the gift are still their customers in five years from now, but there is no data for optimizing this goal directly, because the first of theses kinds of campaigns was two years ago, meaning there is no suitable training data for this objective. So the company opts for using a proxy: The amount of money the customers will be spending in the next 6 months at your company. An explainable model can help to find problems that might arise due to using a proxy objective (possibly mismatched objectives). In the e-commerce example it might turn out that the customers selected by the algorithm are only international students that might spent heavily in a short time, even more after receiving a gift, but usually leave the country and therefore the company as a customer within a few years.In many scientific questions, like in longitudinal observational studies, the researcher wants to uncover causal relationships, like the therapy effect on some disease outcome measurement, but all algorithms only pick up and optimize the correlation which the researcher is only interested in if it is causal. In volatile environments a machine learning model might be outdated at some point because of a data shift. A data shift is a change in the relationships in the data. Explanation make it easier to detect when that happens or if one model is more prone than another to face problems soon. Spam filter for mails use works in a mail to detect if they should reach your inbox or be moved into the spam folder. ‘Viagra’ is one of the words that increase the likelihood that a mail is spam. When ‘viagra’ is suddenly used in a different context, for some obscure reason, suddenly non-spam mails will land in the spam folder (like ‘Random forests are basically a bunch of decision trees on viagra’). An investigation of this problem might reveal very quickly that the data shift (‘viagra’-occurrence / spam reluationship) is caused by the new use of the former spam related word. An explainable model could highlight in the text which sections caused the spam classification. A special case of a data shift is when the outcome of the machine learning model influences its environment and therefore its future training data. An example is the optimization of marketing campaigns. The machinee learning model influences which people will be targeted by the campaign and therefore influence who will be part of the future training set and also possibly invalidate its own predictions. Another topic related to data shift is extrapolation. With simpler, interpretable models it is clearer how predictions will be handled for data points outside of the training data cluster. Sometimes training of machine learning models happens with training data that comes from a different distribution as the live data with which the model will be confronted with in production. The machine learning team basically trains the model on the dedicated training dataset, crosses their fingers and put it into production. The hope is, that the learned associations between the features and the output has a good generalization to new data. The training data for a fracture detection machine learning model might come from labeled radiographs that were done with a handful of different x-ray devices. But the company hopes to use their fracture detection algorithm in their software that they sell to hundreds of hospitals that likely use different machines. Explainable machine learning models can give the engineers the confidence that the model only picked up relationships that will generalize well for new datasets. Even if the problem formalization is complete, there might be an unknown trade-off between objectives like predictive ability and not being racis. A machine learning model that decides for a bank whether customers should receive a loan or not has many objectives to take care of: For the bank it is important that the customers will pay their loan back and not default. This can be directly optimized by training on historical data with customer data and information if they defaulted on a loan or not. The customers with the best scores (highest probability of paying back the loan without problems) are the ones the bank would want to give a loan. At the same time the bank has to ensure that the algorithm is not discriminating against a demographic minority like race or religion. Also there might be other regulatory or legal requirements the bank has to be aware of when giving out loans. These additional objectives cannot be directly optimized, but explanations can fill the gap. There might be legal requirements to have an explainable decision making process. Besides these scenarios with an incompleteness to of the problem formalization, there is also the task of debugging a machine learning model, which is crucial and difficult. Explainability is a useful debugging tool for black box algorithm. So even in low risk environments (e.g. movie recommenders) explainability in the research and development stage is valuable. Also later when some model is used in a product, things can go wrong. And needed for explainability arises when something goes wrong. Because having an explanation for a faulty classification helps to understand the cause of the fault. It delivers a direction for how to fix the system. Consider an example of a husky versus wolf classifier, that missclassifies some huskies as wolfs. If there is an explanation to the classification you can see, that the missclassification happened due to the snow on the image. The classifier learned to use snow as a feature for classifying images as wolfs, which might make sense in terms of separating features in the training data set, but not in the real world use. References "],
["the-bigger-picture.html", "2.3 The bigger picture", " 2.3 The bigger picture Let’s take a look from further away. What do we want to explain, and what kind of ‘layers’ are inbetween? The infographic displays the concepts, see Figure 2.1. The bottom layer is the ‘World’. This could literally be nature itself, like the biology of the human body and how it reacts to medication, but also human behaviour like if people payed back their loans. The ‘World’-layer contains everything that can be observed and is of interest. Ultimately we want to learn something about the ‘World’ and interact with it. The second layer is the ‘Data’-layer. We have to digitalise the ‘World’ in to make it processable for computers and also to store information. The ‘Data’-layer contains anything from images, texts, tabular data and so on. With machine learning on top of the ‘Data’-layer we get to the ‘Black Box Model’-layer. Machine learning algorithms learns with data from the real world to make predictions / classifications or finds structures. Now with the ‘Interpretable models’-layer we come the part that this book is concerned with. On top of the ‘Black-Box-Layer’ we want to have something that helps us deal with the opaqueness of machine learning models. What were the important attributes for a particular diagnosis? Why was a financial transaction classified as fraud? On top of that, there is the ‘Explanations’-layer. I put it as a layer separate from ‘Interpretable models’, since the simple models deal with capturing associations and it is useful to think of the explanation as independent. There are different ways to present the results of a linear regression model for example, it could be a coefficient table, a coefficient plot with confidence intervals, a colored bar chart, a few sentences, … It depends on the target audience what representation which explanation to choose. The last layer is ‘Human’. Look this one is waiving at you because you are reading this book and you are helping to provide better explanations for black box models! Humans are the consumers of the explanations ultimately. FIGURE 2.1: The big picture of explainable machine learning. The real world goes through many layers before it reaches the human in forms of explanations. This layered abstraction also helps in understanding what the difference between statisticians and machine learning practicioners is. Statistician are concerned with the ‘Data’ layer, like planning clinical trials or designing surveys. The they skip the ‘Black Box Model’-layer and go right to the ‘Interpretable Models’ and from there to the explanations for our human. Machine learning specialists are also concerned with the ‘Data’-layer, like collecting labeled samples of skin cancer images or crawling Wikipedia. Then comes the machine learning model. ‘Interpretable models’ and ‘Explanations’ are skipped and the human deals directly with the ‘Black Box Model’. It’s a nice thing, that in explainable machine learning, the work of a statistican and a machine learner fuses and becomes something better. Of course this graphic does not capture everything: Data could come from simulations. Black box models also output predictions that might not even reach humans, but only feed other machines and so on. But overall it is a useful abstraction for understanding what explainable machine learning is. "],
["scope-of-explainability.html", "2.4 Scope of explainability", " 2.4 Scope of explainability An algorithm trains a model, which produces the predictions. Each step can be evaluated in terms of transparency or explainability. 2.4.1 Algorithm transparency How does the algorithm create the model? Algorithm transparency is about how the algorithm learns a model from data and what kind of relationships it is capable of picking up. If you are using convolutional neural networks for classifying images, you can explain that the algorithm learns edge detectors and filters on the lowest layers. This is an understanding of how the algorithm works, but not of the specific model that is learned in the end and not about how single predictions are made. For this level of transparency only knowledge about the algorithm and not about the data or concrete learned models are required. This book focuses on model explainability. Algorithms like the least squares method for linear models are well studied and understood. They score high in transparency. Deep learning approaches (pushing a gradient through a network with millions of weights) are less understood and the inner workings are in the focus on-going research. It is not clear how they exactly work, so they are less transparent. 2.4.2 Global, holistic model explainability How does the trained model make predictions? You could call a model explainable if you can comprehend the whole model at once (Lipton 2016). To explain the global model output, you need the trained model, knowledge about the algorithm and the data. This level of explainability is about understanding how the model makes the decisions, based on a holistic few on it’s features and each learned components like weights, parameters and structures. Which features are the important ones and what kind of interactions are happening? Global model explainability helps to understand the distribution of your target variable based on the features. Arguably, global model explainability is very hard to achieve in practice. Any model that exceeds a handful of parameters or weights, probably won’t fit an average human’s brain capacity. I’d argue that you cannot really imagine a linear model with 5 features and draw in your head the hyperplane that the was estimated in the 5-dimensional feature space. Each feature space with more than 3 dimensions is just not imaginable for humans. Usually when people try to comprehend a model, they look at parts of it, like the weights in linear models. 2.4.3 Global model explainability on a modular level How do parts of the model influence predictions? You might not be able to comprehend a naive bayes model with many hundred features, because there is no way you could hold all the feature weights in your brain’s working memory. But you can understand a single weight easily. Not many models are explainable on a strict parameter level.While global model explainability is usually out of reach, there is a better chance to understand at least some models on a modular level. In the case of linear models parts to understand are the weights and the distribution of the features, for trees it would be splits (used feature and cut-off point) and leaf node predictions. Linear models for example look like they would be, but the interpretation of a single weight is interlocked with all of the other weights. As you will see in Chapter [#limo], the interpretation of a single weight always comes with the footnote that the other input features stay at the same way value, which is not the case in many real world applications. A linear model predicting the rent of a flat, which takes into account both the size of the flat and the number of rooms might have a negative weight for the rooms feature, which is counter intuitive. But it can happen, because there is already the highly correlated flat size feature and in a market where people prefer bigger rooms, a flat with more rooms might be worth less than a flat with more rooms and same square meters. The weights only make sense in the light of the other features used in the model. But arguably a linear models weights still have better explainability than the weights of a deep neural network. 2.4.4 Explain the decision for a single instance Why did the model make a specific decision for an instance? You can go all the way down to a single observation and examine what kind of classification or decision the model gives for this input, and why it made this decision. When you look at one example, the local distribution of the target variable might behave more nicely. Locally it might depend only linearly or monotonic on some variables rather than having a complex dependency on the features. For example the rent of an apartment might not depend linearly on the size, but if you only look at a specific apartment of 100 square meter and check how the prize changes going up plus and minus 10 square meters there is a chance that this sub region in your data space is linear. Local explanations can be more accurate compared to global explanations because of this. 2.4.5 Explain the decisions for a group of instances Why did the model make specific decisions for a group of instances? The model output for multiple instances can be explained by using methods for global model explainability and single instance explanations. The global methods can be applied by taking the group of observations pretending it’s the complete dataset and using the global methods on this subset. The single explanation methods can be used on each instance and listed or aggregated afterwards for the whole group. References "],
["evaluating-explainability.html", "2.5 Evaluating explainability", " 2.5 Evaluating explainability There is no real consensus what explainability in machine learning is. Also it is not clear how to measure it. 2.5.1 Approaches for evaluation of the explanation quality (Doshi-Velez and Kim 2017) proposes 3 major levels of evaluating explainability. - Application level evaluation (real task): Put the explanation into the product and let the end user test it. On an application level the radiologists would test the fracture detection software in order to evaluate the model. This requires a good experimental setup and an idea of how to assess the quality. A good baseline for this is always how good a human would be at explaining the same decision. - Human level evaluation (simple task) is a simplified application level evaluation. The difference is that these experiments are not conducted with the domain experts, but with lay humans. This makes experiments less expensive (especially when the domain experts are radiologists) and it is easier to find more humans. An example would be to show a user different explanations and the human would choose the best . - Function level evaluation (proxy task) does not require any humans. This works best when the class of models used is already evaluated by someone else in a human level evaluation. 2.5.1.1 Function level evaluation Model size is an easy way to measure, but might be too simplistic. Dimensions of interpretability: Model sparsity: How many features are being used by the explanation? Monotonicity: Is there a Monotonicity constraint? Uncertainty: Is a measurement of uncertainty part of the explanation? Interactions: Is the explanation able to include interaction of features? Cognitive processing time: How long does it take to understand the explanation. Feature complexity: What features were used for the explanation? PCA components are harder to understand than word occurrences for example. Description length of explanation If you can ensure that the machine learning model can explain decisions, following traits can also be checked more easily (Doshi-Velez and Kim 2017). Fairness: Unbiased, not discrimating against protected groups (implicit or explicit). An interpretable model can tell you why it decided it decided a certain person is not worthy of a credit and for a human it becomes easy to decide if the decision was based on a learned demographic (e.g. racial) bias. Privacy: sensitive information in the data is protected. Reliability/Robustness: Small changes in the input don’t lead to big changes in the ouput/decision. Causality: Only causal relationships are picked up. So a predicted change in a decision due to arbitrary changes in the input values, are also happening in reality. Usability: Trust: It is easier for humans to trust into a system that explains it’s decisions compared to a black box --> References "],
["simple.html", "Chapter 3 Simple, interpretable models", " Chapter 3 Simple, interpretable models The most straight forward way to achieve explainable machine learning algorithms is to use only a subset algorithms that yield an understandable model structure. These are: Linear models (sparse) Decision trees Decision rules In the following chapters these we will talk about the algorithm with it’s variants. Not in detail, only the basics, because there are already a ton of books, videos, tutorials, papers and so on about them. We will focus on how to interpret the models and why they are explainable. The chapter covers linear models, decision trees, decision rules, neighbour methods and graphical models. "],
["terminology.html", "3.1 Terminology", " 3.1 Terminology Y is the target variable in supervised settings. X are the features or covariates. w are the weights. \\(\\beta\\) are regression weights. "],
["the-dataset-speed-dating.html", "3.2 The dataset: speed dating", " 3.2 The dataset: speed dating For illustration purposes we will be using speed dating data in this book. Missing values were imputed using the mean for numerical features and the mode (most common value) for categorial features. I don’t recommend doing this in practice, but for illustratory purposes, I guess I will not get arrested by the statistics police. The data was gathered in a study (between 2002 and 2004) with candidates doing speed dating. Partners were randomly matched. Each first date lasted 4 minutes. After the speed date, the participants were asked if they would go on a date with this person again. They were also asked to estimate attractiveness, sincerity, intelligence, fun, ambition, and shared interests of the partner. Data Source: http://www.stat.columbia.edu/~gelman/arm/examples/speed.dating/ Paper: http://faculty.chicagobooth.edu/emir.kamenica/documents/genderDifferences.pdf You can look at a sample of 50 dates here: "],
["TubeSpam.html", "3.3 TubeSpam dataset: Spam classification on YouTube comments", " 3.3 TubeSpam dataset: Spam classification on YouTube comments As an example for text classification we will be using 1956 comments from 5 different YouTube videos . Thankfully the authors that used this dataset in a paper about spam classification made the data freely available (Alberto, Lochter, and Almeida 2015). Data was collected through the YouTube API from five of the ten most viewed videos on YouTube in the first half of 2015. All of the 5 videos are music videos. One of them is the wildly popular “Gangnam Style” from korean artist Psy. The other artists where Katy Perry, LMFAO, Eminem and Shakira. You can flip through the comments. The comments had been hand labeled as spam or legitimate. Spam has been coded with a ‘1’ and legitimate comments with a ‘0’. You could also go over to YouTube and have a look at the comment section. But please don’t get trapped in the YouTube hell, ending up watching videos about monkeys stealing and drinking coktails from tourists on the beach. Also the Google Spam detector probably has changed a lot since 2015. Watch the view-record breaking video below “Gangnam Style” below: References "],
["risk-factors-for-cervical-cancer.html", "3.4 Risk factors for cervical cancer", " 3.4 Risk factors for cervical cancer The cervical cancer dataset contains indicators and risk factors for predicting if a woman gets cervical cancer. The features contain demograpics (e.g. age), habits and medical history. The data can be downloaded from the UCI Machine Learning repository. The variable contained in the dataset are: (int) Age (int) Number of sexual partners (int) First sexual intercourse (age) (int) Num of pregnancies (bool) Smokes (bool) Smokes (years) (bool) Smokes (packs/year) (bool) Hormonal Contraceptives (int) Hormonal Contraceptives (years) (bool) IUD (int) IUD (years) (bool) STDs (int) STDs (number) (bool) STDs:condylomatosis (bool) STDs:cervical condylomatosis (bool) STDs:vaginal condylomatosis (bool) STDs:vulvo-perineal condylomatosis (bool) STDs:syphilis (bool) STDs:pelvic inflammatory disease (bool) STDs:genital herpes (bool) STDs:molluscum contagiosum (bool) STDs:AIDS (bool) STDs:HIV (bool) STDs:Hepatitis B (bool) STDs:HPV (int) STDs: Number of diagnosis (int) STDs: Time since first diagnosis (int) STDs: Time since last diagnosis (bool) Dx:Cancer (bool) Dx:CIN (bool) Dx:HPV (bool) Dx (bool) Hinselmann: target variable (bool) Schiller: target variable (bool) Cytology: target variable (bool) Biopsy: target variable As the biopsy serves as the gold standard for diagnosing cervical cancer, the classification tasks in this book used the biopsy outcome as the target. Missing values for each column were imputed by the mode (most frequent value), which is probably a bad solution, because the value of the answer might be correlated with the probability for missingness. There is probably this bias, because the question are of a very private nature. But this is not a book about missing data imputation, so the mode imputation will suffice! (K. Fernandes, Cardoso, and Fernandes 2017) References "],
["overview.html", "3.5 Overview", " 3.5 Overview Algorithm Linear Monotonicity Interaction built-in Linear models Yes Yes No Decision trees No Not by default Yes Decision rules No Not by default Yes Naive bayes Yes Yes No Nearest neighbours No No No "],
["limo.html", "3.6 Linear models", " 3.6 Linear models Linear models have been and are still used by statistician, computer scientists and other people with quantitative problems. They learn straightforward linear (and monotonic) relationships between the target and the features. The target changes by a learned weight depending on the feature. Monotonicity makes the interpretation easy. Linear models can be used to model the dependency of a regression variable (here Y) on K covariates. As the name says, the learned relationships are linear in the form of \\[y_{i} = \\beta_{0} + \\beta_{1} \\cdot x_{i,1} + \\ldots + \\beta_{K} x_{i,K} + \\epsilon_{i}\\] The i-th observation’s outcome is a weighted sum of it’s K features. The \\(\\beta_{k}\\) represent the learned feature weights or coefficients. The \\(\\epsilon_{i}\\) is the error we are still making, the difference between the predicted and actual outcome. The biggest advantage is the linearity: It makes the estimation procedure straight forward and most importantly these linear equations have an easy to understand interpretation. That is one of the main reasons why the linear model and all it’s descendants are so widespread in academic fields like medicine, sociology, psychology and many more quantitative research fields. In this areas it is important to not only predict e.g. the clinical outcome of a patient, but also quantify the influence of the medication while at the same time accounting for things like sex, age and other variables. Linear regression models also come with some assumptions that make them easy to use and interpret but are often not given in reality. The assumptions are: linearity, normality, homoscedasticity, independence, fixed features, abscence of multicollinearity. Linearity: Linear regression models allow the mean of the response to be only a linear combination of the features, which is both the greatest strength and biggest limitation. Linearity makes the estimation procedure easy. Also linearity leads to interpretable models: linear effects are simple to quantify and describe (see also next chapter) and are additive, so it is easy to separate the effects. If you suspect interactions of features or a non-linear association of a feature with the target value, then you can add interaction terms and things like regression splines to estimate non-linear effects. Normality: The target value given the features are assumed to follow a normal distribution. If this assumption is violated, then the estimated confidence intervals of the feature weights are not valid. Any interpretation of the p-values (p-value = Probability that the confidence interval of the feature weight covers the 0) is not valid. Homoscedasticity (constant variance): The variance of the error terms \\[\\epsilon_{i}\\] is assumed to be constant along the whole feature space. Let’s say you want to predict the value of a house given the living area in square meters. You estimate a linear model, which assumes that no matter how big the flat, the error terms around the predicted response have the same variance. This assumption is in reality often violated. In the house example it is plausible that the variance of error terms around the predicted price is higher in bigger houses, since also the prices are higher and there is more wriggle room for prices to vary. Indepence: Each observation is assumed to be independent from the next one. If you have repeated measurements, like multiple records per patient, the data points are not independent from each other and there are special linear model classes to deal with these cases, like mixed effect models or GEEs. Fixed features: The input features are seen as ‘fixed’, carrying no errors or variation, which, of course, is very unrealistic and only makes sense in controlled experimental settings. But not assuming fixed features would mean that you have to fit very complex measurement error models that account for the measurement errors. And usually you don’t want to do that. Abscence of multicollinearity: Basically you don’t want features to be highly correlated, because this messes up the estimation of your models. In a situation where two variables are highly correlated (something &gt;0.9) the linear model will have problems estimating the weights, since the models are additive and the model does not know to which feature to attribute the effects. 3.6.1 Interpretation The interpretation of the coefficients: Continuous regression variable: For an increase of one point of the variable \\(x_{j}\\) the estimated outcome changes by \\(\\beta_{j}\\) Binary categorical variables: One of the variables is the reference level (in some languages the one that was coded in 0). A change of the variable \\(x_{i}\\) the reference level to the other category changes the estimated outcome by \\(\\beta_{i}\\) categorical variables with many levels: One solution to deal with many variables is to one-hot-encode them, meaning each level gets it’s own column. From a categorical variable with L levels, you only need L-1 columsn, otherwise it is over parameterized. The interpretation for each level is then according to the binary variables. Some language like R allow to Intercept \\(\\beta_{0}\\): The interpretation is: Given all continuous variables are zero and the categorical variables are on the reference level, the estimated outcome of \\(y_{i}\\) is \\(\\beta_{0}\\). The interpretation of \\(\\beta_{0}\\) is usually not relevant. Another important measurement for interpreting linear models is the \\(R^2\\) measurement. \\(R^2\\) tells you how much of the total variance of your target variable is explained by the model. The higher \\(R^2\\) the better your model explains the data. The formula to calculate \\(R^2\\) is: \\(R^2 = 1 - SSE/SST\\), where SSE is the squared sum of the error terms (\\(SSE = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\)) and SST is the squared sum of the data variance (\\(SST = \\sum_{i=1}^n (y_i - \\bar{y})^2\\)). \\(R^2\\) ranges between 0 for models that explain nothing and 1 for models that explain all of the datas variance. There is a catch, because \\(R^2\\) increases with the number of features in the model, even if they carry no information about the target value at all. So it is better to use the adjusted R-squared \\(\\bar{R}^2\\), which accounts for number of features used in the model. It’s calculation is \\(\\bar{R}^2 = R^2 - (1-R^2)\\frac{p}{n - p - 1}\\), where p is the number of features and n the number of observations. It isn’t helpful to do interpretation on a model with very low \\(R^2\\) or \\(\\bar{R}^2\\), because basically the model is not explaining much of the variance, so any interpretation of the weights are not meaningful. 3.6.2 Interpretation example Estimate Std. Error (Intercept) 7.3216330 0.2616971 GenderMale 0.2055874 0.0424321 Number of people in wave -0.0190576 0.0051841 Number of date -0.0085710 0.0040642 Age of partner -0.0218426 0.0057671 Race of partner -0.0754781 0.0171996 Correlation of interests 0.1927147 0.0679408 Partners same race:yes 0.1198600 0.0434943 Importance of same religion -0.0191672 0.0082405 Importance of same race -0.0305695 0.0081656 Goal of date:meet new people -0.0269258 0.0473541 Goal of date:date 0.1389435 0.0822913 Goal of date:relationship 0.6144941 0.1123152 Goal of date:experience -0.0090531 0.0894566 Goal of date:other 0.3023967 0.0974087 Dating frequency:Twice a week 0.2680615 0.2194545 Dating frequency:Once a week 0.0242338 0.2021012 Dating frequency:Twice a month -0.0474246 0.1963305 Dating frequency:Once a month -0.1254500 0.1972584 Dating frequency:Several times a year -0.0828013 0.1959933 Dating frequency:Almost never -0.0453842 0.1982375 Interpretation of a numerical variable (‘Age of partner’): An increase of the age of the partner of 1 year increases the Likeness rating by -0.02 given all other features stay the same. Interpretation of a categorical variable (‘Goal of date’ (reference category: ‘Seemed like a fun night out’)): The Likeness rating for a partner is 0.14 higher if the goal is to get a date, compared to if the participants goal was to have a fun night out. Also if participants were ‘Looking for a serious relationship’, the Likeness rating for the partner was 0.61 higher, compared to participants who wanted a fun night out, given all features stay the same. As you can see in the interpretation examples, the interpretations are always coming with the clause that ‘all other features stay the same’. That’s because of the nature of linear models: All features are input linearly into the function with no interactions (unless explicitly specified). The good side is, that is isolates the interpretation. If you think of the features as turn-switches that you can turn up or down, it is nice to see what happens when you would just turn the switch for one feature. 3.6.3 Interpretation templates Interpretation of a numerical feature: An increase of \\(x_{k}\\) by one unit increases the expectation for \\(y\\) by \\(\\beta_x{k}\\) units if all other features X stay the same. Interpretation of a categorical feature: The category coded with 1 of \\(x_{k}\\) increases the expectation for \\(y\\) by \\(\\beta_{k}\\) compared to the reference category (coded with 0). 3.6.4 Visual parameter interpretation 3.6.4.1 Weight plot The information of the coefficient table can also be put into a visualization, which makes the weights and the uncertainty about them can be made understandable on one glance. The weight is displayed as a point and the 95% confidence interval around the point with a line. The 95% confidence interval means that if the linear model was repeated 100 times on TODO: Add interpetation 3.6.4.2 Effect plot The weights of the linear model only have meaning, when combined with the actual features. The weights depend on the scale of the features and will be different if you have a features measuring some height and you switch from inches to centemeters. The weight will change, but the actual relationships in your data will not. Also it is important to know the distribution of your feature in the data, because if you have a very low variance, it means that almost all instances will get a similar contribution from this feature. The effect plot can help to understand how much the combination of a weight and a feature contributes to the predictions in your data. Start with the computation of the effects, which is the weight per feature times the feature of an instance: \\(eff_{i,k} = w_{k} \\cdot x_{i,k}\\). The resulting effects are visualized with boxplots: The box contains the effect range for half of your data (25% to 75% effect quantiles). The line in the box is the median effect, so 50% of the instances have a lower and the other half a higher effect on the prediction than the median value. The whiskers are \\(+/i 1.58 IQR / \\sqrt{n}\\), with IQR being the inter quartile range ($q_{0.75} - q_{0.25}). The points are outlier to the whiskers. TODO: Add interpetation 3.6.5 Explaining single predictions Why did a certain instance get the prediction it got from the linear model? This can again be answered by bringing together the weights and features and computing the effect. Now the effect will tell you how much each feature contributed towards the sum of the prediction. This is only meaningful if you compare the instance specific effects with the mean effects. Let’s have a look at the effect realization for the Likeness rating of one observation (= a date of a participant with a partner). Some features contribute unusually much to the degree to which the participant rates the Likeness towards the dating partner: Goal of date () and Correlation of interests (value = ) and Dating frequency (value = ) 3.6.6 Coding categorical variables: There are several ways to represent a categorical variable, which has an influence on the interpretation: http://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/ and http://heidiseibold.github.io/page7/ Described above is the treatment coding, which is usually sufficient. Using different codings boils down to creating different matrices from your one column with the categorical feature. I present three different codings, but there are many more. The example has six instances and one categorical feature with 3 levels. The first two instances are in category A, instances three and four are in category B and the last two instances are in category C. Treatment coding compares each level to the reference level. The intercept is the mean of the reference group. The first column is the intercept, which is always 1. Column two is an indicator whether instance \\(i\\) is in category B, columns three is an indicator for category C. There is no need for a column for category A, because than the system would be over specified. Knowing that an instance is neither in category B or C is enough. \\[ \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\] Effect coding compares each level to the overall mean of \\(y\\). The first column is again the intercept. The weight \\(\\beta_{0}\\) which is associated to the intercept represents the overall mean and \\(\\beta_{1}\\), the weight for column two is the difference between the overall mean and category B. The overall effect of category B is \\(\\beta_{0}\\) + _{1}$. Interpretation for category C is equivalent. For the reference category A, \\(-(\\beta_{1} + \\beta_{2})\\) is the difference of the category C to the overall mean and \\(\\beta_{0} -(\\beta_{1} + \\beta_{2})\\) the overall effect of category C. \\[ \\begin{pmatrix} 1 &amp; -1 &amp; -1 \\\\ 1 &amp; -1 &amp; -1 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\] Dummy coding compares each level to the level mean of \\(y\\). If all level are have the same frequency the resulting coefficients will be the same as in effect coding. Note that the intercept was dropped here. \\[ \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\] 3.6.7 Assuring sparsity in linear models Lasso, Ridge, elasticnet, forward/backward variable selection, dimensionality reduction, … UNDER CONSTRUCTION 3.6.8 The disadvantages of linear models They can only represent linear relationships as the name suggests. Each non-linearity or interaction has to be hand-crafted and explicitly given to the model as an input feature. Because of possible high correlation between features, it is possible that a feature that is positively correlated with the outcome might get a negative weight in a linear model, because in the high dimensional space it is negatively correlated. An example: You have a model to predict the rent price and have features like number of rooms and size of the flat. Of course flat size and room number are highly correlated, the bigger a flat the more rooms it has. If you now take both variables into a linear model it might happen, that the flat size is the better predictor and get’s a large positive weight. The room number might end up getting a negative weight, because given that a flat has the same size, increasing the number of rooms could make it less valuable. 3.6.9 Towards complexer relationships within linear model class Adding interactions Adding non-linear terms like polynomials Stratifying data by variable and fitting linear models on subsets 3.6.10 Linear models beyond gaussian regression Logistic regression GAMs Quantile regression "],
["decision-trees.html", "3.7 Decision trees", " 3.7 Decision trees Linear models fail in situation where the relationship is non-linear and/or where the features are interacting with each other. Time to shine for the decision trees! Tree-based models partition the data along the features into rectangles. For predicting the outcome in each rectangle it fits a simple model (for example the average of the outcome of the instances that fall into this rectangle). Trees have an intuitive structure starting from a root and splitting into nodes, according to cutoff values of the features. After each split, the instances fall into one of the new nodes. At the end of the training all the instances from the training data set are assigned into one of the leaf nodes. See Figure 3.1 for illustration. There are a lot of different tree algorithms. They differ in structure (number of splits per node), criteria for how to find the splits, when to stop splitting and how to estimate the simple models within the leaf nodes. Classification and regression trees (CART) is one of the more popular algorithms for tree building. This book will only talk about CART, because in the interpretation they are all the same. If you know of some tree algorithm with a different interpretation, I would welcome your feedback. Each of these rectangles is associated with a simple model of the outcome of the interest. This is usually estimated by taking the mean of outcomes from all training instances that fall into a rectangle. I recommend the book ‘The elements of statistical learning’ (Hastie, Tibshirani, and Friedman 2009) for a more detailed introduction. FIGURE 3.1: Exemplary decision tree with artificial data The following formula describes relationship of y and x (in which rectangle does x fall?) \\[\\hat{y}_i = \\hat{f}(x_i) = \\sum_{m = 1}^M c_m I\\{x_i \\in R_m\\}\\] Each instance \\(x_i\\) falls into exactly one leaf node (=rectangle), so \\(I_{\\{x_i \\in R_m\\}}\\) is only 1 for the this single leaf node (\\(I\\) is the identity function which is 1 if \\(x_i \\in R_m\\) and else 0). If \\(x_i\\) falls into leaf node \\(R_l\\), the predicted outcome \\(\\hat{y} = c_l\\), where \\(c_l\\) is the mean of all the training instances in leaf node \\(R_l\\). But where do the ‘rectangles’ come from? This is quite simple: The algorithm takes a feature and tries which cut-off point minimizes the sum of squares if it is a regression task or the Gini index in classification tasks. It’s the cut-off point that makes the two resulting subsets as different as possible in terms of the outcome variable of interest. For categorial features the algorithm tries different groupings by category into to nodes. After this was done for each feature, the algorithm looks for the feature with the best cut-off and chooses this to split the node into two new nodes. The algorithm continues doing this in both new nodes until the stopping criteria is reached. Possible criteria are: A minimum number of observations that have to be in a node before the split, the minimum number of instances that have to be in a terminal node. A common strategy is to grow a tree fully and then cut it back to optimize it’s complexity measure \\(cp\\). 3.7.1 Interpretation It’s easy: Starting from the root node you go to the next nodes and the edges tell you which subsets you are looking at. Once you reach the leaf node, the node tells you the predicted outcome. All the edges are connected by ‘AND’. Template: If feature x is [smaller/bigger] than threshold c AND …, then the predicted value is \\(\\hat{y}\\). 3.7.2 Interpretation example Let’s have a look again at the speed dating example. Again we want to predict the rating from the participants, how much they will like the rating partners. FIGURE 3.2: Regression tree fitted on the speed dating data. Categories of goal of date feature have been abbreviated for readability: f = for fun, m = meet new people, d = date, r = relationship, e = experience, o = other The first split was done in the wave variable, which is the number of dating partners within a wave. In smaller waves the participants were more likely to give the partners a better rating in how much they liked them. The ratings were low (median: 6), when there were 21 or more people in the wave and the goal of the date was either to have fun, to get a date, to meet new people, or to try out speed dating. When the goal was to find a serious relationship or when the stated goal was ‘other’, the rating was higher on average (median of around 7). In waves of size 20 or smaller, participants who gave 5/10 or more to importance of same religion of partner, they also rated lower on average (median around 6). If religion was less important (4 or lower), than they gave higher ratings. 3.7.3 Advantages The tree structure is perfectly suited to cover interactions between features in the data. The data also ends up in distinct groups, which are easier to grasp than points on a hyperplane like in linear regression. The interpretation is arguably pretty straight forward. The tree structure also has a natural visualization, with it’s nodes and edges. 3.7.4 Disadvantages Handling of real linear relationships, that’s what trees suck at. Any real linear relationship between an input feature and the outcome has to be approximated by hard splits, which produces a step function. This is not efficient. This goes hand in hand with lack of smoothness. Slight changes in the input feature can have a big impact on the predicted outcome, which might not be desirable. Imagine a tree that predicts the worth of a house and the tree splits in the square meters multiple times. One of the splits is at 100.5 square meters. When a user measure his house and arrives at 99 square meters, types it into some nice web interface and get’s 200 000 Euro. The user notices that she forgot to measure a small storeroom with 2 square meters. The storeroom has a skewed wall, so she is not sure if she can count it fully towards the whole flat area or only half of the space. So she decides to try both 100.0 and 101.0 square meters. The results: 200 000 Euro and 205 000 Euro, which is quite unintuitive. Trees are also quite unstable, so a few changes in the training data set might create a completely different tree. That’s because each splits depends on the parent split. It does not generate trust if the structure flips so easily. References "],
["other-simple-explainable-models.html", "3.8 Other simple, explainable models", " 3.8 Other simple, explainable models Decision rules Decision tables Association rules RuleFit Nearest neighbours Prototypes Naive bayes -->"],
["model-agnostic-explanations.html", "Chapter 4 Model-agnostic explanations", " Chapter 4 Model-agnostic explanations Separating the explanations from the machine learning model (= model-agnostic explanations) gives some benefits. The big advantage of model-agnostic vs model-specific explanation algorithm is the flexibility. When the explanation system is independently applicable even when the underlying model is switched, it frees the practitioner to use different machine learning models without restrictions. It is also more efficient to build interfaces on top of model-agnostic systems, because this has to be done only once and not for each model-specific explanation system. Usually not one but many types of machine learning models are tested in development time and if you want to compare the models in terms of interpretability this is easier with model-agnostic explanations because the system is the same for both models that are being compared (Ribeiro, Singh, and Guestrin 2016). The alternatives are either using only interpretable models as introduced in Chapter 2, which has the big disadvantage to usually loose accuracy compared to other approaches. The other alternative is to use more flexible model classes that come with built in explanations. The drawback here is that it ties you to this one algorithm and it will be hard to switch to something else. Desirable aspects of a model-agnostic explanation system (Ribeiro, Singh, and Guestrin 2016): - Model flexibility: Not being tied to an underlying particular machine learning model. The method should work for random forests as well as convolutional neural networks - Explanation flexibility: Not being tied to a certain form of explanation. In some cases it might be useful to have a linear formula in other cases some decision rules - Representation flexibility: The explanation system should not have to use the same feature representation as the model that is being explained. So when a text classifier uses abstract word embedding vectors, it might be preferable to use the presence of single words for the explanation. References "],
["global-explain-the-behaviour-of-a-model.html", "4.1 Global: Explain the behaviour of a model", " 4.1 Global: Explain the behaviour of a model 4.1.1 Global surrogate models A surrogate model is a simple, explainable model that explains another complex machine learning model. Models in [#simple] are viable candidates. UNDER CONSTRUCTION 4.1.2 Partial dependence plot The partial dependence plot shows the marginal effect of a variable on the target (regression / classification) (J. H. Friedman 2001). A partial dependence plot can show if the relationship between target and feature is linear, monotonic or something else. In linear regression, those plots will always show a linear relationship. The partial dependence function for regression is defined as: \\[f_S = E_{x_C}[f(x_S, x_C)] = \\int f(x_S, x_C) dP(x_C)\\] The \\(x_S\\) is the set of variables for which the partial dependence should be depicted and \\(x_C\\) are the other variables that were used in the machine learning model. Partial dependence works by averaging out the other variables, so that the remaining function shows the relationsship between the \\(x_S\\), in which we are interested, and the target. \\(x_S\\) is fixed and \\(x_C\\) is varying. The integral is estimated by calculating averages in the training data, which looks like this for regression: \\[ \\hat{f}(x_S) = \\frac{1}{n} \\sum_{i=1}^n f(x_S, x_{Ci}) \\] In this formula, \\(x\\) is the variable for which to calculate the partial dependence, \\(x_{iC}\\) is the other variables and \\(n\\) the number of instances in the data set. For classification it is the logits: \\[ f(x) = \\log p_k(x) - \\frac{1}{K} \\sum_{j=1}^K \\log p_j(x) \\] Partial dependence plots are only partially global: They are global because they take into account all instances, but it is local in the feature, because partial dependence plots only examine one variable, as the name suggests. 4.1.2.1 Examples In practice \\(x_S\\) usually only contains one variable or a maximum of two, because one variable produces 2D plots and two variables produce 3D plots. Everything beyond that is quite tricky. Even 3D on a 2D paper or monitor is already challenging. This example here shows an artificial dataset with two x variables on which a Random Forest was trained. FIGURE 4.1: The plots show the partial dependence plots of x1 and x2. x1 exhibits the V-shaped structure. FIGURE 4.1: The plots show the partial dependence plots of x1 and x2. x1 exhibits the V-shaped structure. Average marginal effects were originally developed for generalized linear models with special emphasis on the logistic regression model (CITATION). Logistic regression models try to model the probability of a binary response using a set of features. The main idea in generalized linear models is to model the expected value of the target feature using a linear combination (also called linear predictor) of the features by weighting them with the coefficients \\(\\beta\\) and a so-called response function \\(h\\), i.e. \\[E(Y|X) = h(x^\\top \\beta).\\] In any case the linear predictor takes values between minus and plus infinity, i.e., for a logistic regression model those values need to be translated into probabilities which is typically done by appling a sigmoid function (e.g., using a logistic function for \\(h\\)) to the linear predictor to make sure that the values are between 0 and 1. Although the logistic regression model is claimed to be an interpretable model (CITATION), the interpretations of the estimated coefficients (feature effects) are not directly related to the probability of the binary response due to the non-linear response function \\(h\\). Instead, the interpretations are done w.r.t. log-odds, e.g. if \\(x\\) increases by one, the log-odds (\\(log \\frac{P(Y = 1)}{P(Y = 0)}\\)) will increase by \\(\\beta\\). Researchers of applied sciences, however, are often interested in making direct interpretations of how changes in \\(x\\) affect the probability \\(P(Y=1)\\). For this purpose one can use average marginal effects. 4.1.3 Feature importance 4.1.3.1 Permutation feature importance The permutation importance measurement was orginially introduced for RandomForests (Breiman 2001). It is calculated on the out-of-bag instances and works by estimating the original model performance and checking what happens with the model performance when you permute each feature. A big loss in performance means a big feature importance. The idea of permutation of features is per se model-agnostic, only the OOB-scheme is specific for ensemble methods. In can be used for any model when a hold-out dataset is used, instead of OOB samples. Of course you could also use the training data, but you risk getting variable importance measures that overfit your training data, since the model was already trained on it. Algorithm (Breiman 2001): Input: Trained model \\(\\hat{f}\\), hold-out dataset \\(D\\), number of permutations \\(n_{perm}\\) Estimate performance \\(Perf\\) of \\(\\hat{f}\\) with \\(D\\) (e.g. MSE for regression or accuracy for classification) For each feature \\(j \\in 1, \\ldots, J\\) do: For \\(i \\in 1,\\ldots , n_{perm}\\) Get \\(D_{j_{perm}}\\) by permuting feature \\(X_j\\) in data \\(D\\). This breaks the association between \\(X_j\\) and \\(Y\\). Estimate performance \\(Perf_{i,j_{perm}}\\) of \\(\\hat{f}\\) with \\(D_{j_{perm}}\\) Calculate permutation variable importance \\(VI_i(X_j) = Perf_{i,j_{perm}} - Perf\\) Calculate mean variable importance: \\(VI(X_j) = \\frac{1}{n_{perm}}\\sum_{i=1}^{n_{perm}} VI_i(X_j)\\) Optional: Calculate p-value \\(p = \\frac{I(Perf_{j_{perm}} &gt; Perf)}{n_{perm}}\\) Sort variables by descending \\(VI\\). The feature with the highest \\(VI\\) measure is the most important globally in your model. With the p-value you can additionally check if a feature importance is significantly different from 0. You might want to adjust your \\(\\alpha\\) confidence level for multiple testing. You can also find the algorithm in more detail in (Strobl et al. 2008). The authors additionally suggest a conditional feature importance measurement, which is not (yet) covered in this book. The standard permutation feature importance only works with marginal feature improvements and cannot distinguish between correlation and spurious correlation. (Strobl et al. 2008) suggest to condition the importance measure also on other features, which makes it possible to account for correlation among the features. 4.1.3.2 Model dependent feature importance Some model classes already come with built in feature importance measurements. A few examples: - RandomForest: Permutation based feature importance - CART and boosting: mean decrease in Gini impurity index - Linear Model: (absolute value of) t-test statistic for each feature References "],
["local-explain-a-single-decisions.html", "4.2 Local: Explain a single decisions", " 4.2 Local: Explain a single decisions Instead of trying to explain predictions or mechanisms of the model globally, local methods focus on explaining single predictions. 4.2.1 Individual Conditional Expectation (ICE) plot The partial dependence plot are a global method, because it does not focus on the partial dependence of a specific instance, but on an average over all. The equivalent to a PDP for local expectations is called individiual conditional expectation (ICE) plot (Goldstein et al. 2015). An ICE plot visualizes the dependence of each instance’s predicted response on a feature. For drawing each line, the \\(x_C\\) are fixed for this one instance, and the \\(x_C\\) is varied on a grid and the \\(\\hat(y)\\) calculated with \\(\\hat(f)\\). So, what do you gain by looking at individual expectations, instead of partial dependencies? This averaged display can obfuscate a heterogeneous relationship that comes from interactions. ICE plots solve this problem by plotting the relationship between feature and predicted response for individual observations. It can be seen as an extension to the standard PDP. PDP can show you how the average relationship between feature \\(x_S\\) and \\(\\hat(y)\\) looks like. This works only well in cases where the interactions between \\(x_S\\) and the remaining \\(x_C\\) are weak. If there are interactions, a ICE plot will give a lot more insight. A more formal definition: In ICE plots, for each observation in \\(\\{(x_{S_i}, x_{C_i})\\}_{i=1}^N\\) the curve \\(\\hat{f}_S^{(i)}\\) is plotted against \\(x_{S_i}\\), while \\(x_{C_i}\\) is kept fixed. #### Example 4.2.1.1 Centered ICE plot There is one issue with the ICE plot: It can be hard to see if the individual conditionl expectations curve differ between individuals when they start at different $. An easy fix is to center the curves at a certain point in \\(x_S\\) and only display the difference in predited response. The resulting plot is called centered ICE plot (c-ICE). It is a kind of anchoring, and doing this at the lower end of \\(x_S\\) is a good choice. The new curves are defined as: \\[\\hat{f}_{cent}^{(i)} = \\hat{f}_i - 1\\hat{f}(x^{\\text{*}}, x_{C_i}), \\] where \\(1\\) is a vector of 1’s with the appropriate dimensions (usually one- or two-dimensional), and \\(\\hat{f}\\) the fitted model. 4.2.1.2 Derivative ICE plot Another way to make it it visually easier to spot heterogenity is to look at the individual derivatives of \\(\\hat(f)\\) with respect to \\(\\x_S\\) instead of the predicted response \\(\\hat(f)\\). The resulting plot is called derivative ICE plot (d-ICE). The derivatives of a function (or curve) tells you in which direction changes occur and if any occur at all. With the derivative ICE plot it is easy to spot value ranges in a feature where the black box’s predicted value changes for (at least some) instances. If there is no interaction between \\(x_S\\) and \\(x_C\\), then \\(\\hat{f}\\) can be expressed as: \\[\\hat{f}(x) = \\hat(f)(x_S, x_C) = g(x_S) + h(x_C), \\text{ so that } \\frac{\\delta\\hat{f}(x)}{\\delta x_S} = g&#39;(x_S)\\] Without interactions, the individual partial derivatives should be the same for all observations. If they differ, it’s because of interactions and it will become visible in the d-ICE plot. In addition to displaying the individual curves for derivative \\(\\hat{f}\\), showing the standard deviation of derivative \\(\\hat{f}\\) helps to highlight regions in \\(x_S\\) with heterogeneity in the estimated derivatives. (Goldstein et al. 2015) 4.2.2 Local surrogate models (LIME) Local interpretable model-agnostic explanations (LIME) is a method for fitting local, interpretable models that can explain single predictions or classifications of any black-box machine learning model. LIME explanations are local surrogate models. Instead of trying to fit a global surrogate model, LIME focuses on a prediction done by a black-box algorithm and explains it’s outcome. The idea is quite simple, really. First of all, forget about the training data and imagine you only have the black box model where you can input data points and get the models outcome. You can probe the box as often as you want. Your goal is to understand why the machine learning model gave the outcome it produced. LIME tests out what happens to the model’s predictions when you put some variations of your data point of interest into the machine learning model. This basically generates a new dataset consisting of the perturbed samples and the associated model’s outcome. On this dataset LIME then trains a simple model weighted by the proximity of the sampled instances to the instance of interest. The simple mode can basically be any from Section [#simple], for example LASSO or a short tree. The learned model should be a good approximation of the machine learning model locally, but it does not have to be so globally. This kind of accuracy is also called local fidelity. The recipe: Choose your instance of interest for which you want to have an explanation of it’s black box outcome Make some variations of the instances and check what the black box predicts in the neighbourhood of the instance of interest. Fit a local, interpretable model on the dataset with the variations Explain prediction by interpreting the local simple model. In the current implementation, only LASSO can be chosen as a simple model. Upfront you have to choose K, the number of features that you want to have in your simple model. The lower the K, the simpler the model is to understand, higher K potentially creates models with higher fidelity. There are different methods for how to fit models with exactly K features. The most natural with LASSO is the lasso path. Starting from a model with a very high regularisation parameter \\(\\lambda\\) yields a model with only the intercept. By refitting the LASSO models with slowly decreasing \\(\\lambda\\) one after each other the features are getting weight estimates different from zero. When K features are in the model, you reached the desired number of features. Other strategies are forward or backward selection of features. This means you either start with the full model (=containing all features) or with a model with only the intercept and then testing which feature would create the biggest improvement when added or removed, until a model with K features are reached. Other simple models like decision trees are currently not implemented. As always, the devil is in the details. In a high-dimensional space, defining a neighbourhood is not trivial. Distance measures are quite arbitrary and distances in different dimensions (aka features) might not be comparable at all. How big should the neighbourhood be you look into? If it is too small, then there might be no difference in the predictions of the machine learning model at all. The other question is: How do you get the variations of the data? This differs depending on the type of data, which can be either text, an image or tabular data. For text and image the solution is turning off and on single words or superpixels. In the case of tabular data LIME creates new samples by pertubing each feature individually, by drawing from a normal distribution with mean and standard deviation from the feature. 4.2.2.1 LIME for tabular data Tabular data means any data that comes in tables, where each row represents an instance and each column a feature. Sampling is not done around the point, but from the training data’s mass center. Has it’s problems. But it increases the likelihood that the outcome for some of the sampled points predictions differ from the data point of interest and that LIME can learn at least some explanation. Figure 4.2 explains how the sampling and local model fitting works. FIGURE 4.2: How LIME sampling works: A) The trainig data has two classes. The most data points have class 0, and the ones with class 1 are grouped in an upside-down V-shape. The plot displays the decision boundaries learned by a machine learning model. In this case it was a Random Forest, but it does not matter, because LIME is model-agnostic and we only care about the decision boundaries. B) The yellow is the instance of interest, for which an explanation is desired. The black dots are data sampled from a normal distribution around the means of the features in the training sample. This has only to be done once and can be reused for other explanations. C) Introducing locality by giving points near the instance of interest a higher weights. D) The colors and signs of the grid display the classifications of the locally learned model form the weighted samples. The white line marks the decision boundary (P(class) = 0.5) at which the classification changes. 4.2.2.1.1 Example Let’s look at a concrete example. We go back to the speed dating dataset and predict the probability that the participant will want to see the partner for a second date. First we train a Random Forest on the classification task. Given correlation of interest, age of partner, field of study and so on, what is the probability that the participant will choose the partner for a second date? The Random Forest has 30 trees. The continuous features are categorised into bins by quantiles for the explanation models. The explanations are set to contain 2 features. FIGURE 4.3: Explanations for two instances in the test data. The first instance got a probability of 0.13 for a match by the Random Forest. The correlation of interests between the participant and the partner is low which reduces the probability of a match for this participant. Also the importance for race of the partner is higher than 6, which lowers the probability of a match. In the second case, the probability for a match is quite high with 0.46. The reasons are the high correlation (&gt;0.43) and that the partner is younger then 25. 4.2.2.2 LIME for images For images the sampling procedure works differently. Instead of sampling single pixels, LIME create variations of the image by turning off superpixel. 4.2.2.3 LIME for text LIME for text works a bit differently than for tabular data. Variation of the point to be explained are created differently: Starting from the original text, new texts are created by randomly removing words from it. 4.2.2.3.1 Example In this example we classify spam vs. ham of YouTube comments. The dataset is described in [#TubeSpam]. The black box model is a decision tree on the document word matrix. Each comment is one document (= one row) and each column is a the number of occurences of a specific word. A decision tree was trained on this data. As discussed in Section [#simple], decision trees are easy to understand, but in this case the tree is very deep. Also in the place of this tree there could have been a recurrent neural network or a support vector machine that was trained on the embeddings from word2vec. The machine learning model was trained on 80% of the approximately 2000 comments. From the remaining comments two were selected for showing the explanations. Let’s look at tow comments of this dataset and the corresponding classes: In the next step we create some variations of the datasets, which are used in a local model. For example some variations of one of the comments. Each column corresponds to one word in the sentence. Each row is a variation. 1 indicates that the word is part of this variation. 0 indicates that the word was removed. The corresponding sentence for the first variation is “Guy in the yellow suit Jae-suk”. References "],
["model-agnostic-why-not-use-them-on-the-data-itself.html", "4.3 Model-agnostic: Why not use them on the data itself?", " 4.3 Model-agnostic: Why not use them on the data itself? Well, nothing stops you from using it on the data itself. "],
["explanation-types.html", "4.4 Explanation types", " 4.4 Explanation types UNDER CONSTRUCTION 4.4.1 Structured output Regression tables, decision tree plots. There are overlaps with visual explanations (Section [#viz-explanation]). 4.4.2 Visualization Easy to understand Visualizations. First choice in image classification tasks. 4.4.3 Natural language (narratives) That’s what humans usually do. 4.4.4 Examples and prototypes -->"],
["references.html", "Chapter 5 References", " Chapter 5 References "]
]
