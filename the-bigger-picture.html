<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Explainable machine learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable.">
  <meta name="generator" content="bookdown 0.4 and GitBook 2.6.7">

  <meta property="og:title" content="Explainable machine learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable." />
  <meta name="github-repo" content="christophM/xai-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Explainable machine learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisons more interpretable." />
  

<meta name="author" content="Christoph Molnar">


<meta name="date" content="2017-08-04">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="when-is-explainability-important.html">
<link rel="next" href="scope-of-explainability.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Explainable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="who-should-read-this-book.html"><a href="who-should-read-this-book.html"><i class="fa fa-check"></i><b>1.1</b> Who should read this book</a></li>
<li class="chapter" data-level="1.2" data-path="outline.html"><a href="outline.html"><i class="fa fa-check"></i><b>1.2</b> Outline</a></li>
<li class="chapter" data-level="1.3" data-path="what-is-machine-learning-and-why-is-it-important.html"><a href="what-is-machine-learning-and-why-is-it-important.html"><i class="fa fa-check"></i><b>1.3</b> What is machine learning and why is it important?</a></li>
<li class="chapter" data-level="1.4" data-path="definitions.html"><a href="definitions.html"><i class="fa fa-check"></i><b>1.4</b> Definitions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="explainability.html"><a href="explainability.html"><i class="fa fa-check"></i><b>2</b> Explainability</a><ul>
<li class="chapter" data-level="2.1" data-path="what-is-explainability.html"><a href="what-is-explainability.html"><i class="fa fa-check"></i><b>2.1</b> What is explainability</a></li>
<li class="chapter" data-level="2.2" data-path="when-is-explainability-important.html"><a href="when-is-explainability-important.html"><i class="fa fa-check"></i><b>2.2</b> When is explainability important?</a></li>
<li class="chapter" data-level="2.3" data-path="the-bigger-picture.html"><a href="the-bigger-picture.html"><i class="fa fa-check"></i><b>2.3</b> The bigger picture</a></li>
<li class="chapter" data-level="2.4" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html"><i class="fa fa-check"></i><b>2.4</b> Scope of explainability</a><ul>
<li class="chapter" data-level="2.4.1" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#algorithm-transparency"><i class="fa fa-check"></i><b>2.4.1</b> Algorithm transparency</a></li>
<li class="chapter" data-level="2.4.2" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#global-holistic-model-explainability"><i class="fa fa-check"></i><b>2.4.2</b> Global, holistic model explainability</a></li>
<li class="chapter" data-level="2.4.3" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#global-model-explainability-on-a-modular-level"><i class="fa fa-check"></i><b>2.4.3</b> Global model explainability on a modular level</a></li>
<li class="chapter" data-level="2.4.4" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#explain-the-decision-for-a-single-instance"><i class="fa fa-check"></i><b>2.4.4</b> Explain the decision for a single instance</a></li>
<li class="chapter" data-level="2.4.5" data-path="scope-of-explainability.html"><a href="scope-of-explainability.html#explain-the-decisions-for-a-group-of-instances"><i class="fa fa-check"></i><b>2.4.5</b> Explain the decisions for a group of instances</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="evaluating-explainability.html"><a href="evaluating-explainability.html"><i class="fa fa-check"></i><b>2.5</b> Evaluating explainability</a><ul>
<li class="chapter" data-level="2.5.1" data-path="evaluating-explainability.html"><a href="evaluating-explainability.html#approaches-for-evaluation-of-the-explanation-quality"><i class="fa fa-check"></i><b>2.5.1</b> Approaches for evaluation of the explanation quality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>3</b> Simple, interpretable models</a><ul>
<li class="chapter" data-level="3.1" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>3.1</b> Terminology</a></li>
<li class="chapter" data-level="3.2" data-path="regression-dataset-bike-sharing-counts.html"><a href="regression-dataset-bike-sharing-counts.html"><i class="fa fa-check"></i><b>3.2</b> Regression dataset: Bike sharing counts</a></li>
<li class="chapter" data-level="3.3" data-path="the-dataset-speed-dating.html"><a href="the-dataset-speed-dating.html"><i class="fa fa-check"></i><b>3.3</b> The dataset: speed dating</a></li>
<li class="chapter" data-level="3.4" data-path="TubeSpam.html"><a href="TubeSpam.html"><i class="fa fa-check"></i><b>3.4</b> TubeSpam dataset: Spam classification on YouTube comments</a></li>
<li class="chapter" data-level="3.5" data-path="risk-factors-for-cervical-cancer.html"><a href="risk-factors-for-cervical-cancer.html"><i class="fa fa-check"></i><b>3.5</b> Risk factors for cervical cancer</a></li>
<li class="chapter" data-level="3.6" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>3.6</b> Overview</a></li>
<li class="chapter" data-level="3.7" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>3.7</b> Linear models</a><ul>
<li class="chapter" data-level="3.7.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>3.7.1</b> Interpretation</a></li>
<li class="chapter" data-level="3.7.2" data-path="limo.html"><a href="limo.html#interpretation-example"><i class="fa fa-check"></i><b>3.7.2</b> Interpretation example</a></li>
<li class="chapter" data-level="3.7.3" data-path="limo.html"><a href="limo.html#interpretation-templates"><i class="fa fa-check"></i><b>3.7.3</b> Interpretation templates</a></li>
<li class="chapter" data-level="3.7.4" data-path="limo.html"><a href="limo.html#visual-parameter-interpretation"><i class="fa fa-check"></i><b>3.7.4</b> Visual parameter interpretation</a></li>
<li class="chapter" data-level="3.7.5" data-path="limo.html"><a href="limo.html#explaining-single-predictions"><i class="fa fa-check"></i><b>3.7.5</b> Explaining single predictions</a></li>
<li class="chapter" data-level="3.7.6" data-path="limo.html"><a href="limo.html#coding-categorical-variables"><i class="fa fa-check"></i><b>3.7.6</b> Coding categorical variables:</a></li>
<li class="chapter" data-level="3.7.7" data-path="limo.html"><a href="limo.html#assuring-sparsity-in-linear-models"><i class="fa fa-check"></i><b>3.7.7</b> Assuring sparsity in linear models</a></li>
<li class="chapter" data-level="3.7.8" data-path="limo.html"><a href="limo.html#the-disadvantages-of-linear-models"><i class="fa fa-check"></i><b>3.7.8</b> The disadvantages of linear models</a></li>
<li class="chapter" data-level="3.7.9" data-path="limo.html"><a href="limo.html#towards-complexer-relationships-within-linear-model-class"><i class="fa fa-check"></i><b>3.7.9</b> Towards complexer relationships within linear model class</a></li>
<li class="chapter" data-level="3.7.10" data-path="limo.html"><a href="limo.html#linear-models-beyond-gaussian-regression"><i class="fa fa-check"></i><b>3.7.10</b> Linear models beyond gaussian regression</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>3.8</b> Decision trees</a><ul>
<li class="chapter" data-level="3.8.1" data-path="decision-trees.html"><a href="decision-trees.html#interpretation-1"><i class="fa fa-check"></i><b>3.8.1</b> Interpretation</a></li>
<li class="chapter" data-level="3.8.2" data-path="decision-trees.html"><a href="decision-trees.html#interpretation-example-1"><i class="fa fa-check"></i><b>3.8.2</b> Interpretation example</a></li>
<li class="chapter" data-level="3.8.3" data-path="decision-trees.html"><a href="decision-trees.html#advantages"><i class="fa fa-check"></i><b>3.8.3</b> Advantages</a></li>
<li class="chapter" data-level="3.8.4" data-path="decision-trees.html"><a href="decision-trees.html#disadvantages"><i class="fa fa-check"></i><b>3.8.4</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="other-simple-explainable-models.html"><a href="other-simple-explainable-models.html"><i class="fa fa-check"></i><b>3.9</b> Other simple, explainable models</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-agnostic-explanations.html"><a href="model-agnostic-explanations.html"><i class="fa fa-check"></i><b>4</b> Model-agnostic explanations</a><ul>
<li class="chapter" data-level="4.1" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html"><i class="fa fa-check"></i><b>4.1</b> Global: Explain the behaviour of a model</a><ul>
<li class="chapter" data-level="4.1.1" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html#global-surrogate-models"><i class="fa fa-check"></i><b>4.1.1</b> Global surrogate models</a></li>
<li class="chapter" data-level="4.1.2" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html#pdp"><i class="fa fa-check"></i><b>4.1.2</b> Partial dependence plot</a></li>
<li class="chapter" data-level="4.1.3" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html#average-marginal-effects"><i class="fa fa-check"></i><b>4.1.3</b> Average Marginal Effects}</a></li>
<li class="chapter" data-level="4.1.4" data-path="global-explain-the-behaviour-of-a-model.html"><a href="global-explain-the-behaviour-of-a-model.html#feature-importance"><i class="fa fa-check"></i><b>4.1.4</b> Feature importance</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="local-explain-a-single-decisions.html"><a href="local-explain-a-single-decisions.html"><i class="fa fa-check"></i><b>4.2</b> Local: Explain a single decisions</a><ul>
<li class="chapter" data-level="4.2.1" data-path="local-explain-a-single-decisions.html"><a href="local-explain-a-single-decisions.html#individual-conditional-expectation-ice-plot"><i class="fa fa-check"></i><b>4.2.1</b> Individual Conditional Expectation (ICE) plot</a></li>
<li class="chapter" data-level="4.2.2" data-path="local-explain-a-single-decisions.html"><a href="local-explain-a-single-decisions.html#local-surrogate-models-lime"><i class="fa fa-check"></i><b>4.2.2</b> Local surrogate models (LIME)</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-agnostic-why-not-use-them-on-the-data-itself.html"><a href="model-agnostic-why-not-use-them-on-the-data-itself.html"><i class="fa fa-check"></i><b>4.3</b> Model-agnostic: Why not use them on the data itself?</a></li>
<li class="chapter" data-level="4.4" data-path="explanation-types.html"><a href="explanation-types.html"><i class="fa fa-check"></i><b>4.4</b> Explanation types</a><ul>
<li class="chapter" data-level="4.4.1" data-path="explanation-types.html"><a href="explanation-types.html#structured-output"><i class="fa fa-check"></i><b>4.4.1</b> Structured output</a></li>
<li class="chapter" data-level="4.4.2" data-path="explanation-types.html"><a href="explanation-types.html#viz-explanation"><i class="fa fa-check"></i><b>4.4.2</b> Visualization</a></li>
<li class="chapter" data-level="4.4.3" data-path="explanation-types.html"><a href="explanation-types.html#natural-language-narratives"><i class="fa fa-check"></i><b>4.4.3</b> Natural language (narratives)</a></li>
<li class="chapter" data-level="4.4.4" data-path="explanation-types.html"><a href="explanation-types.html#examples-and-prototypes"><i class="fa fa-check"></i><b>4.4.4</b> Examples and prototypes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>5</b> References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Explainable machine learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-bigger-picture" class="section level2">
<h2><span class="header-section-number">2.3</span> The bigger picture</h2>
<p>Let’s take a look from further away. What do we want to explain, and what kind of ‘layers’ are inbetween? The infographic displays the concepts, see Figure <a href="the-bigger-picture.html#fig:bigpicture">2.1</a>. The bottom layer is the ‘World’. This could literally be nature itself, like the biology of the human body and how it reacts to medication, but also human behaviour like if people payed back their loans. The ‘World’-layer contains everything that can be observed and is of interest. Ultimately we want to learn something about the ‘World’ and interact with it.</p>
<p>The second layer is the ‘Data’-layer. We have to digitalise the ‘World’ in to make it processable for computers and also to store information. The ‘Data’-layer contains anything from images, texts, tabular data and so on.</p>
<p>With machine learning on top of the ‘Data’-layer we get to the ‘Black Box Model’-layer. Machine learning algorithms learns with data from the real world to make predictions / classifications or finds structures.</p>
<p>Now with the ‘Interpretable models’-layer we come the part that this book is concerned with. On top of the ‘Black-Box-Layer’ we want to have something that helps us deal with the opaqueness of machine learning models. What were the important attributes for a particular diagnosis? Why was a financial transaction classified as fraud?</p>
<p>On top of that, there is the ‘Explanations’-layer. I put it as a layer separate from ‘Interpretable models’, since the simple models deal with capturing associations and it is useful to think of the explanation as independent. There are different ways to present the results of a linear regression model for example, it could be a coefficient table, a coefficient plot with confidence intervals, a colored bar chart, a few sentences, … It depends on the target audience what representation which explanation to choose.</p>
<p>The last layer is ‘Human’. Look this one is waiving at you because you are reading this book and you are helping to provide better explanations for black box models! Humans are the consumers of the explanations ultimately.</p>
<div class="figure"><span id="fig:bigpicture"></span>
<img src="images/big-picture.png" alt="The big picture of explainable machine learning. The real world goes through many layers before it reaches the human in forms of explanations." width="80%" />
<p class="caption">
FIGURE 2.1: The big picture of explainable machine learning. The real world goes through many layers before it reaches the human in forms of explanations.
</p>
</div>
<p>This layered abstraction also helps in understanding what the difference between statisticians and machine learning practicioners is. Statistician are concerned with the ‘Data’ layer, like planning clinical trials or designing surveys. The they skip the ‘Black Box Model’-layer and go right to the ‘Interpretable Models’ and from there to the explanations for our human. Machine learning specialists are also concerned with the ‘Data’-layer, like collecting labeled samples of skin cancer images or crawling Wikipedia. Then comes the machine learning model. ‘Interpretable models’ and ‘Explanations’ are skipped and the human deals directly with the ‘Black Box Model’. It’s a nice thing, that in explainable machine learning, the work of a statistican and a machine learner fuses and becomes something better.</p>
<p>Of course this graphic does not capture everything: Data could come from simulations. Black box models also output predictions that might not even reach humans, but only feed other machines and so on. But overall it is a useful abstraction for understanding what explainable machine learning is.</p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="when-is-explainability-important.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="scope-of-explainability.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/xai-book/edit/master/02-explainability.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
